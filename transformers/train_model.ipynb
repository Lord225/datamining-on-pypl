{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "      <th>args_types</th>\n",
       "      <th>args_defaults</th>\n",
       "      <th>body</th>\n",
       "      <th>docstring</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118132</th>\n",
       "      <td>31048</td>\n",
       "      <td>100</td>\n",
       "      <td>setwinsize</td>\n",
       "      <td>{self,rows,cols}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self.ptyproc.setwinsize(rows, cols)</td>\n",
       "      <td>This sets the terminal window size of the chil...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118133</th>\n",
       "      <td>31251</td>\n",
       "      <td>20</td>\n",
       "      <td>metadata_dict</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return msg_to_json(self.metadata)</td>\n",
       "      <td>PEP 566 compliant JSON-serializable representa...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118134</th>\n",
       "      <td>31364</td>\n",
       "      <td>20</td>\n",
       "      <td>exports</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>result = {}\\nr = self.get_distinfo_resource(EX...</td>\n",
       "      <td>Return the information exported by this distri...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118135</th>\n",
       "      <td>31517</td>\n",
       "      <td>20</td>\n",
       "      <td>log</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>if not objects:\\n    objects = (NewLine(),)\\nr...</td>\n",
       "      <td>Log rich content to the terminal.\\n\\nArgs:\\n  ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118136</th>\n",
       "      <td>31706</td>\n",
       "      <td>20</td>\n",
       "      <td>test_as_import</td>\n",
       "      <td>{script}</td>\n",
       "      <td>{PipTestEnvironment}</td>\n",
       "      <td>{}</td>\n",
       "      <td>import pip._internal.commands.install as inst\\...</td>\n",
       "      <td>test that pip.__init__.py does not shadow\\nthe...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117445</th>\n",
       "      <td>27697</td>\n",
       "      <td>16</td>\n",
       "      <td>isMaskedArray</td>\n",
       "      <td>{x}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return isinstance(x, MaskedArray)</td>\n",
       "      <td>Test whether input is an instance of MaskedArr...</td>\n",
       "      <td>551641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117537</th>\n",
       "      <td>29324</td>\n",
       "      <td>22</td>\n",
       "      <td>describe_timestamp_1d</td>\n",
       "      <td>{data,percentiles}</td>\n",
       "      <td>{Series,Sequence[float]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>from pandas import Series\\nformatted_percentil...</td>\n",
       "      <td>Describe series containing datetime64 dtype.\\n...</td>\n",
       "      <td>551650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117561</th>\n",
       "      <td>29336</td>\n",
       "      <td>22</td>\n",
       "      <td>asfreq</td>\n",
       "      <td>{obj,freq,method,how,normalize,fill_value}</td>\n",
       "      <td>{NDFrameT,bool}</td>\n",
       "      <td>{None,None,False,None}</td>\n",
       "      <td>if isinstance(obj.index, PeriodIndex):\\n    if...</td>\n",
       "      <td>Utility frequency conversion method for Series...</td>\n",
       "      <td>551651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117562</th>\n",
       "      <td>29445</td>\n",
       "      <td>22</td>\n",
       "      <td>_get_custom_index_name</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self.xlabel</td>\n",
       "      <td>Specify whether xlabel/ylabel should be used t...</td>\n",
       "      <td>551652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117636</th>\n",
       "      <td>39350</td>\n",
       "      <td>234</td>\n",
       "      <td>projected</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self._projected</td>\n",
       "      <td>Gets the projected of this V1Volume.  # noqa: ...</td>\n",
       "      <td>551657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120125 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_id  repo                    name  \\\n",
       "118132    31048   100              setwinsize   \n",
       "118133    31251    20           metadata_dict   \n",
       "118134    31364    20                 exports   \n",
       "118135    31517    20                     log   \n",
       "118136    31706    20          test_as_import   \n",
       "...         ...   ...                     ...   \n",
       "117445    27697    16           isMaskedArray   \n",
       "117537    29324    22   describe_timestamp_1d   \n",
       "117561    29336    22                  asfreq   \n",
       "117562    29445    22  _get_custom_index_name   \n",
       "117636    39350   234               projected   \n",
       "\n",
       "                                              args                args_types  \\\n",
       "118132                            {self,rows,cols}                        {}   \n",
       "118133                                      {self}                        {}   \n",
       "118134                                      {self}                        {}   \n",
       "118135                                      {self}                        {}   \n",
       "118136                                    {script}      {PipTestEnvironment}   \n",
       "...                                            ...                       ...   \n",
       "117445                                         {x}                        {}   \n",
       "117537                          {data,percentiles}  {Series,Sequence[float]}   \n",
       "117561  {obj,freq,method,how,normalize,fill_value}           {NDFrameT,bool}   \n",
       "117562                                      {self}                        {}   \n",
       "117636                                      {self}                        {}   \n",
       "\n",
       "                 args_defaults  \\\n",
       "118132                      {}   \n",
       "118133                      {}   \n",
       "118134                      {}   \n",
       "118135                      {}   \n",
       "118136                      {}   \n",
       "...                        ...   \n",
       "117445                      {}   \n",
       "117537                      {}   \n",
       "117561  {None,None,False,None}   \n",
       "117562                      {}   \n",
       "117636                      {}   \n",
       "\n",
       "                                                     body  \\\n",
       "118132         return self.ptyproc.setwinsize(rows, cols)   \n",
       "118133                  return msg_to_json(self.metadata)   \n",
       "118134  result = {}\\nr = self.get_distinfo_resource(EX...   \n",
       "118135  if not objects:\\n    objects = (NewLine(),)\\nr...   \n",
       "118136  import pip._internal.commands.install as inst\\...   \n",
       "...                                                   ...   \n",
       "117445                  return isinstance(x, MaskedArray)   \n",
       "117537  from pandas import Series\\nformatted_percentil...   \n",
       "117561  if isinstance(obj.index, PeriodIndex):\\n    if...   \n",
       "117562                                 return self.xlabel   \n",
       "117636                             return self._projected   \n",
       "\n",
       "                                                docstring      id  \n",
       "118132  This sets the terminal window size of the chil...      18  \n",
       "118133  PEP 566 compliant JSON-serializable representa...      19  \n",
       "118134  Return the information exported by this distri...      20  \n",
       "118135  Log rich content to the terminal.\\n\\nArgs:\\n  ...      22  \n",
       "118136  test that pip.__init__.py does not shadow\\nthe...      23  \n",
       "...                                                   ...     ...  \n",
       "117445  Test whether input is an instance of MaskedArr...  551641  \n",
       "117537  Describe series containing datetime64 dtype.\\n...  551650  \n",
       "117561  Utility frequency conversion method for Series...  551651  \n",
       "117562  Specify whether xlabel/ylabel should be used t...  551652  \n",
       "117636  Gets the projected of this V1Volume.  # noqa: ...  551657  \n",
       "\n",
       "[120125 rows x 9 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "\n",
    "db = sa.create_engine('postgresql://postgres:8W0MQwY4DINCoX@localhost:5432/data-mining').connect()\n",
    "\n",
    "# load 100 samples from function\n",
    "values = pd.read_sql(\"SELECT * FROM functions WHERE docstring is not null\", db)\n",
    "\n",
    "# order by id\n",
    "values = values.sort_values(by='id')\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This sets the terminal window size of the child tty. This will cause\\na SIGWINCH signal to be sent to the child. This does not change the\\nphysical window size. It changes the size reported to TTY-aware\\napplications like vi or curses -- applications that respond to the\\nSIGWINCH signal. ',\n",
       " 'PEP 566 compliant JSON-serializable representation of METADATA or PKG-INFO.\\n\\nThis should return an empty dict if the metadata file is unavailable.\\n\\n:raises NoneMetadataError: If the metadata file is available, but does\\n    not contain valid metadata.',\n",
       " 'Return the information exported by this distribution.\\n:return: A dictionary of exports, mapping an export category to a dict\\n         of :class:`ExportEntry` instances describing the individual\\n         export entries, and keyed by name.',\n",
       " 'Log rich content to the terminal.\\n\\nArgs:\\n    objects (positional args): Objects to log to the terminal.\\n    sep (str, optional): String to write between print data. Defaults to \" \".\\n    end (str, optional): String to write at end of print data. Defaults to \"\\\\\\\\n\".\\n    style (Union[str, Style], optional): A style to apply to output. Defaults to None.\\n    justify (str, optional): One of \"left\", \"right\", \"center\", or \"full\". Defaults to ``None``.\\n    emoji (Optional[bool], optional): Enable emoji code, or ``None`` to use console default. Defaults to None.\\n    markup (Optional[bool], optional): Enable markup, or ``None`` to use console default. Defaults to None.\\n    highlight (Optional[bool], optional): Enable automatic highlighting, or ``None`` to use console default. Defaults to None.\\n    log_locals (bool, optional): Boolean to enable logging of locals where ``log()``\\n        was called. Defaults to False.\\n    _stack_offset (int, optional): Offset of caller from end of call stack. Defaults to 1.',\n",
       " 'test that pip.__init__.py does not shadow\\nthe command submodule with a dictionary',\n",
       " 'Parses input, which is a list of tokens.',\n",
       " 'Test ordering of checkers based on their __gt__ method.',\n",
       " 'Test that a toml file has a pylint config.',\n",
       " 'If an extension requires an issuer, the `issuer` parameter to\\n`X509Extension` provides its value.',\n",
       " 'Tests whether requests can be used importing standard_library modules\\npreviously with the hooks context manager',\n",
       " 'Produce coverage reports.',\n",
       " 'Return a string or list of strings to be displayed after collection\\nhas finished successfully.\\n\\nThese strings will be displayed after the standard \"collected X items\" message.\\n\\n.. versionadded:: 3.2\\n\\n:param config: The pytest config object.\\n:param start_path: The starting dir.\\n:type start_path: pathlib.Path\\n:param startdir: The starting dir (deprecated).\\n:param items: List of pytest items that are going to be executed; this list should not be modified.\\n\\n.. note::\\n\\n    Lines returned by a plugin are displayed before those of plugins which\\n    ran before it.\\n    If you want to have your line(s) displayed first, use\\n    :ref:`trylast=True <plugin-hookorder>`.\\n\\n.. versionchanged:: 7.0.0\\n    The ``start_path`` parameter was added as a :class:`pathlib.Path`\\n    equivalent of the ``startdir`` parameter. The ``startdir`` parameter\\n    has been deprecated.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest plugin can implement this hook.',\n",
       " 'Map errors for Unary-Unary and Stream-Unary gRPC callables.',\n",
       " \"Wait for a job to complete and return the results.\\n\\nIf we can't return the results within the ``wait_timeout``, try to cancel\\nthe job.\",\n",
       " 'Return a string indicating the HTTP request method.',\n",
       " \"Fail unless a warning of class warnClass is triggered\\nby callable_obj when invoked with arguments args and keyword\\narguments kwargs.  If a different type of warning is\\ntriggered, it will not be handled: depending on the other\\nwarning filtering rules in effect, it might be silenced, printed\\nout, or raised as an exception.\\n\\nIf called with callable_obj omitted or None, will return a\\ncontext object used like this::\\n\\n     with self.assertWarns(SomeWarning):\\n         do_something()\\n\\nAn optional keyword argument 'msg' can be provided when assertWarns\\nis used as a context object.\\n\\nThe context manager keeps a reference to the first matching\\nwarning as the 'warning' attribute; similarly, the 'filename'\\nand 'lineno' attributes give you information about the line\\nof Python code from which the warning was triggered.\\nThis allows you to inspect the warning after the assertion::\\n\\n    with self.assertWarns(SomeWarning) as cm:\\n        do_something()\\n    the_warning = cm.warning\\n    self.assertEqual(the_warning.some_attribute, 147)\",\n",
       " '__init__(self, \\\\*, inputCols=None, outputCol=None):',\n",
       " 'Test checking a single file that is excluded.',\n",
       " \"replace_namespaced_persistent_volume_claim_status  # noqa: E501\\n\\nreplace status of the specified PersistentVolumeClaim  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.replace_namespaced_persistent_volume_claim_status(name, namespace, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the PersistentVolumeClaim (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param V1PersistentVolumeClaim body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1PersistentVolumeClaim\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Sets the collision_count of this V1DaemonSetStatus.\\n\\nCount of hash collisions for the DaemonSet. The DaemonSet controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ControllerRevision.  # noqa: E501\\n\\n:param collision_count: The collision_count of this V1DaemonSetStatus.  # noqa: E501\\n:type: int',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Sets the additional_properties of this V1JSONSchemaProps.\\n\\nJSONSchemaPropsOrBool represents JSONSchemaProps or a boolean value. Defaults to true for the boolean property.  # noqa: E501\\n\\n:param additional_properties: The additional_properties of this V1JSONSchemaProps.  # noqa: E501\\n:type: object',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Returns micro-averaged label-based precision.\\n(equals to micro-averaged document-based precision)',\n",
       " 'Deserialize with Futures',\n",
       " 'Format training args to pass in sagemaker_session.train.\\n\\nArgs:\\n    desc (dict): the response from DescribeTrainingJob API.\\n    inputs (list): a list of input data channels.\\n    name (str): the name of the step job.\\n    volume_kms_key (str): The KMS key id to encrypt data on the storage volume attached to\\n        the ML compute instance(s).\\n    encrypt_inter_container_traffic (bool): To encrypt all communications between ML compute\\n        instances in distributed training.\\n    vpc_config (dict): Specifies a VPC that jobs and hosted models have access to.\\n        Control access to and from training and model containers by configuring the VPC\\n\\nReturns (dcit): a dictionary that can be used as args of\\n    sagemaker_session.train method.',\n",
       " 'Retrieves the supported instance types for the model.\\n\\nArgs:\\n    model_id (str): JumpStart model ID of the JumpStart model for which to\\n        retrieve the supported instance types.\\n    model_version (str): Version of the JumpStart model for which to retrieve the\\n        supported instance types.\\n    scope (str): The script type, i.e. what it is used for.\\n        Valid values: \"training\" and \"inference\".\\n    hub_arn (str): The arn of the SageMaker Hub for which to retrieve\\n        model details from. (Default: None).\\n    region (Optional[str]): Region for which to retrieve supported instance types.\\n        (Default: None).\\n    tolerate_vulnerable_model (bool): True if vulnerable versions of model\\n        specifications should be tolerated (exception not raised). If False, raises an\\n        exception if the script used by this version of the model has dependencies with known\\n        security vulnerabilities. (Default: False).\\n    tolerate_deprecated_model (bool): True if deprecated versions of model\\n        specifications should be tolerated (exception not raised). If False, raises\\n        an exception if the version of the model is deprecated. (Default: False).\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions. If not\\n        specified, one is created using the default AWS configuration\\n        chain. (Default: sagemaker.jumpstart.constants.DEFAULT_JUMPSTART_SAGEMAKER_SESSION).\\n    training_instance_type (str): In the case of a model fine-tuned on SageMaker, the training\\n        instance type used for the training job that produced the fine-tuned weights.\\n        Optionally supply this to get a inference instance type conditioned\\n        on the training instance, to ensure compatability of training artifact to inference\\n        instance. (Default: None).\\n    config_name (Optional[str]): Name of the JumpStart Model config to apply. (Default: None).\\nReturns:\\n    list: the supported instance types to use for the model or None.\\n\\nRaises:\\n    ValueError: If the model is not available in the\\n        specified region due to lack of supported computing instances.',\n",
       " 'Returns json representation of S3DataSource object.',\n",
       " 'Use the lineage query to retrieve transform jobs that use this endpoint.\\n\\nArgs:\\n    direction (LineageQueryDirectionEnum, optional): The query direction.\\n\\nReturns:\\n    list of LineageTrialComponent: Lineage trial component that represent Transform jobs.',\n",
       " 'Creates a monitoring schedule to monitor an Amazon SageMaker Endpoint.\\n\\nIf constraints and statistics are provided, or if they are able to be retrieved from a\\nprevious baselining job associated with this monitor, those will be used.\\nIf constraints and statistics cannot be automatically retrieved, baseline_inputs will be\\nrequired in order to kick off a baselining job.\\n\\nArgs:\\n    endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.\\n        This can either be the endpoint name or an EndpointInput. (default: None)\\n    record_preprocessor_script (str): The path to the record preprocessor script. This can\\n        be a local path or an S3 uri.\\n    post_analytics_processor_script (str): The path to the record post-analytics processor\\n        script. This can be a local path or an S3 uri.\\n    output_s3_uri (str): Desired S3 destination of the constraint_violations and\\n        statistics json files.\\n        Default: \"s3://<default_session_bucket>/<job_name>/output\"\\n    constraints (sagemaker.model_monitor.Constraints or str): If provided alongside\\n        statistics, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Constraints object or an s3_uri pointing to a constraints\\n        JSON file.\\n    statistics (sagemaker.model_monitor.Statistic or str): If provided alongside\\n        constraints, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Statistics object or an s3_uri pointing to a statistics\\n        JSON file.\\n    monitor_schedule_name (str): Schedule name. If not specified, the processor generates\\n        a default job name, based on the image name and current timestamp.\\n    schedule_cron_expression (str): The cron expression that dictates the frequency that\\n        this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid\\n        expressions. Default: Daily.\\n    enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of\\n        the baselining or monitoring jobs.\\n    batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to\\n        run the monitoring schedule on the batch transform (default: None)\\n    data_analysis_start_time (str): Start time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)\\n    data_analysis_end_time (str): End time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)',\n",
       " 'Calculate a multidimensional median filter.\\n\\nParameters\\n----------\\n%(input)s\\n%(size_foot)s\\n%(output)s\\n%(mode_reflect)s\\n%(cval)s\\n%(origin_multiple)s\\naxes : tuple of int or None, optional\\n    If None, `input` is filtered along all axes. Otherwise,\\n    `input` is filtered along the specified axes. When `axes` is\\n    specified, any tuples used for `size`, `origin`, and/or `mode`\\n    must match the length of `axes`. The ith entry in any of these tuples\\n    corresponds to the ith entry in `axes`.\\n\\nReturns\\n-------\\nmedian_filter : ndarray\\n    Filtered array. Has the same shape as `input`.\\n\\nSee Also\\n--------\\nscipy.signal.medfilt2d\\n\\nNotes\\n-----\\nFor 2-dimensional images with ``uint8``, ``float32`` or ``float64`` dtypes\\nthe specialised function `scipy.signal.medfilt2d` may be faster. It is\\nhowever limited to constant mode with ``cval=0``.\\n\\nExamples\\n--------\\n>>> from scipy import ndimage, datasets\\n>>> import matplotlib.pyplot as plt\\n>>> fig = plt.figure()\\n>>> plt.gray()  # show the filtered result in grayscale\\n>>> ax1 = fig.add_subplot(121)  # left side\\n>>> ax2 = fig.add_subplot(122)  # right side\\n>>> ascent = datasets.ascent()\\n>>> result = ndimage.median_filter(ascent, size=20)\\n>>> ax1.imshow(ascent)\\n>>> ax2.imshow(result)\\n>>> plt.show()',\n",
       " 'Get information about memory available, not counting swap.',\n",
       " 'Compute the inverse FFT of a signal that has Hermitian symmetry.\\n\\nParameters\\n----------\\nx : array_like\\n    Input array.\\nn : int, optional\\n    Length of the inverse FFT, the number of points along\\n    transformation axis in the input to use.  If `n` is smaller than\\n    the length of the input, the input is cropped. If it is larger,\\n    the input is padded with zeros. If `n` is not given, the length of\\n    the input along the axis specified by `axis` is used.\\naxis : int, optional\\n    Axis over which to compute the inverse FFT. If not given, the last\\n    axis is used.\\nnorm : {\"backward\", \"ortho\", \"forward\"}, optional\\n    Normalization mode (see `fft`). Default is \"backward\".\\noverwrite_x : bool, optional\\n    If True, the contents of `x` can be destroyed; the default is False.\\n    See `fft` for more details.\\nworkers : int, optional\\n    Maximum number of workers to use for parallel computation. If negative,\\n    the value wraps around from ``os.cpu_count()``.\\n    See :func:`~scipy.fft.fft` for more details.\\nplan : object, optional\\n    This argument is reserved for passing in a precomputed plan provided\\n    by downstream FFT vendors. It is currently not used in SciPy.\\n\\n    .. versionadded:: 1.5.0\\n\\nReturns\\n-------\\nout : complex ndarray\\n    The truncated or zero-padded input, transformed along the axis\\n    indicated by `axis`, or the last one if `axis` is not specified.\\n    The length of the transformed axis is ``n//2 + 1``.\\n\\nSee Also\\n--------\\nhfft, irfft\\n\\nNotes\\n-----\\n`hfft`/`ihfft` are a pair analogous to `rfft`/`irfft`, but for the\\nopposite case: here, the signal has Hermitian symmetry in the time\\ndomain and is real in the frequency domain. So, here, it\\'s `hfft`, for\\nwhich you must supply the length of the result if it is to be odd:\\n* even: ``ihfft(hfft(a, 2*len(a) - 2) == a``, within roundoff error,\\n* odd: ``ihfft(hfft(a, 2*len(a) - 1) == a``, within roundoff error.\\n\\nExamples\\n--------\\n>>> from scipy.fft import ifft, ihfft\\n>>> import numpy as np\\n>>> spectrum = np.array([ 15, -4, 0, -1, 0, -4])\\n>>> ifft(spectrum)\\narray([1.+0.j,  2.+0.j,  3.+0.j,  4.+0.j,  3.+0.j,  2.+0.j]) # may vary\\n>>> ihfft(spectrum)\\narray([ 1.-0.j,  2.-0.j,  3.-0.j,  4.-0.j]) # may vary',\n",
       " 'Match the namespace of the element.',\n",
       " 'Remove this state.',\n",
       " 'Create a new Spark configuration.',\n",
       " 'Convert a locale string to a language tag (ex. en_US -> en-US).\\n\\nrefs: BCP 47 (:rfc:`5646`)',\n",
       " 'Get qualified name for given object as a list of string(s).',\n",
       " 'helper context manager that will apply appropriate DDL events\\nto a CREATE or DROP operation.',\n",
       " 'target database must support retrieval of the columns in a view,\\nsimilarly to how a table is inspected.\\n\\nThis does not include the full CREATE VIEW definition.',\n",
       " 'Execute a statement and assert that rows returned equal expected.',\n",
       " 'Generates all URLs and templates',\n",
       " 'Remove None, convert values to bool, check commutativity *in place*.\\n        ',\n",
       " 'This rule covers trigonometric factors by splitting everything into a\\nsum of exponential functions and collecting complex conjugate poles and\\nreal symmetric poles.',\n",
       " 'Print a LaTeX representation of the function defining the curve.\\n\\nParameters\\n==========\\n\\nprinter : Printer\\n    The printer to be used to print the LaTeX string representation.',\n",
       " 'Return a list of all units formed by unit and the given prefixes.\\n\\nYou can use the predefined PREFIXES or BIN_PREFIXES, but you can also\\npass as argument a subdict of them if you do not want all prefixed units.\\n\\n    >>> from sympy.physics.units.prefixes import (PREFIXES,\\n    ...                                                 prefix_unit)\\n    >>> from sympy.physics.units import m\\n    >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\\n    >>> prefix_unit(m, pref)  # doctest: +SKIP\\n    [millimeter, centimeter, decimeter]',\n",
       " 'Tests the coordinate variables functionality',\n",
       " 'Scale the function by a term independent of x.\\n\\nExplanation\\n===========\\n\\nf(x) -> s * f(x)\\n\\nThis is fast, if Fourier series of f(x) is already\\ncomputed.\\n\\nExamples\\n========\\n\\n>>> from sympy import fourier_series, pi\\n>>> from sympy.abc import x\\n>>> s = fourier_series(x**2, (x, -pi, pi))\\n>>> s.scale(2).truncate()\\n-8*cos(x) + 2*cos(2*x) + 2*pi**2/3',\n",
       " 'Access a group of rows and columns by label(s) or a boolean array.\\n\\n``.loc[]`` is primarily label based, but may also be used with a\\nboolean array.\\n\\nAllowed inputs are:\\n\\n- A single label, e.g. ``5`` or ``\\'a\\'``, (note that ``5`` is\\n  interpreted as a *label* of the index, and **never** as an\\n  integer position along the index).\\n- A list or array of labels, e.g. ``[\\'a\\', \\'b\\', \\'c\\']``.\\n- A slice object with labels, e.g. ``\\'a\\':\\'f\\'``.\\n\\n  .. warning:: Note that contrary to usual python slices, **both** the\\n      start and the stop are included\\n\\n- A boolean array of the same length as the axis being sliced,\\n  e.g. ``[True, False, True]``.\\n- An alignable boolean Series. The index of the key will be aligned before\\n  masking.\\n- An alignable Index. The Index of the returned selection will be the input.\\n- A ``callable`` function with one argument (the calling Series or\\n  DataFrame) and that returns valid output for indexing (one of the above)\\n\\nSee more at :ref:`Selection by Label <indexing.label>`.\\n\\nRaises\\n------\\nKeyError\\n    If any items are not found.\\nIndexingError\\n    If an indexed key is passed and its index is unalignable to the frame index.\\n\\nSee Also\\n--------\\nDataFrame.at : Access a single value for a row/column label pair.\\nDataFrame.iloc : Access group of rows and columns by integer position(s).\\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\\n               Series/DataFrame.\\nSeries.loc : Access group of values using labels.\\n\\nExamples\\n--------\\n**Getting values**\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[\"cobra\", \"viper\", \"sidewinder\"],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label. Note this returns the row as a Series.\\n\\n>>> df.loc[\"viper\"]\\nmax_speed    4\\nshield       5\\nName: viper, dtype: int64\\n\\nList of labels. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"]]\\n            max_speed  shield\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label for row and column\\n\\n>>> df.loc[\"cobra\", \"shield\"]\\n2\\n\\nSlice with labels for row and single label for column. As mentioned\\nabove, note that both the start and stop of the slice are included.\\n\\n>>> df.loc[\"cobra\":\"viper\", \"max_speed\"]\\ncobra    1\\nviper    4\\nName: max_speed, dtype: int64\\n\\nBoolean list with the same length as the row axis\\n\\n>>> df.loc[[False, False, True]]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nAlignable boolean Series:\\n\\n>>> df.loc[\\n...     pd.Series([False, True, False], index=[\"viper\", \"sidewinder\", \"cobra\"])\\n... ]\\n                     max_speed  shield\\nsidewinder          7       8\\n\\nIndex (same behavior as ``df.reindex``)\\n\\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\\n       max_speed  shield\\nfoo\\ncobra          1       2\\nviper          4       5\\n\\nConditional that returns a boolean Series\\n\\n>>> df.loc[df[\"shield\"] > 6]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nConditional that returns a boolean Series with column labels specified\\n\\n>>> df.loc[df[\"shield\"] > 6, [\"max_speed\"]]\\n            max_speed\\nsidewinder          7\\n\\nMultiple conditional using ``&`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 1) & (df[\"shield\"] < 8)]\\n            max_speed  shield\\nviper          4       5\\n\\nMultiple conditional using ``|`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 4) | (df[\"shield\"] < 5)]\\n            max_speed  shield\\ncobra               1       2\\nsidewinder          7       8\\n\\nPlease ensure that each condition is wrapped in parentheses ``()``.\\nSee the :ref:`user guide<indexing.boolean>`\\nfor more details and explanations of Boolean indexing.\\n\\n.. note::\\n    If you find yourself using 3 or more conditionals in ``.loc[]``,\\n    consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\\n\\n    See below for using ``.loc[]`` on MultiIndex DataFrames.\\n\\nCallable that returns a boolean Series\\n\\n>>> df.loc[lambda df: df[\"shield\"] == 8]\\n            max_speed  shield\\nsidewinder          7       8\\n\\n**Setting values**\\n\\nSet value for all items matching the list of labels\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"], [\"shield\"]] = 50\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire row\\n\\n>>> df.loc[\"cobra\"] = 10\\n>>> df\\n            max_speed  shield\\ncobra              10      10\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire column\\n\\n>>> df.loc[:, \"max_speed\"] = 30\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper              30      50\\nsidewinder         30      50\\n\\nSet value for rows matching callable condition\\n\\n>>> df.loc[df[\"shield\"] > 35] = 0\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       0\\nsidewinder          0       0\\n\\nAdd value matching location\\n\\n>>> df.loc[\"viper\", \"shield\"] += 5\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       5\\nsidewinder          0       0\\n\\nSetting using a ``Series`` or a ``DataFrame`` sets the values matching the\\nindex labels, not the index positions.\\n\\n>>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\\n>>> df.loc[:] += shuffled_df\\n>>> df\\n            max_speed  shield\\ncobra              60      20\\nviper               0      10\\nsidewinder          0       0\\n\\n**Getting values on a DataFrame with an index that has integer labels**\\n\\nAnother example using integers for the index\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[7, 8, 9],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\nSlice with integer labels for rows. As mentioned above, note that both\\nthe start and stop of the slice are included.\\n\\n>>> df.loc[7:9]\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\n**Getting values with a MultiIndex**\\n\\nA number of examples using a DataFrame with a MultiIndex\\n\\n>>> tuples = [\\n...     (\"cobra\", \"mark i\"),\\n...     (\"cobra\", \"mark ii\"),\\n...     (\"sidewinder\", \"mark i\"),\\n...     (\"sidewinder\", \"mark ii\"),\\n...     (\"viper\", \"mark ii\"),\\n...     (\"viper\", \"mark iii\"),\\n... ]\\n>>> index = pd.MultiIndex.from_tuples(tuples)\\n>>> values = [[12, 2], [0, 4], [10, 20], [1, 4], [7, 1], [16, 36]]\\n>>> df = pd.DataFrame(values, columns=[\"max_speed\", \"shield\"], index=index)\\n>>> df\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSingle label. Note this returns a DataFrame with a single index.\\n\\n>>> df.loc[\"cobra\"]\\n         max_speed  shield\\nmark i          12       2\\nmark ii          0       4\\n\\nSingle index tuple. Note this returns a Series.\\n\\n>>> df.loc[(\"cobra\", \"mark ii\")]\\nmax_speed    0\\nshield       4\\nName: (cobra, mark ii), dtype: int64\\n\\nSingle label for row and column. Similar to passing in a tuple, this\\nreturns a Series.\\n\\n>>> df.loc[\"cobra\", \"mark i\"]\\nmax_speed    12\\nshield        2\\nName: (cobra, mark i), dtype: int64\\n\\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[(\"cobra\", \"mark ii\")]]\\n               max_speed  shield\\ncobra mark ii          0       4\\n\\nSingle tuple for the index with a single label for the column\\n\\n>>> df.loc[(\"cobra\", \"mark i\"), \"shield\"]\\n2\\n\\nSlice from index tuple to single label\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : \"viper\"]\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSlice from index tuple to index tuple\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : (\"viper\", \"mark ii\")]\\n                    max_speed  shield\\ncobra      mark i          12       2\\n           mark ii          0       4\\nsidewinder mark i          10      20\\n           mark ii          1       4\\nviper      mark ii          7       1\\n\\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\\nfor more details and explanations of advanced indexing.',\n",
       " 'Test for stalled tqdm instance and monitor deletion',\n",
       " 'extract my config from a global Config object\\n\\nwill construct a Config object of only the config values that apply to me\\nbased on my mro(), as well as those of my parent(s) if they exist.\\n\\nIf I am Bar and my parent is Foo, and their parent is Tim,\\nthis will return merge following config sections, in this order::\\n\\n    [Bar, Foo.Bar, Tim.Foo.Bar]\\n\\nWith the last item being the highest priority.',\n",
       " 'Optional method. If not provided, the interpreter will use\\n``__iter__`` or the old ``__getitem__`` protocol\\nto implement ``in``.',\n",
       " 'Constuct an object holding a date value.\\n\\nThis function is part of the `DBAPI 2.0 specification\\n<http://www.python.org/dev/peps/pep-0249/>`_.\\n\\n:rtype: :class:`datetime.date`',\n",
       " 'Returns the normalized identifier of any currency code.\\n\\nAccepts a ``locale`` parameter for fined-grained validation, working as\\nthe one defined above in ``list_currencies()`` method.\\n\\nReturns None if the currency is unknown to Babel.',\n",
       " 'Append `leaf` to current line or to new line if appending impossible.',\n",
       " \"Parse Prometheus text format from a file descriptor.\\n\\nThis is a laxer parser than the main Go parser,\\nso successful parsing does not imply that the parsed\\ntext meets the specification.\\n\\nYields Metric's.\",\n",
       " ':param string type:\\n:param integer request_seq: Sequence number of the corresponding request.\\n:param boolean success: Outcome of the request.\\nIf true, the request was successful and the `body` attribute may contain the result of the request.\\nIf the value is false, the attribute `message` contains the error in short form and the `body` may contain additional information (see `ErrorResponse.body.error`).\\n:param string command: The command requested.\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.\\n:param string message: Contains the raw error in short form if `success` is false.\\nThis raw error might be interpreted by the client and is not shown in the UI.\\nSome predefined values exist.\\n:param Capabilities body: The capabilities of this debug adapter.',\n",
       " \":param array areas: Set of logical areas that got invalidated. This property has a hint characteristic: a client can only be expected to make a 'best effort' in honoring the areas but there are no guarantees. If this property is missing, empty, or if values are not understood, the client should assume a single value `all`.\\n:param integer threadId: If specified, the client only needs to refetch data related to this thread.\\n:param integer stackFrameId: If specified, the client only needs to refetch data related to this stack frame (and the `threadId` is ignored).\",\n",
       " 'Query nameservers to find the answer to the question.\\n\\nThis is a convenience function that uses the default resolver\\nobject to make the query.\\n\\nSee ``dns.resolver.Resolver.resolve`` for more information on the\\nparameters.',\n",
       " \"Get log stream for a service.\\nNote: This endpoint works only for services with the ``json-file``\\nor ``journald`` logging drivers.\\n\\nArgs:\\n    service (str): ID or name of the service\\n    details (bool): Show extra details provided to logs.\\n        Default: ``False``\\n    follow (bool): Keep connection open to read logs as they are\\n        sent by the Engine. Default: ``False``\\n    stdout (bool): Return logs from ``stdout``. Default: ``False``\\n    stderr (bool): Return logs from ``stderr``. Default: ``False``\\n    since (int): UNIX timestamp for the logs staring point.\\n        Default: 0\\n    timestamps (bool): Add timestamps to every log line.\\n    tail (string or int): Number of log lines to be returned,\\n        counting from the current end of the logs. Specify an\\n        integer or ``'all'`` to output all log lines.\\n        Default: ``all``\\n    is_tty (bool): Whether the service's :py:class:`ContainerSpec`\\n        enables the TTY option. If omitted, the method will query\\n        the Engine for the information, causing an additional\\n        roundtrip.\\n\\nReturns (generator): Logs for the service.\",\n",
       " \"Apply the chain's changes and write the final result using the passed\\nwrite function.\\n:param bbuf: base buffer containing the base of all deltas contained in this\\n    list. It will only be used if the chunk in question does not have a base\\n    chain.\\n:param write: function taking a string of bytes to write to the output\",\n",
       " 'Create a JSON representation of an instance of MediaUpload.\\n\\nReturns:\\n   string, a JSON representation of this instance, suitable to pass to\\n   from_json().',\n",
       " 'Call the create ad sense link method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.CreateAdSenseLinkRequest):\\n        The request object. Request message to be passed to\\n    CreateAdSenseLink method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.resources.AdSenseLink:\\n        A link between a GA4 Property and an\\n    AdSense for Content ad client.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the get project settings method over gRPC.\\n\\nRetrieves the Settings for the Project.\\n\\nReturns:\\n    Callable[[~.GetProjectSettingsRequest],\\n            ~.ProjectSettings]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Sleep *ds* seconds.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    MetastoreServiceAsyncClient: The constructed client.',\n",
       " \"Merges capacity commitments of the same plan into a single\\ncommitment.\\n\\nThe resulting capacity commitment has the greater\\ncommitment_end_time out of the to-be-merged capacity\\ncommitments.\\n\\nAttempting to merge capacity commitments of different plan will\\nfail with the error code\\n``google.rpc.Code.FAILED_PRECONDITION``.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_reservation_v1\\n\\n    def sample_merge_capacity_commitments():\\n        # Create a client\\n        client = bigquery_reservation_v1.ReservationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_reservation_v1.MergeCapacityCommitmentsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.merge_capacity_commitments(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_reservation_v1.types.MergeCapacityCommitmentsRequest, dict]):\\n        The request object. The request for\\n        [ReservationService.MergeCapacityCommitments][google.cloud.bigquery.reservation.v1.ReservationService.MergeCapacityCommitments].\\n    parent (str):\\n        Parent resource that identifies admin project and\\n        location e.g., ``projects/myproject/locations/us``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    capacity_commitment_ids (MutableSequence[str]):\\n        Ids of capacity commitments to merge.\\n        These capacity commitments must exist\\n        under admin project and location\\n        specified in the parent.\\n        ID is the last portion of capacity\\n        commitment name e.g., 'abc' for\\n        projects/myproject/locations/US/capacityCommitments/abc\\n\\n        This corresponds to the ``capacity_commitment_ids`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_reservation_v1.types.CapacityCommitment:\\n        Capacity commitment is a way to\\n        purchase compute capacity for BigQuery\\n        jobs (in the form of slots) with some\\n        committed period of usage. Annual\\n        commitments renew by default.\\n        Commitments can be removed after their\\n        commitment end time passes.\\n\\n        In order to remove annual commitment,\\n        its plan needs to be changed to monthly\\n        or flex first.\\n\\n        A capacity commitment resource exists as\\n        a child resource of the admin project.\",\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.billing_v1.types.ListProjectBillingInfoRequest):\\n        The initial request object.\\n    response (google.cloud.billing_v1.types.ListProjectBillingInfoResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Returns the specified network firewall policy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.NetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetNetworkFirewallPolicyRequest(\\n            firewall_policy=\"firewall_policy_value\",\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        NetworkFirewallPolicies.Get. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy (str):\\n        Name of the firewall policy to get.\\n        This corresponds to the ``firewall_policy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.FirewallPolicy:\\n        Represents a Firewall Policy\\n        resource.',\n",
       " 'Sets the port of this CoreV1EndpointPort.\\n\\nThe port number of the endpoint.  # noqa: E501\\n\\n:param port: The port of this CoreV1EndpointPort.  # noqa: E501\\n:type: int',\n",
       " 'Post-rpc interceptor for export_deployment_statefile\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Config server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the read repository file method over gRPC.\\n\\nReturns the contents of a file (inside a Repository). The\\nRepository must not have a value for\\n``git_remote_settings.url``.\\n\\nReturns:\\n    Callable[[~.ReadRepositoryFileRequest],\\n            ~.ReadRepositoryFileResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Gets the resource representation for an interactive\\nsession.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataproc_v1\\n\\n    def sample_get_session():\\n        # Create a client\\n        client = dataproc_v1.SessionControllerClient()\\n\\n        # Initialize request argument(s)\\n        request = dataproc_v1.GetSessionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_session(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataproc_v1.types.GetSessionRequest, dict]):\\n        The request object. A request to get the resource\\n        representation for a session.\\n    name (str):\\n        Required. The name of the session to\\n        retrieve.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataproc_v1.types.Session:\\n        A representation of a session.',\n",
       " 'Call the create connection method over HTTP.\\n\\nArgs:\\n    request (~.developer_connect.CreateConnectionRequest):\\n        The request object. Message for creating a Connection\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Post-rpc interceptor for list_participants\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the check grounding method over gRPC.\\n\\nPerforms a grounding check.\\n\\nReturns:\\n    Callable[[~.CheckGroundingRequest],\\n            ~.CheckGroundingResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the purge suggestion deny list\\nentries method over gRPC.\\n\\nPermanently deletes all\\n[SuggestionDenyListEntry][google.cloud.discoveryengine.v1beta.SuggestionDenyListEntry]\\nfor a DataStore.\\n\\nReturns:\\n    Callable[[~.PurgeSuggestionDenyListEntriesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Return a callable for the list sites method over gRPC.\\n\\nLists sites in a given project and location.\\n\\nReturns:\\n    Callable[[~.ListSitesRequest],\\n            ~.ListSitesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AwsClustersAsyncClient: The constructed client.',\n",
       " 'Call the create kms config method over HTTP.\\n\\nArgs:\\n    request (~.kms.CreateKmsConfigRequest):\\n        The request object. CreateKmsConfigRequest creates a KMS\\n    Config.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return if pages/frames are currently being cached.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Return a callable for the list related account groups method over gRPC.\\n\\nList groups of related accounts.\\n\\nReturns:\\n    Callable[[~.ListRelatedAccountGroupsRequest],\\n            ~.ListRelatedAccountGroupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Return a callable for the add control method over gRPC.\\n\\nEnables a Control on the specified ServingConfig. The control is\\nadded in the last position of the list of controls it belongs to\\n(e.g. if it's a facet spec control it will be applied in the\\nlast position of servingConfig.facetSpecIds) Returns a\\nALREADY_EXISTS error if the control has already been applied.\\nReturns a FAILED_PRECONDITION error if the addition could exceed\\nmaximum number of control allowed for that type of control.\\n\\nReturns:\\n    Callable[[~.AddControlRequest],\\n            ~.ServingConfig]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'Return a callable for the list accelerator types method over gRPC.\\n\\nLists accelerator types supported by this API.\\n\\nReturns:\\n    Callable[[~.ListAcceleratorTypesRequest],\\n            ~.ListAcceleratorTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Call the update organization\\nsettings method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.UpdateOrganizationSettingsRequest):\\n            The request object. Request message for updating an\\n        organization's settings.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcs_organization_settings.OrganizationSettings:\\n            User specified settings that are\\n        attached to the Security Command Center\\n        organization.\",\n",
       " 'Begins executing a batch create jobs operation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import talent_v4\\n\\n    def sample_batch_create_jobs():\\n        # Create a client\\n        client = talent_v4.JobServiceClient()\\n\\n        # Initialize request argument(s)\\n        jobs = talent_v4.Job()\\n        jobs.company = \"company_value\"\\n        jobs.requisition_id = \"requisition_id_value\"\\n        jobs.title = \"title_value\"\\n        jobs.description = \"description_value\"\\n\\n        request = talent_v4.BatchCreateJobsRequest(\\n            parent=\"parent_value\",\\n            jobs=jobs,\\n        )\\n\\n        # Make the request\\n        operation = client.batch_create_jobs(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.talent_v4.types.BatchCreateJobsRequest, dict]):\\n        The request object. Request to create a batch of jobs.\\n    parent (str):\\n        Required. The resource name of the tenant under which\\n        the job is created.\\n\\n        The format is\\n        \"projects/{project_id}/tenants/{tenant_id}\". For\\n        example, \"projects/foo/tenants/bar\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    jobs (MutableSequence[google.cloud.talent_v4.types.Job]):\\n        Required. The jobs to be created.\\n        A maximum of 200 jobs can be created in\\n        a batch.\\n\\n        This corresponds to the ``jobs`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.talent_v4.types.BatchCreateJobsResponse` The result of\\n           [JobService.BatchCreateJobs][google.cloud.talent.v4.JobService.BatchCreateJobs].\\n           It\\'s used to replace\\n           [google.longrunning.Operation.response][google.longrunning.Operation.response]\\n           in case of success.',\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Pre-rpc interceptor for get_management_dns_zone_binding\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VmwareEngine server.',\n",
       " 'Call the start scan run method over HTTP.\\n\\nArgs:\\n    request (~.web_security_scanner.StartScanRunRequest):\\n        The request object. Request for the ``StartScanRun`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.scan_run.ScanRun:\\n        A ScanRun is a output-only resource\\n    representing an actual run of the scan.\\n    Next id: 12',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'forward latex should really return nothing in either field if nothing is found.',\n",
       " 'Private function that doesn\\'t support extended axis or keepdims.\\nThese methods are extended to this function using _ureduce\\nSee nanpercentile for parameter usage\\nIt computes the quantiles of the array for the given axis.\\nA linear interpolation is performed based on the `interpolation`.\\n\\nBy default, the method is \"linear\" where alpha == beta == 1 which\\nperforms the 7th method of Hyndman&Fan.\\nWith \"median_unbiased\" we get alpha == beta == 1/3\\nthus the 8th method of Hyndman&Fan.',\n",
       " 'Custom memmap constructor compatible with numpy.memmap.\\n\\nThis function:\\n- is a backport the numpy memmap offset fix (See\\n  https://github.com/numpy/numpy/pull/8443 for more details.\\n  The numpy fix is available starting numpy 1.13)\\n- adds ``unlink_on_gc_collect``, which specifies  explicitly whether\\n  the process re-constructing the memmap owns a reference to the\\n  underlying file. If set to True, it adds a finalizer to the\\n  newly-created memmap that sends a maybe_unlink request for the\\n  memmaped file to resource_tracker.',\n",
       " 'If the most relevant error is an anyOf, then we traverse its context\\nand select the otherwise *least* relevant error, since in this case\\nthat means the most specific, deep, error inside the instance.\\n\\nI.e. since only one of the schemas must match, we look for the most\\nrelevant one.',\n",
       " 'Test that a nested `AtomicString` is not parsed. ',\n",
       " 'Recall the first view and position from the stack.',\n",
       " 'Remove this colorbar from the figure.\\n\\nIf the colorbar was created with ``use_gridspec=True`` the previous\\ngridspec is restored.',\n",
       " 'Tests that for loops at deeper levels are picked up',\n",
       " 'Add a colorbar to a plot.\\n\\nParameters\\n----------\\nmappable\\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\\n    mandatory for the `.Figure.colorbar` method but optional for the\\n    `.pyplot.colorbar` function, which sets the default to the current\\n    image.\\n\\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\\n    generate colorbars not attached to a previously drawn artist, e.g.\\n    ::\\n\\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\\n\\ncax : `~matplotlib.axes.Axes`, optional\\n    Axes into which the colorbar will be drawn.  If `None`, then a new\\n    Axes is created and the space for it will be stolen from the Axes(s)\\n    specified in *ax*.\\n\\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\\n    The one or more parent Axes from which space for a new colorbar Axes\\n    will be stolen. This parameter is only used if *cax* is not set.\\n\\n    Defaults to the Axes that contains the mappable used to create the\\n    colorbar.\\n\\nuse_gridspec : bool, optional\\n    If *cax* is ``None``, a new *cax* is created as an instance of\\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\\n    is ``True``, then *cax* is also positioned with a subplotspec.\\n\\nReturns\\n-------\\ncolorbar : `~matplotlib.colorbar.Colorbar`\\n\\nOther Parameters\\n----------------\\n%(_make_axes_kw_doc)s\\n%(_colormap_kw_doc)s\\n\\nNotes\\n-----\\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\\nincluded automatically.\\n\\nThe *shrink* kwarg provides a simple way to scale the colorbar with\\nrespect to the Axes. Note that if *cax* is specified, it determines the\\nsize of the colorbar, and *shrink* and *aspect* are ignored.\\n\\nFor more precise control, you can manually specify the positions of the\\naxes objects in which the mappable and the colorbar are drawn.  In this\\ncase, do not use any of the Axes properties kwargs.\\n\\nIt is known that some vector graphics viewers (svg and pdf) render\\nwhite gaps between segments of the colorbar.  This is due to bugs in\\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\\nrendered with overlapping segments::\\n\\n    cbar = colorbar()\\n    cbar.solids.set_edgecolor(\"face\")\\n    draw()\\n\\nHowever, this has negative consequences in other circumstances, e.g.\\nwith semi-transparent images (alpha < 1) and colorbar extensions;\\ntherefore, this workaround is not used by default (see issue #1188).',\n",
       " 'Set slider value to *val*.\\n\\nParameters\\n----------\\nval : float',\n",
       " 'Return the ``ZAxis`` (`~.axis3d.Axis`) instance.',\n",
       " 'Whether the returned results should be verbose.',\n",
       " 'Test scale function with non-array',\n",
       " \"Verify MAC challenge\\n\\nIf our message did not include a digest_name prefix, the client is allowed\\nto select a stronger digest_name from _ALLOWED_DIGESTS.\\n\\nIn case our message is prefixed, a client cannot downgrade to a weaker\\nalgorithm, because the MAC is calculated over the entire message\\nincluding the '{digest_name}' prefix.\",\n",
       " 'Given a non-complete graph G, returns a missing edge.',\n",
       " 'Make X orthogonal to the nullspace of L.',\n",
       " 'Determine if a provided dtype is of a specified data type ``kind``.\\n\\nThis function only supports built-in NumPy\\'s data types.\\nThird-party dtypes are not yet supported.\\n\\nParameters\\n----------\\ndtype : dtype\\n    The input dtype.\\nkind : dtype or str or tuple of dtypes/strs.\\n    dtype or dtype kind. Allowed dtype kinds are:\\n    * ``\\'bool\\'`` : boolean kind\\n    * ``\\'signed integer\\'`` : signed integer data types\\n    * ``\\'unsigned integer\\'`` : unsigned integer data types\\n    * ``\\'integral\\'`` : integer data types\\n    * ``\\'real floating\\'`` : real-valued floating-point data types\\n    * ``\\'complex floating\\'`` : complex floating-point data types\\n    * ``\\'numeric\\'`` : numeric data types\\n\\nReturns\\n-------\\nout : bool\\n\\nSee Also\\n--------\\nissubdtype\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> np.isdtype(np.float32, np.float64)\\nFalse\\n>>> np.isdtype(np.float32, \"real floating\")\\nTrue\\n>>> np.isdtype(np.complex128, (\"real floating\", \"complex floating\"))\\nTrue',\n",
       " 'Sets the pod_selector of this V1NetworkPolicySpec.\\n\\n\\n:param pod_selector: The pod_selector of this V1NetworkPolicySpec.  # noqa: E501\\n:type: V1LabelSelector',\n",
       " 'Ensure the logged in user has authorized silent OpenID authorization.\\n\\nSilent OpenID authorization allows access tokens and id tokens to be\\ngranted to clients without any user prompt or interaction.\\n\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n:rtype: True or False\\n\\nMethod is used by:\\n    - OpenIDConnectAuthCode\\n    - OpenIDConnectImplicit\\n    - OpenIDConnectHybrid',\n",
       " 'This property can be used as a prefix for any HTTP method call to return the\\nthe raw response object instead of the parsed content.\\n\\nFor more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers',\n",
       " 'Test that the meter provides a function to create a new ObservableCounter',\n",
       " 'Loads an ASN.1 object of an x509 certificate into a Certificate object\\n\\n:param certificate:\\n    An asn1crypto.x509.Certificate object\\n\\n:return:\\n    A Certificate object',\n",
       " 'Convert a native series structure to a Series object.',\n",
       " 'Return the list of thead row elements from the parsed table element.\\n\\nParameters\\n----------\\ntable : a table element that contains zero or more thead elements.\\n\\nReturns\\n-------\\nlist of node-like\\n    These are the <tr> row elements of a table.',\n",
       " 'Try to format axes if they are datelike.',\n",
       " 'return the root node',\n",
       " 'Convert the data from this selection to the appropriate pandas type.\\n\\nParameters\\n----------\\nvalues : np.ndarray\\nnan_rep : str\\nencoding : str\\nerrors : str',\n",
       " 'Return different versions of data for count times',\n",
       " 'Check if we have a not-outdated version loaded already.',\n",
       " 'Remove a number of characters from the end of the text.',\n",
       " 'This method should only be called once, before the connection is used.',\n",
       " 'Negate a polynomial in ``K[x]``.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x = ring(\"x\", ZZ)\\n\\n>>> R.dup_neg(x**2 - 1)\\n-x**2 + 1',\n",
       " 'Test Subversion.get_vcs_version() with previously cached result.',\n",
       " 'Execute a statement using :sql:`VALUES` with a sequence of parameters.\\n\\n:param cur: the cursor to use to execute the query.\\n\\n:param sql: the query to execute. It must contain a single ``%s``\\n    placeholder, which will be replaced by a `VALUES list`__.\\n    Example: ``\"INSERT INTO mytable (id, f1, f2) VALUES %s\"``.\\n\\n:param argslist: sequence of sequences or dictionaries with the arguments\\n    to send to the query. The type and content must be consistent with\\n    *template*.\\n\\n:param template: the snippet to merge to every item in *argslist* to\\n    compose the query.\\n\\n    - If the *argslist* items are sequences it should contain positional\\n      placeholders (e.g. ``\"(%s, %s, %s)\"``, or ``\"(%s, %s, 42)``\" if there\\n      are constants value...).\\n\\n    - If the *argslist* items are mappings it should contain named\\n      placeholders (e.g. ``\"(%(id)s, %(f1)s, 42)\"``).\\n\\n    If not specified, assume the arguments are sequence and use a simple\\n    positional template (i.e.  ``(%s, %s, ...)``), with the number of\\n    placeholders sniffed by the first element in *argslist*.\\n\\n:param page_size: maximum number of *argslist* items to include in every\\n    statement. If there are more items the function will execute more than\\n    one statement.\\n\\n:param fetch: if `!True` return the query results into a list (like in a\\n    `~cursor.fetchall()`).  Useful for queries with :sql:`RETURNING`\\n    clause.\\n\\n.. __: https://www.postgresql.org/docs/current/static/queries-values.html\\n\\nAfter the execution of the function the `cursor.rowcount` property will\\n**not** contain a total result.\\n\\nWhile :sql:`INSERT` is an obvious candidate for this function it is\\npossible to use it with other statements, for example::\\n\\n    >>> cur.execute(\\n    ... \"create table test (id int primary key, v1 int, v2 int)\")\\n\\n    >>> execute_values(cur,\\n    ... \"INSERT INTO test (id, v1, v2) VALUES %s\",\\n    ... [(1, 2, 3), (4, 5, 6), (7, 8, 9)])\\n\\n    >>> execute_values(cur,\\n    ... \"\"\"UPDATE test SET v1 = data.v1 FROM (VALUES %s) AS data (id, v1)\\n    ... WHERE test.id = data.id\"\"\",\\n    ... [(1, 20), (4, 50)])\\n\\n    >>> cur.execute(\"select * from test order by id\")\\n    >>> cur.fetchall()\\n    [(1, 20, 3), (4, 50, 6), (7, 8, 9)])',\n",
       " 'Finalize build system configuration on win32 platform.',\n",
       " 'Instantiate a cipher object that performs ECB encryption/decryption.\\n\\n:Parameters:\\n  factory : module\\n    The underlying block cipher, a module from ``Crypto.Cipher``.\\n\\nAll keywords are passed to the underlying block cipher.\\nSee the relevant documentation for details (at least ``key`` will need\\nto be present',\n",
       " '`version` is the mypy version string.\\n\\nWe might want to use this to print a warning if the mypy version being used is\\nnewer, or especially older, than we expect (or need).\\n\\nArgs:\\n    version: The mypy version string.\\n\\nReturn:\\n    The Pydantic mypy plugin type.',\n",
       " 'Returns a dict of config keys to values.\\n\\nIt reads configs from toml file and returns `None` if the file is not a toml file.',\n",
       " ':calls: `POST /repos/{owner}/{repo}/forks <https://docs.github.com/en/rest/reference/repos#forks>`_\\n:param organization: :class:`github.Organization.Organization` or string\\n:param name: string\\n:param default_branch_only: bool\\n:rtype: :class:`github.Repository.Repository`',\n",
       " 'Issue the warning :param:`message` for the definition of the given :param:`method`\\n\\nthis helps to log warnings for functions defined prior to finding an issue with them\\n(like hook wrappers being marked in a legacy mechanism)',\n",
       " \"Check whether 'value' should be coerced to 'field' type.\",\n",
       " 'Error 301 -- also relocated (permanently).',\n",
       " 'Issue #96',\n",
       " 'Polls a Cloud Pub/Sub subscription for new GCS events for display.',\n",
       " 'Sets the api_server_id of this V1alpha1ServerStorageVersion.\\n\\nThe ID of the reporting API server.  # noqa: E501\\n\\n:param api_server_id: The api_server_id of this V1alpha1ServerStorageVersion.  # noqa: E501\\n:type: str',\n",
       " \"Differentiate polynomials represented with coefficients.\\n\\np must be a 1-D or 2-D array.  In the 2-D case, each column gives\\nthe coefficients of a polynomial; the first row holds the coefficients\\nassociated with the highest power. m must be a nonnegative integer.\\n(numpy.polyder doesn't handle the 2-D case.)\",\n",
       " 'Return a :class:`.MapperOption` that will indicate to the\\n:class:`_query.Query`\\nthat the main table has been aliased.',\n",
       " \"Initialize an ``AlgorithmEstimator`` instance.\\n\\nArgs:\\n    algorithm_arn (str): algorithm arn used for training. Can be just the name if your\\n        account owns the algorithm.\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker\\n        training jobs and APIsthat create Amazon SageMaker endpoints use this role to\\n        access training data and model artifacts. After the endpoint\\n        is created, the inference code might use the IAM role, if it\\n        needs to access an AWS resource.\\n    instance_count (int or PipelineVariable): Number of Amazon EC2 instances to use\\n        for training.\\n    instance_type (str or PipelineVariable): Type of EC2 instance to use for training,\\n        for example, 'ml.c4.xlarge'.\\n    volume_size (int or PipelineVariable): Size in GB of the EBS volume to use for\\n        storing input data during training (default: 30). Must be large enough to store\\n        training data if File Mode is used (which is the default).\\n    volume_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting\\n        EBS volume attached to the training instance (default: None).\\n    max_run (int or PipelineVariable): Timeout in seconds for training\\n        (default: 24 * 60 * 60).\\n        After this amount of time Amazon SageMaker terminates the\\n        job regardless of its current status.\\n    input_mode (str or PipelineVariable): The input mode that the algorithm supports\\n        (default: 'File'). Valid modes:\\n\\n        * 'File' - Amazon SageMaker copies the training dataset from\\n          the S3 location to a local directory.\\n        * 'Pipe' - Amazon SageMaker streams data directly from S3 to\\n          the container via a Unix-named pipe.\\n\\n        This argument can be overriden on a per-channel basis using\\n        ``sagemaker.inputs.TrainingInput.input_mode``.\\n\\n    output_path (str or PipelineVariable): S3 location for saving the training result\\n        (model artifacts and output files). If not specified,\\n        results are stored to a default bucket. If\\n        the bucket with the specific name does not exist, the\\n        estimator creates the bucket during the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit` method\\n        execution.\\n    output_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting the\\n        training output (default: None). base_job_name (str): Prefix for\\n        training job name when the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit`\\n        method launches. If not specified, the estimator generates a\\n        default job name, based on the training image name and\\n        current timestamp.\\n    sagemaker_session (sagemaker.session.Session): Session object which manages\\n        interactions with Amazon SageMaker APIs and any other AWS services needed. If\\n        not specified, the estimator creates one using the default\\n        AWS configuration chain.\\n    tags (Union[Tags]): Tags for\\n        labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    subnets (list[str] or list[PipelineVariable]): List of subnet ids. If not specified\\n        training job will be created without VPC config.\\n        security_group_ids (list[str]): List of security group ids. If\\n        not specified training job will be created without VPC config.\\n    model_uri (str): URI where a pre-trained model is stored, either locally or in S3\\n        (default: None). If specified, the estimator will create a channel pointing to\\n        the model so the training job can download it. This model\\n        can be a 'model.tar.gz' from a previous training job, or\\n        other artifacts coming from a different source.\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization\\n    model_channel_name (str or PipelineVariable): Name of the channel where 'model_uri'\\n        will be downloaded (default: 'model'). metric_definitions\\n        (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for\\n        the name of the metric, and 'Regex' for the regular\\n        expression used to extract the metric from the logs.\\n    encrypt_inter_container_traffic (bool or PipelineVariable): Specifies whether traffic\\n        between training containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool or PipelineVariable): Specifies whether to use SageMaker\\n        Managed Spot instances for training. If enabled then the\\n        `max_wait` arg should also be set.\\n\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\\n        (default: ``False``).\\n    max_wait (int or PipelineVariable): Timeout in seconds waiting for spot training\\n        instances (default: None). After this amount of time Amazon\\n        SageMaker will stop waiting for Spot instances to become\\n        available (default: ``None``).\\n    **kwargs: Additional kwargs. This is unused. It's only added for AlgorithmEstimator\\n        to ignore the irrelevant arguments.\\n\\nRaises:\\n    ValueError:\\n    - If an AWS IAM Role is not provided.\\n    - Bad value for instance type.\\n    RuntimeError:\\n    - When setting up custom VPC, both subnets and security_group_ids are not provided\\n    - If instance_count > 1 (distributed training) with instance type local or local gpu\\n    - If LocalSession is not used with instance type local or local gpu\\n    - file:// output path used outside of local mode\\n    botocore.exceptions.ClientError:\\n    - algorithm arn is incorrect\\n    - insufficient permission to access/ describe algorithm\\n    - algorithm is in a different region\",\n",
       " 'Sets fields in object based on json of header.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of spec.',\n",
       " 'The full SELECT statement represented by this Query.\\n\\nThe statement by default will not have disambiguating labels\\napplied to the construct unless with_labels(True) is called\\nfirst.',\n",
       " 'Set Additional Model Source to ``this`` model.\\n\\nArgs:\\n    speculative_decoding_config (Optional[Dict[str, Any]]): Speculative decoding config.\\n    accept_eula (Optional[bool]): For models that require a Model Access Config.',\n",
       " 'Decorator to emit telemetry logs for SageMaker Python SDK functions',\n",
       " 'Memoized fast path for _get_funcs instances',\n",
       " 'Access nonzero values, possibly after summing duplicates.\\n\\nParameters\\n----------\\ns : sparse array\\n    Input sparse array.\\n\\nReturns\\n-------\\ndata: ndarray\\n  Nonzero values of the array, with shape (s.nnz,)',\n",
       " 'Compute matrices to transform rot. vector derivatives to angular rates.\\n\\nThe matrices depend on the current attitude represented as a rotation\\nvector.\\n\\nParameters\\n----------\\nrotvecs : ndarray, shape (n, 3)\\n    Set of rotation vectors.\\n\\nReturns\\n-------\\nndarray, shape (n, 3, 3)',\n",
       " 'Return a GeoJSON-like mapping of the LineString geometry.',\n",
       " 'Unsupported.',\n",
       " 'Does s3 multipart chunking work correctly?',\n",
       " 'test #6696',\n",
       " 'Gets the value of withCentering or its default value.',\n",
       " 'Read HTML tables into a ``list`` of ``DataFrame`` objects.\\n\\nParameters\\n----------\\nio : str or file-like\\n    A URL, a file-like object, or a raw string containing HTML. Note that\\n    lxml only accepts the http, FTP and file URL protocols. If you have a\\n    URL that starts with ``\\'https\\'`` you might try removing the ``\\'s\\'``.\\n\\n    .. deprecated:: 4.0.0\\n        Passing html literal strings is deprecated.\\n        Wrap literal string/bytes input in io.StringIO/io.BytesIO instead.\\n\\nmatch : str or compiled regular expression, optional\\n    The set of tables containing text matching this regex or string will be\\n    returned. Unless the HTML is extremely simple you will probably need to\\n    pass a non-empty string here. Defaults to \\'.+\\' (match any non-empty\\n    string). The default value will return all tables contained on a page.\\n    This value is converted to a regular expression so that there is\\n    consistent behavior between Beautiful Soup and lxml.\\n\\nflavor : str or None, container of strings\\n    The parsing engine to use. \\'bs4\\' and \\'html5lib\\' are synonymous with\\n    each other, they are both there for backwards compatibility. The\\n    default of ``None`` tries to use ``lxml`` to parse and if that fails it\\n    falls back on ``bs4`` + ``html5lib``.\\n\\nheader : int or list-like or None, optional\\n    The row (or list of rows for a :class:`~ps.MultiIndex`) to use to\\n    make the columns headers.\\n\\nindex_col : int or list-like or None, optional\\n    The column (or list of columns) to use to create the index.\\n\\nskiprows : int or list-like or slice or None, optional\\n    0-based. Number of rows to skip after parsing the column integer. If a\\n    sequence of integers or a slice is given, will skip the rows indexed by\\n    that sequence.  Note that a single element sequence means \\'skip the nth\\n    row\\' whereas an integer means \\'skip n rows\\'.\\n\\nattrs : dict or None, optional\\n    This is a dictionary of attributes that you can pass to use to identify\\n    the table in the HTML. These are not checked for validity before being\\n    passed to lxml or Beautiful Soup. However, these attributes must be\\n    valid HTML table attributes to work correctly. For example, ::\\n\\n        attrs = {\\'id\\': \\'table\\'}\\n\\n    is a valid attribute dictionary because the \\'id\\' HTML tag attribute is\\n    a valid HTML attribute for *any* HTML tag as per `this document\\n    <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\\n\\n        attrs = {\\'asdf\\': \\'table\\'}\\n\\n    is *not* a valid attribute dictionary because \\'asdf\\' is not a valid\\n    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\\n    table attributes can be found `here\\n    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\\n    working draft of the HTML 5 spec can be found `here\\n    <http://www.w3.org/TR/html-markup/table.html>`__. It contains the\\n    latest information on table attributes for the modern web.\\n\\nparse_dates : bool, optional\\n    See :func:`~ps.read_csv` for more details.\\n\\nthousands : str, optional\\n    Separator to use to parse thousands. Defaults to ``\\',\\'``.\\n\\nencoding : str or None, optional\\n    The encoding used to decode the web page. Defaults to ``None``.``None``\\n    preserves the previous encoding behavior, which depends on the\\n    underlying parser library (e.g., the parser library will try to use\\n    the encoding provided by the document).\\n\\ndecimal : str, default \\'.\\'\\n    Character to recognize as decimal point (example: use \\',\\' for European\\n    data).\\n\\nconverters : dict, default None\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\n\\nna_values : iterable, default None\\n    Custom NA values\\n\\nkeep_default_na : bool, default True\\n    If na_values are specified and keep_default_na is False the default NaN\\n    values are overridden, otherwise they\\'re appended to\\n\\ndisplayed_only : bool, default True\\n    Whether elements with \"display: none\" should be parsed\\n\\nReturns\\n-------\\ndfs : list of DataFrames\\n\\nSee Also\\n--------\\nread_csv\\nDataFrame.to_html',\n",
       " 'Truncate the timestamps for nanoseconds.',\n",
       " 'Test the different timestamp, date, and timedelta types.',\n",
       " 'Return the \"comment\" for the table identified by ``table_name``.\\n\\nGiven a string ``table_name`` and an optional string ``schema``, return\\ntable comment information as a dictionary corresponding to the\\n:class:`.ReflectedTableComment` dictionary.\\n\\nThis is an internal dialect method. Applications should use\\n:meth:`.Inspector.get_table_comment`.\\n\\n:raise: ``NotImplementedError`` for dialects that don\\'t support\\n comments.\\n\\n.. versionadded:: 1.2',\n",
       " 'Driver quirk where the cursor.fetchall() will work even if\\nthe connection has been rolled back.\\n\\nThis generally refers to buffered cursors but also seems to work\\nwith cx_oracle, for example.',\n",
       " 'Target must support simultaneous, independent database connections\\nthat will be used in a readonly fashion.',\n",
       " 'Start date column, if not present already.',\n",
       " 'Returns the typecast or ``None`` of this object as a string.',\n",
       " 'Target must support simultaneous, independent database connections.',\n",
       " 'If ``key`` is in ``dictionary``, set the new value of ``key``\\nto be the union between the old value and ``value``.\\nOtherwise, set the value of ``key`` to ``value.\\n\\nReturns ``True`` if the key already was in the dictionary and\\n``False`` otherwise.',\n",
       " 'Return True if expr1 and expr2 are numerically close.\\n\\nThe expressions must have the same structure, but any Rational, Integer, or\\nFloat numbers they contain are compared approximately using rtol and atol.\\nAny other parts of expressions are compared exactly. However, allowance is\\nmade to allow for the additive and multiplicative identities.\\n\\nRelative tolerance is measured with respect to expr2 so when used in\\ntesting expr2 should be the expected correct answer.\\n\\nExamples\\n========\\n\\n>>> from sympy import exp\\n>>> from sympy.abc import x, y\\n>>> from sympy.core.numbers import all_close\\n>>> expr1 = 0.1*exp(x - y)\\n>>> expr2 = exp(x - y)/10\\n>>> expr1\\n0.1*exp(x - y)\\n>>> expr2\\nexp(x - y)/10\\n>>> expr1 == expr2\\nFalse\\n>>> all_close(expr1, expr2)\\nTrue\\n\\nIdentities are automatically supplied:\\n\\n>>> all_close(x, x + 1e-10)\\nTrue\\n>>> all_close(x, 1.0*x)\\nTrue\\n>>> all_close(x, 1.0*x + 1e-10)\\nTrue',\n",
       " 'Turn all numbers in eq into their polar equivalents (under the standard\\nchoice of argument).\\n\\nNote that no attempt is made to guess a formal convention of adding\\npolar numbers, expressions like $1 + x$ will generally not be altered.\\n\\nNote also that this function does not promote ``exp(x)`` to ``exp_polar(x)``.\\n\\nIf ``subs`` is ``True``, all symbols which are not already polar will be\\nsubstituted for polar dummies; in this case the function behaves much\\nlike :func:`~.posify`.\\n\\nIf ``lift`` is ``True``, both addition statements and non-polar symbols are\\nchanged to their ``polar_lift()``ed versions.\\nNote that ``lift=True`` implies ``subs=False``.\\n\\nExamples\\n========\\n\\n>>> from sympy import polarify, sin, I\\n>>> from sympy.abc import x, y\\n>>> expr = (-x)**y\\n>>> expr.expand()\\n(-x)**y\\n>>> polarify(expr)\\n((_x*exp_polar(I*pi))**_y, {_x: x, _y: y})\\n>>> polarify(expr)[0].expand()\\n_x**_y*exp_polar(_y*I*pi)\\n>>> polarify(x, lift=True)\\npolar_lift(x)\\n>>> polarify(x*(1+y), lift=True)\\npolar_lift(x)*polar_lift(y + 1)\\n\\nAdds are treated carefully:\\n\\n>>> polarify(1 + sin((1 + I)*x))\\n(sin(_x*polar_lift(1 + I)) + 1, {_x: x})',\n",
       " 'Front-end function of the inverse Laplace transform. It tries to apply all\\nknown rules recursively.  If everything else fails, it tries to integrate.',\n",
       " 'Returns denominator of ``a``. ',\n",
       " 'Returns a field associated with ``self``. ',\n",
       " 'Return the Smith Normal Form of a matrix `m` over the ring `domain`.\\nThis will only work if the ring is a principal ideal domain.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_form\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> print(smith_normal_form(m).to_Matrix())\\nMatrix([[1, 0, 0], [0, 10, 0], [0, 0, 30]])',\n",
       " 'Returns ``True`` if ``f`` is a cyclotomic polynomial. ',\n",
       " 'Calls the given callback on the next IOLoop iteration.\\n\\nAs of Tornado 6.0, this method is equivalent to `add_callback`.\\n\\n.. versionadded:: 4.0',\n",
       " 'Put a connection back into the pool.\\n\\n:param conn:\\n    Connection object for the current host and port as returned by\\n    :meth:`._new_conn` or :meth:`._get_conn`.\\n\\nIf the pool is already full, the connection is closed and discarded\\nbecause we exceeded maxsize. If connections are discarded frequently,\\nthen maxsize should be increased.\\n\\nIf the pool is closed, then the connection will be closed and discarded.',\n",
       " \"Create a WebSocket server listening on a Unix socket.\\n\\nThis function is identical to :func:`serve`, except the ``host`` and\\n``port`` arguments are replaced by ``path``. It's only available on Unix.\\n\\nIt's useful for deploying a server behind a reverse proxy such as nginx.\\n\\nArgs:\\n    handler: Connection handler. It receives the WebSocket connection,\\n        which is a :class:`ServerConnection`, in argument.\\n    path: File system path to the Unix socket.\",\n",
       " 'Handshake succeeds without subprotocols.',\n",
       " 'Creates datatype test schema, tables, and inserts test data.',\n",
       " 'Write File.',\n",
       " \"Locale display names for months.\\n\\n>>> Locale('de', 'DE').months['format']['wide'][10]\\nu'Oktober'\",\n",
       " 'Return the era names used by the locale for the specified format.\\n\\n>>> get_era_names(\\'wide\\', locale=\\'en_US\\')[1]\\nu\\'Anno Domini\\'\\n>>> get_era_names(\\'abbreviated\\', locale=\\'de_DE\\')[1]\\nu\\'n. Chr.\\'\\n\\n:param width: the width to use, either \"wide\", \"abbreviated\", or \"narrow\"\\n:param locale: the `Locale` object, or a locale string',\n",
       " 'Return a node that represents the (type) result of an indexing operation,\\ne.g. for tuple unpacking or iteration.',\n",
       " 'Compacts the frames to deduplicate recursive calls.',\n",
       " 'Many of these arguments duplicate and override values that can be\\nprovided in a configuration file.  Parameters that are missing here\\nwill use values from the config file.\\n\\n`data_file` is the base name of the data file to use. The config value\\ndefaults to \".coverage\".  None can be provided to prevent writing a data\\nfile.  `data_suffix` is appended (with a dot) to `data_file` to create\\nthe final file name.  If `data_suffix` is simply True, then a suffix is\\ncreated with the machine and process identity included.\\n\\n`cover_pylib` is a boolean determining whether Python code installed\\nwith the Python interpreter is measured.  This includes the Python\\nstandard library and any packages installed with the interpreter.\\n\\nIf `auto_data` is true, then any existing data file will be read when\\ncoverage measurement starts, and data will be saved automatically when\\nmeasurement stops.\\n\\nIf `timid` is true, then a slower and simpler trace function will be\\nused.  This is important for some environments where manipulation of\\ntracing functions breaks the faster trace function.\\n\\nIf `branch` is true, then branch coverage will be measured in addition\\nto the usual statement coverage.\\n\\n`config_file` determines what configuration file to read:\\n\\n    * If it is \".coveragerc\", it is interpreted as if it were True,\\n      for backward compatibility.\\n\\n    * If it is a string, it is the name of the file to read.  If the\\n      file can\\'t be read, it is an error.\\n\\n    * If it is True, then a few standard files names are tried\\n      (\".coveragerc\", \"setup.cfg\", \"tox.ini\").  It is not an error for\\n      these files to not be found.\\n\\n    * If it is False, then no configuration file is read.\\n\\n`source` is a list of file paths or package names.  Only code located\\nin the trees indicated by the file paths or package names will be\\nmeasured.\\n\\n`source_pkgs` is a list of package names. It works the same as\\n`source`, but can be used to name packages where the name can also be\\ninterpreted as a file path.\\n\\n`include` and `omit` are lists of file name patterns. Files that match\\n`include` will be measured, files that match `omit` will not.  Each\\nwill also accept a single string argument.\\n\\n`debug` is a list of strings indicating what debugging information is\\ndesired.\\n\\n`concurrency` is a string indicating the concurrency library being used\\nin the measured code.  Without this, coverage.py will get incorrect\\nresults if these libraries are in use.  Valid strings are \"greenlet\",\\n\"eventlet\", \"gevent\", \"multiprocessing\", or \"thread\" (the default).\\nThis can also be a list of these strings.\\n\\nIf `check_preimported` is true, then when coverage is started, the\\nalready-imported files will be checked to see if they should be\\nmeasured by coverage.  Importing measured files before coverage is\\nstarted can mean that code is missed.\\n\\n`context` is a string to use as the :ref:`static context\\n<static_contexts>` label for collected data.\\n\\nIf `messages` is true, some messages will be printed to stdout\\nindicating what is happening.\\n\\n.. versionadded:: 4.0\\n    The `concurrency` parameter.\\n\\n.. versionadded:: 4.2\\n    The `concurrency` parameter can now be a list of strings.\\n\\n.. versionadded:: 5.0\\n    The `check_preimported` and `context` parameters.\\n\\n.. versionadded:: 5.3\\n    The `source_pkgs` parameter.\\n\\n.. versionadded:: 6.0\\n    The `messages` parameter.',\n",
       " 'Make sure our htmlcov directory exists.',\n",
       " 'Assert that numbits is good.',\n",
       " 'Make a tree of packages.\\n\\nMakes `width` directories, named d0 .. d{width-1}. Each directory has\\n__init__.py, and `width` files, named f0.py .. f{width-1}.py.  Each\\ndirectory also has `width` sub-directories, in the same fashion, until\\na depth of `depth` is reached.',\n",
       " 'Changes the value of a variable',\n",
       " 'Align the given address to the end of the page it occupies.\\nThat is, to point to the start of the next page.\\n\\n@type  address: int\\n@param address: Memory address.\\n\\n@rtype:  int\\n@return: Aligned memory address.',\n",
       " '@rtype:  bool\\n@return: C{True} if the memory in this region belongs to a mapped file.',\n",
       " 'Set a the value for ``key`` to ``value`` inside the ``config``\\ndict.',\n",
       " 'Unpack the crc32 checksum for the ith object from the index file.',\n",
       " 'All refs present in this container.',\n",
       " \"Return a named tuple containing ISO year, week number, and weekday.\\n\\nThe first ISO week of the year is the (Mon-Sun) week\\ncontaining the year's first Thursday; everything else derives\\nfrom that.\\n\\nThe first week is 1; Monday is 1 ... Sunday is 7.\\n\\nISO calendar algorithm taken from\\nhttp://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\\n(used with permission)\",\n",
       " 'Mirror attributes and methods from the given\\norigin_name attribute of the instance to the\\ndecorated class',\n",
       " 'Post-rpc interceptor for create_analytics_account_link\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the MarketingplatformAdminService server but before\\nit is returned to user code.',\n",
       " 'Call the list google ads links method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.ListGoogleAdsLinksRequest):\\n        The request object. Request message for\\n    ListGoogleAdsLinks RPC.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.analytics_admin.ListGoogleAdsLinksResponse:\\n        Response message for\\n    ListGoogleAdsLinks RPC.',\n",
       " 'Returns matching deployments.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import apigee_registry_v1\\n\\n    def sample_list_api_deployments():\\n        # Create a client\\n        client = apigee_registry_v1.RegistryClient()\\n\\n        # Initialize request argument(s)\\n        request = apigee_registry_v1.ListApiDeploymentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_api_deployments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.apigee_registry_v1.types.ListApiDeploymentsRequest, dict]):\\n        The request object. Request message for\\n        ListApiDeployments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        deployments. Format: ``projects/*/locations/*/apis/*``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.apigee_registry_v1.services.registry.pagers.ListApiDeploymentsPager:\\n        Response message for\\n        ListApiDeployments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ApiHubAsyncClient: The constructed client.',\n",
       " 'Call the get api spec method over HTTP.\\n\\nArgs:\\n    request (~.registry_service.GetApiSpecRequest):\\n        The request object. Request message for GetApiSpec.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.registry_models.ApiSpec:\\n        Describes a version of an API in a\\n    structured way. ApiSpecs provide formal\\n    descriptions that consumers can use to\\n    use a version. ApiSpec resources are\\n    intended to be fully-resolved\\n    descriptions of an ApiVersion. When\\n    specs consist of multiple files, these\\n    should be bundled together (e.g., in a\\n    zip archive) and stored as a unit.\\n    Multiple specs can exist to provide\\n    representations in different API\\n    description formats. Synchronization of\\n    these representations would be provided\\n    by tooling and background services.',\n",
       " 'return a boolean if I am possibly a view',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Returns the specified disk type.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.DiskTypesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetDiskTypeRequest(\\n            disk_type=\"disk_type_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetDiskTypeRequest, dict]):\\n        The request object. A request message for DiskTypes.Get.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone for this\\n        request.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk_type (str):\\n        Name of the disk type to return.\\n        This corresponds to the ``disk_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.DiskType:\\n        Represents a Disk Type resource. Google Compute Engine\\n        has two Disk Type resources: \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDiskTypes)\\n        \\\\* [Zonal](/compute/docs/reference/rest/v1/diskTypes)\\n        You can choose from a variety of disk types based on\\n        your needs. For more information, read Storage options.\\n        The diskTypes resource represents disk types for a zonal\\n        persistent disk. For more information, read Zonal\\n        persistent disks. The regionDiskTypes resource\\n        represents disk types for a regional persistent disk.\\n        For more information, read Regional persistent disks.',\n",
       " 'Retrieves a list of resize requests that are\\ncontained in the managed instance group.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InstanceGroupManagerResizeRequestsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInstanceGroupManagerResizeRequestsRequest(\\n            instance_group_manager=\"instance_group_manager_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInstanceGroupManagerResizeRequestsRequest, dict]):\\n        The request object. A request message for\\n        InstanceGroupManagerResizeRequests.List.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone where the\\n        managed instance group is located. The\\n        name should conform to RFC1035.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    instance_group_manager (str):\\n        The name of the managed instance\\n        group. The name should conform to\\n        RFC1035.\\n\\n        This corresponds to the ``instance_group_manager`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.instance_group_manager_resize_requests.pagers.ListPager:\\n        [Output Only] A list of resize requests.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Snapshot the state of a streaming job.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataflow_v1beta3\\n\\n    def sample_snapshot_job():\\n        # Create a client\\n        client = dataflow_v1beta3.JobsV1Beta3Client()\\n\\n        # Initialize request argument(s)\\n        request = dataflow_v1beta3.SnapshotJobRequest(\\n        )\\n\\n        # Make the request\\n        response = client.snapshot_job(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataflow_v1beta3.types.SnapshotJobRequest, dict]):\\n        The request object. Request to create a snapshot of a\\n        job.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataflow_v1beta3.types.Snapshot:\\n        Represents a snapshot of a job.',\n",
       " 'Use this method to delete a private connectivity\\nconfiguration.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datastream_v1alpha1\\n\\n    def sample_delete_private_connection():\\n        # Create a client\\n        client = datastream_v1alpha1.DatastreamClient()\\n\\n        # Initialize request argument(s)\\n        request = datastream_v1alpha1.DeletePrivateConnectionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_private_connection(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datastream_v1alpha1.types.DeletePrivateConnectionRequest, dict]):\\n        The request object.\\n    name (str):\\n        Required. The name of the private\\n        connectivity configuration to delete.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Call the fetch git hub\\ninstallations method over HTTP.\\n\\n    Args:\\n        request (~.developer_connect.FetchGitHubInstallationsRequest):\\n            The request object. Request for fetching github\\n        installations.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.developer_connect.FetchGitHubInstallationsResponse:\\n            Response of fetching github\\n        installations.',\n",
       " 'Call the update conversation\\nprofile method over HTTP.\\n\\n    Args:\\n        request (~.gcd_conversation_profile.UpdateConversationProfileRequest):\\n            The request object. The request message for\\n        [ConversationProfiles.UpdateConversationProfile][google.cloud.dialogflow.v2.ConversationProfiles.UpdateConversationProfile].\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcd_conversation_profile.ConversationProfile:\\n            Defines the services to connect to\\n        incoming Dialogflow conversations.',\n",
       " 'Updates the specified entity type.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflow_v2\\n\\n    def sample_update_entity_type():\\n        # Create a client\\n        client = dialogflow_v2.EntityTypesClient()\\n\\n        # Initialize request argument(s)\\n        entity_type = dialogflow_v2.EntityType()\\n        entity_type.display_name = \"display_name_value\"\\n        entity_type.kind = \"KIND_REGEXP\"\\n\\n        request = dialogflow_v2.UpdateEntityTypeRequest(\\n            entity_type=entity_type,\\n        )\\n\\n        # Make the request\\n        response = client.update_entity_type(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflow_v2.types.UpdateEntityTypeRequest, dict]):\\n        The request object. The request message for\\n        [EntityTypes.UpdateEntityType][google.cloud.dialogflow.v2.EntityTypes.UpdateEntityType].\\n    entity_type (google.cloud.dialogflow_v2.types.EntityType):\\n        Required. The entity type to update.\\n        This corresponds to the ``entity_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    language_code (str):\\n        Optional. The language used to access language-specific\\n        data. If not specified, the agent\\'s default language is\\n        used. For more information, see `Multilingual intent and\\n        entity\\n        data <https://cloud.google.com/dialogflow/docs/agents-multilingual#intent-entity>`__.\\n\\n        This corresponds to the ``language_code`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflow_v2.types.EntityType:\\n        Each intent parameter has a type, called the entity type, which dictates\\n           exactly how data from an end-user expression is\\n           extracted.\\n\\n           Dialogflow provides predefined system entities that\\n           can match many common types of data. For example,\\n           there are system entities for matching dates, times,\\n           colors, email addresses, and so on. You can also\\n           create your own custom entities for matching custom\\n           data. For example, you could define a vegetable\\n           entity that can match the types of vegetables\\n           available for purchase with a grocery store agent.\\n\\n           For more information, see the [Entity\\n           guide](\\\\ https://cloud.google.com/dialogflow/docs/entities-overview).',\n",
       " 'Call the complete query method over HTTP.\\n\\nArgs:\\n    request (~.completion_service.CompleteQueryRequest):\\n        The request object. Request message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.completion_service.CompleteQueryResponse:\\n        Response message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.',\n",
       " 'Deletes a single Subnet.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import edgenetwork_v1\\n\\n    def sample_delete_subnet():\\n        # Create a client\\n        client = edgenetwork_v1.EdgeNetworkClient()\\n\\n        # Initialize request argument(s)\\n        request = edgenetwork_v1.DeleteSubnetRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_subnet(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.edgenetwork_v1.types.DeleteSubnetRequest, dict]):\\n        The request object. Message for deleting a Subnet\\n    name (str):\\n        Required. Name of the resource\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'List OS policies compliance data for all Compute\\nEngine VM instances in the specified zone.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import osconfig_v1alpha\\n\\n    def sample_list_instance_os_policies_compliances():\\n        # Create a client\\n        client = osconfig_v1alpha.OsConfigZonalServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = osconfig_v1alpha.ListInstanceOSPoliciesCompliancesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_instance_os_policies_compliances(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.osconfig_v1alpha.types.ListInstanceOSPoliciesCompliancesRequest, dict]):\\n        The request object. A request message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n    parent (str):\\n        Required. The parent resource name.\\n\\n        Format: ``projects/{project}/locations/{location}``\\n\\n        For ``{project}``, either Compute Engine project-number\\n        or project-id can be provided.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.osconfig_v1alpha.services.os_config_zonal_service.pagers.ListInstanceOSPoliciesCompliancesPager:\\n        A response message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Return GeoTIFF metadata from first page as dict.',\n",
       " 'Call the list crypto keys method over HTTP.\\n\\nArgs:\\n    request (~.service.ListCryptoKeysRequest):\\n        The request object. Request message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.service.ListCryptoKeysResponse:\\n        Response message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].',\n",
       " 'Lists\\n[CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.security import privateca_v1\\n\\n    def sample_list_certificate_templates():\\n        # Create a client\\n        client = privateca_v1.CertificateAuthorityServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = privateca_v1.ListCertificateTemplatesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_certificate_templates(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.security.privateca_v1.types.ListCertificateTemplatesRequest, dict]):\\n        The request object. Request message for\\n        [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n    parent (str):\\n        Required. The resource name of the location associated\\n        with the\\n        [CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate],\\n        in the format ``projects/*/locations/*``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.security.privateca_v1.services.certificate_authority_service.pagers.ListCertificateTemplatesPager:\\n        Response message for\\n           [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Post-rpc interceptor for update_certificate_template\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'Deletes permanently all user events specified by the\\nfilter provided. Depending on the number of events\\nspecified by the filter, this operation could take hours\\nor days to complete. To test a filter, use the list\\ncommand first.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import retail_v2alpha\\n\\n    def sample_purge_user_events():\\n        # Create a client\\n        client = retail_v2alpha.UserEventServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = retail_v2alpha.PurgeUserEventsRequest(\\n            parent=\"parent_value\",\\n            filter=\"filter_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.purge_user_events(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.retail_v2alpha.types.PurgeUserEventsRequest, dict]):\\n        The request object. Request message for PurgeUserEvents\\n        method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.retail_v2alpha.types.PurgeUserEventsResponse` Response of the PurgeUserEventsRequest. If the long running operation is\\n           successfully done, then this message is returned by\\n           the google.longrunning.Operations.response field.',\n",
       " 'Return a callable for the pause job method over gRPC.\\n\\nPauses a job.\\n\\nIf a job is paused then the system will stop executing the job\\nuntil it is re-enabled via\\n[ResumeJob][google.cloud.scheduler.v1.CloudScheduler.ResumeJob].\\nThe state of the job is stored in\\n[state][google.cloud.scheduler.v1.Job.state]; if paused it will\\nbe set to\\n[Job.State.PAUSED][google.cloud.scheduler.v1.Job.State.PAUSED].\\nA job must be in\\n[Job.State.ENABLED][google.cloud.scheduler.v1.Job.State.ENABLED]\\nto be paused.\\n\\nReturns:\\n    Callable[[~.PauseJobRequest],\\n            ~.Job]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the delete series method over HTTP.\\n\\nArgs:\\n    request (~.streams_service.DeleteSeriesRequest):\\n        The request object. Message for deleting a Series.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return whether the supplied file name fn matches pattern filename.',\n",
       " 'Start the main event loop.\\n\\nThis method is called by `.FigureManagerBase.pyplot_show`, which is the\\nimplementation of `.pyplot.show`.  To customize the behavior of\\n`.pyplot.show`, interactive backends should usually override\\n`~.FigureManagerBase.start_main_loop`; if more customized logic is\\nnecessary, `~.FigureManagerBase.pyplot_show` can also be overridden.',\n",
       " 'Process every newly added tool.',\n",
       " \"Enable or disable IPython GUI event loop integration.\\n\\n%gui [GUINAME]\\n\\nThis magic replaces IPython's threaded shells that were activated\\nusing the (pylab/wthread/etc.) command line flags.  GUI toolkits\\ncan now be enabled at runtime and keyboard\\ninterrupts should work without any problems.  The following toolkits\\nare supported:  wxPython, PyQt4, PyGTK, Tk and Cocoa (OSX)::\\n\\n    %gui wx      # enable wxPython event loop integration\\n    %gui qt      # enable PyQt/PySide event loop integration\\n                 # with the latest version available.\\n    %gui qt6     # enable PyQt6/PySide6 event loop integration\\n    %gui qt5     # enable PyQt5/PySide2 event loop integration\\n    %gui gtk     # enable PyGTK event loop integration\\n    %gui gtk3    # enable Gtk3 event loop integration\\n    %gui gtk4    # enable Gtk4 event loop integration\\n    %gui tk      # enable Tk event loop integration\\n    %gui osx     # enable Cocoa event loop integration\\n                 # (requires %matplotlib 1.1)\\n    %gui         # disable all event loop integration\\n\\nWARNING:  after any of these has been called you can simply create\\nan application object, but DO NOT start the event loop yourself, as\\nwe have already handled that.\",\n",
       " 'Run the conda package manager within the current kernel.\\n\\nUsage:\\n  %micromamba install [pkgs]',\n",
       " 'Sagemath use custom prompt and we broke them in 8.19.',\n",
       " 'Create a table of color schemes.\\n\\nThe table can be created empty and manually filled or it can be\\ncreated with a list of valid color schemes AND the specification for\\nthe default active scheme.',\n",
       " 'Some functions use type vars that are not defined by the class, but rather\\nonly defined in the function. See for example `iter`. In those cases we\\nwant to:\\n\\n1. Search for undefined type vars.\\n2. Infer type vars with the execution state we have.\\n3. Return the union of all type vars that have been found.',\n",
       " 'Provided only as a backward-compatible wrapper around `.SSHConfig`.\\n\\n.. deprecated:: 2.7\\n    Use `SSHConfig.from_file` instead.',\n",
       " 'Binds the app context to the current context.',\n",
       " 'Return the corners of the rectangle, moving anti-clockwise from\\n(x0, y0).',\n",
       " 'Check that the animated artists changed in callbacks are updated.',\n",
       " \"Parameters\\n----------\\nnbins : int or 'auto', optional\\n    Number of ticks. Only used if minor is False.\\nminor : bool, default: False\\n    Indicate if this locator is for minor ticks or not.\",\n",
       " 'Set list of module names to try to load in forkserver process.',\n",
       " 'Convert the given BSON value into our own type.',\n",
       " 'Raise a BulkWriteError from the full bulk api result.',\n",
       " 'Discover python modules and packages in sub-directory.\\n\\nReturns iterator of paths to discovered modules and packages.',\n",
       " 'Node betweenness_centrality helper:\\n\\nSee betweenness_centrality for what you probably want.\\nThis actually computes \"load\" and not betweenness.\\nSee https://networkx.lanl.gov/ticket/103\\n\\nThis calculates the load of each node for paths from a single source.\\n(The fraction of number of shortests paths from source that go\\nthrough each node.)\\n\\nTo get the load for a node you need to do all-pairs shortest paths.\\n\\nIf weight is not None then use Dijkstra for finding shortest paths.',\n",
       " 'Tests for providing an alternate distance metric to the generator.',\n",
       " 'Write a graph `G` in GML format to the file or file handle `path`.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n    The graph to be converted to GML.\\n\\npath : filename or filehandle\\n    The filename or filehandle to write. Files whose names end with .gz or\\n    .bz2 will be compressed.\\n\\nstringizer : callable, optional\\n    A `stringizer` which converts non-int/non-float/non-dict values into\\n    strings. If it cannot convert a value into a string, it should raise a\\n    `ValueError` to indicate that. Default value: None.\\n\\nRaises\\n------\\nNetworkXError\\n    If `stringizer` cannot convert a value into a string, or the value to\\n    convert is not a string while `stringizer` is None.\\n\\nSee Also\\n--------\\nread_gml, generate_gml\\nliteral_stringizer\\n\\nNotes\\n-----\\nGraph attributes named \\'directed\\', \\'multigraph\\', \\'node\\' or\\n\\'edge\\', node attributes named \\'id\\' or \\'label\\', edge attributes\\nnamed \\'source\\' or \\'target\\' (or \\'key\\' if `G` is a multigraph)\\nare ignored because these attribute names are used to encode the graph\\nstructure.\\n\\nGML files are stored using a 7-bit ASCII encoding with any extended\\nASCII characters (iso8859-1) appearing as HTML character entities.\\nWithout specifying a `stringizer`/`destringizer`, the code is capable of\\nwriting `int`/`float`/`str`/`dict`/`list` data as required by the GML\\nspecification.  For writing other data types, and for reading data other\\nthan `str` you need to explicitly supply a `stringizer`/`destringizer`.\\n\\nNote that while we allow non-standard GML to be read from a file, we make\\nsure to write GML format. In particular, underscores are not allowed in\\nattribute names.\\nFor additional documentation on the GML file format, please see the\\n`GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.\\n\\nSee the module docstring :mod:`networkx.readwrite.gml` for more details.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(4)\\n>>> nx.write_gml(G, \"test.gml\")\\n\\nFilenames ending in .gz or .bz2 will be compressed.\\n\\n>>> nx.write_gml(G, \"test.gml.gz\")',\n",
       " \"Get information about the arguments accepted by a code object.\\n\\nThree things are returned: (args, varargs, varkw), where 'args' is\\na list of argument names (possibly containing nested lists), and\\n'varargs' and 'varkw' are the names of the * and ** arguments or None.\",\n",
       " 'validate multi targets that defined between parentheses()',\n",
       " 'Ensures that filename is opened with correct encoding parameter.\\n\\nThis function uses charset_normalizer package, when available, for\\ndetermining the encoding of the file to be opened. When charset_normalizer\\nis not available, the function detects only UTF encodings, otherwise, ASCII\\nencoding is used as fallback.',\n",
       " 'Coefficients should be modifiable ',\n",
       " \"Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.\\n\\nThis function returns the values:\\n\\n.. math:: p(a,b,c) = \\\\sum_{i,j,k} c_{i,j,k} * T_i(a) * T_j(b) * T_k(c)\\n\\nwhere the points ``(a, b, c)`` consist of all triples formed by taking\\n`a` from `x`, `b` from `y`, and `c` from `z`. The resulting points form\\na grid with `x` in the first dimension, `y` in the second, and `z` in\\nthe third.\\n\\nThe parameters `x`, `y`, and `z` are converted to arrays only if they\\nare tuples or a lists, otherwise they are treated as a scalars. In\\neither case, either `x`, `y`, and `z` or their elements must support\\nmultiplication and addition both with themselves and with the elements\\nof `c`.\\n\\nIf `c` has fewer than three dimensions, ones are implicitly appended to\\nits shape to make it 3-D. The shape of the result will be c.shape[3:] +\\nx.shape + y.shape + z.shape.\\n\\nParameters\\n----------\\nx, y, z : array_like, compatible objects\\n    The three dimensional series is evaluated at the points in the\\n    Cartesian product of `x`, `y`, and `z`.  If `x`, `y`, or `z` is a\\n    list or tuple, it is first converted to an ndarray, otherwise it is\\n    left unchanged and, if it isn't an ndarray, it is treated as a\\n    scalar.\\nc : array_like\\n    Array of coefficients ordered so that the coefficients for terms of\\n    degree i,j are contained in ``c[i,j]``. If `c` has dimension\\n    greater than two the remaining indices enumerate multiple sets of\\n    coefficients.\\n\\nReturns\\n-------\\nvalues : ndarray, compatible object\\n    The values of the two dimensional polynomial at points in the Cartesian\\n    product of `x` and `y`.\\n\\nSee Also\\n--------\\nchebval, chebval2d, chebgrid2d, chebval3d\\n\\nNotes\\n-----\",\n",
       " 'Forces the mask to soft',\n",
       " 'Initialize the RuntimeContext\\n\\nReturns:\\n    An instance of RuntimeContext.',\n",
       " 'Returns a `Tracer` for use by the given instrumentation library.\\n\\nThis function is a convenience wrapper for\\nopentelemetry.trace.TracerProvider.get_tracer.\\n\\nIf tracer_provider is omitted the current configured one is used.',\n",
       " 'Test `inject()` method for Format.BINARY.',\n",
       " 'Preloads asn1crypto and optionally oscrypto from a local source checkout,\\nor from a normal install\\n\\n:param require_oscrypto:\\n    A bool if oscrypto needs to be preloaded\\n\\n:param print_info:\\n    A bool if info about asn1crypto and oscrypto should be printed',\n",
       " 'Pairwise frames test_pairwise',\n",
       " 'Sends a Unix signal to the subprocess.\\n\\nUse constants from the :mod:`signal` module to specify which signal.',\n",
       " 'Return a comparison of actual and expected hash values.\\n\\nExample::\\n\\n       Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde\\n                    or 123451234512345123451234512345123451234512345\\n            Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef',\n",
       " 'Return True if `name` is a considered as an archive file.',\n",
       " 'Replace the password in a given url with ****.',\n",
       " 'Return all the distribution names known to this locator.',\n",
       " 'Recursive helper for :func:`_rec_strip`.',\n",
       " 'Test that Link.from_json() produces Links with consistent cache\\nlocations',\n",
       " 'Test deprecated default_value=None behavior for Container subclass traits',\n",
       " 'See 2.3.3 in RFC6979',\n",
       " \"Parse the input into a new type, deferring resolution of the type until the current class\\nis fully defined.\\n\\nThis is useful when you need to reference the class in it's own type annotations.\",\n",
       " \"Don't die on unary +.\",\n",
       " \"Called to perform the setup phase for a test item.\\n\\nThe default implementation runs ``setup()`` on ``item`` and all of its\\nparents (which haven't been setup yet). This includes obtaining the\\nvalues of fixtures required by the item (which haven't been obtained\\nyet).\\n\\n:param item:\\n    The item.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest file can implement this hook. For a given item, only conftest\\nfiles in the item's directory and its parent directories are consulted.\",\n",
       " 'Custom .cfg files with [tool:pytest] section are read correctly',\n",
       " '`pythonpath` kicks early enough to load plugins via -p (#11118).',\n",
       " 'Additional properties to set if ``sourceFormat`` is set to AVRO.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration.FIELDS.avro_options',\n",
       " 'Test whether a resource is enabled.  Known resources are set by\\nregrtest.py.',\n",
       " 'Saves the private key in PKCS#1 DER format.\\n\\n:returns: the DER-encoded private key.\\n:rtype: bytes',\n",
       " 'Compute minimum distances between one point and a set of points.\\n\\nThis function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance).\\n\\nThis is mostly equivalent to calling::\\n\\n    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\\n\\nbut uses much less memory, and is faster for large arrays.\\n\\nThis function works with dense 2D arrays only.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\\n    Array containing points.\\n\\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\\n    Arrays containing points.\\n\\naxis : int, default=1\\n    Axis along which the argmin and distances are to be computed.\\n\\nmetric : str or callable, default=\"euclidean\"\\n    Metric to use for distance computation. Any metric from scikit-learn\\n    or scipy.spatial.distance can be used.\\n\\n    If metric is a callable function, it is called on each\\n    pair of instances (rows) and the resulting value recorded. The callable\\n    should take two arrays as input and return one value indicating the\\n    distance between them. This works for Scipy\\'s metrics, but is less\\n    efficient than passing the metric name as a string.\\n\\n    Distance matrices are not supported.\\n\\n    Valid values for metric are:\\n\\n    - from scikit-learn: [\\'cityblock\\', \\'cosine\\', \\'euclidean\\', \\'l1\\', \\'l2\\',\\n      \\'manhattan\\']\\n\\n    - from scipy.spatial.distance: [\\'braycurtis\\', \\'canberra\\', \\'chebyshev\\',\\n      \\'correlation\\', \\'dice\\', \\'hamming\\', \\'jaccard\\', \\'kulsinski\\',\\n      \\'mahalanobis\\', \\'minkowski\\', \\'rogerstanimoto\\', \\'russellrao\\',\\n      \\'seuclidean\\', \\'sokalmichener\\', \\'sokalsneath\\', \\'sqeuclidean\\',\\n      \\'yule\\']\\n\\n    See the documentation for scipy.spatial.distance for details on these\\n    metrics.\\n\\n    .. note::\\n       `\\'kulsinski\\'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n    .. note::\\n       `\\'matching\\'` has been removed in SciPy 1.9 (use `\\'hamming\\'` instead).\\n\\nmetric_kwargs : dict, default=None\\n    Keyword arguments to pass to specified metric function.\\n\\nReturns\\n-------\\nargmin : numpy.ndarray\\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\\n\\nSee Also\\n--------\\npairwise_distances : Distances between every pair of samples of X and Y.\\npairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\\n    returns the distances.\\n\\nExamples\\n--------\\n>>> from sklearn.metrics.pairwise import pairwise_distances_argmin\\n>>> X = [[0, 0, 0], [1, 1, 1]]\\n>>> Y = [[1, 0, 0], [1, 1, 0]]\\n>>> pairwise_distances_argmin(X, Y)\\narray([0, 1])',\n",
       " 'Internal: Align multiline celltext text to bottom',\n",
       " 'delete_collection_namespaced_deployment  # noqa: E501\\n\\ndelete collection of Deployment  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_deployment_with_http_info(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'list_storage_version_migration  # noqa: E501\\n\\nlist or watch objects of kind StorageVersionMigration  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.list_storage_version_migration(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param bool allow_watch_bookmarks: allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server\\'s discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1alpha1StorageVersionMigrationList\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'override if setattr should do something other than call self.set',\n",
       " 'Sets the _continue of this V1ListMeta.\\n\\ncontinue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message.  # noqa: E501\\n\\n:param _continue: The _continue of this V1ListMeta.  # noqa: E501\\n:type: str',\n",
       " 'Returns true if both objects are not equal',\n",
       " \"Gets the max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n\\nMaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | |  P P  |  P P  |   P   | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed.  # noqa: E501\\n\\n:return: The max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n:rtype: int\",\n",
       " 'Append SCORE_FIELD and SCORE.',\n",
       " \"Return a :class:`~sagemaker.amazon.LinearLearnerModel`.\\n\\nIt references the latest s3 model data produced by this Estimator.\\n\\nArgs:\\n    vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on\\n        the model. Default: use subnets and security groups from this Estimator.\\n        * 'Subnets' (list[str]): List of subnet ids.\\n        * 'SecurityGroupIds' (list[str]): List of security group ids.\\n    **kwargs: Additional kwargs passed to the LinearLearnerModel constructor.\",\n",
       " 'Determines if the ``ast.Call`` node points to an ``ast.Name`` node with a matching name.\\n\\nArgs:\\n    node (ast.Call): a node that represents a function call. For more,\\n        see https://docs.python.org/3/library/ast.html#abstract-grammar.\\n    name (str): the function name.\\n\\nReturns:\\n    bool: if ``node.func`` is an ``ast.Name`` node with a matching name.',\n",
       " 'Construct a dictionary based on the attributes.\\n\\nReturns:\\n    dict represents the attributes.',\n",
       " 'Initialize source algorithm object\\n\\nArgs:\\n    algorithm_name (str): The ARN of an algorithm resource that was used to create the model package.\\n    model_data_url (str, optional): The Amazon S3 path where the model artifacts, which result from model training, are stored (default: None).',\n",
       " 'Initialize a TrainingDetails object.\\n\\nArgs:\\n    objective_function (ObjectiveFunction, optional): The objective function that is optimized during training (default: None).\\n    training_observations (str, optional): Any observations about training (default: None).\\n    training_job_details (TrainingJobDetails, optional): Details about any associated training jobs (default: None).',\n",
       " 'Generates indices to split data into training and test set.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    Training data, where `n_samples` is the number of samples\\n    and `n_features` is the number of features.\\n\\ny : array-like of shape (n_samples,)\\n    The target variable for supervised learning problems.\\n\\ngroups : array-like of shape (n_samples,), default=None\\n    Group labels for the samples used while splitting the dataset into\\n    train/test set.\\n\\nYields\\n------\\ntrain : ndarray\\n    The training set indices for that split.\\n\\ntest : ndarray\\n    The testing set indices for that split.',\n",
       " 'Get feature names from X.\\n\\nSupport for other array containers should place its implementation here.\\n\\nParameters\\n----------\\nX : {ndarray, dataframe} of shape (n_samples, n_features)\\n    Array container to extract feature names.\\n\\n    - pandas dataframe : The columns will be considered to be feature\\n      names. If the dataframe contains non-string feature names, `None` is\\n      returned.\\n    - All other array containers will return `None`.\\n\\nReturns\\n-------\\nnames: ndarray or None\\n    Feature names of `X`. Unrecognized array containers will return `None`.',\n",
       " \"Add a new knot.\\n\\n(Approximately) replicate FITPACK's logic:\\n  1. split the `x` array into knot intervals, ``t(j+k) <= x(i) <= t(j+k+1)``\\n  2. find the interval with the maximum sum of residuals\\n  3. insert a new knot into the middle of that interval.\\n\\nNB: a new knot is in fact an `x` value at the middle of the interval.\\nSo *the knots are a subset of `x`*.\\n\\nThis routine is an analog of\\nhttps://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpcurf.f#L190-L215\\n(cf _split function)\\n\\nand https://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpknot.f\",\n",
       " 'Encode bson.objectid.ObjectId.',\n",
       " 'Sets the value of :py:attr:`labelType`.',\n",
       " 'Returns R^2^, the coefficient of determination.',\n",
       " 'Returns a rotation matrix for a rotation of theta (in radians)\\nabout the 3-axis.\\n\\nExplanation\\n===========\\n\\nFor a right-handed coordinate system, this corresponds to a\\nclockwise rotation around the `z`-axis, given by:\\n\\n.. math::\\n\\n    R  = \\\\begin{bmatrix}\\n             \\\\cos(\\\\theta) & \\\\sin(\\\\theta) & 0 \\\\\\\\\\n            -\\\\sin(\\\\theta) & \\\\cos(\\\\theta) & 0 \\\\\\\\\\n                        0 &            0 & 1\\n        \\\\end{bmatrix}\\n\\nExamples\\n========\\n\\n>>> from sympy import pi, rot_axis3\\n\\nA rotation of pi/3 (60 degrees):\\n\\n>>> theta = pi/3\\n>>> rot_axis3(theta)\\nMatrix([\\n[       1/2, sqrt(3)/2, 0],\\n[-sqrt(3)/2,       1/2, 0],\\n[         0,         0, 1]])\\n\\nIf we rotate by pi/2 (90 degrees):\\n\\n>>> rot_axis3(pi/2)\\nMatrix([\\n[ 0, 1, 0],\\n[-1, 0, 0],\\n[ 0, 0, 1]])\\n\\nSee Also\\n========\\n\\nrot_givens: Returns a Givens rotation matrix (generalized rotation for\\n    any number of dimensions)\\nrot_ccw_axis3: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 3-axis (counterclockwise around the z axis)\\nrot_axis1: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 1-axis (clockwise around the x axis)\\nrot_axis2: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 2-axis (clockwise around the y axis)',\n",
       " 'Apply \"render derived\" to this :class:`_sql.TableValuedAlias`.\\n\\nThis has the effect of the individual column names listed out\\nafter the alias name in the \"AS\" sequence, e.g.:\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.unnest(array([\"one\", \"two\", \"three\"])).\\n                table_valued(\"x\", with_ordinality=\"o\").render_derived()\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.x, anon_1.o\\n    FROM unnest(ARRAY[%(param_1)s, %(param_2)s, %(param_3)s]) WITH ORDINALITY AS anon_1(x, o)\\n\\nThe ``with_types`` keyword will render column types inline within\\nthe alias expression (this syntax currently applies to the\\nPostgreSQL database):\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.json_to_recordset(\\n    ...             \\'[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]\\'\\n    ...         )\\n    ...         .table_valued(column(\"a\", Integer), column(\"b\", String))\\n    ...         .render_derived(with_types=True)\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.a, anon_1.b FROM json_to_recordset(:json_to_recordset_1)\\n    AS anon_1(a INTEGER, b VARCHAR)\\n\\n:param name: optional string name that will be applied to the alias\\n generated.  If left as None, a unique anonymizing name will be used.\\n\\n:param with_types: if True, the derived columns will include the\\n datatype specification with each column. This is a special syntax\\n currently known to be required by PostgreSQL for some SQL functions.',\n",
       " 'Matches any differential equation that nth_algebraic can solve. Uses\\n`sympy.solve` but teaches it how to integrate derivatives.\\n\\nThis involves calling `sympy.solve` and does most of the work of finding a\\nsolution (apart from evaluating the integrals).',\n",
       " \"Create a string item.\\n\\nBy default, this function will create *single line basic* strings, but\\nboolean flags (e.g. ``literal=True`` and/or ``multiline=True``)\\ncan be used for personalization.\\n\\nFor more information, please check the spec: `<https://toml.io/en/v1.0.0#string>`__.\\n\\nCommon escaping rules will be applied for basic strings.\\nThis can be controlled by explicitly setting ``escape=False``.\\nPlease note that, if you disable escaping, you will have to make sure that\\nthe given strings don't contain any forbidden character or sequence.\",\n",
       " 'Return a decorated class with a constructor signature that contain Trait names as kwargs.',\n",
       " 'Figures out the full host name for the given domain part.  The\\ndomain part is a subdomain in case host matching is disabled or\\na full host name.',\n",
       " 'Check if the mimetype indicates JSON data, either\\n:mimetype:`application/json` or :mimetype:`application/*+json`.',\n",
       " 'Sets ``Content-Disposition`` header.',\n",
       " 'Determines if the labels in a domain are a match for labels from a\\nwildcard valid domain name\\n\\n:param domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in the domain name to check\\n\\n:param valid_domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in a wildcard domain pattern\\n\\n:return:\\n    A boolean - if the domain matches the valid domain',\n",
       " 'Indicates whether the :class:`Arrow <arrow.arrow.Arrow>` object is a repeated wall time in the current\\ntimezone.',\n",
       " ':return:\\n    Integer',\n",
       " 'Upload Part.',\n",
       " 'Parse the .c file written by pgen.  (Internal)\\n\\nThe file looks as follows.  The first two lines are always this:\\n\\n#include \"pgenheaders.h\"\\n#include \"grammar.h\"\\n\\nAfter that come four blocks:\\n\\n1) one or more state definitions\\n2) a table defining dfas\\n3) a table defining labels\\n4) a struct defining the grammar\\n\\nA state definition has the following form:\\n- one or more arc arrays, each of the form:\\n  static arc arcs_<n>_<m>[<k>] = {\\n          {<i>, <j>},\\n          ...\\n  };\\n- followed by a state array, of the form:\\n  static state states_<s>[<t>] = {\\n          {<k>, arcs_<n>_<m>},\\n          ...\\n  };',\n",
       " \"Check if a stream's encoding and errors attributes are\\ncompatible with the desired values.\",\n",
       " 'Alias for :meth:`main`.',\n",
       " 'Returns the sample value, or None if not found.\\n\\nThis is inefficient, and intended only for use in unittests.',\n",
       " 'Run a benchmarking experiment and print a table of results.\\n\\nArguments:\\n\\n    py_versions: The Python versions to test.\\n    cov_versions: The coverage versions to test.\\n    projects: The projects to run.\\n    rows: A list of strings chosen from `\"pyver\"`, `\"cov\"`, and `\"proj\"`.\\n    column: The remaining dimension not used in `rows`.\\n    ratios: A list of triples: (title, slug1, slug2).\\n    num_runs: The number of times to run each matrix element.',\n",
       " 'Import modules randomly to stress coverage.',\n",
       " 'Yields a sequence of (PyObjectPtr key, PyObjectPtr value) pairs,\\nanalogous to dict.iteritems()',\n",
       " 'Main entry point to process a list of .py files and inject type inferred declarations.',\n",
       " '>>> in_lambda_in_list_comprehension1()\\n[[0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6]]',\n",
       " 'Build an EDNS option object from wire format.\\n\\n*otype*, an ``int``, is the option type.\\n\\n*parser*, a ``dns.wire.Parser``, the parser, which should be\\nrestricted to the option length.\\n\\nReturns an instance of a subclass of ``dns.edns.Option``.',\n",
       " 'Add the specified rdata to the rdataset.\\n\\nIf the optional *ttl* parameter is supplied, then\\n``self.update_ttl(ttl)`` will be called prior to adding the rdata.\\n\\n*rd*, a ``dns.rdata.Rdata``, the rdata\\n\\n*ttl*, an ``int``, the TTL.\\n\\nRaises ``dns.rdataset.IncompatibleTypes`` if the type and class\\ndo not match the type and class of the rdataset.\\n\\nRaises ``dns.rdataset.DifferingCovers`` if the type is a signature\\ntype and the covered type does not match that of the rdataset.',\n",
       " 'Remove a container. Similar to the ``docker rm`` command.\\n\\nArgs:\\n    container (str): The container to remove\\n    v (bool): Remove the volumes associated with the container\\n    link (bool): Remove the specified link and not the underlying\\n        container\\n    force (bool): Force the removal of a running container (uses\\n        ``SIGKILL``)\\n\\nRaises:\\n    :py:class:`docker.errors.APIError`\\n        If the server returns an error.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsFirewallPolicyRequest):\\n        The request object. A request message for\\n    FirewallPolicies.TestIamPermissions. See\\n    the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'Pre-rpc interceptor for list_terraform_versions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Config server.',\n",
       " 'Gets a previously created question.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataqna_v1alpha\\n\\n    def sample_get_question():\\n        # Create a client\\n        client = dataqna_v1alpha.QuestionServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = dataqna_v1alpha.GetQuestionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_question(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataqna_v1alpha.types.GetQuestionRequest, dict]):\\n        The request object. A request to get a previously created\\n        question.\\n    name (str):\\n        Required. The unique identifier for the question.\\n        Example: ``projects/foo/locations/bar/questions/1234``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataqna_v1alpha.types.Question:\\n        The question resource represents a\\n        natural language query, its settings,\\n        understanding generated by the system,\\n        and answer retrieval status. A question\\n        cannot be modified.',\n",
       " 'Call the get job metrics method over HTTP.\\n\\nArgs:\\n    request (~.metrics.GetJobMetricsRequest):\\n        The request object. Request to get job metrics.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.metrics.JobMetrics:\\n        JobMetrics contains a collection of\\n    metrics describing the detailed progress\\n    of a Dataflow job. Metrics correspond to\\n    user-defined and system-defined metrics\\n    in the job.\\n\\n    This resource captures only the most\\n    recent values of each metric;\\n    time-series data can be queried for them\\n    (under the same metric names) from Cloud\\n    Monitoring.',\n",
       " 'Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.metastore.CreateServiceRequest):\\n        The request object. Request message for\\n    [DataprocMetastore.CreateService][google.cloud.metastore.v1.DataprocMetastore.CreateService].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Pre-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CloudDeploy server.',\n",
       " 'Post-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CloudDeploy server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the get validation result method over gRPC.\\n\\nGets agent validation result. Agent validation is\\nperformed during training time and is updated\\nautomatically when training is completed.\\n\\nReturns:\\n    Callable[[~.GetValidationResultRequest],\\n            ~.ValidationResult]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates a\\n[SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1alpha\\n\\n    def sample_create_sample_query():\\n        # Create a client\\n        client = discoveryengine_v1alpha.SampleQueryServiceClient()\\n\\n        # Initialize request argument(s)\\n        sample_query = discoveryengine_v1alpha.SampleQuery()\\n        sample_query.query_entry.query = \"query_value\"\\n\\n        request = discoveryengine_v1alpha.CreateSampleQueryRequest(\\n            parent=\"parent_value\",\\n            sample_query=sample_query,\\n            sample_query_id=\"sample_query_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_sample_query(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1alpha.types.CreateSampleQueryRequest, dict]):\\n        The request object. Request message for\\n        [SampleQueryService.CreateSampleQuery][google.cloud.discoveryengine.v1alpha.SampleQueryService.CreateSampleQuery]\\n        method.\\n    parent (str):\\n        Required. The parent resource name, such as\\n        ``projects/{project}/locations/{location}/sampleQuerySets/{sampleQuerySet}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query (google.cloud.discoveryengine_v1alpha.types.SampleQuery):\\n        Required. The\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n        to create.\\n\\n        This corresponds to the ``sample_query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query_id (str):\\n        Required. The ID to use for the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        which will become the final component of the\\n        [SampleQuery.name][google.cloud.discoveryengine.v1alpha.SampleQuery.name].\\n\\n        If the caller does not have permission to create the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        regardless of whether or not it exists, a\\n        ``PERMISSION_DENIED`` error is returned.\\n\\n        This field must be unique among all\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]s\\n        with the same\\n        [parent][google.cloud.discoveryengine.v1alpha.CreateSampleQueryRequest.parent].\\n        Otherwise, an ``ALREADY_EXISTS`` error is returned.\\n\\n        This field must conform to\\n        `RFC-1034 <https://tools.ietf.org/html/rfc1034>`__\\n        standard with a length limit of 63 characters.\\n        Otherwise, an ``INVALID_ARGUMENT`` error is returned.\\n\\n        This corresponds to the ``sample_query_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.discoveryengine_v1alpha.types.SampleQuery:\\n        Sample Query captures metadata to be\\n        used for evaluation.',\n",
       " 'Pre-rpc interceptor for create_discovery_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DlpService server.',\n",
       " 'Updates metadata associated with a dataset. Note that this\\nmethod requires the\\n``documentai.googleapis.com/datasets.update`` permission on the\\nproject, which is highly privileged. A user or service account\\nwith this permission can create new processors that can interact\\nwith any gcs bucket in your project.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_update_dataset():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        dataset = documentai_v1beta3.Dataset()\\n        dataset.state = \"INITIALIZED\"\\n\\n        request = documentai_v1beta3.UpdateDatasetRequest(\\n            dataset=dataset,\\n        )\\n\\n        # Make the request\\n        operation = client.update_dataset(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.UpdateDatasetRequest, dict]):\\n        The request object.\\n    dataset (google.cloud.documentai_v1beta3.types.Dataset):\\n        Required. The ``name`` field of the ``Dataset`` is used\\n        to identify the resource to be updated.\\n\\n        This corresponds to the ``dataset`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        The update mask applies to the\\n        resource.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.documentai_v1beta3.types.Dataset` A singleton resource under a\\n           [Processor][google.cloud.documentai.v1beta3.Processor]\\n           which configures a collection of documents.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ManagedIdentitiesServiceAsyncClient: The constructed client.',\n",
       " 'Lists the clusters in a given project and location.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import managedkafka_v1\\n\\n    def sample_list_clusters():\\n        # Create a client\\n        client = managedkafka_v1.ManagedKafkaClient()\\n\\n        # Initialize request argument(s)\\n        request = managedkafka_v1.ListClustersRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_clusters(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.managedkafka_v1.types.ListClustersRequest, dict]):\\n        The request object. Request for ListClusters.\\n    parent (str):\\n        Required. The parent location whose clusters are to be\\n        listed. Structured like\\n        ``projects/{project}/locations/{location}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.managedkafka_v1.services.managed_kafka.pagers.ListClustersPager:\\n        Response for ListClusters.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Creates a new GrpcRoute in a given project and\\nlocation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import network_services_v1\\n\\n    def sample_create_grpc_route():\\n        # Create a client\\n        client = network_services_v1.NetworkServicesClient()\\n\\n        # Initialize request argument(s)\\n        grpc_route = network_services_v1.GrpcRoute()\\n        grpc_route.name = \"name_value\"\\n        grpc_route.hostnames = [\\'hostnames_value1\\', \\'hostnames_value2\\']\\n\\n        request = network_services_v1.CreateGrpcRouteRequest(\\n            parent=\"parent_value\",\\n            grpc_route_id=\"grpc_route_id_value\",\\n            grpc_route=grpc_route,\\n        )\\n\\n        # Make the request\\n        operation = client.create_grpc_route(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.network_services_v1.types.CreateGrpcRouteRequest, dict]):\\n        The request object. Request used by the CreateGrpcRoute\\n        method.\\n    parent (str):\\n        Required. The parent resource of the GrpcRoute. Must be\\n        in the format ``projects/*/locations/global``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route (google.cloud.network_services_v1.types.GrpcRoute):\\n        Required. GrpcRoute resource to be\\n        created.\\n\\n        This corresponds to the ``grpc_route`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route_id (str):\\n        Required. Short name of the GrpcRoute\\n        resource to be created.\\n\\n        This corresponds to the ``grpc_route_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.network_services_v1.types.GrpcRoute` GrpcRoute is the resource defining how gRPC traffic routed by a Mesh\\n           or Gateway resource is routed.',\n",
       " 'Quantize the bitmap to make it 8-bit (paletted). Returns a new\\nFIBitmap object.\\nOnly for 24 bit images.',\n",
       " 'Call the list models method over HTTP.\\n\\nArgs:\\n    request (~.model_service.ListModelsRequest):\\n        The request object. Request for listing models associated\\n    with a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.model_service.ListModelsResponse:\\n        Response to a ListModelRequest.',\n",
       " 'Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Performs bidirectional streaming speech synthesis:\\nreceive audio while sending text.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import texttospeech_v1\\n\\n    async def sample_streaming_synthesize():\\n        # Create a client\\n        client = texttospeech_v1.TextToSpeechAsyncClient()\\n\\n        # Initialize request argument(s)\\n        streaming_config = texttospeech_v1.StreamingSynthesizeConfig()\\n        streaming_config.voice.language_code = \"language_code_value\"\\n\\n        request = texttospeech_v1.StreamingSynthesizeRequest(\\n            streaming_config=streaming_config,\\n        )\\n\\n        # This method expects an iterator which contains\\n        # \\'texttospeech_v1.StreamingSynthesizeRequest\\' objects\\n        # Here we create a generator that yields a single `request` for\\n        # demonstrative purposes.\\n        requests = [request]\\n\\n        def request_generator():\\n            for request in requests:\\n                yield request\\n\\n        # Make the request\\n        stream = await client.streaming_synthesize(requests=request_generator())\\n\\n        # Handle the response\\n        async for response in stream:\\n            print(response)\\n\\nArgs:\\n    requests (AsyncIterator[`google.cloud.texttospeech_v1.types.StreamingSynthesizeRequest`]):\\n        The request object AsyncIterator. Request message for the ``StreamingSynthesize`` method.\\n        Multiple ``StreamingSynthesizeRequest`` messages are\\n        sent in one call. The first message must contain a\\n        ``streaming_config`` that fully specifies the request\\n        configuration and must not contain ``input``. All\\n        subsequent messages must only have ``input`` set.\\n    retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    AsyncIterable[google.cloud.texttospeech_v1.types.StreamingSynthesizeResponse]:\\n        StreamingSynthesizeResponse is the only message returned to the\\n           client by StreamingSynthesize method. A series of\\n           zero or more StreamingSynthesizeResponse messages are\\n           streamed back to the client.',\n",
       " 'Pre-rpc interceptor for get_connector\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VpcAccessService server.',\n",
       " 'Returns `True` for 4xx status codes, `False` otherwise.',\n",
       " 'Print the docstring for an object.\\n\\nIf the given object is a class, it will print both the class and the\\nconstructor docstrings.',\n",
       " 'Idle, the editor bundled with python\\n\\nParameters\\n----------\\nexe : str, None\\n    If none, should be pretty smart about finding the executable.',\n",
       " 'Tests for definition_start_position and definition_end_position',\n",
       " \"Configure the null keyring as the default.\\n\\n>>> fs = getfixture('fs')\\n>>> disable()\\n>>> disable()\\nTraceback (most recent call last):\\n...\\nRuntimeError: Refusing to overwrite...\",\n",
       " 'Parameters\\n----------\\nxy : (float, float)\\n  The lower left corner of the box.\\n\\nwidth : float\\n    The width of the box.\\n\\nheight : float\\n    The height of the box.\\n\\nboxstyle : str or `~matplotlib.patches.BoxStyle`\\n    The style of the fancy box. This can either be a `.BoxStyle`\\n    instance or a string of the style name and optionally comma\\n    separated attributes (e.g. \"Round, pad=0.2\"). This string is\\n    passed to `.BoxStyle` to construct a `.BoxStyle` object. See\\n    there for a full documentation.\\n\\n    The following box styles are available:\\n\\n    %(BoxStyle:table)s\\n\\nmutation_scale : float, default: 1\\n    Scaling factor applied to the attributes of the box style\\n    (e.g. pad or rounding_size).\\n\\nmutation_aspect : float, default: 1\\n    The height of the rectangle will be squeezed by this value before\\n    the mutation and the mutated box will be stretched by the inverse\\n    of it. For example, this allows different horizontal and vertical\\n    padding.\\n\\nOther Parameters\\n----------------\\n**kwargs : `~matplotlib.patches.Patch` properties\\n\\n%(Patch:kwdoc)s',\n",
       " 'Test constrained_layout for nested gridspecs',\n",
       " 'Parameters\\n----------\\nh : list of :mod:`~mpl_toolkits.axes_grid1.axes_size`\\n    sizes for horizontal division',\n",
       " 'Parameters\\n----------\\nresult : np.ndarray\\nfill_value : object, default iNaT\\nconvert : str, dtype or None\\n\\nReturns\\n-------\\nresult : ndarray with values replace by the fill_value\\n\\nmask the result if needed, convert to the provided dtype if its not\\nNone\\n\\nThis is an internal routine.',\n",
       " 'Reset the option store to its initial state\\n\\nReturns\\n-------\\nNone',\n",
       " 'Create a new, parsed `SSHConfig` from the file found at ``path``.\\n\\n.. versionadded:: 2.7',\n",
       " 'Blocking expect',\n",
       " \"Recompute this distribution's dependencies.\",\n",
       " 'Test using global distutils options.\\n(In particular those that disable the actual install action)',\n",
       " 'Create a new hash object.\\n\\n:parameter data:\\n    Optional. The very first chunk of the message to hash.\\n    It is equivalent to an early call to :meth:`SHA224Hash.update`.\\n:type data: byte string/byte array/memoryview\\n\\n:Return: A :class:`SHA224Hash` hash object',\n",
       " ':calls: `GET /users/{user}/events/orgs/{org} <http://docs.github.com/en/rest/reference/activity#events>`_',\n",
       " '(Experimental) Rate limit for GraphQL API, use with caution.',\n",
       " 'Registers a new Algorithm for use when creating and verifying tokens.',\n",
       " 'Transforms a yes/no or stringified bool into a bool.',\n",
       " 'Set an attribute.',\n",
       " 'This is a Sphinx docstring.\\n\\n:raises NameError: Never',\n",
       " 'Depends on `undefined1` in function return annotation. ',\n",
       " 'The value of `OpenSSL.SSL.OP_NO_QUERY_MTU` is 0x1000, the value\\nof `SSL_OP_NO_QUERY_MTU` defined by `openssl/ssl.h`.',\n",
       " \"The 'uses no fixture' error tells the user at collection time\\nthat the parametrize data they've set up doesn't correspond to the\\nfixtures in their test function, rather than silently ignoring this\\nand letting the test potentially pass.\\n\\n#714\",\n",
       " '#3498',\n",
       " 'Call the update topic method over HTTP.\\n\\nArgs:\\n    request (~.pubsub.UpdateTopicRequest):\\n        The request object. Request for the UpdateTopic method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.pubsub.Topic:\\n        A topic resource.',\n",
       " 'Mode of the Wishart distribution\\n\\nOnly valid if the degrees of freedom are greater than the dimension of\\nthe scale matrix.\\n\\nParameters\\n----------\\n%(_doc_default_callparams)s\\n\\nReturns\\n-------\\nmode : float or None\\n    The Mode of the distribution',\n",
       " 'Check for non-CSR input to private method `_silhouette_reduce`.',\n",
       " \"create_ip_address  # noqa: E501\\n\\ncreate an IPAddress  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.create_ip_address_with_http_info(body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param V1beta1IPAddress body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1beta1IPAddress, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Return new instance of configuration.\\n\\nThis method returns newly created, based on default constructor,\\nobject of Configuration class or returns a copy of default\\nconfiguration passed by the set_default method.\\n\\n:return: The configuration object.',\n",
       " 'Gets the rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n\\n\\n:return: The rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n:rtype: V1RollingUpdateDaemonSet',\n",
       " \"Sets the propagation_policy of this V1DeleteOptions.\\n\\nWhether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.  # noqa: E501\\n\\n:param propagation_policy: The propagation_policy of this V1DeleteOptions.  # noqa: E501\\n:type: str\",\n",
       " 'Returns true if both objects are equal',\n",
       " \"Returns the raw data of the ``event``'s latency spikes time series.\\n\\nFor more information see https://redis.io/commands/latency-history\",\n",
       " \"AWS4Auth instances can be created by supplying key scope parameters\\ndirectly or by using an AWS4SigningKey instance:\\n\\n>>> auth = AWS4Auth(access_id, secret_key, region, service\\n...                 [, date][, raise_invalid_date=False][, session_token=None])\\n\\n  or\\n\\n>>> auth = AWS4Auth(access_id, signing_key[, raise_invalid_date=False])\\n\\n  or using auto-refreshed STS temporary creds via botocore RefreshableCredentials\\n  (useful for long-running processes):\\n\\n>>> auth = AWS4Auth(refreshable_credentials=botocore.session.Session().get_credentials(),\\n...                 region='eu-west-1', service='es')\\n\\naccess_id   -- This is your AWS access ID\\nsecret_key  -- This is your AWS secret access key\\nregion      -- The region you're connecting to, as per the list at\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\\n               e.g. us-east-1. For services which don't require a region\\n               (e.g. IAM), use us-east-1.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\nservice     -- The name of the service you're connecting to, as per\\n               endpoints at:\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html\\n               e.g. elasticbeanstalk.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\ndate        -- Date this instance is valid for. 8-digit date as str of the\\n               form YYYYMMDD. Key is only valid for requests with a\\n               Date or X-Amz-Date header matching this date. If date is\\n               not supplied the current date is used.\\nsigning_key -- An AWS4SigningKey instance.\\nraise_invalid_date\\n            -- Must be supplied as keyword argument. AWS4Auth tries to\\n               parse a date from the X-Amz-Date and Date headers of the\\n               request, first trying X-Amz-Date, and then Date if\\n               X-Amz-Date is not present or is in an unrecognised\\n               format. If one or both of the two headers are present\\n               yet neither are in a format which AWS4Auth recognises\\n               then it will remove both headers and replace with a new\\n               X-Amz-Date header using the current date.\\n\\n               If this behaviour is not wanted, set the\\n               raise_invalid_date keyword argument to True, and\\n               instead an InvalidDateError will be raised when neither\\n               date is recognised. If neither header is present at all\\n               then an X-Amz-Date header will still be added containing\\n               the current date.\\n\\n               See the AWS4Auth class docstring for supported date\\n               formats.\\nsession_token\\n            -- Must be supplied as keyword argument. If session_token\\n               is set, then it is used for the x-amz-security-token\\n               header, for use with STS temporary credentials.\\nrefreshable_credentials\\n            -- A botocore.credentials.RefreshableCredentials instance.\\n               Must be supplied as keyword argument. This instance is\\n               used to generate valid per-request static credentials,\\n               without needing to re-generate the AWS4Auth instance.                       \\n               If refreshable_credentials is set, the following arguments\\n               are ignored: access_id, secret_key, signing_key,\\n               session_token.\",\n",
       " 'Rename namespace ``session`` to ``inputs``.',\n",
       " 'Instantiates HubNotebookDocument object.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of hub content document.',\n",
       " 'Return a scalar result corresponding to the given\\ncolumn expression.',\n",
       " 'Initializes a ConstraintViolations object from a file path.\\n\\nArgs:\\n    constraint_violations_file_path (str): The path to the constraint violations file.\\n    kms_key (str): The kms_key to use when encrypting the file in S3.\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions (default: None). If not\\n        specified, one is created using the default AWS configuration\\n        chain.\\n\\nReturns:\\n    sagemaker.model_monitor.ConstraintViolations: The instance of ConstraintViolations\\n        generated from the local file path.',\n",
       " \"Save this hyperparameter to be applied to the graph_manager object when\\nit's ready.\",\n",
       " 'Iterate over the points in the grid.\\n\\nReturns\\n-------\\nparams : iterator over dict of str to any\\n    Yields dictionaries mapping each estimator parameter to one of its\\n    allowed values.',\n",
       " \"Find root of a function within an interval using bisection.\\n\\nBasic bisection routine to find a root of the function `f` between the\\narguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\\nSlow but sure.\\n\\nParameters\\n----------\\nf : function\\n    Python function returning a number.  `f` must be continuous, and\\n    f(a) and f(b) must have opposite signs.\\na : scalar\\n    One end of the bracketing interval [a,b].\\nb : scalar\\n    The other end of the bracketing interval [a,b].\\nxtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter must be positive.\\nrtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter cannot be smaller than its default value of\\n    ``4*np.finfo(float).eps``.\\nmaxiter : int, optional\\n    If convergence is not achieved in `maxiter` iterations, an error is\\n    raised. Must be >= 0.\\nargs : tuple, optional\\n    Containing extra arguments for the function `f`.\\n    `f` is called by ``apply(f, (x)+args)``.\\nfull_output : bool, optional\\n    If `full_output` is False, the root is returned. If `full_output` is\\n    True, the return value is ``(x, r)``, where x is the root, and r is\\n    a `RootResults` object.\\ndisp : bool, optional\\n    If True, raise RuntimeError if the algorithm didn't converge.\\n    Otherwise, the convergence status is recorded in a `RootResults`\\n    return object.\\n\\nReturns\\n-------\\nroot : float\\n    Root of `f` between `a` and `b`.\\nr : `RootResults` (present if ``full_output = True``)\\n    Object containing information about the convergence. In particular,\\n    ``r.converged`` is True if the routine converged.\\n\\nExamples\\n--------\\n\\n>>> def f(x):\\n...     return (x**2 - 1)\\n\\n>>> from scipy import optimize\\n\\n>>> root = optimize.bisect(f, 0, 2)\\n>>> root\\n1.0\\n\\n>>> root = optimize.bisect(f, -2, 0)\\n>>> root\\n-1.0\\n\\nSee Also\\n--------\\nbrentq, brenth, bisect, newton\\nfixed_point : scalar fixed-point finder\\nfsolve : n-dimensional root-finding\",\n",
       " 'Checks whether a matrix contains only independent rows of another',\n",
       " \"Compare to output from 'qvoronoi o Fv < data' to Voronoi()\",\n",
       " 'Return True if a Geometry is prepared.\\n\\nNote that it is not necessary to check if a geometry is already prepared\\nbefore preparing it. It is more efficient to call ``prepare`` directly\\nbecause it will skip geometries that are already prepared.\\n\\nThis function will return False for missing geometries (None).\\n\\nParameters\\n----------\\ngeometry : Geometry or array_like\\n    Geometry or geometries to check.\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\nis_valid_input : check if an object is a geometry or None\\nprepare : prepare a geometry\\n\\nExamples\\n--------\\n>>> from shapely import Point, prepare\\n>>> geometry = Point(0, 0)\\n>>> is_prepared(Point(0, 0))\\nFalse\\n>>> prepare(geometry)\\n>>> is_prepared(geometry)\\nTrue\\n>>> is_prepared(None)\\nFalse',\n",
       " 'Is prerelease.',\n",
       " 'Trim values at input threshold(s).\\n\\nAssigns values outside boundary-to-boundary values.\\n\\nParameters\\n----------\\nlower : float or int, default None\\n    Minimum threshold value. All values below this threshold will be set to it.\\nupper : float or int, default None\\n    Maximum threshold value. All values above this threshold will be set to it.\\n\\nReturns\\n-------\\nDataFrame\\n    DataFrame with the values outside the clip boundaries replaced.\\n\\nExamples\\n--------\\n>>> ps.DataFrame({\\'A\\': [0, 2, 4]}).clip(1, 3)\\n   A\\n0  1\\n1  2\\n2  3\\n\\nNotes\\n-----\\nOne difference between this implementation and pandas is that running\\npd.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1) will crash with \"TypeError: \\'<=\\' not supported\\nbetween instances of \\'str\\' and \\'int\\'\" while ps.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1)\\nwill output the original DataFrame, simply ignoring the incompatible types.',\n",
       " 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\nfrom the uniform distribution U(0.0, 1.0).\\n\\n.. versionadded:: 1.1.0\\n\\nParameters\\n----------\\nsc : :py:class:`pyspark.SparkContext`\\n    SparkContext used to create the RDD.\\nnumRows : int\\n    Number of Vectors in the RDD.\\nnumCols : int\\n    Number of elements in each Vector.\\nnumPartitions : int, optional\\n    Number of partitions in the RDD.\\nseed : int, optional\\n    Seed for the RNG that generates the seed for the generator in each partition.\\n\\nReturns\\n-------\\n:py:class:`pyspark.RDD`\\n    RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n>>> mat.shape\\n(10, 10)\\n>>> bool(mat.max() <= 1.0 and mat.min() >= 0.0)\\nTrue\\n>>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n4',\n",
       " \"Return ``eq`` with non-commutative objects replaced with Dummy\\nsymbols. A dictionary that can be used to restore the original\\nvalues is returned: if it is None, the expression is noncommutative\\nand cannot be made commutative. The third value returned is a list\\nof any non-commutative symbols that appear in the returned equation.\\n\\nExplanation\\n===========\\n\\nAll non-commutative objects other than Symbols are replaced with\\na non-commutative Symbol. Identical objects will be identified\\nby identical symbols.\\n\\nIf there is only 1 non-commutative object in an expression it will\\nbe replaced with a commutative symbol. Otherwise, the non-commutative\\nentities are retained and the calling routine should handle\\nreplacements in this case since some care must be taken to keep\\ntrack of the ordering of symbols when they occur within Muls.\\n\\nParameters\\n==========\\n\\nname : str\\n    ``name``, if given, is the name that will be used with numbered Dummy\\n    variables that will replace the non-commutative objects and is mainly\\n    used for doctesting purposes.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.secondquant import Commutator, NO, F, Fd\\n>>> from sympy import symbols\\n>>> from sympy.core.exprtools import _mask_nc\\n>>> from sympy.abc import x, y\\n>>> A, B, C = symbols('A,B,C', commutative=False)\\n\\nOne nc-symbol:\\n\\n>>> _mask_nc(A**2 - x**2, 'd')\\n(_d0**2 - x**2, {_d0: A}, [])\\n\\nMultiple nc-symbols:\\n\\n>>> _mask_nc(A**2 - B**2, 'd')\\n(A**2 - B**2, {}, [A, B])\\n\\nAn nc-object with nc-symbols but no others outside of it:\\n\\n>>> _mask_nc(1 + x*Commutator(A, B), 'd')\\n(_d0*x + 1, {_d0: Commutator(A, B)}, [])\\n>>> _mask_nc(NO(Fd(x)*F(y)), 'd')\\n(_d0, {_d0: NO(CreateFermion(x)*AnnihilateFermion(y))}, [])\\n\\nMultiple nc-objects:\\n\\n>>> eq = x*Commutator(A, B) + x*Commutator(A, C)*Commutator(A, B)\\n>>> _mask_nc(eq, 'd')\\n(x*_d0 + x*_d1*_d0, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1])\\n\\nMultiple nc-objects and nc-symbols:\\n\\n>>> eq = A*Commutator(A, B) + B*Commutator(A, C)\\n>>> _mask_nc(eq, 'd')\\n(A*_d0 + B*_d1, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1, A, B])\",\n",
       " 'Returns the wavefunction psi_{n} for the One-dimensional harmonic oscillator.\\n\\nParameters\\n==========\\n\\nn :\\n    the \"nodal\" quantum number.  Corresponds to the number of nodes in the\\n    wavefunction.  ``n >= 0``\\nx :\\n    x coordinate.\\nm :\\n    Mass of the particle.\\nomega :\\n    Angular frequency of the oscillator.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.qho_1d import psi_n\\n>>> from sympy.abc import m, x, omega\\n>>> psi_n(0, x, m, omega)\\n(m*omega)**(1/4)*exp(-m*omega*x**2/(2*hbar))/(hbar**(1/4)*pi**(1/4))',\n",
       " 'Factor out gcd of the elements of a matrix.\\n\\nRequires ``gcd`` in the ground domain.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DM\\n>>> from sympy import ZZ\\n>>> M = DM([[2, 4], [4, 12]], ZZ)\\n>>> content, M_primitive = M.primitive()\\n>>> content\\n2\\n>>> M_primitive\\nDomainMatrix([[1, 2], [2, 6]], (2, 2), ZZ)\\n>>> content * M_primitive == M\\nTrue\\n>>> M_primitive.content() == ZZ(1)\\nTrue\\n\\nSee Also\\n========\\n\\ncontent\\ncancel_denom',\n",
       " 'Returns\\n=======\\n\\nsize: int\\n    The size of set T. Set T is the set of all possible\\n    monomials of the n variables for degree equal to the\\n    degree_m',\n",
       " 'Each entry fundamental matrix can be interpreted as\\nthe expected number of times the chains is in state j\\nif it started in state i.\\n\\nReferences\\n==========\\n\\n.. [1] https://lips.cs.princeton.edu/the-fundamental-matrix-of-a-finite-markov-chain/',\n",
       " 'Build up the response data into a bytearray.',\n",
       " 'This injects a type check into an assignment expression (a := foo()).',\n",
       " 'Test that bare HTTPSConnection can connect, make requests',\n",
       " 'Create a handshake response to reject the connection.\\n\\nA short plain text response is the best fallback when failing to\\nestablish a WebSocket connection.\\n\\nYou must send the handshake response with :meth:`send_response`.\\n\\nYou may modify the response before sending it, for example by changing\\nHTTP headers.\\n\\nArgs:\\n    status: HTTP status code.\\n    text: HTTP response body; it will be encoded to UTF-8.\\n\\nReturns:\\n    HTTP response to send to the client.',\n",
       " 'Parse upgrade command syntax :target to retrieve the target revision\\nand given the :current_revisions stamp of the database.\\n\\nReturns a tuple of Revision objects which should be iterated/upgraded\\nto. The target may be specified in absolute form, or relative to\\n:current_revisions.',\n",
       " ':return:\\n    A unicode string',\n",
       " \"Upload a file to an S3 object.\\n\\nVariants have also been injected into S3 client, Bucket and Object.\\nYou don't have to use S3Transfer.upload_file() directly.\\n\\n.. seealso::\\n    :py:meth:`S3.Client.upload_file`\\n    :py:meth:`S3.Client.upload_fileobj`\",\n",
       " 'Get the value of a command option.',\n",
       " 'Get a stringified version of the param for use in error messages to\\nindicate which param caused the error.',\n",
       " 'The bulk of the command line interface to coverage.py.\\n\\n`argv` is the argument list to process.\\n\\nReturns 0 if all is well, 1 if something went wrong.',\n",
       " 'Construct and initialize a new FileDisposition object.',\n",
       " 'Find strings in `v`, and replace backslashes with slashes throughout.',\n",
       " 'Update a class object.',\n",
       " 'Notify breakpoints of a single step exception event.\\n\\n@type  event: L{ExceptionEvent}\\n@param event: Single step exception event.\\n\\n@rtype:  bool\\n@return: C{True} to call the user-defined handle, C{False} otherwise.',\n",
       " 'Dump the x86/x64 processor register values.\\nThe output mimics that of the WinDBG debugger.\\n\\n@type  registers: dict( str S{->} int )\\n@param registers: Dictionary mapping register names to their values.\\n\\n@type  arch: str\\n@param arch: Architecture of the machine whose registers were dumped.\\n    Defaults to the current architecture.\\n    Currently only the following architectures are supported:\\n     - L{win32.ARCH_I386}\\n     - L{win32.ARCH_AMD64}\\n\\n@rtype:  str\\n@return: Text suitable for logging.',\n",
       " 'Convert a space-separated list of EDNS flag text values into a EDNS\\nflags value.\\n\\nReturns an ``int``',\n",
       " 'see ``_offset_v2``',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Pre-rpc interceptor for create_rollup_property\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'Return a callable for the list playbooks method over gRPC.\\n\\nReturns a list of playbooks in the specified agent.\\n\\nReturns:\\n    Callable[[~.ListPlaybooksRequest],\\n            ~.ListPlaybooksResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.TestIamPermissionsRequest):\\n        The request object for TestIamPermissions method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    iam_policy_pb2.TestIamPermissionsResponse: Response from TestIamPermissions method.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Deletes an existing catalog specified by the catalog\\nID.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_biglake_v1\\n\\n    def sample_delete_catalog():\\n        # Create a client\\n        client = bigquery_biglake_v1.MetastoreServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_biglake_v1.DeleteCatalogRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete_catalog(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_biglake_v1.types.DeleteCatalogRequest, dict]):\\n        The request object. Request message for the DeleteCatalog\\n        method.\\n    name (str):\\n        Required. The name of the catalog to delete. Format:\\n        projects/{project_id_or_number}/locations/{location_id}/catalogs/{catalog_id}\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_biglake_v1.types.Catalog:\\n        Catalog is the container of\\n        databases.',\n",
       " 'Return a callable for the get attestor method over gRPC.\\n\\nGets an\\n[attestor][google.cloud.binaryauthorization.v1.Attestor].\\nReturns NOT_FOUND if the\\n[attestor][google.cloud.binaryauthorization.v1.Attestor] does\\nnot exist.\\n\\nReturns:\\n    Callable[[~.GetAttestorRequest],\\n            ~.Attestor]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Unassigns a license from a user.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import commerce_consumer_procurement_v1\\n\\n    def sample_unassign():\\n        # Create a client\\n        client = commerce_consumer_procurement_v1.LicenseManagementServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = commerce_consumer_procurement_v1.UnassignRequest(\\n            parent=\"parent_value\",\\n            usernames=[\\'usernames_value1\\', \\'usernames_value2\\'],\\n        )\\n\\n        # Make the request\\n        response = client.unassign(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.commerce_consumer_procurement_v1.types.UnassignRequest, dict]):\\n        The request object. Request message for\\n        [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].\\n    parent (str):\\n        Required. License pool name.\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    usernames (MutableSequence[str]):\\n        Required. Username. Format: ``name@domain.com``.\\n        This corresponds to the ``usernames`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.commerce_consumer_procurement_v1.types.UnassignResponse:\\n        Response message for\\n           [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].',\n",
       " 'Call the list network endpoints method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListNetworkEndpointsGlobalNetworkEndpointGroupsRequest):\\n        The request object. A request message for\\n    GlobalNetworkEndpointGroups.ListNetworkEndpoints.\\n    See the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.NetworkEndpointGroupsListNetworkEndpoints:',\n",
       " 'Retrieves the list of interconnect attachments\\ncontained within the specified region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InterconnectAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInterconnectAttachmentsRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInterconnectAttachmentsRequest, dict]):\\n        The request object. A request message for\\n        InterconnectAttachments.List. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.interconnect_attachments.pagers.ListPager:\\n        Response to the list request, and\\n        contains a list of interconnect\\n        attachments.  Iterating over this object\\n        will yield results and resolve\\n        additional pages automatically.',\n",
       " 'Deletes the specified NetworkAttachment in the given\\nscope\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.NetworkAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteNetworkAttachmentRequest(\\n            network_attachment=\"network_attachment_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteNetworkAttachmentRequest, dict]):\\n        The request object. A request message for\\n        NetworkAttachments.Delete. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region of this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    network_attachment (str):\\n        Name of the NetworkAttachment\\n        resource to delete.\\n\\n        This corresponds to the ``network_attachment`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.compute_v1.types.ListRegionUrlMapsRequest):\\n        The initial request object.\\n    response (google.cloud.compute_v1.types.UrlMapList):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Creates an analysis. The long running operation is\\ndone when the analysis has completed.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contact_center_insights_v1\\n\\n    def sample_create_analysis():\\n        # Create a client\\n        client = contact_center_insights_v1.ContactCenterInsightsClient()\\n\\n        # Initialize request argument(s)\\n        request = contact_center_insights_v1.CreateAnalysisRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.create_analysis(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contact_center_insights_v1.types.CreateAnalysisRequest, dict]):\\n        The request object. The request to create an analysis.\\n    parent (str):\\n        Required. The parent resource of the\\n        analysis.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    analysis (google.cloud.contact_center_insights_v1.types.Analysis):\\n        Required. The analysis to create.\\n        This corresponds to the ``analysis`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.contact_center_insights_v1.types.Analysis`\\n        The analysis resource.',\n",
       " 'Pre-rpc interceptor for get_phrase_matcher\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ContactCenterInsights server.',\n",
       " 'Call the execute question method over HTTP.\\n\\nArgs:\\n    request (~.question_service.ExecuteQuestionRequest):\\n        The request object. Request to execute an interpretation.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.question.Question:\\n        The question resource represents a\\n    natural language query, its settings,\\n    understanding generated by the system,\\n    and answer retrieval status. A question\\n    cannot be modified.',\n",
       " 'Instantiates the job controller client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,JobControllerTransport,Callable[..., JobControllerTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the JobControllerTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'Returns the list of all experiments in the specified\\n[Environment][google.cloud.dialogflow.cx.v3.Environment].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflowcx_v3\\n\\n    def sample_list_experiments():\\n        # Create a client\\n        client = dialogflowcx_v3.ExperimentsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflowcx_v3.ListExperimentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_experiments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflowcx_v3.types.ListExperimentsRequest, dict]):\\n        The request object. The request message for\\n        [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n    parent (str):\\n        Required. The\\n        [Environment][google.cloud.dialogflow.cx.v3.Environment]\\n        to list all environments for. Format:\\n        ``projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/environments/<Environment ID>``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflowcx_v3.services.experiments.pagers.ListExperimentsPager:\\n        The response message for\\n           [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the delete agent method over gRPC.\\n\\nDeletes the specified agent.\\n\\nReturns:\\n    Callable[[~.DeleteAgentRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the start experiment method over gRPC.\\n\\nStarts the specified\\n[Experiment][google.cloud.dialogflow.cx.v3beta1.Experiment].\\nThis rpc only changes the state of experiment from PENDING to\\nRUNNING.\\n\\nReturns:\\n    Callable[[~.StartExperimentRequest],\\n            ~.Experiment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns True if the graph G is a triad, else False.\\n\\nParameters\\n----------\\nG : graph\\n   A NetworkX Graph\\n\\nReturns\\n-------\\nistriad : boolean\\n   Whether G is a valid triad\\n\\nExamples\\n--------\\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\\n>>> nx.is_triad(G)\\nTrue\\n>>> G.add_edge(0, 1)\\n>>> nx.is_triad(G)\\nFalse',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Deletes the processor, unloads all deployed model\\nartifacts if it was enabled and then deletes all\\nartifacts associated with this processor.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_delete_processor():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentProcessorServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = documentai_v1beta3.DeleteProcessorRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_processor(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.DeleteProcessorRequest, dict]):\\n        The request object. Request message for the\\n        [DeleteProcessor][google.cloud.documentai.v1beta3.DocumentProcessorService.DeleteProcessor]\\n        method.\\n    name (str):\\n        Required. The processor resource name\\n        to be deleted.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " \"Publish events to a subscriber's channel.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import eventarc_publishing_v1\\n\\n    def sample_publish_events():\\n        # Create a client\\n        client = eventarc_publishing_v1.PublisherClient()\\n\\n        # Initialize request argument(s)\\n        request = eventarc_publishing_v1.PublishEventsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.publish_events(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.eventarc_publishing_v1.types.PublishEventsRequest, dict]):\\n        The request object. The request message for the\\n        PublishEvents method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.eventarc_publishing_v1.types.PublishEventsResponse:\\n        The response message for the\\n        PublishEvents method.\",\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the create comment method over gRPC.\\n\\nCreates a new comment on an order.\\n\\nReturns:\\n    Callable[[~.CreateCommentRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the delete hardware group method over HTTP.\\n\\nArgs:\\n    request (~.service.DeleteHardwareGroupRequest):\\n        The request object. A request to delete a hardware group.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.network_management_v1.types.ListConnectivityTestsRequest):\\n        The initial request object.\\n    response (google.cloud.network_management_v1.types.ListConnectivityTestsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Return a callable for the get inventory method over gRPC.\\n\\nGet inventory data for the specified VM instance. If the VM has\\nno associated inventory, the message ``NOT_FOUND`` is returned.\\n\\nReturns:\\n    Callable[[~.GetInventoryRequest],\\n            ~.Inventory]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Read image data from file and return as numpy array.\\n\\nRaise ValueError if format is unsupported.\\n\\nParameters\\n----------\\nout : numpy.ndarray, str, or file-like object; optional\\n    Buffer where image data will be saved.\\n    If None (default), a new array will be created.\\n    If numpy.ndarray, a writable array of compatible dtype and shape.\\n    If 'memmap', directly memory-map the image data in the TIFF file\\n    if possible; else create a memory-mapped array in a temporary file.\\n    If str or open file, the file name or file object used to\\n    create a memory-map to an array stored in a binary file on disk.\\nsqueeze : bool\\n    If True, all length-1 dimensions (except X and Y) are\\n    squeezed out from the array.\\n    If False, the shape of the returned array might be different from\\n    the page.shape.\\nlock : {RLock, NullContext}\\n    A reentrant lock used to synchronize reads from file.\\n    If None (default), the lock of the parent's filehandle is used.\\nreopen : bool\\n    If True (default) and the parent file handle is closed, the file\\n    is temporarily re-opened and closed if no exception occurs.\\nmaxsize: int or None\\n    Maximum size of data before a ValueError is raised.\\n    Can be used to catch DOS. Default: 16 TB.\\nvalidate : bool\\n    If True (default), validate various parameters.\\n    If None, only validate parameters and return None.\",\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    RecaptchaEnterpriseServiceAsyncClient: The constructed client.',\n",
       " 'Return a callable for the mark recommendation failed method over gRPC.\\n\\nMarks the Recommendation State as Failed. Users can use this\\nmethod to indicate to the Recommender API that they have applied\\nthe recommendation themselves, and the operation failed. This\\nstops the recommendation content from being updated. Associated\\ninsights are frozen and placed in the ACCEPTED state.\\n\\nMarkRecommendationFailed can be applied to recommendations in\\nACTIVE, CLAIMED, SUCCEEDED, or FAILED state.\\n\\nRequires the recommender.*.update IAM permission for the\\nspecified recommender.\\n\\nReturns:\\n    Callable[[~.MarkRecommendationFailedRequest],\\n            ~.Recommendation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the remove fulfillment places method over gRPC.\\n\\nWe recommend that you use the\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nmethod instead of the\\n[ProductService.RemoveFulfillmentPlaces][google.cloud.retail.v2alpha.ProductService.RemoveFulfillmentPlaces]\\nmethod.\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nachieves the same results but provides more fine-grained control\\nover ingesting local inventory data.\\n\\nIncrementally removes place IDs from a\\n[Product.fulfillment_info.place_ids][google.cloud.retail.v2alpha.FulfillmentInfo.place_ids].\\n\\nThis process is asynchronous and does not require the\\n[Product][google.cloud.retail.v2alpha.Product] to exist before\\nupdating fulfillment information. If the request is valid, the\\nupdate will be enqueued and processed downstream. As a\\nconsequence, when a response is returned, the removed place IDs\\nare not immediately manifested in the\\n[Product][google.cloud.retail.v2alpha.Product] queried by\\n[ProductService.GetProduct][google.cloud.retail.v2alpha.ProductService.GetProduct]\\nor\\n[ProductService.ListProducts][google.cloud.retail.v2alpha.ProductService.ListProducts].\\n\\nThe returned [Operation][google.longrunning.Operation]s will be\\nobsolete after 1 day, and\\n[GetOperation][google.longrunning.Operations.GetOperation] API\\nwill return NOT_FOUND afterwards.\\n\\nIf conflicting updates are issued, the\\n[Operation][google.longrunning.Operation]s associated with the\\nstale updates will not be marked as\\n[done][google.longrunning.Operation.done] until being obsolete.\\n\\nReturns:\\n    Callable[[~.RemoveFulfillmentPlacesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get big query export method over gRPC.\\n\\nGets a BigQuery export.\\n\\nReturns:\\n    Callable[[~.GetBigQueryExportRequest],\\n            ~.BigQueryExport]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list operations method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.ListOperationsRequest):\\n        The request object for ListOperations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    operations_pb2.ListOperationsResponse: Response from ListOperations method.',\n",
       " 'Pre-rpc interceptor for analyze_asset\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Warehouse server.',\n",
       " 'Post-rpc interceptor for create_clone_job\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the VmMigration server but before\\nit is returned to user code.',\n",
       " 'Call the list locations method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.ListLocationsRequest):\\n        The request object for ListLocations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.ListLocationsResponse: Response from ListLocations method.',\n",
       " \"Process the response from an HTTP request that canceled the upload.\\n\\nThis is everything that must be done after a request that doesn't\\nrequire network I/O (or other I/O). This is based on the `sans-I/O`_\\nphilosophy.\\n\\nArgs:\\n    response (object): The HTTP response object.\\n\\nRaises:\\n    ~google.resumable_media.common.InvalidResponse: If the status\\n        code is not 204.\\n\\n.. _sans-I/O: https://sans-io.readthedocs.io/\",\n",
       " 'maybe_int : NUMBER\\n| empty',\n",
       " 'Like os.path.walk, but follows symlinks on POSIX systems.\\n\\nIf the symlinks create a loop, this function will never finish.',\n",
       " 'Run a test only if the client is not connected to a mongos.',\n",
       " 'create a unix-style long description of the file (like ls -l)',\n",
       " \"Set how the Axes adjusts to achieve the required aspect ratio.\\n\\nParameters\\n----------\\nadjustable : {'box', 'datalim'}\\n    If 'box', change the physical dimensions of the Axes.\\n    If 'datalim', change the ``x`` or ``y`` data limits. This\\n    may ignore explicitly defined axis limits.\\n\\nshare : bool, default: False\\n    If ``True``, apply the settings to all shared Axes.\\n\\nSee Also\\n--------\\nmatplotlib.axes.Axes.set_aspect\\n    For a description of aspect handling.\\n\\nNotes\\n-----\\nShared Axes (of which twinned Axes are a special case)\\nimpose restrictions on how aspect ratios can be imposed.\\nFor twinned Axes, use 'datalim'.  For Axes that share both\\nx and y, use 'box'.  Otherwise, either 'datalim' or 'box'\\nmay be used.  These limitations are partly a requirement\\nto avoid over-specification, and partly a result of the\\nparticular implementation we are currently using, in\\nwhich the adjustments for aspect ratios are done sequentially\\nand independently on each Axes as it is drawn.\",\n",
       " 'Return the size of the image as tuple (numrows, numcols).',\n",
       " 'Function used for unpickling proxy objects.',\n",
       " 'Helper function to implement map, starmap and their async counterparts.',\n",
       " 'Create a Timestamp from posix timestamp in nanoseconds.\\n\\n:param int unix_ns: Posix timestamp in nanoseconds.\\n:rtype: Timestamp',\n",
       " 'Compute the Katz centrality for the graph G.\\n\\nKatz centrality computes the centrality for a node based on the centrality\\nof its neighbors. It is a generalization of the eigenvector centrality. The\\nKatz centrality for node $i$ is\\n\\n.. math::\\n\\n    x_i = \\\\alpha \\\\sum_{j} A_{ij} x_j + \\\\beta,\\n\\nwhere $A$ is the adjacency matrix of graph G with eigenvalues $\\\\lambda$.\\n\\nThe parameter $\\\\beta$ controls the initial centrality and\\n\\n.. math::\\n\\n    \\\\alpha < \\\\frac{1}{\\\\lambda_{\\\\max}}.\\n\\nKatz centrality computes the relative influence of a node within a\\nnetwork by measuring the number of the immediate neighbors (first\\ndegree nodes) and also all other nodes in the network that connect\\nto the node under consideration through these immediate neighbors.\\n\\nExtra weight can be provided to immediate neighbors through the\\nparameter $\\\\beta$.  Connections made with distant neighbors\\nare, however, penalized by an attenuation factor $\\\\alpha$ which\\nshould be strictly less than the inverse largest eigenvalue of the\\nadjacency matrix in order for the Katz centrality to be computed\\ncorrectly. More information is provided in [1]_.\\n\\nParameters\\n----------\\nG : graph\\n  A NetworkX graph\\n\\nalpha : float\\n  Attenuation factor\\n\\nbeta : scalar or dictionary, optional (default=1.0)\\n  Weight attributed to the immediate neighborhood. If not a scalar the\\n  dictionary must have an value for every node.\\n\\nnormalized : bool\\n  If True normalize the resulting values.\\n\\nweight : None or string, optional\\n  If None, all edge weights are considered equal.\\n  Otherwise holds the name of the edge attribute used as weight.\\n  In this measure the weight is interpreted as the connection strength.\\n\\nReturns\\n-------\\nnodes : dictionary\\n   Dictionary of nodes with Katz centrality as the value.\\n\\nRaises\\n------\\nNetworkXError\\n   If the parameter `beta` is not a scalar but lacks a value for at least\\n   one node\\n\\nExamples\\n--------\\n>>> import math\\n>>> G = nx.path_graph(4)\\n>>> phi = (1 + math.sqrt(5)) / 2.0  # largest eigenvalue of adj matrix\\n>>> centrality = nx.katz_centrality_numpy(G, 1 / phi)\\n>>> for n, c in sorted(centrality.items()):\\n...     print(f\"{n} {c:.2f}\")\\n0 0.37\\n1 0.60\\n2 0.60\\n3 0.37\\n\\nSee Also\\n--------\\nkatz_centrality\\neigenvector_centrality_numpy\\neigenvector_centrality\\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\\n\\nNotes\\n-----\\nKatz centrality was introduced by [2]_.\\n\\nThis algorithm uses a direct linear solver to solve the above equation.\\nThe parameter ``alpha`` should be strictly less than the inverse of largest\\neigenvalue of the adjacency matrix for there to be a solution.\\nYou can use ``max(nx.adjacency_spectrum(G))`` to get $\\\\lambda_{\\\\max}$ the largest\\neigenvalue of the adjacency matrix.\\n\\nFor strongly connected graphs, as $\\\\alpha \\\\to 1/\\\\lambda_{\\\\max}$, and $\\\\beta > 0$,\\nKatz centrality approaches the results for eigenvector centrality.\\n\\nFor directed graphs this finds \"left\" eigenvectors which corresponds\\nto the in-edges in the graph. For out-edges Katz centrality,\\nfirst reverse the graph with ``G.reverse()``.\\n\\nReferences\\n----------\\n.. [1] Mark E. J. Newman:\\n   Networks: An Introduction.\\n   Oxford University Press, USA, 2010, p. 173.\\n.. [2] Leo Katz:\\n   A New Status Index Derived from Sociometric Index.\\n   Psychometrika 18(1):39–43, 1953\\n   https://link.springer.com/content/pdf/10.1007/BF02289026.pdf',\n",
       " 'Remove node attributes from all nodes in the graph.\\n\\nParameters\\n----------\\nG : NetworkX Graph\\n\\n*attr_names : List of Strings\\n    The attribute names to remove from the graph.\\n\\nnbunch : List of Nodes\\n    Remove the node attributes only from the nodes in this list.\\n\\nExamples\\n--------\\n>>> G = nx.Graph()\\n>>> G.add_nodes_from([1, 2, 3], color=\"blue\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{1: \\'blue\\', 2: \\'blue\\', 3: \\'blue\\'}\\n>>> nx.remove_node_attributes(G, \"color\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{}',\n",
       " 'Check the message is formatted correctly for the decimal value.\\nAlso check the message when input includes inf or nan (gh12200)',\n",
       " 'Check that ._metadata attributes are equivalent.',\n",
       " 'Check for the `min_count` keyword. Returns True if below `min_count` (when\\nmissing value should be returned from the reduction).\\n\\nParameters\\n----------\\nshape : tuple\\n    The shape of the values (`values.shape`).\\nmask : ndarray[bool] or None\\n    Boolean numpy array (typically of same shape as `shape`) or None.\\nmin_count : int\\n    Keyword passed through from sum/prod call.\\n\\nReturns\\n-------\\nbool',\n",
       " 'Issue #96 (for newbytes instead of newobject)',\n",
       " \"Convert ``color_spec`` to an openpyxl v2 Color object.\\n\\nParameters\\n----------\\ncolor_spec : str, dict\\n    A 32-bit ARGB hex string, or a dict with zero or more of the\\n    following keys.\\n        'rgb'\\n        'indexed'\\n        'auto'\\n        'theme'\\n        'tint'\\n        'index'\\n        'type'\\n\\nReturns\\n-------\\ncolor : openpyxl.styles.Color\",\n",
       " 'A simplified json_normalize\\n\\nConverts a nested dict into a flat dict (\"record\"), unlike json_normalize,\\nit does not attempt to extract a subset of the data.\\n\\nParameters\\n----------\\nds : dict or list of dicts\\nprefix: the prefix, optional, default: \"\"\\nsep : str, default \\'.\\'\\n    Nested records will generate names separated by sep,\\n    e.g., for sep=\\'.\\', { \\'foo\\' : { \\'bar\\' : 0 } } -> foo.bar\\nlevel: int, optional, default: 0\\n    The number of levels in the json string.\\n\\nmax_level: int, optional, default: None\\n    The max depth to normalize.\\n\\nReturns\\n-------\\nd - dict or list of dicts, matching `ds`\\n\\nExamples\\n--------\\n>>> nested_to_record(\\n...     dict(flat1=1, dict1=dict(c=1, d=2), nested=dict(e=dict(c=1, d=2), d=2))\\n... )\\n{\\'flat1\\': 1, \\'dict1.c\\': 1, \\'dict1.d\\': 2, \\'nested.e.c\\': 1, \\'nested.e.d\\': 2, \\'nested.d\\': 2}',\n",
       " \"Converts lists of lists/tuples into DataFrames with proper type inference\\nand optional (e.g. string to datetime) conversion. Also enables iterating\\nlazily over chunks of large files\\n\\nParameters\\n----------\\ndata : file-like object or list\\ndelimiter : separator character to use\\ndialect : str or csv.Dialect instance, optional\\n    Ignored if delimiter is longer than 1 character\\nnames : sequence, default\\nheader : int, default 0\\n    Row to use to parse column labels. Defaults to the first row. Prior\\n    rows will be discarded\\nindex_col : int or list, optional\\n    Column or columns to use as the (possibly hierarchical) index\\nhas_index_names: bool, default False\\n    True if the cols defined in index_col have an index name and are\\n    not in the header.\\nna_values : scalar, str, list-like, or dict, optional\\n    Additional strings to recognize as NA/NaN.\\nkeep_default_na : bool, default True\\nthousands : str, optional\\n    Thousands separator\\ncomment : str, optional\\n    Comment out remainder of line\\nparse_dates : bool, default False\\ndate_format : str or dict of column -> format, default ``None``\\n\\n    .. versionadded:: 2.0.0\\nskiprows : list of integers\\n    Row numbers to skip\\nskipfooter : int\\n    Number of line at bottom of file to skip\\nconverters : dict, optional\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\nencoding : str, optional\\n    Encoding to use for UTF when reading/writing (ex. 'utf-8')\\nfloat_precision : str, optional\\n    Specifies which converter the C engine should use for floating-point\\n    values. The options are `None` or `high` for the ordinary converter,\\n    `legacy` for the original lower precision pandas converter, and\\n    `round_trip` for the round-trip converter.\",\n",
       " \"This reads until EOF using readline() and returns a list containing\\nthe lines thus read. The optional 'sizehint' argument is ignored.\\nRemember, because this reads until EOF that means the child\\nprocess should have closed its stdout. If you run this method on\\na child that is still running with its stdout open then this\\nmethod will block until it timesout.\",\n",
       " \"Return the address of the remote side of this Channel, if possible.\\n\\nThis simply wraps `.Transport.getpeername`, used to provide enough of a\\nsocket-like interface to allow asyncore to work. (asyncore likes to\\ncall ``'getpeername'``.)\",\n",
       " 'Returns a generator of `funcdef` nodes.',\n",
       " \"Fixture for 'na_action' argument in sort_values/sort_index/rank.\",\n",
       " 'Test the wheel cache filters on wheel name when several wheels\\nfor different package are stored under the same cache directory.',\n",
       " 'Get the sha256 digest of a string\\n\\nSupports the `usedforsecurity` argument for Python 3.9+ to allow running on\\na FIPS-enabled system.',\n",
       " \"create a modified version of this path. A 'rev' argument\\nindicates a new revision.\\nthe following keyword arguments modify various path parts::\\n\\n  http://host.com/repo/path/file.ext\\n  |-----------------------|          dirname\\n                            |------| basename\\n                            |--|     purebasename\\n                                |--| ext\",\n",
       " 'Assign |ASN.1| type component by position.\\n\\nEquivalent to Python sequence item assignment operation (e.g. `[]`).\\n\\nParameters\\n----------\\nidx : :class:`int`\\n    Component index (zero-based). Must either refer to existing\\n    component (if *componentType* is set) or to N+1 component\\n    otherwise. In the latter case a new component of given ASN.1\\n    type gets instantiated and appended to |ASN.1| sequence.\\n\\nKeyword Args\\n------------\\nvalue: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\\n    A Python value to initialize |ASN.1| component with (if *componentType* is set)\\n    or ASN.1 value object to assign to |ASN.1| component.\\n    If `value` is not given, schema object will be set as a component.\\n\\nverifyConstraints : :class:`bool`\\n     If :obj:`False`, skip constraints validation\\n\\nmatchTags: :class:`bool`\\n     If :obj:`False`, skip component tags matching\\n\\nmatchConstraints: :class:`bool`\\n     If :obj:`False`, skip component constraints matching\\n\\nReturns\\n-------\\nself',\n",
       " 'Avoid extraneous whitespace around an operator.\\n\\nOkay: a = 12 + 3\\nE221: a = 4  + 5\\nE222: a = 4 +  5\\nE223: a = 4\\\\t+ 5\\nE224: a = 4 +\\\\t5',\n",
       " 'The error reported for source files which end prematurely causing a\\nsyntax error reflects the cause for the syntax error.',\n",
       " 'Test line referrals.',\n",
       " 'The check',\n",
       " 'Similar, but with a subscript in a key-value pair rather than the test\\nSee https://github.com/pylint-dev/pylint/issues/6069',\n",
       " 'The type name of the exception.',\n",
       " 'Raise pytest.skip() if all examples in the given DocTest have the SKIP\\noption set.',\n",
       " 'See :meth:`Pytester.parseconfigure`.',\n",
       " 'Regarding issue pytest-xdist#241.\\n\\nThis test came originally from test_remote.py in xdist (ca03269).',\n",
       " 'From the backport of the email package',\n",
       " 'patch_service_cidr_status  # noqa: E501\\n\\npartially update status of the specified ServiceCIDR  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.patch_service_cidr_status(name, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ServiceCIDR (required)\\n:param object body: (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param bool force: Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1beta1ServiceCIDR\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'test router-router MQ devices',\n",
       " 'Tests for the regular expressions used in ISO8601Highlighter.',\n",
       " 'Placeholder docstring',\n",
       " 'Update to enable or disable remote debug for a training job.\\n\\nThis method updates the ``_enable_remote_debug`` parameter\\nand enables or disables remote debug for a training job',\n",
       " 'Create an Amazon SageMaker training job.\\n\\nArgs:\\n    input_mode (str): The input mode that the algorithm supports. Valid modes:\\n        * \\'File\\' - Amazon SageMaker copies the training dataset from the S3 location to\\n        a directory in the Docker container.\\n        * \\'Pipe\\' - Amazon SageMaker streams data directly from S3 to the container via a\\n        Unix-named pipe.\\n        * \\'FastFile\\' - Amazon SageMaker streams data from S3 on demand instead of\\n        downloading the entire dataset before training begins.\\n    input_config (list): A list of Channel objects. Each channel is a named input source.\\n        Please refer to the format details described:\\n        https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\\n        jobs and APIs that create Amazon SageMaker endpoints use this role to access\\n        training data and model artifacts. You must grant sufficient permissions to this\\n        role.\\n    job_name (str): Name of the training job being created.\\n    output_config (dict): The S3 URI where you want to store the training results and\\n        optional KMS key ID.\\n    resource_config (dict): Contains values for ResourceConfig:\\n        * instance_count (int): Number of EC2 instances to use for training.\\n        The key in resource_config is \\'InstanceCount\\'.\\n        * instance_type (str): Type of EC2 instance to use for training, for example,\\n        \\'ml.c4.xlarge\\'. The key in resource_config is \\'InstanceType\\'.\\n    vpc_config (dict): Contains values for VpcConfig:\\n        * subnets (list[str]): List of subnet ids.\\n        The key in vpc_config is \\'Subnets\\'.\\n        * security_group_ids (list[str]): List of security group ids.\\n        The key in vpc_config is \\'SecurityGroupIds\\'.\\n    hyperparameters (dict): Hyperparameters for model training. The hyperparameters are\\n        made accessible as a dict[str, str] to the training code on SageMaker. For\\n        convenience, this accepts other types for keys and values, but ``str()`` will be\\n        called to convert them before training.\\n    stop_condition (dict): Defines when training shall finish. Contains entries that can\\n        be understood by the service like ``MaxRuntimeInSeconds``.\\n    tags (Optional[Tags]): Tags for labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    metric_definitions (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: \\'Name\\' for\\n        the name of the metric, and \\'Regex\\' for the regular expression used to extract the\\n        metric from the logs.\\n    enable_network_isolation (bool): Whether to request for the training job to run with\\n        network isolation or not.\\n    image_uri (str): Docker image containing training code.\\n    training_image_config(dict): Training image configuration.\\n        Optionally, the dict can contain \\'TrainingRepositoryAccessMode\\' and\\n        \\'TrainingRepositoryCredentialsProviderArn\\' (under \\'TrainingRepositoryAuthConfig\\').\\n        For example,\\n\\n        .. code:: python\\n\\n            training_image_config = {\\n                \"TrainingRepositoryAccessMode\": \"Vpc\",\\n                \"TrainingRepositoryAuthConfig\": {\\n                    \"TrainingRepositoryCredentialsProviderArn\":\\n                      \"arn:aws:lambda:us-west-2:1234567890:function:test\"\\n                },\\n            }\\n\\n        If TrainingRepositoryAccessMode is set to Vpc, the training image is accessed\\n        through a private Docker registry in customer Vpc. If it\\'s set to Platform or None,\\n        the training image is accessed through ECR.\\n        If TrainingRepositoryCredentialsProviderArn is provided, the credentials to\\n        authenticate to the private Docker registry will be retrieved from this AWS Lambda\\n        function. (default: ``None``). When it\\'s set to None, SageMaker will not do\\n        authentication before pulling the image in the private Docker registry.\\n    container_entry_point (List[str]): Optional. The entrypoint script for a Docker\\n        container used to run a training job. This script takes precedence over\\n        the default train processing instructions.\\n    container_arguments (List[str]): Optional. The arguments for a container used to run\\n        a training job.\\n    algorithm_arn (str): Algorithm Arn from Marketplace.\\n    encrypt_inter_container_traffic (bool): Specifies whether traffic between training\\n        containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool): whether to use spot instances for training.\\n    checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\\n        that the algorithm persists (if any) during training. (default:\\n        ``None``).\\n    checkpoint_local_path (str): The local path that the algorithm\\n        writes its checkpoints to. SageMaker will persist all files\\n        under this path to `checkpoint_s3_uri` continually during\\n        training. On job startup the reverse happens - data from the\\n        s3 location is downloaded to this path before the algorithm is\\n        started. If the path is unset then SageMaker assumes the\\n        checkpoints will be provided under `/opt/ml/checkpoints/`.\\n        (default: ``None``).\\n    experiment_config (dict[str, str]): Experiment management configuration.\\n        Optionally, the dict can contain four keys:\\n        \\'ExperimentName\\', \\'TrialName\\',  \\'TrialComponentDisplayName\\' and \\'RunName\\'.\\n        The behavior of setting these keys is as follows:\\n        * If `ExperimentName` is supplied but `TrialName` is not a Trial will be\\n        automatically created and the job\\'s Trial Component associated with the Trial.\\n        * If `TrialName` is supplied and the Trial already exists the job\\'s Trial Component\\n        will be associated with the Trial.\\n        * If both `ExperimentName` and `TrialName` are not supplied the trial component\\n        will be unassociated.\\n        * `TrialComponentDisplayName` is used for display in Studio.\\n        * `RunName` is used to record an experiment run.\\n    enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\\n        Series. For more information see:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html\\n        #SageMaker-Type\\n        -AlgorithmSpecification-EnableSageMakerMetricsTimeSeries\\n        (default: ``None``).\\n    profiler_rule_configs (list[dict]): A list of profiler rule\\n        configurations.src/sagemaker/lineage/artifact.py:285\\n    profiler_config (dict): Configuration for how profiling information is emitted\\n        with SageMaker Profiler. (default: ``None``).\\n    remote_debug_config(dict): Configuration for RemoteDebug. (default: ``None``)\\n        The dict can contain \\'EnableRemoteDebug\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            remote_debug_config = {\\n                \"EnableRemoteDebug\": True,\\n            }\\n    session_chaining_config(dict): Configuration for SessionChaining. (default: ``None``)\\n        The dict can contain \\'EnableSessionTagChaining\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            session_chaining_config = {\\n                \"EnableSessionTagChaining\": True,\\n            }\\n    environment (dict[str, str]) : Environment variables to be set for\\n        use during training job (default: ``None``)\\n    retry_strategy(dict): Defines RetryStrategy for InternalServerFailures.\\n        * max_retry_attsmpts (int): Number of times a job should be retried.\\n        The key in RetryStrategy is \\'MaxRetryAttempts\\'.\\n    infra_check_config(dict): Infra check configuration.\\n        Optionally, the dict can contain \\'EnableInfraCheck\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            infra_check_config = {\\n                \"EnableInfraCheck\": True,\\n            }\\nReturns:\\n    str: ARN of the training job, if it is created.\\n\\nRaises:\\n    - botocore.exceptions.ClientError: If Sagemaker throws an exception while creating\\n    training job.\\n    - ValueError: If both image_uri and algorithm are provided, or if neither is provided.',\n",
       " 'Convenience method for accessing the SageMaker session.\\n\\nIt access :class:`~sagemaker.session.Session` object associated with the estimator\\nfor the ``HyperparameterTuner``.',\n",
       " 'Check for correspondence between linkage and condensed distance matrices.\\n\\nThey must have the same number of original observations for\\nthe check to succeed.\\n\\nThis function is useful as a sanity check in algorithms that make\\nextensive use of linkage and distance matrices that must\\ncorrespond to the same set of original observations.\\n\\nParameters\\n----------\\nZ : array_like\\n    The linkage matrix to check for correspondence.\\nY : array_like\\n    The condensed distance matrix to check for correspondence.\\n\\nReturns\\n-------\\nb : bool\\n    A boolean indicating whether the linkage matrix and distance\\n    matrix could possibly correspond to one another.\\n\\nSee Also\\n--------\\nlinkage : for a description of what a linkage matrix is.\\n\\nExamples\\n--------\\n>>> from scipy.cluster.hierarchy import ward, correspond\\n>>> from scipy.spatial.distance import pdist\\n\\nThis method can be used to check if a given linkage matrix ``Z`` has been\\nobtained from the application of a cluster method over a dataset ``X``:\\n\\n>>> X = [[0, 0], [0, 1], [1, 0],\\n...      [0, 4], [0, 3], [1, 4],\\n...      [4, 0], [3, 0], [4, 1],\\n...      [4, 4], [3, 4], [4, 3]]\\n>>> X_condensed = pdist(X)\\n>>> Z = ward(X_condensed)\\n\\nHere, we can compare ``Z`` and ``X`` (in condensed form):\\n\\n>>> correspond(Z, X_condensed)\\nTrue',\n",
       " 'Fit the gradient boosting model.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The input samples.\\n\\ny : array-like of shape (n_samples,)\\n    Target values.\\n\\nsample_weight : array-like of shape (n_samples,) default=None\\n    Weights of training data.\\n\\n    .. versionadded:: 0.23\\n\\nReturns\\n-------\\nself : object\\n    Fitted estimator.',\n",
       " 'Informative warnings should be raised when mixing sklearn and joblib API',\n",
       " \"Options\\n-------\\nnit : int, optional\\n    Number of iterations to make. If omitted (default), make as many\\n    as required to meet tolerances.\\ndisp : bool, optional\\n    Print status to stdout on every iteration.\\nmaxiter : int, optional\\n    Maximum number of iterations to make.\\nftol : float, optional\\n    Relative tolerance for the residual. If omitted, not used.\\nfatol : float, optional\\n    Absolute tolerance (in max-norm) for the residual.\\n    If omitted, default is 6e-6.\\nxtol : float, optional\\n    Relative minimum step size. If omitted, not used.\\nxatol : float, optional\\n    Absolute minimum step size, as determined from the Jacobian\\n    approximation. If the step size is smaller than this, optimization\\n    is terminated as successful. If omitted, not used.\\ntol_norm : function(vector) -> scalar, optional\\n    Norm to use in convergence check. Default is the maximum norm.\\nline_search : {None, 'armijo' (default), 'wolfe'}, optional\\n    Which type of a line search to use to determine the step size in\\n    the direction given by the Jacobian approximation. Defaults to\\n    'armijo'.\\njac_options : dict, optional\\n    Options for the respective Jacobian approximation.\\n\\n    alpha : float, optional\\n        Initial guess for the Jacobian is (-1/alpha).\\n    reduction_method : str or tuple, optional\\n        Method used in ensuring that the rank of the Broyden\\n        matrix stays low. Can either be a string giving the\\n        name of the method, or a tuple of the form ``(method,\\n        param1, param2, ...)`` that gives the name of the\\n        method and values for additional parameters.\\n\\n        Methods available:\\n\\n        - ``restart``: drop all matrix columns. Has no extra parameters.\\n        - ``simple``: drop oldest matrix column. Has no extra parameters.\\n        - ``svd``: keep only the most significant SVD components.\\n          Takes an extra parameter, ``to_retain``, which determines the\\n          number of SVD components to retain when rank reduction is done.\\n          Default is ``max_rank - 2``.\\n\\n    max_rank : int, optional\\n        Maximum rank for the Broyden matrix.\\n        Default is infinity (i.e., no rank reduction).\\n\\nExamples\\n--------\\n>>> def func(x):\\n...     return np.cos(x) + x[::-1] - [1, 2, 3, 4]\\n...\\n>>> from scipy import optimize\\n>>> res = optimize.root(func, [1, 1, 1, 1], method='broyden1', tol=1e-14)\\n>>> x = res.x\\n>>> x\\narray([4.04674914, 3.91158389, 2.71791677, 1.61756251])\\n>>> np.cos(x) + x[::-1]\\narray([1., 2., 3., 4.])\",\n",
       " 'Check stats.rankdata with an array of length 1.',\n",
       " 'List commits in reverse chronological order.\\n\\nOnly the first `num_commits` are shown.',\n",
       " 'Returns UNIX timestamp (integer) representing the time\\nwhen the item was created.\\n\\n.. versionadded:: 1.1',\n",
       " 'Return a buffer object which allows access to our memory region from our offset\\nto the window size. Please note that it might be smaller than you requested when calling use_region()\\n\\n**Note:** You can only obtain a buffer if this instance is_valid() !\\n\\n**Note:** buffers should not be cached passed the duration of your access as it will\\nprevent resources from being freed even though they might not be accounted for anymore !',\n",
       " 'Test focus within.',\n",
       " \"Round each value in a Series to the given number of decimals.\\n\\nParameters\\n----------\\ndecimals : int\\n    Number of decimal places to round to (default: 0).\\n    If decimals are negative, it specifies the number of\\n    positions to the left of the decimal point.\\n\\nReturns\\n-------\\nSeries object\\n\\nSee Also\\n--------\\nDataFrame.round\\n\\nExamples\\n--------\\n>>> df = ps.Series([0.028208, 0.038683, 0.877076], name='x')\\n>>> df\\n0    0.028208\\n1    0.038683\\n2    0.877076\\nName: x, dtype: float64\\n\\n>>> df.round(2)\\n0    0.03\\n1    0.04\\n2    0.88\\nName: x, dtype: float64\",\n",
       " 'For python2 compatibility.',\n",
       " \"Generates data for a given partition and returns an iterator of tuples or rows.\\n\\nThis method is invoked once per partition to read the data. Implementing\\nthis method is required for stream reader. You can initialize any\\nnon-serializable resources required for reading data from the data source\\nwithin this method.\\n\\nNotes\\n-----\\nThis method is static and stateless. You shouldn't access mutable class member\\nor keep in memory state between different invocations of read().\\n\\nParameters\\n----------\\npartition : :class:`InputPartition`\\n    The partition to read. It must be one of the partition values returned by\\n    :meth:`DataSourceStreamReader.partitions`.\\n\\nReturns\\n-------\\niterator of tuples or PyArrow's `RecordBatch`\\n    An iterator of tuples or rows. Each tuple or row will be converted to a row\\n    in the final DataFrame.\\n    It can also return an iterator of PyArrow's `RecordBatch` if the data source\\n    supports it.\",\n",
       " 'test #9635',\n",
       " 'Just parse the simple ones.',\n",
       " \"test that 'literal binds' mode works - no bound params.\",\n",
       " 'Clear the current scope, if any.',\n",
       " 'Target must support simultaneous, independent database cursors\\non a single connection.',\n",
       " \"Encodes a plaintext into popular Morse Code with letters\\nseparated by ``sep`` and words by a double ``sep``.\\n\\nExamples\\n========\\n\\n>>> from sympy.crypto.crypto import encode_morse\\n>>> msg = 'ATTACK RIGHT FLANK'\\n>>> encode_morse(msg)\\n'.-|-|-|.-|-.-.|-.-||.-.|..|--.|....|-||..-.|.-..|.-|-.|-.-'\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Morse_code\",\n",
       " 'Returns True if the axis of the pure quaternions seen as 3D vectors\\n``q1``, ``q2``, and ``q3`` are coplanar.\\n\\nExplanation\\n===========\\n\\nThree pure quaternions are vector coplanar if the quaternions seen as 3D vectors are coplanar.\\n\\nParameters\\n==========\\n\\nq1\\n    A pure Quaternion.\\nq2\\n    A pure Quaternion.\\nq3\\n    A pure Quaternion.\\n\\nReturns\\n=======\\n\\nTrue : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar.\\nFalse : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are not coplanar.\\nNone : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar is unknown.\\n\\nExamples\\n========\\n\\n>>> from sympy.algebras.quaternion import Quaternion\\n>>> q1 = Quaternion(0, 4, 4, 4)\\n>>> q2 = Quaternion(0, 8, 8, 8)\\n>>> q3 = Quaternion(0, 24, 24, 24)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nTrue\\n\\n>>> q1 = Quaternion(0, 8, 16, 8)\\n>>> q2 = Quaternion(0, 8, 3, 12)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nFalse\\n\\nSee Also\\n========\\n\\naxis\\nis_pure',\n",
       " \"Convert SymPy's expression to ``dtype``. \",\n",
       " 'Construct new raw ``RootSum`` instance. ',\n",
       " 'Prepend a minus sign to a pretty form. ',\n",
       " 'Equality as defined by https://tools.ietf.org/html/rfc5280#section-7.1\\n\\n:param other:\\n    Another RDNSequence object\\n\\n:return:\\n    A boolean',\n",
       " 'This extension is used to prevent mapping of the any policy to\\nspecific requirements\\n\\n:return:\\n    None or a Integer object',\n",
       " 'Builds a scope and request body into a WSGI environ object.',\n",
       " 'Dispatch on engine function decorator.',\n",
       " \"Get Pandas DataFrame of tables filtered by a search string.\\n\\nParameters\\n----------\\ntext\\n    Select only tables with the given string in table's properties.\\ncatalog_id\\n    The ID of the Data Catalog from which to retrieve Databases.\\n    If ``None`` is provided, the AWS account ID is used by default.\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** receive ``None``.\\n\\nReturns\\n-------\\n    Iterator of tables.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> df_tables = wr.catalog.search_tables(text='my_property')\",\n",
       " 'Write records stored in a DataFrame into Microsoft SQL Server.\\n\\nParameters\\n----------\\ndf\\n    Pandas DataFrame https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\\ncon\\n    Use pyodbc.connect() to use credentials directly or wr.sqlserver.connect() to fetch it from the Glue Catalog.\\ntable\\n    Table name\\nschema\\n    Schema name\\nmode\\n    Append, overwrite or upsert.\\n\\n    - append: Inserts new records into table.\\n    - overwrite: Drops table and recreates.\\n    - upsert: Perform an upsert which checks for conflicts on columns given by ``upsert_conflict_columns`` and sets the new values on conflicts. Note that column names of the Dataframe will be used for this operation, as if ``use_column_names`` was set to True.\\n\\nindex\\n    True to store the DataFrame index as a column in the table,\\n    otherwise False to ignore it.\\ndtype\\n    Dictionary of columns names and Microsoft SQL Server types to be casted.\\n    Useful when you have columns with undetermined or mixed data types.\\n    (e.g. {\\'col name\\': \\'TEXT\\', \\'col2 name\\': \\'FLOAT\\'})\\nvarchar_lengths\\n    Dict of VARCHAR length by columns. (e.g. {\"col1\": 10, \"col5\": 200}).\\nuse_column_names\\n    If set to True, will use the column names of the DataFrame for generating the INSERT SQL Query.\\n    E.g. If the DataFrame has two columns `col1` and `col3` and `use_column_names` is True, data will only be\\n    inserted into the database columns `col1` and `col3`.\\nuspert_conflict_columns\\n    List of columns to be used as conflict columns in the upsert operation.\\nchunksize\\n    Number of rows which are inserted with each SQL query. Defaults to inserting 200 rows per query.\\nfast_executemany\\n    Mode of execution which greatly reduces round trips for a DBAPI executemany() call when using\\n    Microsoft ODBC drivers, for limited size batches that fit in memory. `False` by default.\\n\\n    https://github.com/mkleehammer/pyodbc/wiki/Cursor#executemanysql-params-with-fast_executemanytrue\\n\\n    Note: when using this mode, pyodbc converts the Python parameter values to their ODBC \"C\" equivalents,\\n    based on the target column types in the database which may lead to subtle data type conversion\\n    differences depending on whether fast_executemany is True or False.\\n\\nExamples\\n--------\\nWriting to Microsoft SQL Server using a Glue Catalog Connections\\n\\n>>> import awswrangler as wr\\n>>> with wr.sqlserver.connect(connection=\"MY_GLUE_CONNECTION\", odbc_driver_version=17) as con:\\n...     wr.sqlserver.to_sql(\\n...         df=df,\\n...         table=\"table\",\\n...         schema=\"dbo\",\\n...         con=con\\n...     )',\n",
       " 'Get a list of available services that can be loaded as resource\\nclients via :py:meth:`Session.resource`.\\n\\n:rtype: list\\n:return: List of service names',\n",
       " \"Convert and validate a value against the option's\\n:attr:`type`, :attr:`multiple`, and :attr:`nargs`.\",\n",
       " 'Reset all resolver configuration to the defaults.',\n",
       " 'The coefficient of kinetic friction.',\n",
       " 'Call the optimize tours method over HTTP.\\n\\nArgs:\\n    request (~.route_optimization_service.OptimizeToursRequest):\\n        The request object. Request to be given to a tour\\n    optimization solver which defines the\\n    shipment model to solve as well as\\n    optimization parameters.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.route_optimization_service.OptimizeToursResponse:\\n        Response after solving a tour\\n    optimization problem containing the\\n    routes followed by each vehicle, the\\n    shipments which have been skipped and\\n    the overall cost of the solution.',\n",
       " 'Tests against python glob to check if our posix tests are accurate.',\n",
       " 'Make a blank PluginOptions, mostly used for tests.',\n",
       " 'Handles an HTTP exception.  By default this will invoke the\\nregistered error handlers and fall back to returning the\\nexception as response.\\n\\n.. versionchanged:: 1.0.3\\n    ``RoutingException``, used internally for actions such as\\n     slash redirects during routing, is not passed to error\\n     handlers.\\n\\n.. versionchanged:: 1.0\\n    Exceptions are looked up by code *and* by MRO, so\\n    ``HTTPException`` subclasses can be handled with a catch-all\\n    handler for the base ``HTTPException``.\\n\\n.. versionadded:: 0.3',\n",
       " 'Check a ref name for validity.\\n\\nThis is based on the rules described in :manpage:`git-check-ref-format(1)`.',\n",
       " 'API to retrieve a list of ``TaxonomyCategory`` objects.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.ads import admanager_v1\\n\\n    def sample_list_taxonomy_categories():\\n        # Create a client\\n        client = admanager_v1.TaxonomyCategoryServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = admanager_v1.ListTaxonomyCategoriesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_taxonomy_categories(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.ads.admanager_v1.types.ListTaxonomyCategoriesRequest, dict]):\\n        The request object. Request object for ``ListTaxonomyCategories`` method.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        TaxonomyCategories. Format: ``networks/{network_code}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.ads.admanager_v1.services.taxonomy_category_service.pagers.ListTaxonomyCategoriesPager:\\n        Response object for ListTaxonomyCategoriesRequest containing matching\\n           TaxonomyCategory objects.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the update google ads link method over gRPC.\\n\\nUpdates a GoogleAdsLink on a property\\n\\nReturns:\\n    Callable[[~.UpdateGoogleAdsLinkRequest],\\n            ~.GoogleAdsLink]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the update channel group method over gRPC.\\n\\nUpdates a ChannelGroup.\\n\\nReturns:\\n    Callable[[~.UpdateChannelGroupRequest],\\n            ~.ChannelGroup]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list display video360\\nadvertiser links method over HTTP.\\n\\n    Args:\\n        request (~.analytics_admin.ListDisplayVideo360AdvertiserLinksRequest):\\n            The request object. Request message for\\n        ListDisplayVideo360AdvertiserLinks RPC.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.analytics_admin.ListDisplayVideo360AdvertiserLinksResponse:\\n            Response message for\\n        ListDisplayVideo360AdvertiserLinks RPC.',\n",
       " 'Pre-rpc interceptor for list_participant_sessions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ConferenceRecordsService server.',\n",
       " 'Return a callable for the list versions method over gRPC.\\n\\nList API versions of an API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.ListVersionsRequest],\\n            ~.ListVersionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the create external api method over gRPC.\\n\\nCreate an External API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.CreateExternalApiRequest],\\n            ~.ExternalApi]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.apphub_service.CreateServiceRequest):\\n        The request object. Request for CreateService.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsStoragePoolRequest):\\n        The request object. A request message for\\n    StoragePools.TestIamPermissions. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'Searches for documents using provided\\n[SearchDocumentsRequest][google.cloud.contentwarehouse.v1.SearchDocumentsRequest].\\nThis call only returns documents that the caller has permission\\nto search against.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contentwarehouse_v1\\n\\n    def sample_search_documents():\\n        # Create a client\\n        client = contentwarehouse_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = contentwarehouse_v1.SearchDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_documents(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contentwarehouse_v1.types.SearchDocumentsRequest, dict]):\\n        The request object. Request message for\\n        DocumentService.SearchDocuments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        documents. Format:\\n        projects/{project_number}/locations/{location}.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.contentwarehouse_v1.services.document_service.pagers.SearchDocumentsPager:\\n        Response message for\\n        DocumentService.SearchDocuments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Pre-rpc interceptor for batch_update_entities\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the EntityTypes server.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Parses a session path into its component segments.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Return a callable for the initialize encryption spec method over gRPC.\\n\\nInitializes a location-level encryption key\\nspecification.  An error will be thrown if the location\\nhas resources already created before the initialization.\\nOnce the encryption specification is initialized at a\\nlocation, it is immutable and all newly created\\nresources under the location will be encrypted with the\\nexisting specification.\\n\\nReturns:\\n    Callable[[~.InitializeEncryptionSpecRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the update intent method over gRPC.\\n\\nUpdates the specified intent.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\nReturns:\\n    Callable[[~.UpdateIntentRequest],\\n            ~.Intent]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the streaming analyze content method over gRPC.\\n\\nAdds a text (e.g., chat) or audio (e.g., phone recording)\\nmessage from a participant into the conversation. Note: This\\nmethod is only available through the gRPC API (not REST).\\n\\nThe top-level message sent to the client by the server is\\n``StreamingAnalyzeContentResponse``. Multiple response messages\\ncan be returned in order. The first one or more messages contain\\nthe ``recognition_result`` field. Each result represents a more\\ncomplete transcript of what the user said. The next message\\ncontains the ``reply_text`` field, and potentially the\\n``reply_audio`` and/or the ``automated_agent_reply`` fields.\\n\\nNote: Always use agent versions for production traffic sent to\\nvirtual agents. See `Versions and\\nenvironments <https://cloud.google.com/dialogflow/es/docs/agents-versions>`__.\\n\\nReturns:\\n    Callable[[~.StreamingAnalyzeContentRequest],\\n            ~.StreamingAnalyzeContentResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    EventarcAsyncClient: The constructed client.',\n",
       " 'jsonpath : fields_or_any',\n",
       " 'Returns a list of services that allow you to opt into audit logs\\nthat are not generated by default.\\n\\nTo learn more about audit logs, see the `Logging\\ndocumentation <https://cloud.google.com/logging/docs/audit>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iam_admin_v1\\n\\n    def sample_query_auditable_services():\\n        # Create a client\\n        client = iam_admin_v1.IAMClient()\\n\\n        # Initialize request argument(s)\\n        request = iam_admin_v1.QueryAuditableServicesRequest(\\n        )\\n\\n        # Make the request\\n        response = client.query_auditable_services(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iam_admin_v1.types.QueryAuditableServicesRequest, dict]):\\n        The request object. A request to get the list of\\n        auditable services for a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iam_admin_v1.types.QueryAuditableServicesResponse:\\n        A response containing a list of\\n        auditable services for a resource.',\n",
       " 'Post-rpc interceptor for delete_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the IDS server but before\\nit is returned to user code.',\n",
       " 'Call the create ekm connection method over HTTP.\\n\\nArgs:\\n    request (~.ekm_service.CreateEkmConnectionRequest):\\n        The request object. Request message for\\n    [EkmService.CreateEkmConnection][google.cloud.kms.v1.EkmService.CreateEkmConnection].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.ekm_service.EkmConnection:\\n        An [EkmConnection][google.cloud.kms.v1.EkmConnection]\\n    represents an individual EKM connection. It can be used\\n    for creating [CryptoKeys][google.cloud.kms.v1.CryptoKey]\\n    and\\n    [CryptoKeyVersions][google.cloud.kms.v1.CryptoKeyVersion]\\n    with a\\n    [ProtectionLevel][google.cloud.kms.v1.ProtectionLevel]\\n    of\\n    [EXTERNAL_VPC][CryptoKeyVersion.ProtectionLevel.EXTERNAL_VPC],\\n    as well as performing cryptographic operations using\\n    keys created within the\\n    [EkmConnection][google.cloud.kms.v1.EkmConnection].',\n",
       " 'Pre-rpc interceptor for get_crypto_key\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the KeyManagementService server.',\n",
       " 'Post-rpc interceptor for revoke_certificate\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'Post-rpc interceptor for delete_collector\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the RapidMigrationAssessment server but before\\nit is returned to user code.',\n",
       " 'Initiates a failover of the primary node to current\\nreplica node for a specific STANDARD tier Cloud\\nMemorystore for Redis instance.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import redis_v1beta1\\n\\n    def sample_failover_instance():\\n        # Create a client\\n        client = redis_v1beta1.CloudRedisClient()\\n\\n        # Initialize request argument(s)\\n        request = redis_v1beta1.FailoverInstanceRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.failover_instance(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.redis_v1beta1.types.FailoverInstanceRequest, dict]):\\n        The request object. Request for\\n        [Failover][google.cloud.redis.v1beta1.CloudRedis.FailoverInstance].\\n    name (str):\\n        Required. Redis instance resource name using the form:\\n        ``projects/{project_id}/locations/{location_id}/instances/{instance_id}``\\n        where ``location_id`` refers to a GCP region.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    data_protection_mode (google.cloud.redis_v1beta1.types.FailoverInstanceRequest.DataProtectionMode):\\n        Optional. Available data protection modes that the user\\n        can choose. If it\\'s unspecified, data protection mode\\n        will be LIMITED_DATA_LOSS by default.\\n\\n        This corresponds to the ``data_protection_mode`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.redis_v1beta1.types.Instance` A\\n        Memorystore for Redis instance.',\n",
       " 'Pre-rpc interceptor for set_default_branch\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CatalogService server.',\n",
       " 'Gets metadata of a repository.\\n\\n**Host: Data Plane**\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securesourcemanager_v1\\n\\n    def sample_get_repository():\\n        # Create a client\\n        client = securesourcemanager_v1.SecureSourceManagerClient()\\n\\n        # Initialize request argument(s)\\n        request = securesourcemanager_v1.GetRepositoryRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_repository(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securesourcemanager_v1.types.GetRepositoryRequest, dict]):\\n        The request object. GetRepositoryRequest is the request\\n        for getting a repository.\\n    name (str):\\n        Required. Name of the repository to retrieve. The format\\n        is\\n        ``projects/{project_number}/locations/{location_id}/repositories/{repository_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securesourcemanager_v1.types.Repository:\\n        Metadata of a Secure Source Manager\\n        repository.',\n",
       " 'Return a callable for the delete service method over gRPC.\\n\\nDeletes a service. This also deletes all endpoints\\nassociated with the service.\\n\\nReturns:\\n    Callable[[~.DeleteServiceRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns a fully-qualified folder string.',\n",
       " 'Return a callable for the list queues method over gRPC.\\n\\nLists queues.\\n\\nQueues are returned in lexicographical order.\\n\\nReturns:\\n    Callable[[~.ListQueuesRequest],\\n            ~.ListQueuesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " \"Call the update migrating vm method over HTTP.\\n\\nArgs:\\n    request (~.vmmigration.UpdateMigratingVmRequest):\\n        The request object. Request message for\\n    'UpdateMigratingVm' request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.\",\n",
       " 'Return a callable for the update vmware engine network method over gRPC.\\n\\nModifies a VMware Engine network resource. Only the following\\nfields can be updated: ``description``. Only fields specified in\\n``updateMask`` are applied.\\n\\nReturns:\\n    Callable[[~.UpdateVmwareEngineNetworkRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Sends a single request, without handling any redirections.',\n",
       " 'Yield Traversable objects in self',\n",
       " 'The __init__ file can be searched in a directory. If found return it, else\\nNone.',\n",
       " 'Returns a checksum for the source.',\n",
       " 'Parse multiple statements into a list until one of the end tokens\\nis reached.  This is used to parse the body of statements as it also\\nparses template data if appropriate.  The parser checks first if the\\ncurrent token is a colon and skips it if there is one.  Then it checks\\nfor the block end and parses until if one of the `end_tokens` is\\nreached.  Per default the active token in the stream at the end of\\nthe call is the matched end token.  If this is not wanted `drop_needle`\\ncan be set to `True` and the end token is removed.',\n",
       " 'Get items to delete to keep the store under size, file, & age limits.',\n",
       " 'Schedule a func to be run',\n",
       " 'Specifically, `const` validation applies for Draft 7.',\n",
       " 'Executes core chain rules.',\n",
       " 'Create an instant of an simple tag processor.\\n\\nArguments:\\n    pattern: A regular expression that matches a pattern.\\n    tag: Tag of element.',\n",
       " 'Return the axis label as a Text instance.\\n\\n.. admonition:: Discouraged\\n\\n   This overrides `.Artist.get_label`, which is for legend labels, with a new\\n   semantic. It is recommended to use the attribute ``Axis.label`` instead.',\n",
       " \"Add *tool* to `ToolManager`.\\n\\nIf successful, adds a new event ``tool_trigger_{name}`` where\\n``{name}`` is the *name* of the tool; the event is fired every time the\\ntool is triggered.\\n\\nParameters\\n----------\\nname : str\\n    Name of the tool, treated as the ID, has to be unique.\\ntool : type\\n    Class of the tool to be added.  A subclass will be used\\n    instead if one was registered for the current canvas class.\\n*args, **kwargs\\n    Passed to the *tool*'s constructor.\\n\\nSee Also\\n--------\\nmatplotlib.backend_tools.ToolBase : The base class for tools.\",\n",
       " 'Convert a premultiplied ARGB32 buffer to an unmultiplied RGBA8888 buffer.',\n",
       " \"Set the rotation of the text.\\n\\nParameters\\n----------\\ns : float or {'vertical', 'horizontal'}\\n    The rotation angle in degrees in mathematically positive direction\\n    (counterclockwise). 'horizontal' equals 0, 'vertical' equals 90.\",\n",
       " 'Button release event handler.',\n",
       " 'Encode bson.decimal128.Decimal128.',\n",
       " 'Return internal buffer contents as bytes object',\n",
       " 'Tests for conditions specific to\\nparse_edge_list method',\n",
       " 'Find shortest weighted paths and lengths between all nodes.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\ncutoff : integer or float, optional\\n    Length (sum of edge weights) at which the search is stopped.\\n    If cutoff is provided, only return paths with summed weight <= cutoff.\\n\\nweight : string or function\\n    If this is a string, then edge weights will be accessed via the\\n    edge attribute with this key (that is, the weight of the edge\\n    joining `u` to `v` will be ``G.edge[u][v][weight]``). If no\\n    such edge attribute exists, the weight of the edge is assumed to\\n    be one.\\n\\n    If this is a function, the weight of an edge is the value\\n    returned by the function. The function must accept exactly three\\n    positional arguments: the two endpoints of an edge and the\\n    dictionary of edge attributes for that edge. The function must\\n    return a number or None to indicate a hidden edge.\\n\\nYields\\n------\\n(node, (distance, path)) : (node obj, (dict, dict))\\n    Each source node has two associated dicts. The first holds distance\\n    keyed by target and the second holds paths keyed by target.\\n    (See single_source_dijkstra for the source/target node terminology.)\\n    If desired you can apply `dict()` to this function to create a dict\\n    keyed by source node to the two dicts.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(5)\\n>>> len_path = dict(nx.all_pairs_dijkstra(G))\\n>>> len_path[3][0][1]\\n2\\n>>> for node in [0, 1, 2, 3, 4]:\\n...     print(f\"3 - {node}: {len_path[3][0][node]}\")\\n3 - 0: 3\\n3 - 1: 2\\n3 - 2: 1\\n3 - 3: 0\\n3 - 4: 1\\n>>> len_path[3][1][1]\\n[3, 2, 1]\\n>>> for n, (dist, path) in nx.all_pairs_dijkstra(G):\\n...     print(path[1])\\n[0, 1]\\n[1]\\n[2, 1]\\n[3, 2, 1]\\n[4, 3, 2, 1]\\n\\nNotes\\n-----\\nEdge weight attributes must be numerical.\\nDistances are calculated as sums of weighted edges traversed.\\n\\nThe yielded dicts only have keys for reachable nodes.',\n",
       " 'Converts a SciPy sparse array in **Coordinate** format to an iterable\\nof weighted edge triples.',\n",
       " 'get_dirs_from_args() skips over non-existing directories and files',\n",
       " 'Check if file is in free format Fortran.',\n",
       " 'Returns the dtype unchanged if it contained no metadata or a copy of the\\ndtype if it (or any of its structure dtypes) contained metadata.\\n\\nThis utility is used by `np.save` and `np.savez` to drop metadata before\\nsaving.\\n\\n.. note::\\n\\n    Due to its limitation this function may move to a more appropriate\\n    home or change in the future and is considered semi-public API only.\\n\\n.. warning::\\n\\n    This function does not preserve more strange things like record dtypes\\n    and user dtypes may simply return the wrong thing.  If you need to be\\n    sure about the latter, check the result with:\\n    ``np.can_cast(new_dtype, dtype, casting=\"no\")``.',\n",
       " 'Decrypts AES/RC4/RC2/3DES/DES ciphertext via CryptoAPI\\n\\n:param cipher:\\n    A unicode string of \"aes\", \"des\", \"tripledes_2key\", \"tripledes_3key\",\\n    \"rc2\", \"rc4\"\\n\\n:param key:\\n    The encryption key - a byte string 5-16 bytes long\\n\\n:param data:\\n    The ciphertext - a byte string\\n\\n:param iv:\\n    The initialization vector - a byte string - unused for RC4\\n\\n:param padding:\\n    Boolean, if padding should be used - unused for RC4\\n\\n:raises:\\n    ValueError - when any of the parameters contain an invalid value\\n    TypeError - when any of the parameters are of the wrong type\\n    OSError - when an error is returned by the OS crypto library\\n\\n:return:\\n    A byte string of the plaintext',\n",
       " 'Numpy version of itertools.product.\\nSometimes faster (for large inputs)...\\n\\nParameters\\n----------\\nX : list-like of list-likes\\n\\nReturns\\n-------\\nproduct : list of ndarrays\\n\\nExamples\\n--------\\n>>> cartesian_product([list(\"ABC\"), [1, 2]])\\n[array([\\'A\\', \\'A\\', \\'B\\', \\'B\\', \\'C\\', \\'C\\'], dtype=\\'<U1\\'), array([1, 2, 1, 2, 1, 2])]\\n\\nSee Also\\n--------\\nitertools.product : Cartesian product of input iterables.  Equivalent to\\n    nested for-loops.',\n",
       " 'compute and set our version',\n",
       " 'Return a Timezone instance given its name.',\n",
       " 'specifier_qualifier_list    : specifier_qualifier_list type_specifier_no_typeid\\n        ',\n",
       " 'struct_declaration : pppragma_directive\\n        ',\n",
       " ':param e: key or index of element on value\\n:return: raw values for element if self._items is dict and contain needed element',\n",
       " ':calls: `GET /projects/columns/{column_id} <https://docs.github.com/en/rest/reference/projects#get-a-project-column>`_',\n",
       " ':type: string',\n",
       " 'Parse and handle errors of a toml configuration file.\\n\\nRaises ``tomllib.TOMLDecodeError``.',\n",
       " 'This is a numpy docstring.\\n\\nRaises\\n------\\n~re.error\\n    Sometimes',\n",
       " 'Name used safely inside the loop.',\n",
       " 'Build an API representation of this object.\\n\\nReturns:\\n    Dict[str, Any]:\\n        A dictionary in the format used by the BigQuery API.',\n",
       " 'Read everything up to one of the chars in endchars.\\n\\nThis is outside the formal grammar.  The InvalidMailbox TokenList that is\\nreturned acts like a Mailbox, but the data attributes are None.',\n",
       " 'Reads the robots.txt URL and feeds it to the parser.',\n",
       " 'Tests whether the fixer_util.is_encoding_comment() function is working.',\n",
       " 'Issue #43: Is shebang line preserved as the first\\nline by futurize when followed by a docstring?',\n",
       " 'Tests whether itertools.zip_longest is available.',\n",
       " \"Retrieve timestamp at which the object's retention period expires.\\n\\nSee https://cloud.google.com/storage/docs/json_api/v1/objects\\n\\n:rtype: :class:`datetime.datetime` or ``NoneType``\\n:returns: Datetime object parsed from RFC3339 valid timestamp, or\\n          ``None`` if the property is not set locally.\",\n",
       " \"read_namespaced_network_policy  # noqa: E501\\n\\nread the specified NetworkPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.read_namespaced_network_policy_with_http_info(name, namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the NetworkPolicy (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1NetworkPolicy, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Sets the resource_rules of this V1MatchResources.\\n\\nResourceRules describes what operations on what resources/subresources the ValidatingAdmissionPolicy matches. The policy cares about an operation if it matches _any_ Rule.  # noqa: E501\\n\\n:param resource_rules: The resource_rules of this V1MatchResources.  # noqa: E501\\n:type: list[V1NamedRuleWithOperations]',\n",
       " 'Sets the config_source of this V1NodeSpec.\\n\\n\\n:param config_source: The config_source of this V1NodeSpec.  # noqa: E501\\n:type: V1NodeConfigSource',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Sets the max_surge of this V1RollingUpdateDaemonSet.\\n\\nThe maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up to a minimum of 1. Default value is 0. Example: when this is set to 30%, at most 30% of the total number of nodes that should be running the daemon pod (i.e. status.desiredNumberScheduled) can have their a new pod created before the old pod is marked as deleted. The update starts by launching new pods on 30% of nodes. Once an updated pod is available (Ready for at least minReadySeconds) the old DaemonSet pod on that node is marked deleted. If the old pod becomes unavailable for any reason (Ready transitions to false, is evicted, or is drained) an updated pod is immediatedly created on that node without considering surge limits. Allowing surge implies the possibility that the resources consumed by the daemonset on any given node can double if the readiness check fails, and so resource intensive daemonsets should take into account that they may cause evictions during disruption.  # noqa: E501\\n\\n:param max_surge: The max_surge of this V1RollingUpdateDaemonSet.  # noqa: E501\\n:type: object',\n",
       " 'Return a dictionary of memory stats\\n\\nFor more information see https://redis.io/commands/memory-stats',\n",
       " \"Increment a bitfield by a given amount.\\n:param fmt: format-string for the bitfield being updated, e.g. 'u8'\\n    for an unsigned 8-bit integer.\\n:param offset: offset (in number of bits). If prefixed with a\\n    '#', this is an offset multiplier, e.g. given the arguments\\n    fmt='u8', offset='#2', the offset will be 16.\\n:param int increment: value to increment the bitfield by.\\n:param str overflow: overflow algorithm. Defaults to WRAP, but other\\n    acceptable values are SAT and FAIL. See the Redis docs for\\n    descriptions of these algorithms.\\n:returns: a :py:class:`BitFieldOperation` instance.\",\n",
       " 'Placeholder docstring',\n",
       " 'Ingest a single Dataframe row into FeatureStore.\\n\\nArgs:\\n    data_frame (DataFrame): source DataFrame to be ingested.\\n    row (Iterable[tuple[Any, ...]]): current row that is being ingested\\n    feature_group_name (str): name of the Feature Group.\\n    feature_definitions (Dict[str, Dict[Any, Any]]):  dictionary of feature definitions.\\n        where the key is the feature name and the value is the FeatureDefinition.\\n        The FeatureDefinition contains the data type of the feature.\\n    sagemaker_fs_runtime_client (Session): session instance to perform boto calls.\\n    failed_rows (List[int]): list of indices from the data frame for which ingestion failed.\\n    target_stores (Sequence[TargetStoreEnum]): stores to be used for ingestion.\\n\\n\\nReturns:\\n    int of row indices that failed to be ingested.',\n",
       " 'Retrieves instance types GPU info of the given region.\\n\\nArgs:\\n    region (str): The AWS region.\\n\\nReturns:\\n    dict[str, dict[str, int]]: A dictionary that contains instance types as keys\\n                               and GPU info as values or empty dictionary if the\\n                               config for the given region is not found.\\n\\nRaises:\\n    ValueError: If no config found.',\n",
       " 'Return ``True`` if the given instance has locally\\nmodified attributes.\\n\\n.. container:: class_bases\\n\\n    Proxied for the :class:`_orm.Session` class on\\n    behalf of the :class:`_orm.scoping.scoped_session` class.\\n\\nThis method retrieves the history for each instrumented\\nattribute on the instance and performs a comparison of the current\\nvalue to its previously flushed or committed value, if any.\\n\\nIt is in effect a more expensive and accurate\\nversion of checking for the given instance in the\\n:attr:`.Session.dirty` collection; a full test for\\neach attribute\\'s net \"dirty\" status is performed.\\n\\nE.g.::\\n\\n    return session.is_modified(someobject)\\n\\nA few caveats to this method apply:\\n\\n* Instances present in the :attr:`.Session.dirty` collection may\\n  report ``False`` when tested with this method.  This is because\\n  the object may have received change events via attribute mutation,\\n  thus placing it in :attr:`.Session.dirty`, but ultimately the state\\n  is the same as that loaded from the database, resulting in no net\\n  change here.\\n* Scalar attributes may not have recorded the previously set\\n  value when a new value was applied, if the attribute was not loaded,\\n  or was expired, at the time the new value was received - in these\\n  cases, the attribute is assumed to have a change, even if there is\\n  ultimately no net change against its database value. SQLAlchemy in\\n  most cases does not need the \"old\" value when a set event occurs, so\\n  it skips the expense of a SQL call if the old value isn\\'t present,\\n  based on the assumption that an UPDATE of the scalar value is\\n  usually needed, and in those few cases where it isn\\'t, is less\\n  expensive on average than issuing a defensive SELECT.\\n\\n  The \"old\" value is fetched unconditionally upon set only if the\\n  attribute container has the ``active_history`` flag set to ``True``.\\n  This flag is set typically for primary key attributes and scalar\\n  object references that are not a simple many-to-one.  To set this\\n  flag for any arbitrary mapped column, use the ``active_history``\\n  argument with :func:`.column_property`.\\n\\n:param instance: mapped instance to be tested for pending changes.\\n:param include_collections: Indicates if multivalued collections\\n should be included in the operation.  Setting this to ``False`` is a\\n way to detect only local-column based properties (i.e. scalar columns\\n or many-to-one foreign keys) that would result in an UPDATE for this\\n instance upon flush.',\n",
       " 'Initializes a `DelayedReturn` object.\\n\\nArgs:\\n    function_step: A `sagemaker.workflow.step._FunctionStep` instance.\\n    reference_path: A tuple that represents the path to the child member.',\n",
       " 'For one-to-many collections, produce a :class:`_dml.Insert` which\\nwill insert new rows in terms of this this instance-local\\n:class:`_orm.WriteOnlyCollection`.\\n\\nThis construct is only supported for a :class:`_orm.Relationship`\\nthat does **not** include the :paramref:`_orm.relationship.secondary`\\nparameter.  For relationships that refer to a many-to-many table,\\nuse ordinary bulk insert techniques to produce new objects, then\\nuse :meth:`_orm.AbstractCollectionWriter.add_all` to associate them\\nwith the collection.',\n",
       " 'Check scores attribute shape',\n",
       " 'Generate (train, test) indices',\n",
       " 'Perform spherical Voronoi calculation, but not the sorting of\\nvertices in the Voronoi polygons.',\n",
       " \"Compute the determinant of a matrix\\n\\nThe determinant is a scalar that is a function of the associated square\\nmatrix coefficients. The determinant value is zero for singular matrices.\\n\\nParameters\\n----------\\na : (..., M, M) array_like\\n    Input array to compute determinants for.\\noverwrite_a : bool, optional\\n    Allow overwriting data in a (may enhance performance).\\ncheck_finite : bool, optional\\n    Whether to check that the input matrix contains only finite numbers.\\n    Disabling may give a performance gain, but may result in problems\\n    (crashes, non-termination) if the inputs do contain infinities or NaNs.\\n\\nReturns\\n-------\\ndet : (...) float or complex\\n    Determinant of `a`. For stacked arrays, a scalar is returned for each\\n    (m, m) slice in the last two dimensions of the input. For example, an\\n    input of shape (p, q, m, m) will produce a result of shape (p, q). If\\n    all dimensions are 1 a scalar is returned regardless of ndim.\\n\\nNotes\\n-----\\nThe determinant is computed by performing an LU factorization of the\\ninput with LAPACK routine 'getrf', and then calculating the product of\\ndiagonal entries of the U factor.\\n\\nEven if the input array is single precision (float32 or complex64), the\\nresult will be returned in double precision (float64 or complex128) to\\nprevent overflows.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy import linalg\\n>>> a = np.array([[1,2,3], [4,5,6], [7,8,9]])  # A singular matrix\\n>>> linalg.det(a)\\n0.0\\n>>> b = np.array([[0,2,3], [4,5,6], [7,8,9]])\\n>>> linalg.det(b)\\n3.0\\n>>> # An array with the shape (3, 2, 2, 2)\\n>>> c = np.array([[[[1., 2.], [3., 4.]],\\n...                [[5., 6.], [7., 8.]]],\\n...               [[[9., 10.], [11., 12.]],\\n...                [[13., 14.], [15., 16.]]],\\n...               [[[17., 18.], [19., 20.]],\\n...                [[21., 22.], [23., 24.]]]])\\n>>> linalg.det(c)  # The resulting shape is (3, 2)\\narray([[-2., -2.],\\n       [-2., -2.],\\n       [-2., -2.]])\\n>>> linalg.det(c[0, 0])  # Confirm the (0, 0) slice, [[1, 2], [3, 4]]\\n-2.0\",\n",
       " 'Butterworth filter order selection.\\n\\nReturn the order of the lowest order digital or analog Butterworth filter\\nthat loses no more than `gpass` dB in the passband and has at least\\n`gstop` dB attenuation in the stopband.\\n\\nParameters\\n----------\\nwp, ws : float\\n    Passband and stopband edge frequencies.\\n\\n    For digital filters, these are in the same units as `fs`. By default,\\n    `fs` is 2 half-cycles/sample, so these are normalized from 0 to 1,\\n    where 1 is the Nyquist frequency. (`wp` and `ws` are thus in\\n    half-cycles / sample.) For example:\\n\\n        - Lowpass:   wp = 0.2,          ws = 0.3\\n        - Highpass:  wp = 0.3,          ws = 0.2\\n        - Bandpass:  wp = [0.2, 0.5],   ws = [0.1, 0.6]\\n        - Bandstop:  wp = [0.1, 0.6],   ws = [0.2, 0.5]\\n\\n    For analog filters, `wp` and `ws` are angular frequencies (e.g., rad/s).\\ngpass : float\\n    The maximum loss in the passband (dB).\\ngstop : float\\n    The minimum attenuation in the stopband (dB).\\nanalog : bool, optional\\n    When True, return an analog filter, otherwise a digital filter is\\n    returned.\\nfs : float, optional\\n    The sampling frequency of the digital system.\\n\\n    .. versionadded:: 1.2.0\\n\\nReturns\\n-------\\nord : int\\n    The lowest order for a Butterworth filter which meets specs.\\nwn : ndarray or float\\n    The Butterworth natural frequency (i.e. the \"3dB frequency\"). Should\\n    be used with `butter` to give filter results. If `fs` is specified,\\n    this is in the same units, and `fs` must also be passed to `butter`.\\n\\nSee Also\\n--------\\nbutter : Filter design using order and critical points\\ncheb1ord : Find order and critical points from passband and stopband spec\\ncheb2ord, ellipord\\niirfilter : General filter design using order and critical frequencies\\niirdesign : General filter design using passband and stopband spec\\n\\nExamples\\n--------\\nDesign an analog bandpass filter with passband within 3 dB from 20 to\\n50 rad/s, while rejecting at least -40 dB below 14 and above 60 rad/s.\\nPlot its frequency response, showing the passband and stopband\\nconstraints in gray.\\n\\n>>> from scipy import signal\\n>>> import matplotlib.pyplot as plt\\n>>> import numpy as np\\n\\n>>> N, Wn = signal.buttord([20, 50], [14, 60], 3, 40, True)\\n>>> b, a = signal.butter(N, Wn, \\'band\\', True)\\n>>> w, h = signal.freqs(b, a, np.logspace(1, 2, 500))\\n>>> plt.semilogx(w, 20 * np.log10(abs(h)))\\n>>> plt.title(\\'Butterworth bandpass filter fit to constraints\\')\\n>>> plt.xlabel(\\'Frequency [rad/s]\\')\\n>>> plt.ylabel(\\'Amplitude [dB]\\')\\n>>> plt.grid(which=\\'both\\', axis=\\'both\\')\\n>>> plt.fill([1,  14,  14,   1], [-40, -40, 99, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.fill([20, 20,  50,  50], [-99, -3, -3, -99], \\'0.9\\', lw=0) # pass\\n>>> plt.fill([60, 60, 1e9, 1e9], [99, -40, -40, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.axis([10, 100, -60, 3])\\n>>> plt.show()',\n",
       " 'Identity matrix in sparse format\\n\\nReturns an identity matrix with shape (n,n) using a given\\nsparse format and dtype. This differs from `eye_array` in\\nthat it has a square shape with ones only on the main diagonal.\\nIt is thus the multiplicative identity. `eye_array` allows\\nrectangular shapes and the diagonal can be offset from the main one.\\n\\n.. warning::\\n\\n    This function returns a sparse matrix -- not a sparse array.\\n    You are encouraged to use ``eye_array`` to take advantage\\n    of the sparse array functionality.\\n\\nParameters\\n----------\\nn : int\\n    Shape of the identity matrix.\\ndtype : dtype, optional\\n    Data type of the matrix\\nformat : str, optional\\n    Sparse format of the result, e.g., format=\"csr\", etc.\\n\\nExamples\\n--------\\n>>> import scipy as sp\\n>>> sp.sparse.identity(3).toarray()\\narray([[ 1.,  0.,  0.],\\n       [ 0.,  1.,  0.],\\n       [ 0.,  0.,  1.]])\\n>>> sp.sparse.identity(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse matrix of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>\\n>>> sp.sparse.eye_array(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse array of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>',\n",
       " 'Partial singular value decomposition of a sparse matrix using ARPACK.\\n\\nCompute the largest or smallest `k` singular values and corresponding\\nsingular vectors of a sparse matrix `A`. The order in which the singular\\nvalues are returned is not guaranteed.\\n\\nIn the descriptions below, let ``M, N = A.shape``.\\n\\nParameters\\n----------\\nA : sparse matrix or LinearOperator\\n    Matrix to decompose.\\nk : int, optional\\n    Number of singular values and singular vectors to compute.\\n    Must satisfy ``1 <= k <= min(M, N) - 1``.\\n    Default is 6.\\nncv : int, optional\\n    The number of Lanczos vectors generated.\\n    The default is ``min(n, max(2*k + 1, 20))``.\\n    If specified, must satisfy ``k + 1 < ncv < min(M, N)``; ``ncv > 2*k``\\n    is recommended.\\ntol : float, optional\\n    Tolerance for singular values. Zero (default) means machine precision.\\nwhich : {\\'LM\\', \\'SM\\'}\\n    Which `k` singular values to find: either the largest magnitude (\\'LM\\')\\n    or smallest magnitude (\\'SM\\') singular values.\\nv0 : ndarray, optional\\n    The starting vector for iteration:\\n    an (approximate) left singular vector if ``N > M`` and a right singular\\n    vector otherwise. Must be of length ``min(M, N)``.\\n    Default: random\\nmaxiter : int, optional\\n    Maximum number of Arnoldi update iterations allowed;\\n    default is ``min(M, N) * 10``.\\nreturn_singular_vectors : {True, False, \"u\", \"vh\"}\\n    Singular values are always computed and returned; this parameter\\n    controls the computation and return of singular vectors.\\n\\n    - ``True``: return singular vectors.\\n    - ``False``: do not return singular vectors.\\n    - ``\"u\"``: if ``M <= N``, compute only the left singular vectors and\\n      return ``None`` for the right singular vectors. Otherwise, compute\\n      all singular vectors.\\n    - ``\"vh\"``: if ``M > N``, compute only the right singular vectors and\\n      return ``None`` for the left singular vectors. Otherwise, compute\\n      all singular vectors.\\n\\nsolver :  {\\'arpack\\', \\'propack\\', \\'lobpcg\\'}, optional\\n        This is the solver-specific documentation for ``solver=\\'arpack\\'``.\\n        :ref:`\\'lobpcg\\' <sparse.linalg.svds-lobpcg>` and\\n        :ref:`\\'propack\\' <sparse.linalg.svds-propack>`\\n        are also supported.\\nrandom_state : {None, int, `numpy.random.Generator`,\\n                `numpy.random.RandomState`}, optional\\n\\n    Pseudorandom number generator state used to generate resamples.\\n\\n    If `random_state` is ``None`` (or `np.random`), the\\n    `numpy.random.RandomState` singleton is used.\\n    If `random_state` is an int, a new ``RandomState`` instance is used,\\n    seeded with `random_state`.\\n    If `random_state` is already a ``Generator`` or ``RandomState``\\n    instance then that instance is used.\\noptions : dict, optional\\n    A dictionary of solver-specific options. No solver-specific options\\n    are currently supported; this parameter is reserved for future use.\\n\\nReturns\\n-------\\nu : ndarray, shape=(M, k)\\n    Unitary matrix having left singular vectors as columns.\\ns : ndarray, shape=(k,)\\n    The singular values.\\nvh : ndarray, shape=(k, N)\\n    Unitary matrix having right singular vectors as rows.\\n\\nNotes\\n-----\\nThis is a naive implementation using ARPACK as an eigensolver\\non ``A.conj().T @ A`` or ``A @ A.conj().T``, depending on which one is more\\nefficient.\\n\\nExamples\\n--------\\nConstruct a matrix ``A`` from singular values and vectors.\\n\\n>>> import numpy as np\\n>>> from scipy.stats import ortho_group\\n>>> from scipy.sparse import csc_matrix, diags\\n>>> from scipy.sparse.linalg import svds\\n>>> rng = np.random.default_rng()\\n>>> orthogonal = csc_matrix(ortho_group.rvs(10, random_state=rng))\\n>>> s = [0.0001, 0.001, 3, 4, 5]  # singular values\\n>>> u = orthogonal[:, :5]         # left singular vectors\\n>>> vT = orthogonal[:, 5:].T      # right singular vectors\\n>>> A = u @ diags(s) @ vT\\n\\nWith only three singular values/vectors, the SVD approximates the original\\nmatrix.\\n\\n>>> u2, s2, vT2 = svds(A, k=3, solver=\\'arpack\\')\\n>>> A2 = u2 @ np.diag(s2) @ vT2\\n>>> np.allclose(A2, A.toarray(), atol=1e-3)\\nTrue\\n\\nWith all five singular values/vectors, we can reproduce the original\\nmatrix.\\n\\n>>> u3, s3, vT3 = svds(A, k=5, solver=\\'arpack\\')\\n>>> A3 = u3 @ np.diag(s3) @ vT3\\n>>> np.allclose(A3, A.toarray())\\nTrue\\n\\nThe singular values match the expected singular values, and the singular\\nvectors are as expected up to a difference in sign.\\n\\n>>> (np.allclose(s3, s) and\\n...  np.allclose(np.abs(u3), np.abs(u.toarray())) and\\n...  np.allclose(np.abs(vT3), np.abs(vT.toarray())))\\nTrue\\n\\nThe singular vectors are also orthogonal.\\n\\n>>> (np.allclose(u3.T @ u3, np.eye(5)) and\\n...  np.allclose(vT3 @ vT3.T, np.eye(5)))\\nTrue',\n",
       " 'Gauss-Jacobi (shifted) quadrature.\\n\\nCompute the sample points and weights for Gauss-Jacobi (shifted)\\nquadrature. The sample points are the roots of the nth degree\\nshifted Jacobi polynomial, :math:`G^{p,q}_n(x)`. These sample\\npoints and weights correctly integrate polynomials of degree\\n:math:`2n - 1` or less over the interval :math:`[0, 1]` with\\nweight function :math:`w(x) = (1 - x)^{p-q} x^{q-1}`. See 22.2.2\\nin [AS]_ for details.\\n\\nParameters\\n----------\\nn : int\\n    quadrature order\\np1 : float\\n    (p1 - q1) must be > -1\\nq1 : float\\n    q1 must be > 0\\nmu : bool, optional\\n    If True, return the sum of the weights, optional.\\n\\nReturns\\n-------\\nx : ndarray\\n    Sample points\\nw : ndarray\\n    Weights\\nmu : float\\n    Sum of the weights\\n\\nSee Also\\n--------\\nscipy.integrate.fixed_quad\\n\\nReferences\\n----------\\n.. [AS] Milton Abramowitz and Irene A. Stegun, eds.\\n    Handbook of Mathematical Functions with Formulas,\\n    Graphs, and Mathematical Tables. New York: Dover, 1972.',\n",
       " 'Type is treated as case insensitive in HTML.',\n",
       " 'Test direction in `iframe`.',\n",
       " 'Model intercept.',\n",
       " 'Standard deviation of the StandardScalerModel.',\n",
       " 'Gets the value of fitLinear or its default value.',\n",
       " 'Returns registered Documenter classes',\n",
       " 'Invalidate mocked modules on sys.modules.',\n",
       " 'Target database must support EXCEPT or equivalent (i.e. MINUS).',\n",
       " \"FastIntFlag still causes elements to be global symbols.\\n\\nWhile we do this and haven't yet changed it, make sure conflicting\\nint values for the same name don't come in.\",\n",
       " 'Inserts *token* before *where*.',\n",
       " 'Returns the class of objects of this category.\\n\\nExamples\\n========\\n\\n>>> from sympy.categories import Object, Category\\n>>> from sympy import FiniteSet\\n>>> A = Object(\"A\")\\n>>> B = Object(\"B\")\\n>>> K = Category(\"K\", FiniteSet(A, B))\\n>>> K.objects\\nClass({Object(\"A\"), Object(\"B\")})',\n",
       " 'Check if ``a`` belongs to this domain. ',\n",
       " \"Convert GMPY's ``mpz`` to ``dtype``. \",\n",
       " \"Convert ``ModularInteger(int)`` to GMPY's ``mpz``. \",\n",
       " 'Return a sparse DomainMatrix representation of *self*.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy import QQ\\n>>> A = DomainMatrix([[1, 0],[0, 2]], (2, 2), QQ)\\n>>> A.rep\\n[[1, 0], [0, 2]]\\n>>> B = A.to_sparse()\\n>>> B.rep\\n{0: {0: 1}, 1: {1: 2}}',\n",
       " 'Return the Smith-Normal form decomposition of matrix `m`.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_decomp\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> a, s, t = smith_normal_decomp(m)\\n>>> assert a == s * m * t',\n",
       " 'Computes the ``m``-th order indefinite integral of ``f`` in ``x_j``. ',\n",
       " 'Return height of the complex isolating interval. ',\n",
       " \"Fateman's GCD benchmark: sparse inputs (deg f ~ vars f) \",\n",
       " 'Returns the roots of characteristic equation of constant coefficient\\nlinear ODE and list of collectterms which is later on used by simplification\\nto use collect on solution.\\n\\nThe parameter `r` is a dict of order:coeff terms, where order is the order of the\\nderivative on each term, and coeff is the coefficient of that derivative.',\n",
       " 'Returns a fully-qualified backup string.',\n",
       " \"Return maximum delimiter priority inside `node`.\\n\\nThis is specific to atoms with contents contained in a pair of parentheses.\\nIf `node` isn't an atom or there are no enclosing parentheses, returns 0.\",\n",
       " 'a => b',\n",
       " 'Returns the default stream encoding if not found.',\n",
       " 'Add a new state to the machine and return it.',\n",
       " 'Classify a node.\\n\\nA node which contains a CNAME or RRSIG(CNAME) is a\\n``NodeKind.CNAME`` node.\\n\\nA node which contains only \"neutral\" types, i.e. types allowed to\\nco-exist with a CNAME, is a ``NodeKind.NEUTRAL`` node.  The neutral\\ntypes are NSEC, NSEC3, KEY, and their associated RRSIGS.  An empty node\\nis also considered neutral.\\n\\nA node which contains some rdataset which is not a CNAME, RRSIG(CNAME),\\nor a neutral type is a a ``NodeKind.REGULAR`` node.  Regular nodes are\\nalso commonly referred to as \"other data\".',\n",
       " 'Register a URL value preprocessor function for all view\\nfunctions in the application. These functions will be called before the\\n:meth:`before_request` functions.\\n\\nThe function can modify the values captured from the matched url before\\nthey are passed to the view. For example, this can be used to pop a\\ncommon language code value and place it in ``g`` rather than pass it to\\nevery view.\\n\\nThe function is passed the endpoint name and values dict. The return\\nvalue is ignored.\\n\\nThis is available on both app and blueprint objects. When used on an app, this\\nis called for every request. When used on a blueprint, this is called for\\nrequests that the blueprint handles. To register with a blueprint and affect\\nevery request, use :meth:`.Blueprint.app_url_value_preprocessor`.',\n",
       " 'Gets configuration metadata about a specific audience list. This\\nmethod can be used to understand an audience list after it has\\nbeen created.\\n\\nSee `Creating an Audience\\nList <https://developers.google.com/analytics/devguides/reporting/data/v1/audience-list-basics>`__\\nfor an introduction to Audience Lists with examples.\\n\\nThis method is available at beta stability at\\n`audienceExports.get <https://developers.google.com/analytics/devguides/reporting/data/v1/rest/v1beta/properties.audienceExports/get>`__.\\nTo give your feedback on this API, complete the `Google\\nAnalytics Audience Export API\\nFeedback <https://forms.gle/EeA5u5LW6PEggtCEA>`__ form.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.analytics import data_v1alpha\\n\\n    def sample_get_audience_list():\\n        # Create a client\\n        client = data_v1alpha.AlphaAnalyticsDataClient()\\n\\n        # Initialize request argument(s)\\n        request = data_v1alpha.GetAudienceListRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_audience_list(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.analytics.data_v1alpha.types.GetAudienceListRequest, dict]):\\n        The request object. A request to retrieve configuration\\n        metadata about a specific audience list.\\n    name (str):\\n        Required. The audience list resource name. Format:\\n        ``properties/{property}/audienceLists/{audience_list}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.analytics.data_v1alpha.types.AudienceList:\\n        An audience list is a list of users\\n        in an audience at the time of the list\\'s\\n        creation. One audience may have multiple\\n        audience lists created for different\\n        days.',\n",
       " '@rtype:  int\\n@return: Process global ID.',\n",
       " 'Return a callable for the create build trigger method over gRPC.\\n\\nCreates a new ``BuildTrigger``.\\n\\nThis API is experimental.\\n\\nReturns:\\n    Callable[[~.CreateBuildTriggerRequest],\\n            ~.BuildTrigger]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Transfers customer entitlements from their current reseller to\\nGoogle.\\n\\nPossible error codes:\\n\\n-  PERMISSION_DENIED: The customer doesn\\'t belong to the\\n   reseller.\\n-  INVALID_ARGUMENT: Required request parameters are missing or\\n   invalid.\\n-  NOT_FOUND: The customer or offer resource was not found.\\n-  ALREADY_EXISTS: The SKU was already transferred for the\\n   customer.\\n-  CONDITION_NOT_MET or FAILED_PRECONDITION:\\n\\n   -  The SKU requires domain verification to transfer, but the\\n      domain is not verified.\\n   -  An Add-On SKU (example, Vault or Drive) is missing the\\n      pre-requisite SKU (example, G Suite Basic).\\n   -  (Developer accounts only) Reseller and resold domain must\\n      meet the following naming requirements:\\n\\n      -  Domain names must start with goog-test.\\n      -  Domain names must include the reseller domain.\\n\\n-  INTERNAL: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n-  UNKNOWN: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n\\nReturn value: The ID of a long-running operation.\\n\\nTo get the results of the operation, call the GetOperation\\nmethod of CloudChannelOperationsService. The response will\\ncontain google.protobuf.Empty on success. The Operation metadata\\nwill contain an instance of\\n[OperationMetadata][google.cloud.channel.v1.OperationMetadata].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import channel_v1\\n\\n    def sample_transfer_entitlements_to_google():\\n        # Create a client\\n        client = channel_v1.CloudChannelServiceClient()\\n\\n        # Initialize request argument(s)\\n        entitlements = channel_v1.Entitlement()\\n        entitlements.offer = \"offer_value\"\\n\\n        request = channel_v1.TransferEntitlementsToGoogleRequest(\\n            parent=\"parent_value\",\\n            entitlements=entitlements,\\n        )\\n\\n        # Make the request\\n        operation = client.transfer_entitlements_to_google(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.channel_v1.types.TransferEntitlementsToGoogleRequest, dict]):\\n        The request object. Request message for\\n        [CloudChannelService.TransferEntitlementsToGoogle][google.cloud.channel.v1.CloudChannelService.TransferEntitlementsToGoogle].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Returns a specified regional persistent disk.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.RegionDisksClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetRegionDiskRequest(\\n            disk=\"disk_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetRegionDiskRequest, dict]):\\n        The request object. A request message for\\n        RegionDisks.Get. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk (str):\\n        Name of the regional persistent disk\\n        to return.\\n\\n        This corresponds to the ``disk`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.Disk:\\n        Represents a Persistent Disk resource. Google Compute\\n        Engine has two Disk resources: \\\\*\\n        [Zonal](/compute/docs/reference/rest/v1/disks) \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDisks)\\n        Persistent disks are required for running your VM\\n        instances. Create both boot and non-boot (data)\\n        persistent disks. For more information, read Persistent\\n        Disks. For more storage options, read Storage options.\\n        The disks resource represents a zonal persistent disk.\\n        For more information, read Zonal persistent disks. The\\n        regionDisks resource represents a regional persistent\\n        disk. For more information, read Regional resources.',\n",
       " 'Creates a new network firewall policy in the\\nspecified project and region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_insert():\\n        # Create a client\\n        client = compute_v1.RegionNetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.InsertRegionNetworkFirewallPolicyRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.insert(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.InsertRegionNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        RegionNetworkFirewallPolicies.Insert.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy_resource (google.cloud.compute_v1.types.FirewallPolicy):\\n        The body resource for this request\\n        This corresponds to the ``firewall_policy_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Call the list document schemas method over HTTP.\\n\\nArgs:\\n    request (~.document_schema_service.ListDocumentSchemasRequest):\\n        The request object. Request message for\\n    DocumentSchemaService.ListDocumentSchemas.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_schema_service.ListDocumentSchemasResponse:\\n        Response message for\\n    DocumentSchemaService.ListDocumentSchemas.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TemplatesServiceAsyncClient: The constructed client.',\n",
       " 'Pre-rpc interceptor for batch_delete_test_cases\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the TestCases server.',\n",
       " 'Return a callable for the update participant method over gRPC.\\n\\nUpdates the specified participant.\\n\\nReturns:\\n    Callable[[~.UpdateParticipantRequest],\\n            ~.Participant]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a asset path into its component segments.',\n",
       " 'Return a callable for the list preference sets method over gRPC.\\n\\nLists all the preference sets in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListPreferenceSetsRequest],\\n            ~.ListPreferenceSetsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the create grpc route method over gRPC.\\n\\nCreates a new GrpcRoute in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateGrpcRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return string with information about image sequence.',\n",
       " 'Post-rpc interceptor for get_replay\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Simulator server but before\\nit is returned to user code.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TagKeysAsyncClient: The constructed client.',\n",
       " 'Updates the mute state of a finding.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securitycenter_v1\\n\\n    def sample_set_mute():\\n        # Create a client\\n        client = securitycenter_v1.SecurityCenterClient()\\n\\n        # Initialize request argument(s)\\n        request = securitycenter_v1.SetMuteRequest(\\n            name=\"name_value\",\\n            mute=\"UNDEFINED\",\\n        )\\n\\n        # Make the request\\n        response = client.set_mute(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securitycenter_v1.types.SetMuteRequest, dict]):\\n        The request object. Request message for updating a\\n        finding\\'s mute status.\\n    name (str):\\n        Required. The `relative resource\\n        name <https://cloud.google.com/apis/design/resource_names#relative_resource_name>`__\\n        of the finding. Example:\\n        ``organizations/{organization_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``folders/{folder_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``projects/{project_id}/sources/{source_id}/findings/{finding_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    mute (google.cloud.securitycenter_v1.types.Finding.Mute):\\n        Required. The desired state of the\\n        Mute.\\n\\n        This corresponds to the ``mute`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securitycenter_v1.types.Finding:\\n        Security Command Center finding.\\n\\n        A finding is a record of assessment data\\n        like security, risk, health, or privacy,\\n        that is ingested into Security Command\\n        Center for presentation, notification,\\n        analysis, policy testing, and\\n        enforcement. For example, a cross-site\\n        scripting (XSS) vulnerability in an App\\n        Engine application is a finding.',\n",
       " 'Return a callable for the update deployment method over gRPC.\\n\\nUpdates a deployment.\\n\\nReturns:\\n    Callable[[~.UpdateDeploymentRequest],\\n            ~.Deployment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'texttospeech.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'Return a callable for the delete product method over gRPC.\\n\\nPermanently deletes a product and its reference\\nimages.\\nMetadata of the product and all its images will be\\ndeleted right away, but search queries against\\nProductSets containing the product may still work until\\nall related caches are refreshed.\\n\\nReturns:\\n    Callable[[~.DeleteProductRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for search_index_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Warehouse server but before\\nit is returned to user code.',\n",
       " 'Returns a fully-qualified account string.',\n",
       " 'Call the list data sources method over HTTP.\\n\\nArgs:\\n    request (~.datasources.ListDataSourcesRequest):\\n        The request object. Request message for the\\n    ListDataSources method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.datasources.ListDataSourcesResponse:\\n        Response message for the\\n    ListDataSources method.',\n",
       " 'SPE specific metadata.\\n\\nParameters\\n----------\\nindex : int\\n    Ignored as SPE files only store global metadata.\\nexclude_applied : bool\\n    Ignored. Exists for API compatibility.\\nchar_encoding : str\\n    The encoding to use when parsing strings.\\nsdt_control : bool\\n    If `True`, decode special metadata written by the\\n    SDT-control software if present.\\n\\nReturns\\n-------\\nmetadata : dict\\n    Key-value pairs of metadata.\\n\\nNotes\\n-----\\nSPE v3 stores metadata as XML, whereas SPE v2 uses a binary format.\\n\\n.. rubric:: Supported SPE v2 Metadata fields\\n\\nROIs : list of dict\\n    Regions of interest used for recording images. Each dict has the\\n    \"top_left\" key containing x and y coordinates of the top left corner,\\n    the \"bottom_right\" key with x and y coordinates of the bottom right\\n    corner, and the \"bin\" key with number of binned pixels in x and y\\n    directions.\\ncomments : list of str\\n    The SPE format allows for 5 comment strings of 80 characters each.\\ncontroller_version : int\\n    Hardware version\\nlogic_output : int\\n    Definition of output BNC\\namp_hi_cap_low_noise : int\\n    Amp switching mode\\nmode : int\\n    Timing mode\\nexp_sec : float\\n    Alternative exposure in seconds\\ndate : str\\n    Date string\\ndetector_temp : float\\n    Detector temperature\\ndetector_type : int\\n    CCD / diode array type\\nst_diode : int\\n    Trigger diode\\ndelay_time : float\\n    Used with async mode\\nshutter_control : int\\n    Normal, disabled open, or disabled closed\\nabsorb_live : bool\\n    on / off\\nabsorb_mode : int\\n    Reference strip or file\\ncan_do_virtual_chip : bool\\n    True or False whether chip can do virtual chip\\nthreshold_min_live : bool\\n    on / off\\nthreshold_min_val : float\\n    Threshold minimum value\\nthreshold_max_live : bool\\n    on / off\\nthreshold_max_val : float\\n    Threshold maximum value\\ntime_local : str\\n    Experiment local time\\ntime_utc : str\\n    Experiment UTC time\\nadc_offset : int\\n    ADC offset\\nadc_rate : int\\n    ADC rate\\nadc_type : int\\n    ADC type\\nadc_resolution : int\\n    ADC resolution\\nadc_bit_adjust : int\\n    ADC bit adjust\\ngain : int\\n    gain\\nsw_version : str\\n    Version of software which created this file\\nspare_4 : bytes\\n    Reserved space\\nreadout_time : float\\n    Experiment readout time\\ntype : str\\n    Controller type\\nclockspeed_us : float\\n    Vertical clock speed in microseconds\\nreadout_mode : [\"full frame\", \"frame transfer\", \"kinetics\", \"\"]\\n    Readout mode. Empty string means that this was not set by the\\n    Software.\\nwindow_size : int\\n    Window size for Kinetics mode\\nfile_header_ver : float\\n    File header version\\nchip_size : [int, int]\\n    x and y dimensions of the camera chip\\nvirt_chip_size : [int, int]\\n    Virtual chip x and y dimensions\\npre_pixels : [int, int]\\n    Pre pixels in x and y dimensions\\npost_pixels : [int, int],\\n    Post pixels in x and y dimensions\\ngeometric : list of {\"rotate\", \"reverse\", \"flip\"}\\n    Geometric operations\\nsdt_major_version : int\\n    (only for files created by SDT-control)\\n    Major version of SDT-control software\\nsdt_minor_version : int\\n    (only for files created by SDT-control)\\n    Minor version of SDT-control software\\nsdt_controller_name : str\\n    (only for files created by SDT-control)\\n    Controller name\\nexposure_time : float\\n    (only for files created by SDT-control)\\n    Exposure time in seconds\\ncolor_code : str\\n    (only for files created by SDT-control)\\n    Color channels used\\ndetection_channels : int\\n    (only for files created by SDT-control)\\n    Number of channels\\nbackground_subtraction : bool\\n    (only for files created by SDT-control)\\n    Whether background subtraction war turned on\\nem_active : bool\\n    (only for files created by SDT-control)\\n    Whether EM was turned on\\nem_gain : int\\n    (only for files created by SDT-control)\\n    EM gain\\nmodulation_active : bool\\n    (only for files created by SDT-control)\\n    Whether laser modulation (“attenuate”) was turned on\\npixel_size : float\\n    (only for files created by SDT-control)\\n    Camera pixel size\\nsequence_type : str\\n    (only for files created by SDT-control)\\n    Type of sequnce (standard, TOCCSL, arbitrary, …)\\ngrid : float\\n    (only for files created by SDT-control)\\n    Sequence time unit (“grid size”) in seconds\\nn_macro : int\\n    (only for files created by SDT-control)\\n    Number of macro loops\\ndelay_macro : float\\n    (only for files created by SDT-control)\\n    Time between macro loops in seconds\\nn_mini : int\\n    (only for files created by SDT-control)\\n    Number of mini loops\\ndelay_mini : float\\n    (only for files created by SDT-control)\\n    Time between mini loops in seconds\\nn_micro : int (only for files created by SDT-control)\\n    Number of micro loops\\ndelay_micro : float (only for files created by SDT-control)\\n    Time between micro loops in seconds\\nn_subpics : int\\n    (only for files created by SDT-control)\\n    Number of sub-pictures\\ndelay_shutter : float\\n    (only for files created by SDT-control)\\n    Camera shutter delay in seconds\\ndelay_prebleach : float\\n    (only for files created by SDT-control)\\n    Pre-bleach delay in seconds\\nbleach_time : float\\n    (only for files created by SDT-control)\\n    Bleaching time in seconds\\nrecovery_time : float\\n    (only for files created by SDT-control)\\n    Recovery time in seconds\\ncomment : str\\n    (only for files created by SDT-control)\\n    User-entered comment. This replaces the \"comments\" field.\\ndatetime : datetime.datetime\\n    (only for files created by SDT-control)\\n    Combines the \"date\" and \"time_local\" keys. The latter two plus\\n    \"time_utc\" are removed.\\nmodulation_script : str\\n    (only for files created by SDT-control)\\n    Laser modulation script. Replaces the \"spare_4\" key.\\nbleach_piezo_active : bool\\n    (only for files created by SDT-control)\\n    Whether piezo for bleaching was enabled',\n",
       " 'Get the global InteractiveShell instance.\\n\\nReturns None if no InteractiveShell instance is registered.',\n",
       " 'Activate the interactive debugger.\\n\\nThis magic command support two ways of activating debugger.\\nOne is to activate debugger before executing code.  This way, you\\ncan set a break point, to step through the code from the point.\\nYou can use this mode by giving statements to execute and optionally\\na breakpoint.\\n\\nThe other one is to activate debugger in post-mortem mode.  You can\\nactivate this mode simply running %debug without any argument.\\nIf an exception has just occurred, this lets you inspect its stack\\nframes interactively.  Note that this will always work only on the last\\ntraceback that occurred, so you must call this quickly after an\\nexception that you wish to inspect has fired, because if another one\\noccurs, it clobbers the previous one.\\n\\nIf you want IPython to automatically do this on every exception, see\\nthe %pdb magic for more details.\\n\\n.. versionchanged:: 7.3\\n    When running code, user variables are no longer expanded,\\n    the magic line is always left unmodified.',\n",
       " \"var_expand on invalid formats shouldn't raise\",\n",
       " 'Check whether the instance conforms to the given format.\\n\\nArguments:\\n\\n    instance (*any primitive type*, i.e. str, number, bool):\\n\\n        The instance to check\\n\\n    format:\\n\\n        The format that instance should conform to\\n\\nReturns:\\n\\n    bool: whether it conformed',\n",
       " 'Return the x-axis view limits.\\n\\nReturns\\n-------\\nleft, right : (float, float)\\n    The current x-axis limits in data coordinates.\\n\\nSee Also\\n--------\\n.Axes.set_xlim\\n.Axes.set_xbound, .Axes.get_xbound\\n.Axes.invert_xaxis, .Axes.xaxis_inverted\\n\\nNotes\\n-----\\nThe x-axis may be inverted, in which case the *left* value will\\nbe greater than the *right* value.',\n",
       " 'Draw a multibyte character from a Type 3 font as an XObject.',\n",
       " 'Return a list of (ind0, ind1) such that ``mask[ind0:ind1].all()`` is\\nTrue and we cover all such regions.',\n",
       " 'You cannot define methods out of the spec',\n",
       " 'Send a batch of write operations to the server.\\n\\nRequests are passed as a list of write operation instances (\\n:class:`~pymongo.operations.InsertOne`,\\n:class:`~pymongo.operations.UpdateOne`,\\n:class:`~pymongo.operations.UpdateMany`,\\n:class:`~pymongo.operations.ReplaceOne`,\\n:class:`~pymongo.operations.DeleteOne`, or\\n:class:`~pymongo.operations.DeleteMany`).\\n\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634ef\\')}\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  >>> # DeleteMany, UpdateOne, and UpdateMany are also available.\\n  ...\\n  >>> from pymongo import InsertOne, DeleteOne, ReplaceOne\\n  >>> requests = [InsertOne({\\'y\\': 1}), DeleteOne({\\'x\\': 1}),\\n  ...             ReplaceOne({\\'w\\': 1}, {\\'z\\': 1}, upsert=True)]\\n  >>> result = db.test.bulk_write(requests)\\n  >>> result.inserted_count\\n  1\\n  >>> result.deleted_count\\n  1\\n  >>> result.modified_count\\n  0\\n  >>> result.upserted_ids\\n  {2: ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  {\\'y\\': 1, \\'_id\\': ObjectId(\\'54f62ee2fba5226811f634f1\\')}\\n  {\\'z\\': 1, \\'_id\\': ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n\\n:param requests: A list of write operations (see examples above).\\n:param ordered: If ``True`` (the default) requests will be\\n    performed on the server serially, in the order provided. If an error\\n    occurs all remaining operations are aborted. If ``False`` requests\\n    will be performed on the server in arbitrary order, possibly in\\n    parallel, and all operations will be attempted.\\n:param bypass_document_validation: (optional) If ``True``, allows the\\n    write to opt-out of document level validation. Default is\\n    ``False``.\\n:param session: a\\n    :class:`~pymongo.client_session.ClientSession`.\\n:param comment: A user-provided comment to attach to this\\n    command.\\n:param let: Map of parameter names and values. Values must be\\n    constant or closed expressions that do not reference document\\n    fields. Parameters can then be accessed as variables in an\\n    aggregate expression context (e.g. \"$$var\").\\n\\n:return: An instance of :class:`~pymongo.results.BulkWriteResult`.\\n\\n.. seealso:: :ref:`writes-and-ids`\\n\\n.. note:: `bypass_document_validation` requires server version\\n  **>= 3.2**\\n\\n.. versionchanged:: 4.1\\n   Added ``comment`` parameter.\\n   Added ``let`` parameter.\\n\\n.. versionchanged:: 3.6\\n   Added ``session`` parameter.\\n\\n.. versionchanged:: 3.2\\n  Added bypass_document_validation support\\n\\n.. versionadded:: 3.0',\n",
       " 'Test a complete undirected graph.',\n",
       " 'Test that the new (>=v1.15) implementation (see #10073) is equal to the original (<=v1.14) ',\n",
       " 'Load data from a text file.\\n\\nParameters\\n----------\\nfname : file, str, pathlib.Path, list of str, generator\\n    File, filename, list, or generator to read.  If the filename\\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\\n    that generators must return bytes or strings. The strings\\n    in a list or produced by a generator are treated as lines.\\ndtype : data-type, optional\\n    Data-type of the resulting array; default: float.  If this is a\\n    structured data-type, the resulting array will be 1-dimensional, and\\n    each row will be interpreted as an element of the array.  In this\\n    case, the number of columns used must match the number of fields in\\n    the data-type.\\ncomments : str or sequence of str or None, optional\\n    The characters or list of characters used to indicate the start of a\\n    comment. None implies no comments. For backwards compatibility, byte\\n    strings will be decoded as \\'latin1\\'. The default is \\'#\\'.\\ndelimiter : str, optional\\n    The character used to separate the values. For backwards compatibility,\\n    byte strings will be decoded as \\'latin1\\'. The default is whitespace.\\n\\n    .. versionchanged:: 1.23.0\\n       Only single character delimiters are supported. Newline characters\\n       cannot be used as the delimiter.\\n\\nconverters : dict or callable, optional\\n    Converter functions to customize value parsing. If `converters` is\\n    callable, the function is applied to all columns, else it must be a\\n    dict that maps column number to a parser function.\\n    See examples for further details.\\n    Default: None.\\n\\n    .. versionchanged:: 1.23.0\\n       The ability to pass a single callable to be applied to all columns\\n       was added.\\n\\nskiprows : int, optional\\n    Skip the first `skiprows` lines, including comments; default: 0.\\nusecols : int or sequence, optional\\n    Which columns to read, with 0 being the first. For example,\\n    ``usecols = (1,4,5)`` will extract the 2nd, 5th and 6th columns.\\n    The default, None, results in all columns being read.\\nunpack : bool, optional\\n    If True, the returned array is transposed, so that arguments may be\\n    unpacked using ``x, y, z = loadtxt(...)``.  When used with a\\n    structured data-type, arrays are returned for each field.\\n    Default is False.\\nndmin : int, optional\\n    The returned array will have at least `ndmin` dimensions.\\n    Otherwise mono-dimensional axes will be squeezed.\\n    Legal values: 0 (default), 1 or 2.\\nencoding : str, optional\\n    Encoding used to decode the inputfile. Does not apply to input streams.\\n    The special value \\'bytes\\' enables backward compatibility workarounds\\n    that ensures you receive byte arrays as results if possible and passes\\n    \\'latin1\\' encoded strings to converters. Override this value to receive\\n    unicode arrays and pass strings as input to converters.  If set to None\\n    the system default is used. The default value is \\'bytes\\'.\\n\\n    .. versionchanged:: 2.0\\n        Before NumPy 2, the default was ``\\'bytes\\'`` for Python 2\\n        compatibility. The default is now ``None``.\\n\\nmax_rows : int, optional\\n    Read `max_rows` rows of content after `skiprows` lines. The default is\\n    to read all the rows. Note that empty rows containing no data such as\\n    empty lines and comment lines are not counted towards `max_rows`,\\n    while such lines are counted in `skiprows`.\\n\\n    .. versionchanged:: 1.23.0\\n        Lines containing no data, including comment lines (e.g., lines\\n        starting with \\'#\\' or as specified via `comments`) are not counted\\n        towards `max_rows`.\\nquotechar : unicode character or None, optional\\n    The character used to denote the start and end of a quoted item.\\n    Occurrences of the delimiter or comment characters are ignored within\\n    a quoted item. The default value is ``quotechar=None``, which means\\n    quoting support is disabled.\\n\\n    If two consecutive instances of `quotechar` are found within a quoted\\n    field, the first is treated as an escape character. See examples.\\n\\n    .. versionadded:: 1.23.0\\n${ARRAY_FUNCTION_LIKE}\\n\\n    .. versionadded:: 1.20.0\\n\\nReturns\\n-------\\nout : ndarray\\n    Data read from the text file.\\n\\nSee Also\\n--------\\nload, fromstring, fromregex\\ngenfromtxt : Load data with missing values handled as specified.\\nscipy.io.loadmat : reads MATLAB data files\\n\\nNotes\\n-----\\nThis function aims to be a fast reader for simply formatted files.  The\\n`genfromtxt` function provides more sophisticated handling of, e.g.,\\nlines with missing values.\\n\\nEach row in the input text file must have the same number of values to be\\nable to read all values. If all rows do not have same number of values, a\\nsubset of up to n columns (where n is the least number of values present\\nin all rows) can be read by specifying the columns via `usecols`.\\n\\nThe strings produced by the Python float.hex method can be used as\\ninput for floats.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from io import StringIO   # StringIO behaves like a file object\\n>>> c = StringIO(\"0 1\\\\n2 3\")\\n>>> np.loadtxt(c)\\narray([[0., 1.],\\n       [2., 3.]])\\n\\n>>> d = StringIO(\"M 21 72\\\\nF 35 58\")\\n>>> np.loadtxt(d, dtype={\\'names\\': (\\'gender\\', \\'age\\', \\'weight\\'),\\n...                      \\'formats\\': (\\'S1\\', \\'i4\\', \\'f4\\')})\\narray([(b\\'M\\', 21, 72.), (b\\'F\\', 35, 58.)],\\n      dtype=[(\\'gender\\', \\'S1\\'), (\\'age\\', \\'<i4\\'), (\\'weight\\', \\'<f4\\')])\\n\\n>>> c = StringIO(\"1,0,2\\\\n3,0,4\")\\n>>> x, y = np.loadtxt(c, delimiter=\\',\\', usecols=(0, 2), unpack=True)\\n>>> x\\narray([1., 3.])\\n>>> y\\narray([2., 4.])\\n\\nThe `converters` argument is used to specify functions to preprocess the\\ntext prior to parsing. `converters` can be a dictionary that maps\\npreprocessing functions to each column:\\n\\n>>> s = StringIO(\"1.618, 2.296\\\\n3.141, 4.669\\\\n\")\\n>>> conv = {\\n...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\\n...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\\n... }\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([[1., 3.],\\n       [3., 5.]])\\n\\n`converters` can be a callable instead of a dictionary, in which case it\\nis applied to all columns:\\n\\n>>> s = StringIO(\"0xDE 0xAD\\\\n0xC0 0xDE\")\\n>>> import functools\\n>>> conv = functools.partial(int, base=16)\\n>>> np.loadtxt(s, converters=conv)\\narray([[222., 173.],\\n       [192., 222.]])\\n\\nThis example shows how `converters` can be used to convert a field\\nwith a trailing minus sign into a negative number.\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> def conv(fld):\\n...     return -float(fld[:-1]) if fld.endswith(\"-\") else float(fld)\\n...\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nUsing a callable as the converter can be particularly useful for handling\\nvalues with different formatting, e.g. floats with underscores:\\n\\n>>> s = StringIO(\"1 2.7 100_000\")\\n>>> np.loadtxt(s, converters=float)\\narray([1.e+00, 2.7e+00, 1.e+05])\\n\\nThis idea can be extended to automatically handle values specified in\\nmany different formats, such as hex values:\\n\\n>>> def conv(val):\\n...     try:\\n...         return float(val)\\n...     except ValueError:\\n...         return float.fromhex(val)\\n>>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\\n\\nOr a format where the ``-`` sign comes after the number:\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> conv = lambda x: -float(x[:-1]) if x.endswith(\"-\") else float(x)\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nSupport for quoted fields is enabled with the `quotechar` parameter.\\nComment and delimiter characters are ignored when they appear within a\\nquoted item delineated by `quotechar`:\\n\\n>>> s = StringIO(\\'\"alpha, #42\", 10.0\\\\n\"beta, #64\", 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nQuoted fields can be separated by multiple whitespace characters:\\n\\n>>> s = StringIO(\\'\"alpha, #42\"       10.0\\\\n\"beta, #64\" 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nTwo consecutive quote characters within a quoted field are treated as a\\nsingle escaped character:\\n\\n>>> s = StringIO(\\'\"Hello, my name is \"\"Monty\"\"!\"\\')\\n>>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar=\\'\"\\')\\narray(\\'Hello, my name is \"Monty\"!\\', dtype=\\'<U26\\')\\n\\nRead subset of columns when all rows do not contain equal number of values:\\n\\n>>> d = StringIO(\"1 2\\\\n2 4\\\\n3 9 12\\\\n4 16 20\")\\n>>> np.loadtxt(d, usecols=(0, 1))\\narray([[ 1.,  2.],\\n       [ 2.,  4.],\\n       [ 3.,  9.],\\n       [ 4., 16.]])',\n",
       " 'Re-pack the fields of a structured array or dtype in memory.\\n\\nThe memory layout of structured datatypes allows fields at arbitrary\\nbyte offsets. This means the fields can be separated by padding bytes,\\ntheir offsets can be non-monotonically increasing, and they can overlap.\\n\\nThis method removes any overlaps and reorders the fields in memory so they\\nhave increasing byte offsets, and adds or removes padding bytes depending\\non the `align` option, which behaves like the `align` option to\\n`numpy.dtype`.\\n\\nIf `align=False`, this method produces a \"packed\" memory layout in which\\neach field starts at the byte the previous field ended, and any padding\\nbytes are removed.\\n\\nIf `align=True`, this methods produces an \"aligned\" memory layout in which\\neach field\\'s offset is a multiple of its alignment, and the total itemsize\\nis a multiple of the largest alignment, by adding padding bytes as needed.\\n\\nParameters\\n----------\\na : ndarray or dtype\\n   array or dtype for which to repack the fields.\\nalign : boolean\\n   If true, use an \"aligned\" memory layout, otherwise use a \"packed\" layout.\\nrecurse : boolean\\n   If True, also repack nested structures.\\n\\nReturns\\n-------\\nrepacked : ndarray or dtype\\n   Copy of `a` with fields repacked, or `a` itself if no repacking was\\n   needed.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n\\n>>> from numpy.lib import recfunctions as rfn\\n>>> def print_offsets(d):\\n...     print(\"offsets:\", [d.fields[name][1] for name in d.names])\\n...     print(\"itemsize:\", d.itemsize)\\n...\\n>>> dt = np.dtype(\\'u1, <i8, <f8\\', align=True)\\n>>> dt\\ndtype({\\'names\\': [\\'f0\\', \\'f1\\', \\'f2\\'], \\'formats\\': [\\'u1\\', \\'<i8\\', \\'<f8\\'], \\'offsets\\': [0, 8, 16], \\'itemsize\\': 24}, align=True)\\n>>> print_offsets(dt)\\noffsets: [0, 8, 16]\\nitemsize: 24\\n>>> packed_dt = rfn.repack_fields(dt)\\n>>> packed_dt\\ndtype([(\\'f0\\', \\'u1\\'), (\\'f1\\', \\'<i8\\'), (\\'f2\\', \\'<f8\\')])\\n>>> print_offsets(packed_dt)\\noffsets: [0, 1, 9]\\nitemsize: 17',\n",
       " 'Test that bin width for integer data is at least 1.',\n",
       " 'Returns the data as a recarray.',\n",
       " 'Invalidate an authorization code after use.\\n\\n:param client_id: Unicode client identifier.\\n:param code: The authorization code grant (request.code).\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n\\nMethod is used by:\\n    - Authorization Code Grant',\n",
       " 'Write an object to file specified by a pathlib.Path and read it back\\n\\nParameters\\n----------\\nwriter : callable bound to pandas object\\n    IO writing function (e.g. DataFrame.to_csv )\\nreader : callable\\n    IO reading function (e.g. pd.read_csv )\\npath : str, default None\\n    The path where the object is written and then read.\\n\\nReturns\\n-------\\npandas object\\n    The original object that was serialized and then re-read.',\n",
       " 'For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results\\nin PeriodArray.  We cannot use ._ndarray directly for the affected\\nmethods because the i8 data has different semantics on NaT values.',\n",
       " ':calls: `POST /gists <http://docs.github.com/en/rest/reference/gists>`_',\n",
       " \"`argslist_list` is a list that can contain an argslist as a first item, but\\nmost not. It's basically the items between the parameter brackets (which is\\nat most one item).\\nThis function modifies the parser structure. It generates `Param` objects\\nfrom the normal ast. Those param objects do not exist in a normal ast, but\\nmake the evaluation of the ast tree so much easier.\\nYou could also say that this function replaces the argslist node with a\\nlist of Param objects.\",\n",
       " 'Erases from the current cursor position to the end of the current\\nline.',\n",
       " 'Apply reduction rules to `word` excluding the reduction rule\\nfor the lhs equal to `exclude`',\n",
       " 'Initialize a SpecifierSet instance.\\n\\n:param specifiers:\\n    The string representation of a specifier or a comma-separated list of\\n    specifiers which will be parsed and normalized before use.\\n:param prereleases:\\n    This tells the SpecifierSet if it should accept prerelease versions if\\n    applicable or not. The default of ``None`` will autodetect it from the\\n    given specifiers.\\n\\n:raises InvalidSpecifier:\\n    If the given ``specifiers`` are not parseable than this exception will be\\n    raised.',\n",
       " 'Register `provider_factory` to make providers for `loader_type`\\n\\n`loader_type` is the type or class of a PEP 302 ``module.__loader__``,\\nand `provider_factory` is a function that, passed a *module* object,\\nreturns an ``IResourceProvider`` for that module.',\n",
       " ':return: runtime directory tied to the user, e.g. ``~/Library/Caches/TemporaryItems/$appname/$version``',\n",
       " \"Connect to *address* and return the socket object.\\n\\nConvenience function.  Connect to *address* (a 2-tuple ``(host,\\nport)``) and return the socket object.  Passing the optional\\n*timeout* parameter will set the timeout on the socket instance\\nbefore attempting to connect.  If no *timeout* is supplied, the\\nglobal default timeout setting returned by :func:`socket.getdefaulttimeout`\\nis used.  If *source_address* is set it must be a tuple of (host, port)\\nfor the socket to bind as a source address before making the connection.\\nAn host of '' or port 0 tells the OS to use the default.\",\n",
       " 'return a Traceback instance wrapping part of this Traceback\\n\\nby provding any combination of path, lineno and firstlineno, the\\nfirst frame to start the to-be-returned traceback is determined\\n\\nthis allows cutting the first part of a Traceback instance e.g.\\nfor formatting reasons (removing some uninteresting bits that deal\\nwith handling of the exception/traceback)',\n",
       " 'A wrapper around typing.List that adds validation.\\n\\nArgs:\\n    item_type: The type of the items in the list.\\n    min_length: The minimum length of the list. Defaults to None.\\n    max_length: The maximum length of the list. Defaults to None.\\n    unique_items: Whether the items in the list must be unique. Defaults to None.\\n        !!! warning Deprecated\\n            The `unique_items` parameter is deprecated, use `Set` instead.\\n            See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.\\n\\nReturns:\\n    The wrapped list type.',\n",
       " 'Check if nodes.Name corresponds to first attribute variable name.\\n\\nName is `self` for method, `cls` for classmethod and `mcs` for metaclass.\\nStatic methods return False.',\n",
       " 'Format and print messages in the context of the path.',\n",
       " 'Return triangle having side of length l1 on the x-axis.',\n",
       " \"Checks that base_var is not seen as defined outsite '__init__'\\n        \",\n",
       " 'https://github.com/pylint-dev/pylint/issues/7131',\n",
       " 'The path from which pytest was invoked.\\n\\n.. versionadded:: 7.0.0',\n",
       " 'Ensure we can collect files with weird file extensions as Python\\nmodules (#2369)',\n",
       " 'Union[google.cloud.bigquery.ExternalConfig, None]: Configuration for\\nan external data source (defaults to :data:`None`).\\n\\nRaises:\\n    ValueError: For invalid value types.',\n",
       " \"Return items as ``(key, value)`` pairs.\\n\\nReturns:\\n    Iterable[Tuple[str, object]]:\\n        The ``(key, value)`` pairs representing this row.\\n\\nExamples:\\n\\n    >>> list(Row(('a', 'b'), {'x': 0, 'y': 1}).items())\\n    [('x', 'a'), ('y', 'b')]\",\n",
       " \"Coerce 'value' to a datetime, if set or not nullable.\\n\\nArgs:\\n    value (str): The timestamp.\\n\\n    field (google.cloud.bigquery.schema.SchemaField):\\n        The field corresponding to the value.\\n\\nReturns:\\n    Optional[datetime.datetime]:\\n        The parsed datetime object from\\n        ``value`` if the ``field`` is not null (otherwise it is\\n        :data:`None`).\",\n",
       " 'Return the estimated number of bytes processed by the query.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobStatistics2.FIELDS.estimated_bytes_processed\\n\\nReturns:\\n    Optional[int]:\\n        number of DML rows affected by the job, or None if job is not\\n        yet complete.',\n",
       " 'Move an existing element to the end (or beginning if last==False).\\n\\nRaises KeyError if the element does not exist.\\nWhen last=True, acts like a fast version of self[key]=self.pop(key).',\n",
       " 'Call finish_request.\\n\\nOverridden by ForkingMixIn and ThreadingMixIn.',\n",
       " 'Note the case normalization of header names here, to\\n.capitalize()-case.  This should be preserved for\\nbackwards-compatibility.  (In the HTTP case, normalization to\\n.title()-case is done by urllib2 before sending headers to\\nhttp.client).\\n\\nNote that e.g. r.has_header(\"spam-EggS\") is currently False, and\\nr.get_header(\"spam-EggS\") returns None, but that could be changed in\\nfuture.\\n\\nMethod r.remove_header should remove items both from r.headers and\\nr.unredirected_hdrs dictionaries',\n",
       " 'Prepending the signature with zeroes should be detected.',\n",
       " 'Input: a list of UserDicts.',\n",
       " 'Output: Using maxcolwidth in conjunction with disable_parsenum is honored',\n",
       " 'Sets the service of this AdmissionregistrationV1WebhookClientConfig.\\n\\n\\n:param service: The service of this AdmissionregistrationV1WebhookClientConfig.  # noqa: E501\\n:type: AdmissionregistrationV1ServiceReference',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Gets the azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n\\n\\n:return: The azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n:rtype: V1AzureDiskVolumeSource',\n",
       " 'Gets the number of this V1ServiceBackendPort.  # noqa: E501\\n\\nnumber is the numerical port number (e.g. 80) on the Service. This is a mutually exclusive setting with \"Name\".  # noqa: E501\\n\\n:return: The number of this V1ServiceBackendPort.  # noqa: E501\\n:rtype: int',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Compute and set the OOB score and attributes.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The data matrix.\\ny : ndarray of shape (n_samples, n_outputs)\\n    The target matrix.\\nscoring_function : callable, default=None\\n    Scoring function for OOB score. Default depends on whether\\n    this is a regression (R2 score) or classification problem\\n    (accuracy score).',\n",
       " 'This provides dict.get method functionality with type checking',\n",
       " \"Ellipsoidal harmonic functions F^p_n(l)\\n\\nThese are also known as Lame functions of the second kind, and are\\nsolutions to the Lame equation:\\n\\n.. math:: (s^2 - h^2)(s^2 - k^2)F''(s)\\n          + s(2s^2 - h^2 - k^2)F'(s) + (a - q s^2)F(s) = 0\\n\\nwhere :math:`q = (n+1)n` and :math:`a` is the eigenvalue (not\\nreturned) corresponding to the solutions.\\n\\nParameters\\n----------\\nh2 : float\\n    ``h**2``\\nk2 : float\\n    ``k**2``; should be larger than ``h**2``\\nn : int\\n    Degree.\\np : int\\n    Order, can range between [1,2n+1].\\ns : float\\n    Coordinate\\n\\nReturns\\n-------\\nF : float\\n    The harmonic :math:`F^p_n(s)`\\n\\nSee Also\\n--------\\nellip_harm, ellip_normal\\n\\nNotes\\n-----\\nLame functions of the second kind are related to the functions of the first kind:\\n\\n.. math::\\n\\n   F^p_n(s)=(2n + 1)E^p_n(s)\\\\int_{0}^{1/s}\\n   \\\\frac{du}{(E^p_n(1/u))^2\\\\sqrt{(1-u^2k^2)(1-u^2h^2)}}\\n\\n.. versionadded:: 0.15.0\\n\\nExamples\\n--------\\n>>> from scipy.special import ellip_harm_2\\n>>> w = ellip_harm_2(5,8,2,1,10)\\n>>> w\\n0.00108056853382\",\n",
       " 'Results above from SAS PROC NPAR1WAY, e.g.\\n\\nDATA myData;\\nINPUT X Y;\\nCARDS;\\n1 1\\n1 2\\n1 3\\n1 4\\n2 1.5\\n2 2\\n2 2.5\\nods graphics on;\\nproc npar1way AB data=myData;\\n    class X;\\n    EXACT;\\nrun;\\nods graphics off;\\n\\nNote: SAS provides Pr >= |S-Mean|, which is different from our\\ndefinition of a two-sided p-value.',\n",
       " 'Match default.',\n",
       " 'See the cancel function in sympy.polys',\n",
       " 'Format the argument signature of *self.object*.\\n\\nShould return None if the object does not have a signature.',\n",
       " 'Outputs the table of ``op``.',\n",
       " 'extension version of the same test in test_mapper.\\n\\nfixes #3408',\n",
       " \"Return a bool indicating whether the error between z1 and z2\\nis $\\\\le$ ``tol``.\\n\\nExamples\\n========\\n\\nIf ``tol`` is ``None`` then ``True`` will be returned if\\n:math:`|z1 - z2|\\\\times 10^p \\\\le 5` where $p$ is minimum value of the\\ndecimal precision of each value.\\n\\n>>> from sympy import comp, pi\\n>>> pi4 = pi.n(4); pi4\\n3.142\\n>>> comp(_, 3.142)\\nTrue\\n>>> comp(pi4, 3.141)\\nFalse\\n>>> comp(pi4, 3.143)\\nFalse\\n\\nA comparison of strings will be made\\nif ``z1`` is a Number and ``z2`` is a string or ``tol`` is ''.\\n\\n>>> comp(pi4, 3.1415)\\nTrue\\n>>> comp(pi4, 3.1415, '')\\nFalse\\n\\nWhen ``tol`` is provided and $z2$ is non-zero and\\n:math:`|z1| > 1` the error is normalized by :math:`|z1|`:\\n\\n>>> abs(pi4 - 3.14)/pi4\\n0.000509791731426756\\n>>> comp(pi4, 3.14, .001)  # difference less than 0.1%\\nTrue\\n>>> comp(pi4, 3.14, .0005)  # difference less than 0.1%\\nFalse\\n\\nWhen :math:`|z1| \\\\le 1` the absolute error is used:\\n\\n>>> 1/pi4\\n0.3183\\n>>> abs(1/pi4 - 0.3183)/(1/pi4)\\n3.07371499106316e-5\\n>>> abs(1/pi4 - 0.3183)\\n9.78393554684764e-6\\n>>> comp(1/pi4, 0.3183, 1e-5)\\nTrue\\n\\nTo see if the absolute error between ``z1`` and ``z2`` is less\\nthan or equal to ``tol``, call this as ``comp(z1 - z2, 0, tol)``\\nor ``comp(z1 - z2, tol=tol)``:\\n\\n>>> abs(pi4 - 3.14)\\n0.00160156249999988\\n>>> comp(pi4 - 3.14, 0, .002)\\nTrue\\n>>> comp(pi4 - 3.14, 0, .001)\\nFalse\",\n",
       " 'Converts a ``Feedback`` object to SymPy Expr.\\n\\nExamples\\n========\\n\\n>>> from sympy.abc import s, a, b\\n>>> from sympy.physics.control.lti import TransferFunction, Feedback\\n>>> from sympy import Expr\\n>>> tf1 = TransferFunction(a+s, 1, s)\\n>>> tf2 = TransferFunction(b+s, 1, s)\\n>>> fd1 = Feedback(tf1, tf2)\\n>>> fd1.to_expr()\\n(a + s)/((a + s)*(b + s) + 1)\\n>>> isinstance(_, Expr)\\nTrue',\n",
       " 'Returns the number of inputs of the system.',\n",
       " 'Returns state space model where numerical expressions are evaluated into floating point numbers.',\n",
       " 'Convert a mpmath ``mpf`` object to ``dtype``. ',\n",
       " 'This supports the functions that compute Hermite Normal Form.\\n\\nExplanation\\n===========\\n\\nLet x, y be the coefficients returned by the extended Euclidean\\nAlgorithm, so that x*a + y*b = g. In the algorithms for computing HNF,\\nit is critical that x, y not only satisfy the condition of being small\\nin magnitude -- namely that |x| <= |b|/g, |y| <- |a|/g -- but also that\\ny == 0 when a | b.',\n",
       " \"Reduce a system of inequalities with nested absolute values.\\n\\nExamples\\n========\\n\\n>>> from sympy import reduce_abs_inequalities, Abs, Symbol\\n>>> x = Symbol('x', extended_real=True)\\n\\n>>> reduce_abs_inequalities([(Abs(3*x - 5) - 7, '<'),\\n... (Abs(x + 25) - 13, '>')], x)\\n(-2/3 < x) & (x < 4) & (((-oo < x) & (x < -38)) | ((-12 < x) & (x < oo)))\\n\\n>>> reduce_abs_inequalities([(Abs(x - 4) + Abs(3*x - 5) - 7, '<')], x)\\n(1/2 < x) & (x < 4)\\n\\nSee Also\\n========\\n\\nreduce_abs_inequality\",\n",
       " 'The usage is simple: you just pass the match method the current\\npath info as well as the method (which defaults to `GET`).  The\\nfollowing things can then happen:\\n\\n- you receive a `NotFound` exception that indicates that no URL is\\n  matching.  A `NotFound` exception is also a WSGI application you\\n  can call to get a default page not found page (happens to be the\\n  same object as `werkzeug.exceptions.NotFound`)\\n\\n- you receive a `MethodNotAllowed` exception that indicates that there\\n  is a match for this URL but not for the current request method.\\n  This is useful for RESTful applications.\\n\\n- you receive a `RequestRedirect` exception with a `new_url`\\n  attribute.  This exception is used to notify you about a request\\n  Werkzeug requests from your WSGI application.  This is for example the\\n  case if you request ``/foo`` although the correct URL is ``/foo/``\\n  You can use the `RequestRedirect` instance as response-like object\\n  similar to all other subclasses of `HTTPException`.\\n\\n- you receive a ``WebsocketMismatch`` exception if the only\\n  match is a WebSocket rule but the bind is an HTTP request, or\\n  if the match is an HTTP rule but the bind is a WebSocket\\n  request.\\n\\n- you get a tuple in the form ``(endpoint, arguments)`` if there is\\n  a match (unless `return_rule` is True, in which case you get a tuple\\n  in the form ``(rule, arguments)``)\\n\\nIf the path info is not passed to the match method the default path\\ninfo of the map is used (defaults to the root URL if not defined\\nexplicitly).\\n\\nAll of the exceptions raised are subclasses of `HTTPException` so they\\ncan be used as WSGI responses. They will all render generic error or\\nredirect pages.\\n\\nHere is a small example for matching:\\n\\n>>> m = Map([\\n...     Rule(\\'/\\', endpoint=\\'index\\'),\\n...     Rule(\\'/downloads/\\', endpoint=\\'downloads/index\\'),\\n...     Rule(\\'/downloads/<int:id>\\', endpoint=\\'downloads/show\\')\\n... ])\\n>>> urls = m.bind(\"example.com\", \"/\")\\n>>> urls.match(\"/\", \"GET\")\\n(\\'index\\', {})\\n>>> urls.match(\"/downloads/42\")\\n(\\'downloads/show\\', {\\'id\\': 42})\\n\\nAnd here is what happens on redirect and missing URLs:\\n\\n>>> urls.match(\"/downloads\")\\nTraceback (most recent call last):\\n  ...\\nRequestRedirect: http://example.com/downloads/\\n>>> urls.match(\"/missing\")\\nTraceback (most recent call last):\\n  ...\\nNotFound: 404 Not Found\\n\\n:param path_info: the path info to use for matching.  Overrides the\\n                  path info specified on binding.\\n:param method: the HTTP method used for matching.  Overrides the\\n               method specified on binding.\\n:param return_rule: return the rule that matched instead of just the\\n                    endpoint (defaults to `False`).\\n:param query_args: optional query arguments that are used for\\n                   automatic redirects as string or dictionary.  It\\'s\\n                   currently not possible to use the query arguments\\n                   for URL matching.\\n:param websocket: Match WebSocket instead of HTTP requests. A\\n    websocket request has a ``ws`` or ``wss``\\n    :attr:`url_scheme`. This overrides that detection.\\n\\n.. versionadded:: 1.0\\n    Added ``websocket``.\\n\\n.. versionchanged:: 0.8\\n    ``query_args`` can be a string.\\n\\n.. versionadded:: 0.7\\n    Added ``query_args``.\\n\\n.. versionadded:: 0.6\\n    Added ``return_rule``.',\n",
       " 'Sets vertical border characters.\\n\\n╔═══════════════╤══════════════════════════╤══════════════════╗\\n║ ISBN          │ Title                    │ Author           ║\\n╠═══════1═══════╪══════════════════════════╪══════════════════╣\\n║ 99921-58-10-7 │ Divine Comedy            │ Dante Alighieri  ║\\n║ 9971-5-0210-0 │ A Tale of Two Cities     │ Charles Dickens  ║\\n╟───────2───────┼──────────────────────────┼──────────────────╢\\n║ 960-425-059-0 │ The Lord of the Rings    │ J. R. R. Tolkien ║\\n║ 80-902734-1-6 │ And Then There Were None │ Agatha Christie  ║\\n╚═══════════════╧══════════════════════════╧══════════════════╝',\n",
       " 'Initialize the metric object as a child, i.e. when it has labels (if any) set.\\n\\nThis is factored as a separate function to allow for deferred initialization.',\n",
       " 'Concatenate a list of points or None into a single point array or None, with NaNs used to\\nseparate each line.',\n",
       " 'How much RAM is this process using? (Windows)',\n",
       " \"Returns the source and destination addresses of the last taken branch.\\n\\n@rtype: tuple( int, int )\\n@return: Source and destination addresses of the last taken branch.\\n\\n@raise WindowsError:\\n    Raises an exception on error.\\n\\n@raise NotImplementedError:\\n    Current architecture is not C{i386} or C{amd64}.\\n\\n@warning:\\n    This method uses the processor's machine specific registers (MSR).\\n    It could potentially brick your machine.\\n    It works on my machine, but your mileage may vary.\\n\\n@note:\\n    It doesn't seem to work in VMWare or VirtualBox machines.\\n    Maybe it fails in other virtualization/emulation environments,\\n    no extensive testing was made so far.\",\n",
       " 'Disassemble instructions from the address space of the process.\\n\\n@type  lpAddress: int\\n@param lpAddress: Memory address where to read the code from.\\n\\n@type  dwSize: int\\n@param dwSize: Size of binary code to disassemble.\\n\\n@rtype:  list of tuple( long, int, str, str )\\n@return: List of tuples. Each tuple represents an assembly instruction\\n    and contains:\\n     - Memory address of instruction.\\n     - Size of instruction in bytes.\\n     - Disassembly line of instruction.\\n     - Hexadecimal dump of instruction.',\n",
       " 'Return a tuple of arguments that must be passed to __new__ in order to support pickling this object.',\n",
       " 'Extract the URL prefix from a regex match.\\n\\nArgs:\\n  mat: A regex match object.\\nReturns: The URL prefix, defined as the text before the match in the\\n    original string. Normalized to start with one leading slash and end\\n    with zero.',\n",
       " 'Decorate a WebSocket function.\\n\\nRead more about it in the\\n[FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\\n\\n**Example**\\n\\n```python\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```',\n",
       " 'Check that /items/{item_id} returns expected data',\n",
       " 'Post-rpc interceptor for list_api_operations\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ApiHub server but before\\nit is returned to user code.',\n",
       " 'If the given type is `typing.Iterable[T]`',\n",
       " 'Call the update backup method over HTTP.\\n\\nArgs:\\n    request (~.backupvault.UpdateBackupRequest):\\n        The request object. Request message for updating a\\n    Backup.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return a single Task.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import batch_v1\\n\\n    def sample_get_task():\\n        # Create a client\\n        client = batch_v1.BatchServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = batch_v1.GetTaskRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_task(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.batch_v1.types.GetTaskRequest, dict]):\\n        The request object. Request for a single Task by name.\\n    name (str):\\n        Required. Task name.\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.batch_v1.types.Task:\\n        A Cloud Batch task.',\n",
       " 'Sets the default network tier of the project. The\\ndefault network tier is used when an\\naddress/forwardingRule/instance is created without\\nspecifying the network tier field.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_default_network_tier():\\n        # Create a client\\n        client = compute_v1.ProjectsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetDefaultNetworkTierProjectRequest(\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_default_network_tier(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetDefaultNetworkTierProjectRequest, dict]):\\n        The request object. A request message for\\n        Projects.SetDefaultNetworkTier. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    projects_set_default_network_tier_request_resource (google.cloud.compute_v1.types.ProjectsSetDefaultNetworkTierRequest):\\n        The body resource for this request\\n        This corresponds to the ``projects_set_default_network_tier_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Post-rpc interceptor for get_participant\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'Call the get method over HTTP.\\n\\nArgs:\\n    request (~.compute.GetPublicDelegatedPrefixeRequest):\\n        The request object. A request message for\\n    PublicDelegatedPrefixes.Get. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.PublicDelegatedPrefix:\\n        A PublicDelegatedPrefix resource\\n    represents an IP block within a\\n    PublicAdvertisedPrefix that is\\n    configured within a single cloud scope\\n    (global or region). IPs in the block can\\n    be allocated to resources within that\\n    scope. Public delegated prefixes may be\\n    further broken up into smaller IP blocks\\n    in the same scope as the parent block.',\n",
       " 'Post-rpc interceptor for compute_repository_access_token_status\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Dataform server but before\\nit is returned to user code.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsRequest):\\n        The initial request object.\\n    response (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Pre-rpc interceptor for alter_metadata_resource_location\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DataprocMetastore server.',\n",
       " 'Post-rpc interceptor for submit_answer_feedback\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Sessions server but before\\nit is returned to user code.',\n",
       " 'Call the create discovery config method over HTTP.\\n\\nArgs:\\n    request (~.dlp.CreateDiscoveryConfigRequest):\\n        The request object. Request message for\\n    CreateDiscoveryConfig.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.dlp.DiscoveryConfig:\\n        Configuration for discovery to scan resources for\\n    profile generation. Only one discovery configuration may\\n    exist per organization, folder, or project.\\n\\n    The generated data profiles are retained according to\\n    the [data retention policy]\\n    (https://cloud.google.com/sensitive-data-protection/docs/data-profiles#retention).',\n",
       " 'Set the session to be used when the TLS/SSL connection is established.\\n\\n:param session: A Session instance representing the session to use.\\n:returns: None\\n\\n.. versionadded:: 0.14',\n",
       " 'Call the list processor types method over HTTP.\\n\\nArgs:\\n    request (~.document_processor_service.ListProcessorTypesRequest):\\n        The request object. Request message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method. Some processor types may require the project be\\n    added to an allowlist.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_processor_service.ListProcessorTypesResponse:\\n        Response message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method.',\n",
       " 'Return a callable for the delete http route method over gRPC.\\n\\nDeletes a single HttpRoute.\\n\\nReturns:\\n    Callable[[~.DeleteHttpRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a collector path into its component segments.',\n",
       " 'Returns the specified key.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import recaptchaenterprise_v1\\n\\n    def sample_get_key():\\n        # Create a client\\n        client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = recaptchaenterprise_v1.GetKeyRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_key(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.recaptchaenterprise_v1.types.GetKeyRequest, dict]):\\n        The request object. The get key request message.\\n    name (str):\\n        Required. The name of the requested key, in the format\\n        ``projects/{project}/keys/{key}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.recaptchaenterprise_v1.types.Key:\\n        A key used to identify and configure\\n        applications (web and/or mobile) that\\n        use reCAPTCHA Enterprise.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Returns a fully-qualified mute_config string.',\n",
       " 'Post-rpc interceptor for list_effective_event_threat_detection_custom_modules\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SecurityCenter server but before\\nit is returned to user code.',\n",
       " '`Connection.bio_read` raises `TypeError` if passed a non-integer\\nargument.',\n",
       " 'Lists all AdaptiveMtFiles associated to an\\nAdaptiveMtDataset.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import translate_v3\\n\\n    def sample_list_adaptive_mt_files():\\n        # Create a client\\n        client = translate_v3.TranslationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = translate_v3.ListAdaptiveMtFilesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_adaptive_mt_files(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.translate_v3.types.ListAdaptiveMtFilesRequest, dict]):\\n        The request object. The request to list all AdaptiveMt\\n        files under a given dataset.\\n    parent (str):\\n        Required. The resource name of the project from which to\\n        list the Adaptive MT files.\\n        ``projects/{project}/locations/{location}/adaptiveMtDatasets/{dataset}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.translate_v3.services.translation_service.pagers.ListAdaptiveMtFilesPager:\\n        The response for listing all\\n        AdaptiveMt files under a given dataset.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Call the delete application method over HTTP.\\n\\nArgs:\\n    request (~.platform.DeleteApplicationRequest):\\n        The request object. Message for deleting an Application.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return a callable for the create private connection method over gRPC.\\n\\nCreates a new private connection that can be used for\\naccessing private Clouds.\\n\\nReturns:\\n    Callable[[~.CreatePrivateConnectionRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Convert a av.VideoFrame into a ndarray\\n\\nParameters\\n----------\\nframe : av.VideoFrame\\n    The frame to unpack.\\nformat : str\\n    If not None, convert the frame to the given format before unpacking.',\n",
       " 'Loads the encoded object. This function raises\\n:class:`.BadPayload` if the payload is not valid. The\\n``serializer`` parameter can be used to override the serializer\\nstored on the class. The encoded ``payload`` should always be\\nbytes.',\n",
       " 'Call the callable with the arguments and keyword arguments\\nprovided but inject the active context or environment as first\\nargument if the callable has :func:`pass_context` or\\n:func:`pass_environment`.',\n",
       " 'Format a date the way Atom likes it (RFC3339?)',\n",
       " 'Removes PEM-encoding from a public key, private key or certificate. If the\\nprivate key is encrypted, the password will be used to decrypt it.\\n\\n:param data:\\n    A byte string of the PEM-encoded data\\n\\n:param password:\\n    A byte string of the encryption password, or None\\n\\n:return:\\n    A 3-element tuple in the format: (key_type, algorithm, der_bytes). The\\n    key_type will be a unicode string of \"public key\", \"private key\" or\\n    \"certificate\". The algorithm will be a unicode string of \"rsa\", \"dsa\"\\n    or \"ec\".',\n",
       " 'Calculate any free parameters based on the current cmap and norm,\\nand do all the drawing.',\n",
       " 'Generate a new value for this ObjectId.',\n",
       " 'GH 43787\\n\\nTest correct handling of UTF-8 chars when memory_map=True and encoding is UTF-8',\n",
       " \"Returns a bipartite graph from two given degree sequences using a\\nHavel-Hakimi style construction.\\n\\nThe graph is composed of two partitions. Set A has nodes 0 to\\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\\nNodes from the set A are connected to nodes in the set B by\\nconnecting the highest degree nodes in set A to the highest degree\\nnodes in set B until all stubs are connected.\\n\\nParameters\\n----------\\naseq : list\\n   Degree sequence for node set A.\\nbseq : list\\n   Degree sequence for node set B.\\ncreate_using : NetworkX graph instance, optional\\n   Return graph of this type.\\n\\nNotes\\n-----\\nThe sum of the two sequences must be equal: sum(aseq)=sum(bseq)\\nIf no graph type is specified use MultiGraph with parallel edges.\\nIf you want a graph with no parallel edges use create_using=Graph()\\nbut then the resulting degree sequences might not be exact.\\n\\nThe nodes are assigned the attribute 'bipartite' with the value 0 or 1\\nto indicate which bipartite set the node belongs to.\\n\\nThis function is not imported in the main namespace.\\nTo use it use nx.bipartite.havel_hakimi_graph\",\n",
       " 'Tests for forbidding self-loops.',\n",
       " 'Return a list of -arch flags for every supported architecture.',\n",
       " 'Tests that it is possible to return to file processing mode\\nCLI :: :\\nBUG: numpy-gh #20520',\n",
       " 'Test that the API Counter is an abstract class.',\n",
       " 'The time the process has been running',\n",
       " 'TAG[.postDISTANCE[.dev0]+gHEX] .\\n\\nThe \".dev0\" means dirty. Note that .dev0 sorts backwards\\n(a dirty tree will appear \"older\" than the corresponding clean one),\\nbut you shouldn\\'t be releasing software with -dirty anyways.\\n\\nExceptions:\\n1: no tags. 0.postDISTANCE[.dev0]',\n",
       " 'Create a new DataFrame by selecting a subset of columns by index.',\n",
       " 'Highlight the minimum with a style.\\n\\nParameters\\n----------\\n%(subset)s\\n%(color)s\\naxis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n    Apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n    (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n    with ``axis=None``.\\n%(props)s\\n\\n    .. versionadded:: 1.3.0\\n\\nReturns\\n-------\\nStyler\\n    Instance of class where min value is highlighted in given style.\\n\\nSee Also\\n--------\\nStyler.highlight_null: Highlight missing values with a style.\\nStyler.highlight_max: Highlight the maximum with a style.\\nStyler.highlight_between: Highlight a defined range with a style.\\nStyler.highlight_quantile: Highlight values defined by a quantile with a style.\\n\\nExamples\\n--------\\n>>> df = pd.DataFrame({\"A\": [2, 1], \"B\": [3, 4]})\\n>>> df.style.highlight_min(color=\"yellow\")  # doctest: +SKIP\\n\\nPlease see:\\n`Table Visualization <../../user_guide/style.ipynb>`_ for more examples.',\n",
       " \"Convert css-string to sequence of tuples format if needed.\\n'color:red; border:1px solid black;' -> [('color', 'red'),\\n                                         ('border','1px solid red')]\",\n",
       " 'verify that select works when a channel is already closed.',\n",
       " 'Normalize the given rows of a RECORD file.\\n\\nItems in each row are converted into str. Rows are then sorted to make\\nthe value more predictable for tests.\\n\\nEach row is a 3-tuple (path, hash, size) and corresponds to a record of\\na RECORD file (see PEP 376 and PEP 427 for details).  For the rows\\npassed to this function, the size can be an integer as an int or string,\\nor the empty string.',\n",
       " 'Return our best guess of encoding for the given *term*.',\n",
       " \"Verifies that header parts don't contain leading whitespace\\nreserved characters, or return characters.\\n\\n:param header: tuple, in the format (name, value).\",\n",
       " 'Get the current row style.',\n",
       " 'Test that wrong hash in direct URL dependency stops installation.',\n",
       " 'Test that we fall back to setuptools develop when using a backend that\\ndoes not support build_editable. Since there is a pyproject.toml,\\nthe prepare_metadata_for_build_wheel hook is called.',\n",
       " 'Test the log message for an invalid Requires-Python.',\n",
       " 'Evaluate a polynomial at ``x_0 = a`` in ``K[X]`` using the Horner scheme.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x,y = ring(\"x,y\", ZZ)\\n\\n>>> R.dmp_eval(2*x*y + 3*x + y + 2, 2)\\n5*y + 8',\n",
       " 'The source type of system site packages\\nmust not be falsely identified as \"directory\".',\n",
       " 'commit with support for non-recursive commits ',\n",
       " 'Builds a function definition.\\n        ',\n",
       " 'struct_or_union : STRUCT\\n| UNION',\n",
       " 'Check if field name is in validator fields.\\n\\nArgs:\\n    info: The field info.\\n    field: The field name to check.\\n\\nReturns:\\n    `True` if field name is in validator fields, `False` otherwise.',\n",
       " 'Tests that a SymilarChecker can return and reduce mapped data.',\n",
       " 'list index is a str constant',\n",
       " 'Say it load',\n",
       " 'An assignment assumed to execute in one Try should continue to be\\nassumed to execute in a consecutive Try.',\n",
       " 'https://github.com/pylint-dev/pylint/issues/5965',\n",
       " \"Tests that an error in the initializer for the parallel jobs doesn't\\nlead to a deadlock.\",\n",
       " 'Import and return a module from the given path, which can be a file (a module) or\\na directory (a package).\\n\\n:param path:\\n    Path to the file to import.\\n\\n:param mode:\\n    Controls the underlying import mechanism that will be used:\\n\\n    * ImportMode.prepend: the directory containing the module (or package, taking\\n      `__init__.py` files into account) will be put at the *start* of `sys.path` before\\n      being imported with `importlib.import_module`.\\n\\n    * ImportMode.append: same as `prepend`, but the directory will be appended\\n      to the end of `sys.path`, if not already in `sys.path`.\\n\\n    * ImportMode.importlib: uses more fine control mechanisms provided by `importlib`\\n      to import the module, which avoids having to muck with `sys.path` at all. It effectively\\n      allows having same-named test modules in different places.\\n\\n:param root:\\n    Used as an anchor when mode == ImportMode.importlib to obtain\\n    a unique name for the module being imported so it can safely be stored\\n    into ``sys.modules``.\\n\\n:param consider_namespace_packages:\\n    If True, consider namespace packages when resolving module names.\\n\\n:raises ImportPathMismatchError:\\n    If after importing the given `path` and the module `__file__`\\n    are different. Only raised in `prepend` and `append` modes.',\n",
       " 'Shortcut for .makefile() with a .txt extension.\\n\\nDefaults to the test name with a \\'.txt\\' extension, e.g test_foobar.txt, overwriting\\nexisting files.\\n\\nExamples:\\n\\n.. code-block:: python\\n\\n    def test_something(pytester):\\n        # Initial file is created test_something.txt.\\n        pytester.maketxtfile(\"foobar\")\\n        # To create multiple files, pass kwargs accordingly.\\n        pytester.maketxtfile(custom=\"foobar\")\\n        # At this point, both \\'test_something.txt\\' & \\'custom.txt\\' exist in the test directory.',\n",
       " 'Check asynctest support (#7110)',\n",
       " 'As above, but explicitly feeding in a long on Py2. Note that\\nchecks like:\\n    isinstance(n, int)\\nare fragile on Py2, because isinstance(10L, int) is False.',\n",
       " \"It would nice if the b'' literal syntax could be coaxed into producing\\nbytes objects somehow ... ;)\",\n",
       " 'Test spec addition using :data:`+=` operator.',\n",
       " 'delete_validating_admission_policy  # noqa: E501\\n\\ndelete a ValidatingAdmissionPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_validating_admission_policy_with_http_info(name, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ValidatingAdmissionPolicy (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'get_api_versions  # noqa: E501\\n\\nget available API versions  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.get_api_versions_with_http_info(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1APIGroupList, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Gets the git_repo of this V1Volume.  # noqa: E501\\n\\n\\n:return: The git_repo of this V1Volume.  # noqa: E501\\n:rtype: V1GitRepoVolumeSource',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Re-subscribe to any channels and patterns previously subscribed to',\n",
       " 'When out of connections, block until another connection is released\\nto the pool',\n",
       " 'Appends an arbitrary number of strings to use as path constants',\n",
       " 'Search recommendation based on recommendation id',\n",
       " 'Get a metric group.\\n\\nArgs:\\n    group_name (str): The metric group name.',\n",
       " 'Check that check_is_fitted passes for stateless estimators.',\n",
       " 'Entry point for bootstrap script',\n",
       " 'Whitespace sensitive char-n-gram tokenization.\\n\\nTokenize text_document into a sequence of character n-grams\\noperating only inside word boundaries. n-grams at the edges\\nof words are padded with space.',\n",
       " 'Returns whether the kernel is stationary.',\n",
       " 'Is there a file at the given path',\n",
       " 'Objective function',\n",
       " 'Generate argument list for calling the C++ function',\n",
       " 'Merge multiple polygons into one.\\n\\nThis is an optimized version of union which assumes the polygons to be\\nnon-overlapping.\\n\\nParameters\\n----------\\na, b : Geometry or array_like\\n    Geometry or geometries to merge (union).\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\ncoverage_union_all\\n\\nExamples\\n--------\\n>>> from shapely import normalize, Polygon\\n>>> polygon = Polygon([(0, 0), (0, 1), (1, 1), (1, 0), (0, 0)])\\n>>> normalize(coverage_union(polygon, Polygon([(1, 0), (1, 1), (2, 1), (2, 0), (1, 0)])))\\n<POLYGON ((0 0, 0 1, 1 1, 2 1, 2 0, 1 0, 0 0))>\\n\\nUnion with None returns same polygon\\n\\n>>> normalize(coverage_union(polygon, None))\\n<POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))>',\n",
       " 'Test when there is no root due to double root tags.',\n",
       " 'Return any actually-specified arguments.',\n",
       " 'Check if a file uses PySpark-specific errors correctly.\\n\\nParameters\\n----------\\nfile_path : str\\n    Path to the file to check.\\npyspark_error_list : list of str\\n    List of PySpark-specific error names.\\n\\nReturns\\n-------\\nlist of str\\n    A list of strings describing the errors found in the file, with line numbers.',\n",
       " 'Gets the value of numPartitions or its default value.',\n",
       " 'Saves metadata + Params to: path + \"/metadata\"\\n\\n- class\\n- timestamp\\n- sparkVersion\\n- uid\\n- paramMap\\n- defaultParamMap (since 2.4.0)\\n- (optionally, extra metadata)\\n\\nParameters\\n----------\\nextraMetadata : dict, optional\\n    Extra metadata to be saved at same level as uid, paramMap, etc.\\nparamMap : dict, optional\\n    If given, this is saved in the \"paramMap\" field.',\n",
       " 'Convert string styled html_css_files to tuple styled one.',\n",
       " 'this is func1',\n",
       " 'Remove the remote with the given name.\\n\\n:return:\\n    The passed remote name to remove',\n",
       " 'Add one or more :class:`_sql.CTE` constructs to this statement.\\n\\nThis method will associate the given :class:`_sql.CTE` constructs with\\nthe parent statement such that they will each be unconditionally\\nrendered in the WITH clause of the final statement, even if not\\nreferenced elsewhere within the statement or any sub-selects.\\n\\nThe optional :paramref:`.HasCTE.add_cte.nest_here` parameter when set\\nto True will have the effect that each given :class:`_sql.CTE` will\\nrender in a WITH clause rendered directly along with this statement,\\nrather than being moved to the top of the ultimate rendered statement,\\neven if this statement is rendered as a subquery within a larger\\nstatement.\\n\\nThis method has two general uses. One is to embed CTE statements that\\nserve some purpose without being referenced explicitly, such as the use\\ncase of embedding a DML statement such as an INSERT or UPDATE as a CTE\\ninline with a primary statement that may draw from its results\\nindirectly.  The other is to provide control over the exact placement\\nof a particular series of CTE constructs that should remain rendered\\ndirectly in terms of a particular statement that may be nested in a\\nlarger statement.\\n\\nE.g.::\\n\\n    from sqlalchemy import table, column, select\\n    t = table(\\'t\\', column(\\'c1\\'), column(\\'c2\\'))\\n\\n    ins = t.insert().values({\"c1\": \"x\", \"c2\": \"y\"}).cte()\\n\\n    stmt = select(t).add_cte(ins)\\n\\nWould render::\\n\\n    WITH anon_1 AS\\n    (INSERT INTO t (c1, c2) VALUES (:param_1, :param_2))\\n    SELECT t.c1, t.c2\\n    FROM t\\n\\nAbove, the \"anon_1\" CTE is not referenced in the SELECT\\nstatement, however still accomplishes the task of running an INSERT\\nstatement.\\n\\nSimilarly in a DML-related context, using the PostgreSQL\\n:class:`_postgresql.Insert` construct to generate an \"upsert\"::\\n\\n    from sqlalchemy import table, column\\n    from sqlalchemy.dialects.postgresql import insert\\n\\n    t = table(\"t\", column(\"c1\"), column(\"c2\"))\\n\\n    delete_statement_cte = (\\n        t.delete().where(t.c.c1 < 1).cte(\"deletions\")\\n    )\\n\\n    insert_stmt = insert(t).values({\"c1\": 1, \"c2\": 2})\\n    update_statement = insert_stmt.on_conflict_do_update(\\n        index_elements=[t.c.c1],\\n        set_={\\n            \"c1\": insert_stmt.excluded.c1,\\n            \"c2\": insert_stmt.excluded.c2,\\n        },\\n    ).add_cte(delete_statement_cte)\\n\\n    print(update_statement)\\n\\nThe above statement renders as::\\n\\n    WITH deletions AS\\n    (DELETE FROM t WHERE t.c1 < %(c1_1)s)\\n    INSERT INTO t (c1, c2) VALUES (%(c1)s, %(c2)s)\\n    ON CONFLICT (c1) DO UPDATE SET c1 = excluded.c1, c2 = excluded.c2\\n\\n.. versionadded:: 1.4.21\\n\\n:param \\\\*ctes: zero or more :class:`.CTE` constructs.\\n\\n .. versionchanged:: 2.0  Multiple CTE instances are accepted\\n\\n:param nest_here: if True, the given CTE or CTEs will be rendered\\n as though they specified the :paramref:`.HasCTE.cte.nesting` flag\\n to ``True`` when they were added to this :class:`.HasCTE`.\\n Assuming the given CTEs are not referenced in an outer-enclosing\\n statement as well, the CTEs given should render at the level of\\n this statement when this flag is given.\\n\\n .. versionadded:: 2.0\\n\\n .. seealso::\\n\\n    :paramref:`.HasCTE.cte.nesting`',\n",
       " 'test #8759.\\n\\nthis should raise an error.',\n",
       " 'same as precision_numerics_many_significant_digits but within the\\ncontext of a CAST statement (hello MySQL)',\n",
       " 'Handler for the absolute value.\\n\\nExamples\\n========\\n\\n>>> from sympy import Q, Abs\\n>>> from sympy.assumptions.refine import refine_abs\\n>>> from sympy.abc import x\\n>>> refine_abs(Abs(x), Q.real(x))\\n>>> refine_abs(Abs(x), Q.positive(x))\\nx\\n>>> refine_abs(Abs(x), Q.negative(x))\\n-x',\n",
       " 'return the non-strict version of the inequality or self\\n\\nEXAMPLES\\n========\\n\\n>>> from sympy.abc import x\\n>>> (x < 1).weak\\nx <= 1\\n>>> _.weak\\nx <= 1',\n",
       " 'Performs the Lenstra–Lenstra–Lovász (LLL) basis reduction algorithm\\nand returns the reduced basis and transformation matrix.\\n\\nExplanation\\n===========\\n\\nParameters, algorithm and basis are the same as for :meth:`lll` except that\\nthe return value is a tuple `(B, T)` with `B` the reduced basis and\\n`T` a transformation matrix. The original basis `A` is transformed to\\n`B` with `T*A == B`. If only `B` is needed then :meth:`lll` should be\\nused as it is a little faster.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.domains import ZZ, QQ\\n>>> from sympy.polys.matrices import DM\\n>>> X = DM([[1, 0, 0, 0, -20160],\\n...         [0, 1, 0, 0, 33768],\\n...         [0, 0, 1, 0, 39578],\\n...         [0, 0, 0, 1, 47757]], ZZ)\\n>>> B, T = X.lll_transform(delta=QQ(5, 6))\\n>>> T * X == B\\nTrue\\n\\nSee also\\n========\\n\\nlll',\n",
       " 'Iterate over a mapping that might have a list of values, yielding\\nall key, value pairs. Almost like iter_multi_items but only allows\\nlists, not tuples, of values so tuples can be used for files.',\n",
       " 'Exhaust the stream by reading until the limit is reached or the client\\ndisconnects, returning the remaining data.\\n\\n.. versionchanged:: 2.3\\n    Return the remaining data.\\n\\n.. versionchanged:: 2.2.3\\n    Handle case where wrapped stream returns fewer bytes than requested.',\n",
       " \"An HTTPS request to an HTTP server doesn't show a traceback.\\nhttps://github.com/pallets/werkzeug/pull/838\",\n",
       " 'Include the given datetime instance in the recurrence set\\nexclusion list. Dates included that way will not be generated,\\neven if some inclusive rrule or rdate matches them. ',\n",
       " 'Send an item immediately if it can be done without waiting.\\n\\n:param item: the item to send\\n:raises ~anyio.ClosedResourceError: if this send stream has been closed\\n:raises ~anyio.BrokenResourceError: if the stream has been closed from the\\n    receiving end\\n:raises ~anyio.WouldBlock: if the buffer is full and there are no tasks waiting\\n    to receive',\n",
       " 'Returns whether node is a statement node.',\n",
       " 'Handle wr.s3.read_parquet_metadata internally.',\n",
       " 'Pre-rpc interceptor for get_aws_open_id_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AwsClusters server.',\n",
       " 'Yields:\\n    All ranges of @string which, if @string were to be split there,\\n    would result in the splitting of an f-expression (which is NOT\\n    allowed).',\n",
       " 'Store one or more lines of input.\\n\\nIf input lines are not newline-terminated, a newline is automatically\\nappended.',\n",
       " \"Windows: if 'enabled_flag' is True, enable the UNICODE and\\n_UNICODE defines in C, and declare the types like TCHAR and LPTCSTR\\nto be (pointers to) wchar_t.  If 'enabled_flag' is False,\\ndeclare these types to be (pointers to) plain 8-bit characters.\\nThis is mostly for backward compatibility; you usually want True.\",\n",
       " 'Decide whether to trace execution in `filename`.\\n\\nCalls `_should_trace_internal`, and returns the FileDisposition.',\n",
       " \"Assert that the hrefs in htmlcov/*.html are valid.\\n\\nDoesn't check external links (those with a protocol).\",\n",
       " \"Return the base directory containing Cython's caches.\\n\\nPriority:\\n\\n1. CYTHON_CACHE_DIR\\n2. (OS X): ~/Library/Caches/Cython\\n   (posix not OS X): XDG_CACHE_HOME/cython if XDG_CACHE_HOME defined\\n3. ~/.cython\",\n",
       " 'Post a metric.',\n",
       " ':param string type:\\n:param string event:\\n:param StoppedEventBody body:\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.',\n",
       " ':param array breakpoints: The instruction references of the breakpoints',\n",
       " 'Iterate over the entries in this pack index.\\n\\nReturns: iterator over tuples with object name, offset in packfile and\\n    crc32 checksum.',\n",
       " 'Iterate over cached submodules.\\n\\nArgs:\\n  store: Object store to iterate\\n  root_tree_id: SHA of root tree\\n\\nReturns:\\n  Iterator over over (path, sha) tuples',\n",
       " 'Releases the file lock. Please note, that the lock is only completely released, if the lock counter is 0.\\nAlso note, that the lock file itself is not automatically deleted.\\n\\n:param force: If true, the lock counter is ignored and the lock is released in every case/',\n",
       " 'Parses a custom_metric path into its component segments.',\n",
       " 'Pre-rpc interceptor for update_enhanced_measurement_settings\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'Post-rpc interceptor for update_repository\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ArtifactRegistry server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the list catalogs method over gRPC.\\n\\nList all catalogs in a specified project.\\n\\nReturns:\\n    Callable[[~.ListCatalogsRequest],\\n            ~.ListCatalogsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Post-rpc interceptor for get_system_policy\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SystemPolicyV1 server but before\\nit is returned to user code.',\n",
       " 'Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'compute.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'Deletes the specified target pool.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.TargetPoolsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteTargetPoolRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n            target_pool=\"target_pool_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteTargetPoolRequest, dict]):\\n        The request object. A request message for\\n        TargetPools.Delete. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    target_pool (str):\\n        Name of the TargetPool resource to\\n        delete.\\n\\n        This corresponds to the ``target_pool`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Call the delete method over HTTP.\\n\\nArgs:\\n    request (~.compute.DeleteVpnTunnelRequest):\\n        The request object. A request message for\\n    VpnTunnels.Delete. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Return a callable for the set logging service method over gRPC.\\n\\nSets the logging service for a specific cluster.\\n\\nReturns:\\n    Callable[[~.SetLoggingServiceRequest],\\n            Awaitable[~.Operation]]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Return a callable for the process open lineage run event method over gRPC.\\n\\nCreates new lineage events together with their\\nparents: process and run. Updates the process and run if\\nthey already exist. Mapped from Open Lineage\\nspecification:\\n\\nhttps://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json.\\n\\nReturns:\\n    Callable[[~.ProcessOpenLineageRunEventRequest],\\n            ~.ProcessOpenLineageRunEventResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataplex_v1.types.ListDataTaxonomiesRequest):\\n        The initial request object.\\n    response (google.cloud.dataplex_v1.types.ListDataTaxonomiesResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Return a callable for the list session entity types method over gRPC.\\n\\nReturns the list of all session entity types in the\\nspecified session.\\n\\nReturns:\\n    Callable[[~.ListSessionEntityTypesRequest],\\n            ~.ListSessionEntityTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Bulk import of multiple\\n[Document][google.cloud.discoveryengine.v1.Document]s. Request\\nprocessing may be synchronous. Non-existing items are created.\\n\\nNote: It is possible for a subset of the\\n[Document][google.cloud.discoveryengine.v1.Document]s to be\\nsuccessfully updated.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1\\n\\n    def sample_import_documents():\\n        # Create a client\\n        client = discoveryengine_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = discoveryengine_v1.ImportDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.import_documents(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1.types.ImportDocumentsRequest, dict]):\\n        The request object. Request message for Import methods.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.discoveryengine_v1.types.ImportDocumentsResponse` Response of the\\n           [ImportDocumentsRequest][google.cloud.discoveryengine.v1.ImportDocumentsRequest].\\n           If the long running operation is done, then this\\n           message is returned by the\\n           google.longrunning.Operations.response field if the\\n           operation was successful.',\n",
       " 'Call the update participant method over HTTP.\\n\\nArgs:\\n    request (~.gcd_participant.UpdateParticipantRequest):\\n        The request object. The request message for\\n    [Participants.UpdateParticipant][google.cloud.dialogflow.v2.Participants.UpdateParticipant].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.gcd_participant.Participant:\\n        Represents a conversation participant\\n    (human agent, virtual agent, end-user).',\n",
       " 'Return a callable for the batch create target sites method over gRPC.\\n\\nCreates [TargetSite][google.cloud.discoveryengine.v1.TargetSite]\\nin a batch.\\n\\nReturns:\\n    Callable[[~.BatchCreateTargetSitesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a file_store_data_profile path into its component segments.',\n",
       " 'Return a callable for the list volume backups method over gRPC.\\n\\nLists the VolumeBackups for a given Backup.\\n\\nReturns:\\n    Callable[[~.ListVolumeBackupsRequest],\\n            ~.ListVolumeBackupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AttachedClustersAsyncClient: The constructed client.',\n",
       " 'Resets an Identity Aware Proxy (IAP) OAuth client\\nsecret. Useful if the secret was compromised. Requires\\nthat the client is owned by IAP.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iap_v1\\n\\n    def sample_reset_identity_aware_proxy_client_secret():\\n        # Create a client\\n        client = iap_v1.IdentityAwareProxyOAuthServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = iap_v1.ResetIdentityAwareProxyClientSecretRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.reset_identity_aware_proxy_client_secret(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iap_v1.types.ResetIdentityAwareProxyClientSecretRequest, dict]):\\n        The request object. The request sent to\\n        ResetIdentityAwareProxyClientSecret.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iap_v1.types.IdentityAwareProxyClient:\\n        Contains the data that describes an\\n        Identity Aware Proxy owned client.',\n",
       " 'Call the list assets method over HTTP.\\n\\nArgs:\\n    request (~.migrationcenter.ListAssetsRequest):\\n        The request object. Message for requesting a list of\\n    assets.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.migrationcenter.ListAssetsResponse:\\n        Response message for listing assets.',\n",
       " 'Post-rpc interceptor for update_http_route\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the NetworkServices server but before\\nit is returned to user code.',\n",
       " 'Deletes a single Exadata Infrastructure.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import oracledatabase_v1\\n\\n    def sample_delete_cloud_exadata_infrastructure():\\n        # Create a client\\n        client = oracledatabase_v1.OracleDatabaseClient()\\n\\n        # Initialize request argument(s)\\n        request = oracledatabase_v1.DeleteCloudExadataInfrastructureRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_cloud_exadata_infrastructure(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.oracledatabase_v1.types.DeleteCloudExadataInfrastructureRequest, dict]):\\n        The request object. The request for ``CloudExadataInfrastructure.Delete``.\\n    name (str):\\n        Required. The name of the Cloud Exadata Infrastructure\\n        in the following format:\\n        projects/{project}/locations/{location}/cloudExadataInfrastructures/{cloud_exadata_infrastructure}.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Parses a ssh_public_key path into its component segments.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsRequest):\\n        The initial request object.\\n    response (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Create the client designed to process long-running operations.\\n\\nThis property caches on the instance; repeated calls return the same\\nclient.',\n",
       " 'Post-rpc interceptor for get_tag_value\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the TagValues server but before\\nit is returned to user code.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " ':param sec_key_ref:\\n    A Security framework SecKeyRef value from loading/importing the\\n    key\\n\\n:param asn1:\\n    An asn1crypto.keys.PrivateKeyInfo object',\n",
       " 'Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'run.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " 'Return a callable for the test iam permissions method over gRPC.\\n\\nReturns the permissions that a caller has on the\\nspecified source.\\n\\nReturns:\\n    Callable[[~.TestIamPermissionsRequest],\\n            ~.TestIamPermissionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list event threat\\ndetection custom modules method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.ListEventThreatDetectionCustomModulesRequest):\\n            The request object. Request to list Event Threat\\n        Detection custom modules.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.securitycenter_service.ListEventThreatDetectionCustomModulesResponse:\\n            Response for listing Event Threat\\n        Detection custom modules.',\n",
       " 'Post-rpc interceptor for get_company\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CompanyService server but before\\nit is returned to user code.',\n",
       " 'Searches across deployment revisions.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import telcoautomation_v1alpha1\\n\\n    def sample_search_deployment_revisions():\\n        # Create a client\\n        client = telcoautomation_v1alpha1.TelcoAutomationClient()\\n\\n        # Initialize request argument(s)\\n        request = telcoautomation_v1alpha1.SearchDeploymentRevisionsRequest(\\n            parent=\"parent_value\",\\n            query=\"query_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_deployment_revisions(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.telcoautomation_v1alpha1.types.SearchDeploymentRevisionsRequest, dict]):\\n        The request object. Request object for ``SearchDeploymentRevisions``.\\n    parent (str):\\n        Required. The name of parent orchestration cluster\\n        resource. Format should be -\\n        \"projects/{project_id}/locations/{location_name}/orchestrationClusters/{orchestration_cluster}\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    query (str):\\n        Required. Supported queries:\\n\\n        1. \"\"                       : Lists all\\n            revisions across all deployments.\\n        2. \"latest=true\"            : Lists\\n            latest revisions across all\\n            deployments.\\n        3. \"name={name}\"            : Lists all\\n            revisions of deployment with name\\n            {name}.\\n        4. \"name={name} latest=true\": Lists\\n            latest revision of deployment with\\n            name {name}\\n\\n        This corresponds to the ``query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.telcoautomation_v1alpha1.services.telco_automation.pagers.SearchDeploymentRevisionsPager:\\n        Response object for SearchDeploymentRevisions.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the list slates method over gRPC.\\n\\nLists all slates in the specified project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListSlatesRequest],\\n            ~.ListSlatesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Cell magics declared via a class, #2',\n",
       " 'A list of the captured rich display outputs, if any.\\n\\nIf you have a CapturedIO object ``c``, these can be displayed in IPython\\nusing::\\n\\n    from IPython.display import display\\n    for o in c.outputs:\\n        display(o)',\n",
       " 'Returns an instance of a compressor file object.',\n",
       " 'preprocess old style comments.\\n\\nexample:\\n\\nfrom mako.ext.preprocessors import convert_comments\\nt = Template(..., preprocessor=convert_comments)',\n",
       " 'Return text with backslash escapes undone (backslashes are restored). ',\n",
       " 'Demonstrates displaying different variables through shade and color.',\n",
       " 'Copy properties from *other* to *self*.',\n",
       " 'Return the depth of the axis used by the picker.',\n",
       " 'Read one page from the file. Return True if successful,\\nFalse if there were no more pages.',\n",
       " \"Return this line's `~matplotlib.transforms.TransformedPath`.\",\n",
       " 'Convert the given BSON value into our own type.',\n",
       " 'Raise error on missing token.',\n",
       " '`MeterProvider.get_meter` arguments are used to create an\\n`InstrumentationScope` object on the created `Meter`.',\n",
       " 'Register a function to convert a core foundation data type into its\\nequivalent in python\\n\\n:param type_id:\\n    The CFTypeId for the type\\n\\n:param callback:\\n    A callback to pass the CFType object to',\n",
       " 'Simple helper method to create data for to ``to_dict(orient=\"split\")``\\nto create the main output data',\n",
       " 'Return a string representation for a particular Series.',\n",
       " 'Appends lines to a buffer.\\n\\nParameters\\n----------\\nbuf\\n    The buffer to write to\\nlines\\n    The lines to append.',\n",
       " 'Check that two objects are not approximately equal.\\n\\nParameters\\n----------\\na : object\\n    The first object to compare.\\nb : object\\n    The second object to compare.\\n**kwargs\\n    The arguments passed to `tm.assert_almost_equal`.',\n",
       " 'Iterate through file entries declared in this distribution.\\n\\nFor modern .dist-info distributions, this is the files listed in the\\n``RECORD`` metadata file. For legacy setuptools distributions, this\\ncomes from ``installed-files.txt``, with entries normalized to be\\ncompatible with the format used by ``RECORD``.\\n\\n:return: An iterator for listed entries, or None if the distribution\\n    contains neither ``RECORD`` nor ``installed-files.txt``.',\n",
       " 'Dispatches a hook dictionary on a given piece of data.',\n",
       " 'Verify that installing works from a link with an extra if there is an indirect\\ndependency on that same package with the same extra (#12372).',\n",
       " 'Returns a dictionary of token types, matched to their action in the parser.\\n\\nOnly returns token types that are accepted by the current state.\\n\\nUpdated by ``feed_token()``.',\n",
       " 'Alternate implementation using /proc/cpuinfo.\\nmin and max frequencies are not available and are set to None.',\n",
       " 'Re-run tests which failed on last run.',\n",
       " 'The context argument for `PydanticKnownError` requires a number or str type, so we do a simple repr() coercion for types like timedelta.\\n\\nSee tests/test_types.py::test_annotated_metadata_any_order for some context.',\n",
       " 'Grant owner access to the current entity.',\n",
       " 'Return the more user-friendly information about the location of a warning, or None.',\n",
       " \"Regression test for importing a submodule 'foo.bar' while there is a 'bar' directory\\nreachable from sys.path -- ensuring the top-level module does not end up imported as a namespace\\npackage.\\n\\n#12194\\nhttps://github.com/pytest-dev/pytest/pull/12208#issuecomment-2056458432\",\n",
       " 'Get the next page in the iterator.\\n\\nReturns:\\n    Page: The next page in the iterator or :data:`None` if\\n        there are no pages left.',\n",
       " \"Patch a table's metadata.\",\n",
       " 'Return query result and display a progress bar while the query running, if tqdm is installed.\\n\\nArgs:\\n    query_job:\\n        The job representing the execution of the query on the server.\\n    progress_bar_type:\\n        The type of progress bar to use to show query progress.\\n    max_results:\\n        The maximum number of rows the row iterator should return.\\n\\nReturns:\\n    A row iterator over the query results.',\n",
       " 'The configuration for this copy job.',\n",
       " 'Optional[str]: Description of the destination table.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.description',\n",
       " \"Scan the input for current state's tokens starting at ``current_offset``.\\n\\nArgs:\\n    state (LexerState): The current lexer state.\\n    current_offset (int): The offset in the input text, i.e. the number\\n        of characters already scanned so far.\\n\\nYields:\\n    The next ``Token`` or ``StateTransition`` instance.\",\n",
       " 'Sets the terminating of this V1EndpointConditions.\\n\\nterminating indicates that this endpoint is terminating. A nil value indicates an unknown state. Consumers should interpret this unknown state to mean that the endpoint is not terminating.  # noqa: E501\\n\\n:param terminating: The terminating of this V1EndpointConditions.  # noqa: E501\\n:type: bool',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Returns the hyperparameters as a dictionary to use for training.\\n\\nThe :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which\\ntrains the model, calls this method to find the hyperparameters.\\n\\nReturns:\\n    dict[str, str]: The hyperparameters.',\n",
       " 'Instantiate a new SparkOutputManager for PySpark functions.\\n\\nArgs:\\n    feature_store_manager_factory (FeatureStoreManagerFactory): A factory to provide\\n        that provides a FeatureStoreManager that handles data ingestion to a Feature Group.\\n        The factory lazily loads the FeatureStoreManager.\\n\\nReturns:\\n    SparkOutputReceiver: An instance that handles outputs of the wrapped function.',\n",
       " 'Returns schema version.',\n",
       " 'Initialize the class.\\n\\nArgs:\\n    force (bool): If True, render colorizes output no matter where the\\n        output is (default: False).',\n",
       " 'Describe the latest baselining job kicked off by the suggest workflow.',\n",
       " 'Deletes the Amazon SageMaker models backing this predictor.',\n",
       " 'Validate mutually exclusive property file / s3uri',\n",
       " 'Compute gradient and hessian of loss w.r.t raw_prediction.\\n\\nParameters\\n----------\\ny_true : C-contiguous array of shape (n_samples,)\\n    Observed, true target values.\\nraw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\\n    Raw prediction values (in link space).\\nsample_weight : None or C-contiguous array of shape (n_samples,)\\n    Sample weights.\\ngradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the gradient is stored. If None, a new array\\n    might be created.\\nhessian_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the hessian is stored. If None, a new array\\n    might be created.\\nn_threads : int, default=1\\n    Might use openmp thread parallelism.\\n\\nReturns\\n-------\\ngradient : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise gradients.\\n\\nhessian : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise hessians.',\n",
       " 'Check that expected error are raised during fit.',\n",
       " 'Time conventional Voronoi diagram calculation.',\n",
       " 'Construct the sigma matrix in SVD from singular values and size M, N.\\n\\nParameters\\n----------\\ns : (M,) or (N,) array_like\\n    Singular values\\nM : int\\n    Size of the matrix whose singular values are `s`.\\nN : int\\n    Size of the matrix whose singular values are `s`.\\n\\nReturns\\n-------\\nS : (M, N) ndarray\\n    The S-matrix in the singular value decomposition\\n\\nSee Also\\n--------\\nsvd : Singular value decomposition of a matrix\\nsvdvals : Compute singular values of a matrix.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy.linalg import diagsvd\\n>>> vals = np.array([1, 2, 3])  # The array representing the computed svd\\n>>> diagsvd(vals, 3, 4)\\narray([[1, 0, 0, 0],\\n       [0, 2, 0, 0],\\n       [0, 0, 3, 0]])\\n>>> diagsvd(vals, 4, 3)\\narray([[1, 0, 0],\\n       [0, 2, 0],\\n       [0, 0, 3],\\n       [0, 0, 0]])',\n",
       " 'Minimize a function using a nonlinear conjugate gradient algorithm.\\n\\nParameters\\n----------\\nf : callable, ``f(x, *args)``\\n    Objective function to be minimized. Here `x` must be a 1-D array of\\n    the variables that are to be changed in the search for a minimum, and\\n    `args` are the other (fixed) parameters of `f`.\\nx0 : ndarray\\n    A user-supplied initial estimate of `xopt`, the optimal value of `x`.\\n    It must be a 1-D array of values.\\nfprime : callable, ``fprime(x, *args)``, optional\\n    A function that returns the gradient of `f` at `x`. Here `x` and `args`\\n    are as described above for `f`. The returned value must be a 1-D array.\\n    Defaults to None, in which case the gradient is approximated\\n    numerically (see `epsilon`, below).\\nargs : tuple, optional\\n    Parameter values passed to `f` and `fprime`. Must be supplied whenever\\n    additional fixed parameters are needed to completely specify the\\n    functions `f` and `fprime`.\\ngtol : float, optional\\n    Stop when the norm of the gradient is less than `gtol`.\\nnorm : float, optional\\n    Order to use for the norm of the gradient\\n    (``-np.inf`` is min, ``np.inf`` is max).\\nepsilon : float or ndarray, optional\\n    Step size(s) to use when `fprime` is approximated numerically. Can be a\\n    scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\\n    floating point machine precision.  Usually ``sqrt(eps)`` is about\\n    1.5e-8.\\nmaxiter : int, optional\\n    Maximum number of iterations to perform. Default is ``200 * len(x0)``.\\nfull_output : bool, optional\\n    If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\\n    addition to `xopt`.  See the Returns section below for additional\\n    information on optional return values.\\ndisp : bool, optional\\n    If True, return a convergence message, followed by `xopt`.\\nretall : bool, optional\\n    If True, add to the returned values the results of each iteration.\\ncallback : callable, optional\\n    An optional user-supplied function, called after each iteration.\\n    Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\\nc1 : float, default: 1e-4\\n    Parameter for Armijo condition rule.\\nc2 : float, default: 0.4\\n    Parameter for curvature condition rule.\\n\\nReturns\\n-------\\nxopt : ndarray\\n    Parameters which minimize f, i.e., ``f(xopt) == fopt``.\\nfopt : float, optional\\n    Minimum value found, f(xopt). Only returned if `full_output` is True.\\nfunc_calls : int, optional\\n    The number of function_calls made. Only returned if `full_output`\\n    is True.\\ngrad_calls : int, optional\\n    The number of gradient calls made. Only returned if `full_output` is\\n    True.\\nwarnflag : int, optional\\n    Integer value with warning status, only returned if `full_output` is\\n    True.\\n\\n    0 : Success.\\n\\n    1 : The maximum number of iterations was exceeded.\\n\\n    2 : Gradient and/or function calls were not changing. May indicate\\n        that precision was lost, i.e., the routine did not converge.\\n\\n    3 : NaN result encountered.\\n\\nallvecs : list of ndarray, optional\\n    List of arrays, containing the results at each iteration.\\n    Only returned if `retall` is True.\\n\\nSee Also\\n--------\\nminimize : common interface to all `scipy.optimize` algorithms for\\n           unconstrained and constrained minimization of multivariate\\n           functions. It provides an alternative way to call\\n           ``fmin_cg``, by specifying ``method=\\'CG\\'``.\\n\\nNotes\\n-----\\nThis conjugate gradient algorithm is based on that of Polak and Ribiere\\n[1]_.\\n\\nConjugate gradient methods tend to work better when:\\n\\n1. `f` has a unique global minimizing point, and no local minima or\\n   other stationary points,\\n2. `f` is, at least locally, reasonably well approximated by a\\n   quadratic function of the variables,\\n3. `f` is continuous and has a continuous gradient,\\n4. `fprime` is not too large, e.g., has a norm less than 1000,\\n5. The initial guess, `x0`, is reasonably close to `f` \\'s global\\n   minimizing point, `xopt`.\\n\\nParameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\\n\\nReferences\\n----------\\n.. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\\n\\nExamples\\n--------\\nExample 1: seek the minimum value of the expression\\n``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\\nof the parameters and an initial guess ``(u, v) = (0, 0)``.\\n\\n>>> import numpy as np\\n>>> args = (2, 3, 7, 8, 9, 10)  # parameter values\\n>>> def f(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\\n>>> def gradf(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     gu = 2*a*u + b*v + d     # u-component of the gradient\\n...     gv = b*u + 2*c*v + e     # v-component of the gradient\\n...     return np.asarray((gu, gv))\\n>>> x0 = np.asarray((0, 0))  # Initial guess.\\n>>> from scipy import optimize\\n>>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\\nOptimization terminated successfully.\\n         Current function value: 1.617021\\n         Iterations: 4\\n         Function evaluations: 8\\n         Gradient evaluations: 8\\n>>> res1\\narray([-1.80851064, -0.25531915])\\n\\nExample 2: solve the same problem using the `minimize` function.\\n(This `myopts` dictionary shows all of the available options,\\nalthough in practice only non-default values would be needed.\\nThe returned value will be a dictionary.)\\n\\n>>> opts = {\\'maxiter\\' : None,    # default value.\\n...         \\'disp\\' : True,    # non-default value.\\n...         \\'gtol\\' : 1e-5,    # default value.\\n...         \\'norm\\' : np.inf,  # default value.\\n...         \\'eps\\' : 1.4901161193847656e-08}  # default value.\\n>>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\\n...                          method=\\'CG\\', options=opts)\\nOptimization terminated successfully.\\n        Current function value: 1.617021\\n        Iterations: 4\\n        Function evaluations: 8\\n        Gradient evaluations: 8\\n>>> res2.x  # minimum found\\narray([-1.80851064, -0.25531915])',\n",
       " 'This test is for backwards compatibility post scipy 1.13.\\nThe behavior observed here is what is to be expected\\nwith the older matrix classes. This test comes with the\\nexception of dok_matrix, which was not working pre scipy1.12\\n(unlike the rest of these).',\n",
       " 'Call this method once you are done using the instance. It is automatically\\ncalled on destruction, and should be called just in time to allow system\\nresources to be freed.\\n\\nOnce you called end_access, you must call begin access before reusing this instance!',\n",
       " \"When join conditions don't express the left side explicitly,\\ndetermine if an existing FROM or entity in this query\\ncan serve as the left hand side.\",\n",
       " 'Test that any link will not match in XHTML (all links are unvisited).',\n",
       " 'Test input direction when input is the root.',\n",
       " 'Runs distributed training.\\n\\nParameters\\n----------\\ntrain_object : callable object or str\\n    Either a PyTorch function, PyTorch Lightning function, or the path to a python file\\n    that launches distributed training.\\nargs :\\n    If train_object is a python function and not a path to a python file, args need\\n    to be the input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, 1e-3, 64)\\n\\n    where train is a function and 1e-3 and 64 are regular numeric inputs to the function.\\n\\n    If train_object is a python file, then args would be the command-line arguments for\\n    that python file which are all in the form of strings. An example would be\\n\\n    >>> distributor.run(\"/path/to/train.py\", \"--learning-rate=1e-3\", \"--batch-size=64\")\\n\\n    where since the input is a path, all of the parameters are strings that can be\\n    handled by argparse in that python file.\\nkwargs :\\n    If train_object is a python function and not a path to a python file, kwargs need\\n    to be the key-word input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, tol=1e-3, max_iter=64)\\n\\n    where train is a function of 2 arguments `tol` and `max_iter`.\\n\\n    If train_object is a python file, then you should not set kwargs arguments.\\n\\nReturns\\n-------\\n    Returns the output of train_object called with args inside spark rank 0 task if the\\n    train_object is a Callable with an expected output. Returns None if train_object is\\n    a file.',\n",
       " 'Rows of the RowMatrix stored as an RDD of vectors.\\n\\nExamples\\n--------\\n>>> mat = RowMatrix(sc.parallelize([[1, 2, 3], [4, 5, 6]]))\\n>>> rows = mat.rows\\n>>> rows.first()\\nDenseVector([1.0, 2.0, 3.0])',\n",
       " \"test additional use case that wasn't considered for #8372\",\n",
       " 'Filter ``only`` nodes which do not match *tags*.',\n",
       " 'Fetch the first column of the first row, and close the result set.\\n\\nReturns ``None`` if there are no rows to fetch.\\n\\nNo validation is performed to test if additional rows remain.\\n\\nAfter calling this method, the object is fully closed,\\ne.g. the :meth:`_engine.CursorResult.close`\\nmethod will have been called.\\n\\n:return: a Python scalar value, or ``None`` if no rows remain.',\n",
       " 'Return the attribute name that should be used to refer from one\\nclass to another, for a collection reference.\\n\\nThe default implementation is::\\n\\n    return referred_cls.__name__.lower() + \"_collection\"\\n\\nAlternate implementations\\ncan be specified using the\\n:paramref:`.AutomapBase.prepare.name_for_collection_relationship`\\nparameter.\\n\\n:param base: the :class:`.AutomapBase` class doing the prepare.\\n\\n:param local_cls: the class to be mapped on the local side.\\n\\n:param referred_cls: the class to be mapped on the referring side.\\n\\n:param constraint: the :class:`_schema.ForeignKeyConstraint` that is being\\n inspected to produce this relationship.',\n",
       " 'Tries to find more initial conditions by substituting the initial\\nvalue point in the differential equation.',\n",
       " 'Converts a term in the expansion of a function from binary to its\\nvariable form (for POS).',\n",
       " \"Young's Modulus of the Beam. \",\n",
       " 'Expands the transfer function matrix',\n",
       " 'The length of the spring at which it produces no force.',\n",
       " 'Tests max degrees function.',\n",
       " 'Returns the first `n` terms of the composed formal power series.\\nTerm by term logic is implemented here.\\n\\nExplanation\\n===========\\n\\nThe coefficient sequence of the :obj:`FormalPowerSeriesCompose` object is the generic sequence.\\nIt is multiplied by ``bell_seq`` to get a sequence, whose terms are added up to get\\nthe final terms for the polynomial.\\n\\nExamples\\n========\\n\\n>>> from sympy import fps, sin, exp\\n>>> from sympy.abc import x\\n>>> f1 = fps(exp(x))\\n>>> f2 = fps(sin(x))\\n>>> fcomp = f1.compose(f2, x)\\n\\n>>> fcomp._eval_terms(6)\\n-x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\n>>> fcomp._eval_terms(8)\\nx**7/90 - x**6/240 - x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\nSee Also\\n========\\n\\nsympy.series.formal.FormalPowerSeries.compose\\nsympy.series.formal.FormalPowerSeries.coeff_bell',\n",
       " 'Apply ``_hasattrs`` and ``_hastypes`` to ``expr``. ',\n",
       " 'Show that options are not passed for older versions of requests.',\n",
       " 'Test CLI --manpath',\n",
       " 'Completion hints for argcomplete',\n",
       " 'validate object to defined invariants.',\n",
       " 'Encodes the value using DER\\n\\n:param force:\\n    If the encoded contents already exist, clear them and regenerate\\n    to ensure they are in DER format instead of BER format\\n\\n:return:\\n    A byte string of the DER-encoded value',\n",
       " 'The native Python datatype representation of this value\\n\\n:return:\\n    A byte string or None',\n",
       " 'Writes the loaded data back out to the JSON index.',\n",
       " 'Should determine names and address with a hostname override.',\n",
       " 'Property suppress_warnings.',\n",
       " 'Filter only running futures.',\n",
       " 'StringTransformer instances have a call signature that mirrors that of\\nthe Transformer type.\\n\\nRaises:\\n    CannotTransform(...) if the concrete StringTransformer class is unable\\n    to transform @line.',\n",
       " \"Acquire the GIL. The thread's thread state must have been initialized\\nby a previous `put_release_gil`\",\n",
       " 'Delete a specific SLO.\\n\\n:param id: SLO id to delete\\n:type id: str\\n\\n:returns: SLO ids removed',\n",
       " \"Sets the tracing function with the pydev debug function and initializes needed facilities.\\n\\n:param host: the user may specify another host, if the debug server is not in the same machine (default is the local\\n    host)\\n\\n:param stdout_to_server: when this is true, the stdout is passed to the debug server\\n\\n:param stderr_to_server: when this is true, the stderr is passed to the debug server\\n    so that they are printed in its console and not in this process console.\\n\\n:param port: specifies which port to use for communicating with the server (note that the server must be started\\n    in the same port). @note: currently it's hard-coded at 5678 in the client\\n\\n:param suspend: whether a breakpoint should be emulated as soon as this function is called.\\n\\n:param trace_only_current_thread: determines if only the current thread will be traced or all current and future\\n    threads will also have the tracing enabled.\\n\\n:param overwrite_prev_trace: deprecated\\n\\n:param patch_multiprocessing: if True we'll patch the functions which create new processes so that launched\\n    processes are debugged.\\n\\n:param stop_at_frame: if passed it'll stop at the given frame, otherwise it'll stop in the function which\\n    called this method.\\n\\n:param wait_for_ready_to_run: if True settrace will block until the ready_to_run flag is set to True,\\n    otherwise, it'll set ready_to_run to True and this function won't block.\\n\\n    Note that if wait_for_ready_to_run == False, there are no guarantees that the debugger is synchronized\\n    with what's configured in the client (IDE), the only guarantee is that when leaving this function\\n    the debugger will be already connected.\\n\\n:param dont_trace_start_patterns: if set, then any path that starts with one fo the patterns in the collection\\n    will not be traced\\n\\n:param dont_trace_end_patterns: if set, then any path that ends with one fo the patterns in the collection\\n    will not be traced\\n\\n:param access_token: token to be sent from the client (i.e.: IDE) to the debugger when a connection\\n    is established (verified by the debugger).\\n\\n:param client_access_token: token to be sent from the debugger to the client (i.e.: IDE) when\\n    a connection is established (verified by the client).\\n\\n:param notify_stdin:\\n    If True sys.stdin will be patched to notify the client when a message is requested\\n    from the IDE. This is done so that when reading the stdin the client is notified.\\n    Clients may need this to know when something that is being written should be interpreted\\n    as an input to the process or as a command to be evaluated.\\n    Note that parallel-python has issues with this (because it tries to assert that sys.stdin\\n    is of a given type instead of just checking that it has what it needs).\\n\\n:param protocol:\\n    When using in Eclipse the protocol should not be passed, but when used in VSCode\\n    or some other IDE/editor that accepts the Debug Adapter Protocol then 'dap' should\\n    be passed.\",\n",
       " 'check version string found_version >= expected_min_or_eq_to_version\\n\\nIf dev/prerelease tags result in TypeError for string-number comparison,\\nit is assumed that the dependency is satisfied.\\nUsers on dev branches are responsible for keeping their own packages up to date.',\n",
       " '@type  crash: L{Crash}\\n@param crash: Crash object.\\n\\n@rtype:  bool\\n@return:\\n    C{True} if a Crash object with the same key is in the container.',\n",
       " '@rtype:  int\\n@return: Exit code of the thread.',\n",
       " '@type  dwThreadId: int\\n@param dwThreadId: Global thread ID.\\n\\n@type  hThread: L{ThreadHandle}\\n@param hThread: (Optional) Handle to the thread.\\n\\n@type  process: L{Process}\\n@param process: (Optional) Parent Process object.',\n",
       " ...]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['docstring'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# good model for code summarization\n",
    "MODEL = 'google/pegasus-cnn_dailymail'\n",
    "\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    function plural(self,plural): \\n if self.local...\n",
      "1    function max_unavailable(self): \\n return self...\n",
      "2    function pods(self): \\n return self._pods \\n \\...\n",
      "3    function start(self): \\n if self.thread is not...\n",
      "4    function tree(self): \\n from rich.styled impor...\n",
      "5    function test_reenable_highlighting(): \\n cons...\n",
      "6    function is_pipeline_variable(var): \\n return ...\n",
      "7    function fixture_fixture_intended_uses_example...\n",
      "8    function _add_prefix_for_feature_names_out(sel...\n",
      "9    function std(self): \\n kwds['moments'] = 'v'\\n...\n",
      "Name: summarization_input, dtype: object\n",
      "function max_unavailable(self): \n",
      " return self._max_unavailable \n",
      " \n",
      " docstring: Gets the max_unavailable of this V1PodDisruptionBudgetSpec.  # noqa: E501\n",
      "\n",
      "An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions by specifying 0. This is a mutually exclusive setting with \"minAvailable\".  # noqa: E501\n",
      "\n",
      ":return: The max_unavailable of this V1PodDisruptionBudgetSpec.  # noqa: E501\n",
      ":rtype: object \n",
      " short docstring: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try summarization of docstrings\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_summarization_input(row):\n",
    "    docstring = row['docstring']\n",
    "    body = row['body']\n",
    "    function_name = row['name']\n",
    "    args = row['args']\n",
    "    args = args.replace('{', '').replace('}', '')\n",
    "\n",
    "    if len(body) > 50:\n",
    "        body = body[:50] + '...'\n",
    "\n",
    "    return f\"def {function_name}({args}): \\n {body} \\n \\n docstring: {docstring} \\n short docstring: \\n\"\n",
    "values['summarization_input'] = values.apply(generate_summarization_input, axis=1)\n",
    "\n",
    "print(values['summarization_input'])\n",
    "\n",
    "input_ids = [tokenizer.encode(x, return_tensors='pt', max_length=4096) for x in values['summarization_input']]\n",
    "\n",
    "print(values['summarization_input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd407ce969594c5d997c877b1451cd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[1;32m---> 11\u001b[0m values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_docstring\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [summarize_docstring(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstring\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "Cell \u001b[1;32mIn[160], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m     summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[1;32m---> 11\u001b[0m values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_docstring\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43msummarize_docstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstring\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "Cell \u001b[1;32mIn[160], line 7\u001b[0m, in \u001b[0;36msummarize_docstring\u001b[1;34m(docstring)\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize to docstring: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m docstring, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:2246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2239\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2240\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2241\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2243\u001b[0m     )\n\u001b[0;32m   2245\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2247\u001b[0m         input_ids,\n\u001b[0;32m   2248\u001b[0m         beam_scorer,\n\u001b[0;32m   2249\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2250\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2251\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2252\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2254\u001b[0m     )\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2259\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2260\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2267\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\generation\\utils.py:3455\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3452\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[0;32m   3454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3455\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3458\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3459\u001b[0m     outputs,\n\u001b[0;32m   3460\u001b[0m     model_kwargs,\n\u001b[0;32m   3461\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3462\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1353\u001b[0m, in \u001b[0;36mPegasusForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1349\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1350\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1351\u001b[0m         )\n\u001b[1;32m-> 1353\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1372\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1215\u001b[0m, in \u001b[0;36mPegasusModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1208\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1209\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1210\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1211\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1212\u001b[0m     )\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1215\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:1047\u001b[0m, in \u001b[0;36mPegasusDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1034\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1035\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1036\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1044\u001b[0m         use_cache,\n\u001b[0;32m   1045\u001b[0m     )\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1047\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:426\u001b[0m, in \u001b[0;36mPegasusDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    425\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 426\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    435\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:201\u001b[0m, in \u001b[0;36mPegasusAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    198\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_states, value_states)\n\u001b[0;32m    200\u001b[0m proj_shape \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 201\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mproj_shape)\n\u001b[0;32m    202\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mproj_shape)\n\u001b[0;32m    203\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mproj_shape)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:141\u001b[0m, in \u001b[0;36mPegasusAttention._shape\u001b[1;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mview(bsz, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    146\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Generate summaries for the docstrings\n",
    "def summarize_docstring(docstring):\n",
    "    inputs = tokenizer.encode(\"summarize to docstring: \" + docstring, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "values['short_docstring'] = [summarize_docstring(doc) for doc in tqdm(values['docstring'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                              Sets the plural of this V1CustomResourceDefinitionNames.<n>param plural: The plural of this V1CustomResourceDefinitionNames.\n",
      "1    An eviction is allowed if at most \"maxUnavailable\" pods selected by \"selector\" are unavailable after the eviction, i.e. even in absence of the evicted pod.<n>This is a mutually exclusive setting with \"minAvailable\"\n",
      "2                                                                                                                                             Gets the pods of this V2MetricSpec. # noqa: E501 :rtype: V2PodsMetricSource .\n",
      "3                            Creates a new zmq Context used for garbage collection.<n>Under most circumstances, this will only be called once per process.<n>Summarize to docstring: Start a new garbage collection thread.\n",
      "4                                                        Get a tree renderable to show layout structure .<n>Use this to show layout structure in a tree .<n>Get a tree renderable to show layout structure in a docstring .\n",
      "5                            When highlighting is disabled, it can be reenabled in print() .<n>Use the following code to highlight a line in a document .<n>Use the following code to highlight a line in a text document .\n",
      "6                                                                            Check if the variable is a pipeline variable Args: var (object): The variable to be verified.<n>Returns: bool: True if it is, False otherwise.\n",
      "7                                                       summarize to docstring: Example intended uses instance.<n>This page includes a summary of the example used in this page .<n>This page was last modified on 18 June.\n",
      "8                                                        Returns ------- feature_names_out : ndarray of shape (n_features), dtype=str Transformed feature names.<n>Adds feature names out that includes transformer names .\n",
      "9                                                     Returns ------- : float standard deviation of the distribution .<n> Parameters ---------- arg1, arg2, arg3,... : array_like .<n>loc : array_like, optional location .\n",
      "Name: short_docstring, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(values['short_docstring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function plural(self,plural): \n",
      " if self.local_vars_configuration.client_side_valid... \n",
      " \n",
      " long docstring: Sets the plural of this V1CustomResourceDefinitionNames.\n",
      "\n",
      "plural is the plural name of the resource to serve. The custom resources are served under `/apis/<group>/<version>/.../<plural>`. Must match the name of the CustomResourceDefinition (in the form `<names.plural>.<group>`). Must be all lowercase.  # noqa: E501\n",
      "\n",
      ":param plural: The plural of this V1CustomResourceDefinitionNames.  # noqa: E501\n",
      ":type: str \n",
      "\n",
      " short docsting: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(values['summarization_input'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sets the plural of this V1CustomResourceDefinitionNames.<n>param plural: The plural of this V1CustomResourceDefinitionNames.\n"
     ]
    }
   ],
   "source": [
    "print(values['short_docstring'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
