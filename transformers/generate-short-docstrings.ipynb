{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "      <th>args_types</th>\n",
       "      <th>args_defaults</th>\n",
       "      <th>body</th>\n",
       "      <th>docstring</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>31048</td>\n",
       "      <td>100</td>\n",
       "      <td>setwinsize</td>\n",
       "      <td>{self,rows,cols}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self.ptyproc.setwinsize(rows, cols)</td>\n",
       "      <td>This sets the terminal window size of the chil...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>31251</td>\n",
       "      <td>20</td>\n",
       "      <td>metadata_dict</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return msg_to_json(self.metadata)</td>\n",
       "      <td>PEP 566 compliant JSON-serializable representa...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>31364</td>\n",
       "      <td>20</td>\n",
       "      <td>exports</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>result = {}\\nr = self.get_distinfo_resource(EX...</td>\n",
       "      <td>Return the information exported by this distri...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>31517</td>\n",
       "      <td>20</td>\n",
       "      <td>log</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>if not objects:\\n    objects = (NewLine(),)\\nr...</td>\n",
       "      <td>Log rich content to the terminal.\\n\\nArgs:\\n  ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>31706</td>\n",
       "      <td>20</td>\n",
       "      <td>test_as_import</td>\n",
       "      <td>{script}</td>\n",
       "      <td>{PipTestEnvironment}</td>\n",
       "      <td>{}</td>\n",
       "      <td>import pip._internal.commands.install as inst\\...</td>\n",
       "      <td>test that pip.__init__.py does not shadow\\nthe...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119824</th>\n",
       "      <td>27697</td>\n",
       "      <td>16</td>\n",
       "      <td>isMaskedArray</td>\n",
       "      <td>{x}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return isinstance(x, MaskedArray)</td>\n",
       "      <td>Test whether input is an instance of MaskedArr...</td>\n",
       "      <td>551641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119916</th>\n",
       "      <td>29324</td>\n",
       "      <td>22</td>\n",
       "      <td>describe_timestamp_1d</td>\n",
       "      <td>{data,percentiles}</td>\n",
       "      <td>{Series,Sequence[float]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>from pandas import Series\\nformatted_percentil...</td>\n",
       "      <td>Describe series containing datetime64 dtype.\\n...</td>\n",
       "      <td>551650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119940</th>\n",
       "      <td>29336</td>\n",
       "      <td>22</td>\n",
       "      <td>asfreq</td>\n",
       "      <td>{obj,freq,method,how,normalize,fill_value}</td>\n",
       "      <td>{NDFrameT,bool}</td>\n",
       "      <td>{None,None,False,None}</td>\n",
       "      <td>if isinstance(obj.index, PeriodIndex):\\n    if...</td>\n",
       "      <td>Utility frequency conversion method for Series...</td>\n",
       "      <td>551651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119941</th>\n",
       "      <td>29445</td>\n",
       "      <td>22</td>\n",
       "      <td>_get_custom_index_name</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self.xlabel</td>\n",
       "      <td>Specify whether xlabel/ylabel should be used t...</td>\n",
       "      <td>551652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120015</th>\n",
       "      <td>39350</td>\n",
       "      <td>234</td>\n",
       "      <td>projected</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self._projected</td>\n",
       "      <td>Gets the projected of this V1Volume.  # noqa: ...</td>\n",
       "      <td>551657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120125 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_id  repo                    name  \\\n",
       "386       31048   100              setwinsize   \n",
       "387       31251    20           metadata_dict   \n",
       "388       31364    20                 exports   \n",
       "389       31517    20                     log   \n",
       "390       31706    20          test_as_import   \n",
       "...         ...   ...                     ...   \n",
       "119824    27697    16           isMaskedArray   \n",
       "119916    29324    22   describe_timestamp_1d   \n",
       "119940    29336    22                  asfreq   \n",
       "119941    29445    22  _get_custom_index_name   \n",
       "120015    39350   234               projected   \n",
       "\n",
       "                                              args                args_types  \\\n",
       "386                               {self,rows,cols}                        {}   \n",
       "387                                         {self}                        {}   \n",
       "388                                         {self}                        {}   \n",
       "389                                         {self}                        {}   \n",
       "390                                       {script}      {PipTestEnvironment}   \n",
       "...                                            ...                       ...   \n",
       "119824                                         {x}                        {}   \n",
       "119916                          {data,percentiles}  {Series,Sequence[float]}   \n",
       "119940  {obj,freq,method,how,normalize,fill_value}           {NDFrameT,bool}   \n",
       "119941                                      {self}                        {}   \n",
       "120015                                      {self}                        {}   \n",
       "\n",
       "                 args_defaults  \\\n",
       "386                         {}   \n",
       "387                         {}   \n",
       "388                         {}   \n",
       "389                         {}   \n",
       "390                         {}   \n",
       "...                        ...   \n",
       "119824                      {}   \n",
       "119916                      {}   \n",
       "119940  {None,None,False,None}   \n",
       "119941                      {}   \n",
       "120015                      {}   \n",
       "\n",
       "                                                     body  \\\n",
       "386            return self.ptyproc.setwinsize(rows, cols)   \n",
       "387                     return msg_to_json(self.metadata)   \n",
       "388     result = {}\\nr = self.get_distinfo_resource(EX...   \n",
       "389     if not objects:\\n    objects = (NewLine(),)\\nr...   \n",
       "390     import pip._internal.commands.install as inst\\...   \n",
       "...                                                   ...   \n",
       "119824                  return isinstance(x, MaskedArray)   \n",
       "119916  from pandas import Series\\nformatted_percentil...   \n",
       "119940  if isinstance(obj.index, PeriodIndex):\\n    if...   \n",
       "119941                                 return self.xlabel   \n",
       "120015                             return self._projected   \n",
       "\n",
       "                                                docstring      id  \n",
       "386     This sets the terminal window size of the chil...      18  \n",
       "387     PEP 566 compliant JSON-serializable representa...      19  \n",
       "388     Return the information exported by this distri...      20  \n",
       "389     Log rich content to the terminal.\\n\\nArgs:\\n  ...      22  \n",
       "390     test that pip.__init__.py does not shadow\\nthe...      23  \n",
       "...                                                   ...     ...  \n",
       "119824  Test whether input is an instance of MaskedArr...  551641  \n",
       "119916  Describe series containing datetime64 dtype.\\n...  551650  \n",
       "119940  Utility frequency conversion method for Series...  551651  \n",
       "119941  Specify whether xlabel/ylabel should be used t...  551652  \n",
       "120015  Gets the projected of this V1Volume.  # noqa: ...  551657  \n",
       "\n",
       "[120125 rows x 9 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "\n",
    "# this is a mess\n",
    "\n",
    "db = sa.create_engine('postgresql://postgres:8W0MQwY4DINCoX@localhost:5432/data-mining').connect()\n",
    "\n",
    "# load 100 samples from function\n",
    "values = pd.read_sql(\"SELECT * FROM functions WHERE docstring is not null ORDER BY id\", db)\n",
    "\n",
    "# sort by id\n",
    "values = values.sort_values(by='id')\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This sets the terminal window size of the child tty. This will cause\\na SIGWINCH signal to be sent to the child. This does not change the\\nphysical window size. It changes the size reported to TTY-aware\\napplications like vi or curses -- applications that respond to the\\nSIGWINCH signal. ',\n",
       " 'PEP 566 compliant JSON-serializable representation of METADATA or PKG-INFO.\\n\\nThis should return an empty dict if the metadata file is unavailable.\\n\\n:raises NoneMetadataError: If the metadata file is available, but does\\n    not contain valid metadata.',\n",
       " 'Return the information exported by this distribution.\\n:return: A dictionary of exports, mapping an export category to a dict\\n         of :class:`ExportEntry` instances describing the individual\\n         export entries, and keyed by name.',\n",
       " 'Log rich content to the terminal.\\n\\nArgs:\\n    objects (positional args): Objects to log to the terminal.\\n    sep (str, optional): String to write between print data. Defaults to \" \".\\n    end (str, optional): String to write at end of print data. Defaults to \"\\\\\\\\n\".\\n    style (Union[str, Style], optional): A style to apply to output. Defaults to None.\\n    justify (str, optional): One of \"left\", \"right\", \"center\", or \"full\". Defaults to ``None``.\\n    emoji (Optional[bool], optional): Enable emoji code, or ``None`` to use console default. Defaults to None.\\n    markup (Optional[bool], optional): Enable markup, or ``None`` to use console default. Defaults to None.\\n    highlight (Optional[bool], optional): Enable automatic highlighting, or ``None`` to use console default. Defaults to None.\\n    log_locals (bool, optional): Boolean to enable logging of locals where ``log()``\\n        was called. Defaults to False.\\n    _stack_offset (int, optional): Offset of caller from end of call stack. Defaults to 1.',\n",
       " 'test that pip.__init__.py does not shadow\\nthe command submodule with a dictionary',\n",
       " 'Parses input, which is a list of tokens.',\n",
       " 'Test ordering of checkers based on their __gt__ method.',\n",
       " 'Test that a toml file has a pylint config.',\n",
       " 'If an extension requires an issuer, the `issuer` parameter to\\n`X509Extension` provides its value.',\n",
       " 'Tests whether requests can be used importing standard_library modules\\npreviously with the hooks context manager',\n",
       " 'Produce coverage reports.',\n",
       " 'Return a string or list of strings to be displayed after collection\\nhas finished successfully.\\n\\nThese strings will be displayed after the standard \"collected X items\" message.\\n\\n.. versionadded:: 3.2\\n\\n:param config: The pytest config object.\\n:param start_path: The starting dir.\\n:type start_path: pathlib.Path\\n:param startdir: The starting dir (deprecated).\\n:param items: List of pytest items that are going to be executed; this list should not be modified.\\n\\n.. note::\\n\\n    Lines returned by a plugin are displayed before those of plugins which\\n    ran before it.\\n    If you want to have your line(s) displayed first, use\\n    :ref:`trylast=True <plugin-hookorder>`.\\n\\n.. versionchanged:: 7.0.0\\n    The ``start_path`` parameter was added as a :class:`pathlib.Path`\\n    equivalent of the ``startdir`` parameter. The ``startdir`` parameter\\n    has been deprecated.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest plugin can implement this hook.',\n",
       " 'Map errors for Unary-Unary and Stream-Unary gRPC callables.',\n",
       " \"Wait for a job to complete and return the results.\\n\\nIf we can't return the results within the ``wait_timeout``, try to cancel\\nthe job.\",\n",
       " 'Return a string indicating the HTTP request method.',\n",
       " \"Fail unless a warning of class warnClass is triggered\\nby callable_obj when invoked with arguments args and keyword\\narguments kwargs.  If a different type of warning is\\ntriggered, it will not be handled: depending on the other\\nwarning filtering rules in effect, it might be silenced, printed\\nout, or raised as an exception.\\n\\nIf called with callable_obj omitted or None, will return a\\ncontext object used like this::\\n\\n     with self.assertWarns(SomeWarning):\\n         do_something()\\n\\nAn optional keyword argument 'msg' can be provided when assertWarns\\nis used as a context object.\\n\\nThe context manager keeps a reference to the first matching\\nwarning as the 'warning' attribute; similarly, the 'filename'\\nand 'lineno' attributes give you information about the line\\nof Python code from which the warning was triggered.\\nThis allows you to inspect the warning after the assertion::\\n\\n    with self.assertWarns(SomeWarning) as cm:\\n        do_something()\\n    the_warning = cm.warning\\n    self.assertEqual(the_warning.some_attribute, 147)\",\n",
       " '__init__(self, \\\\*, inputCols=None, outputCol=None):',\n",
       " 'Test checking a single file that is excluded.',\n",
       " \"replace_namespaced_persistent_volume_claim_status  # noqa: E501\\n\\nreplace status of the specified PersistentVolumeClaim  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.replace_namespaced_persistent_volume_claim_status(name, namespace, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the PersistentVolumeClaim (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param V1PersistentVolumeClaim body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1PersistentVolumeClaim\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Sets the collision_count of this V1DaemonSetStatus.\\n\\nCount of hash collisions for the DaemonSet. The DaemonSet controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ControllerRevision.  # noqa: E501\\n\\n:param collision_count: The collision_count of this V1DaemonSetStatus.  # noqa: E501\\n:type: int',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Sets the additional_properties of this V1JSONSchemaProps.\\n\\nJSONSchemaPropsOrBool represents JSONSchemaProps or a boolean value. Defaults to true for the boolean property.  # noqa: E501\\n\\n:param additional_properties: The additional_properties of this V1JSONSchemaProps.  # noqa: E501\\n:type: object',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Returns micro-averaged label-based precision.\\n(equals to micro-averaged document-based precision)',\n",
       " 'Deserialize with Futures',\n",
       " 'Format training args to pass in sagemaker_session.train.\\n\\nArgs:\\n    desc (dict): the response from DescribeTrainingJob API.\\n    inputs (list): a list of input data channels.\\n    name (str): the name of the step job.\\n    volume_kms_key (str): The KMS key id to encrypt data on the storage volume attached to\\n        the ML compute instance(s).\\n    encrypt_inter_container_traffic (bool): To encrypt all communications between ML compute\\n        instances in distributed training.\\n    vpc_config (dict): Specifies a VPC that jobs and hosted models have access to.\\n        Control access to and from training and model containers by configuring the VPC\\n\\nReturns (dcit): a dictionary that can be used as args of\\n    sagemaker_session.train method.',\n",
       " 'Retrieves the supported instance types for the model.\\n\\nArgs:\\n    model_id (str): JumpStart model ID of the JumpStart model for which to\\n        retrieve the supported instance types.\\n    model_version (str): Version of the JumpStart model for which to retrieve the\\n        supported instance types.\\n    scope (str): The script type, i.e. what it is used for.\\n        Valid values: \"training\" and \"inference\".\\n    hub_arn (str): The arn of the SageMaker Hub for which to retrieve\\n        model details from. (Default: None).\\n    region (Optional[str]): Region for which to retrieve supported instance types.\\n        (Default: None).\\n    tolerate_vulnerable_model (bool): True if vulnerable versions of model\\n        specifications should be tolerated (exception not raised). If False, raises an\\n        exception if the script used by this version of the model has dependencies with known\\n        security vulnerabilities. (Default: False).\\n    tolerate_deprecated_model (bool): True if deprecated versions of model\\n        specifications should be tolerated (exception not raised). If False, raises\\n        an exception if the version of the model is deprecated. (Default: False).\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions. If not\\n        specified, one is created using the default AWS configuration\\n        chain. (Default: sagemaker.jumpstart.constants.DEFAULT_JUMPSTART_SAGEMAKER_SESSION).\\n    training_instance_type (str): In the case of a model fine-tuned on SageMaker, the training\\n        instance type used for the training job that produced the fine-tuned weights.\\n        Optionally supply this to get a inference instance type conditioned\\n        on the training instance, to ensure compatability of training artifact to inference\\n        instance. (Default: None).\\n    config_name (Optional[str]): Name of the JumpStart Model config to apply. (Default: None).\\nReturns:\\n    list: the supported instance types to use for the model or None.\\n\\nRaises:\\n    ValueError: If the model is not available in the\\n        specified region due to lack of supported computing instances.',\n",
       " 'Returns json representation of S3DataSource object.',\n",
       " 'Use the lineage query to retrieve transform jobs that use this endpoint.\\n\\nArgs:\\n    direction (LineageQueryDirectionEnum, optional): The query direction.\\n\\nReturns:\\n    list of LineageTrialComponent: Lineage trial component that represent Transform jobs.',\n",
       " 'Creates a monitoring schedule to monitor an Amazon SageMaker Endpoint.\\n\\nIf constraints and statistics are provided, or if they are able to be retrieved from a\\nprevious baselining job associated with this monitor, those will be used.\\nIf constraints and statistics cannot be automatically retrieved, baseline_inputs will be\\nrequired in order to kick off a baselining job.\\n\\nArgs:\\n    endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.\\n        This can either be the endpoint name or an EndpointInput. (default: None)\\n    record_preprocessor_script (str): The path to the record preprocessor script. This can\\n        be a local path or an S3 uri.\\n    post_analytics_processor_script (str): The path to the record post-analytics processor\\n        script. This can be a local path or an S3 uri.\\n    output_s3_uri (str): Desired S3 destination of the constraint_violations and\\n        statistics json files.\\n        Default: \"s3://<default_session_bucket>/<job_name>/output\"\\n    constraints (sagemaker.model_monitor.Constraints or str): If provided alongside\\n        statistics, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Constraints object or an s3_uri pointing to a constraints\\n        JSON file.\\n    statistics (sagemaker.model_monitor.Statistic or str): If provided alongside\\n        constraints, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Statistics object or an s3_uri pointing to a statistics\\n        JSON file.\\n    monitor_schedule_name (str): Schedule name. If not specified, the processor generates\\n        a default job name, based on the image name and current timestamp.\\n    schedule_cron_expression (str): The cron expression that dictates the frequency that\\n        this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid\\n        expressions. Default: Daily.\\n    enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of\\n        the baselining or monitoring jobs.\\n    batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to\\n        run the monitoring schedule on the batch transform (default: None)\\n    data_analysis_start_time (str): Start time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)\\n    data_analysis_end_time (str): End time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)',\n",
       " 'Calculate a multidimensional median filter.\\n\\nParameters\\n----------\\n%(input)s\\n%(size_foot)s\\n%(output)s\\n%(mode_reflect)s\\n%(cval)s\\n%(origin_multiple)s\\naxes : tuple of int or None, optional\\n    If None, `input` is filtered along all axes. Otherwise,\\n    `input` is filtered along the specified axes. When `axes` is\\n    specified, any tuples used for `size`, `origin`, and/or `mode`\\n    must match the length of `axes`. The ith entry in any of these tuples\\n    corresponds to the ith entry in `axes`.\\n\\nReturns\\n-------\\nmedian_filter : ndarray\\n    Filtered array. Has the same shape as `input`.\\n\\nSee Also\\n--------\\nscipy.signal.medfilt2d\\n\\nNotes\\n-----\\nFor 2-dimensional images with ``uint8``, ``float32`` or ``float64`` dtypes\\nthe specialised function `scipy.signal.medfilt2d` may be faster. It is\\nhowever limited to constant mode with ``cval=0``.\\n\\nExamples\\n--------\\n>>> from scipy import ndimage, datasets\\n>>> import matplotlib.pyplot as plt\\n>>> fig = plt.figure()\\n>>> plt.gray()  # show the filtered result in grayscale\\n>>> ax1 = fig.add_subplot(121)  # left side\\n>>> ax2 = fig.add_subplot(122)  # right side\\n>>> ascent = datasets.ascent()\\n>>> result = ndimage.median_filter(ascent, size=20)\\n>>> ax1.imshow(ascent)\\n>>> ax2.imshow(result)\\n>>> plt.show()',\n",
       " 'Get information about memory available, not counting swap.',\n",
       " 'Compute the inverse FFT of a signal that has Hermitian symmetry.\\n\\nParameters\\n----------\\nx : array_like\\n    Input array.\\nn : int, optional\\n    Length of the inverse FFT, the number of points along\\n    transformation axis in the input to use.  If `n` is smaller than\\n    the length of the input, the input is cropped. If it is larger,\\n    the input is padded with zeros. If `n` is not given, the length of\\n    the input along the axis specified by `axis` is used.\\naxis : int, optional\\n    Axis over which to compute the inverse FFT. If not given, the last\\n    axis is used.\\nnorm : {\"backward\", \"ortho\", \"forward\"}, optional\\n    Normalization mode (see `fft`). Default is \"backward\".\\noverwrite_x : bool, optional\\n    If True, the contents of `x` can be destroyed; the default is False.\\n    See `fft` for more details.\\nworkers : int, optional\\n    Maximum number of workers to use for parallel computation. If negative,\\n    the value wraps around from ``os.cpu_count()``.\\n    See :func:`~scipy.fft.fft` for more details.\\nplan : object, optional\\n    This argument is reserved for passing in a precomputed plan provided\\n    by downstream FFT vendors. It is currently not used in SciPy.\\n\\n    .. versionadded:: 1.5.0\\n\\nReturns\\n-------\\nout : complex ndarray\\n    The truncated or zero-padded input, transformed along the axis\\n    indicated by `axis`, or the last one if `axis` is not specified.\\n    The length of the transformed axis is ``n//2 + 1``.\\n\\nSee Also\\n--------\\nhfft, irfft\\n\\nNotes\\n-----\\n`hfft`/`ihfft` are a pair analogous to `rfft`/`irfft`, but for the\\nopposite case: here, the signal has Hermitian symmetry in the time\\ndomain and is real in the frequency domain. So, here, it\\'s `hfft`, for\\nwhich you must supply the length of the result if it is to be odd:\\n* even: ``ihfft(hfft(a, 2*len(a) - 2) == a``, within roundoff error,\\n* odd: ``ihfft(hfft(a, 2*len(a) - 1) == a``, within roundoff error.\\n\\nExamples\\n--------\\n>>> from scipy.fft import ifft, ihfft\\n>>> import numpy as np\\n>>> spectrum = np.array([ 15, -4, 0, -1, 0, -4])\\n>>> ifft(spectrum)\\narray([1.+0.j,  2.+0.j,  3.+0.j,  4.+0.j,  3.+0.j,  2.+0.j]) # may vary\\n>>> ihfft(spectrum)\\narray([ 1.-0.j,  2.-0.j,  3.-0.j,  4.-0.j]) # may vary',\n",
       " 'Match the namespace of the element.',\n",
       " 'Remove this state.',\n",
       " 'Create a new Spark configuration.',\n",
       " 'Convert a locale string to a language tag (ex. en_US -> en-US).\\n\\nrefs: BCP 47 (:rfc:`5646`)',\n",
       " 'Get qualified name for given object as a list of string(s).',\n",
       " 'helper context manager that will apply appropriate DDL events\\nto a CREATE or DROP operation.',\n",
       " 'target database must support retrieval of the columns in a view,\\nsimilarly to how a table is inspected.\\n\\nThis does not include the full CREATE VIEW definition.',\n",
       " 'Execute a statement and assert that rows returned equal expected.',\n",
       " 'Generates all URLs and templates',\n",
       " 'Remove None, convert values to bool, check commutativity *in place*.\\n        ',\n",
       " 'This rule covers trigonometric factors by splitting everything into a\\nsum of exponential functions and collecting complex conjugate poles and\\nreal symmetric poles.',\n",
       " 'Print a LaTeX representation of the function defining the curve.\\n\\nParameters\\n==========\\n\\nprinter : Printer\\n    The printer to be used to print the LaTeX string representation.',\n",
       " 'Return a list of all units formed by unit and the given prefixes.\\n\\nYou can use the predefined PREFIXES or BIN_PREFIXES, but you can also\\npass as argument a subdict of them if you do not want all prefixed units.\\n\\n    >>> from sympy.physics.units.prefixes import (PREFIXES,\\n    ...                                                 prefix_unit)\\n    >>> from sympy.physics.units import m\\n    >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\\n    >>> prefix_unit(m, pref)  # doctest: +SKIP\\n    [millimeter, centimeter, decimeter]',\n",
       " 'Tests the coordinate variables functionality',\n",
       " 'Scale the function by a term independent of x.\\n\\nExplanation\\n===========\\n\\nf(x) -> s * f(x)\\n\\nThis is fast, if Fourier series of f(x) is already\\ncomputed.\\n\\nExamples\\n========\\n\\n>>> from sympy import fourier_series, pi\\n>>> from sympy.abc import x\\n>>> s = fourier_series(x**2, (x, -pi, pi))\\n>>> s.scale(2).truncate()\\n-8*cos(x) + 2*cos(2*x) + 2*pi**2/3',\n",
       " 'Access a group of rows and columns by label(s) or a boolean array.\\n\\n``.loc[]`` is primarily label based, but may also be used with a\\nboolean array.\\n\\nAllowed inputs are:\\n\\n- A single label, e.g. ``5`` or ``\\'a\\'``, (note that ``5`` is\\n  interpreted as a *label* of the index, and **never** as an\\n  integer position along the index).\\n- A list or array of labels, e.g. ``[\\'a\\', \\'b\\', \\'c\\']``.\\n- A slice object with labels, e.g. ``\\'a\\':\\'f\\'``.\\n\\n  .. warning:: Note that contrary to usual python slices, **both** the\\n      start and the stop are included\\n\\n- A boolean array of the same length as the axis being sliced,\\n  e.g. ``[True, False, True]``.\\n- An alignable boolean Series. The index of the key will be aligned before\\n  masking.\\n- An alignable Index. The Index of the returned selection will be the input.\\n- A ``callable`` function with one argument (the calling Series or\\n  DataFrame) and that returns valid output for indexing (one of the above)\\n\\nSee more at :ref:`Selection by Label <indexing.label>`.\\n\\nRaises\\n------\\nKeyError\\n    If any items are not found.\\nIndexingError\\n    If an indexed key is passed and its index is unalignable to the frame index.\\n\\nSee Also\\n--------\\nDataFrame.at : Access a single value for a row/column label pair.\\nDataFrame.iloc : Access group of rows and columns by integer position(s).\\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\\n               Series/DataFrame.\\nSeries.loc : Access group of values using labels.\\n\\nExamples\\n--------\\n**Getting values**\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[\"cobra\", \"viper\", \"sidewinder\"],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label. Note this returns the row as a Series.\\n\\n>>> df.loc[\"viper\"]\\nmax_speed    4\\nshield       5\\nName: viper, dtype: int64\\n\\nList of labels. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"]]\\n            max_speed  shield\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label for row and column\\n\\n>>> df.loc[\"cobra\", \"shield\"]\\n2\\n\\nSlice with labels for row and single label for column. As mentioned\\nabove, note that both the start and stop of the slice are included.\\n\\n>>> df.loc[\"cobra\":\"viper\", \"max_speed\"]\\ncobra    1\\nviper    4\\nName: max_speed, dtype: int64\\n\\nBoolean list with the same length as the row axis\\n\\n>>> df.loc[[False, False, True]]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nAlignable boolean Series:\\n\\n>>> df.loc[\\n...     pd.Series([False, True, False], index=[\"viper\", \"sidewinder\", \"cobra\"])\\n... ]\\n                     max_speed  shield\\nsidewinder          7       8\\n\\nIndex (same behavior as ``df.reindex``)\\n\\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\\n       max_speed  shield\\nfoo\\ncobra          1       2\\nviper          4       5\\n\\nConditional that returns a boolean Series\\n\\n>>> df.loc[df[\"shield\"] > 6]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nConditional that returns a boolean Series with column labels specified\\n\\n>>> df.loc[df[\"shield\"] > 6, [\"max_speed\"]]\\n            max_speed\\nsidewinder          7\\n\\nMultiple conditional using ``&`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 1) & (df[\"shield\"] < 8)]\\n            max_speed  shield\\nviper          4       5\\n\\nMultiple conditional using ``|`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 4) | (df[\"shield\"] < 5)]\\n            max_speed  shield\\ncobra               1       2\\nsidewinder          7       8\\n\\nPlease ensure that each condition is wrapped in parentheses ``()``.\\nSee the :ref:`user guide<indexing.boolean>`\\nfor more details and explanations of Boolean indexing.\\n\\n.. note::\\n    If you find yourself using 3 or more conditionals in ``.loc[]``,\\n    consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\\n\\n    See below for using ``.loc[]`` on MultiIndex DataFrames.\\n\\nCallable that returns a boolean Series\\n\\n>>> df.loc[lambda df: df[\"shield\"] == 8]\\n            max_speed  shield\\nsidewinder          7       8\\n\\n**Setting values**\\n\\nSet value for all items matching the list of labels\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"], [\"shield\"]] = 50\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire row\\n\\n>>> df.loc[\"cobra\"] = 10\\n>>> df\\n            max_speed  shield\\ncobra              10      10\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire column\\n\\n>>> df.loc[:, \"max_speed\"] = 30\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper              30      50\\nsidewinder         30      50\\n\\nSet value for rows matching callable condition\\n\\n>>> df.loc[df[\"shield\"] > 35] = 0\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       0\\nsidewinder          0       0\\n\\nAdd value matching location\\n\\n>>> df.loc[\"viper\", \"shield\"] += 5\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       5\\nsidewinder          0       0\\n\\nSetting using a ``Series`` or a ``DataFrame`` sets the values matching the\\nindex labels, not the index positions.\\n\\n>>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\\n>>> df.loc[:] += shuffled_df\\n>>> df\\n            max_speed  shield\\ncobra              60      20\\nviper               0      10\\nsidewinder          0       0\\n\\n**Getting values on a DataFrame with an index that has integer labels**\\n\\nAnother example using integers for the index\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[7, 8, 9],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\nSlice with integer labels for rows. As mentioned above, note that both\\nthe start and stop of the slice are included.\\n\\n>>> df.loc[7:9]\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\n**Getting values with a MultiIndex**\\n\\nA number of examples using a DataFrame with a MultiIndex\\n\\n>>> tuples = [\\n...     (\"cobra\", \"mark i\"),\\n...     (\"cobra\", \"mark ii\"),\\n...     (\"sidewinder\", \"mark i\"),\\n...     (\"sidewinder\", \"mark ii\"),\\n...     (\"viper\", \"mark ii\"),\\n...     (\"viper\", \"mark iii\"),\\n... ]\\n>>> index = pd.MultiIndex.from_tuples(tuples)\\n>>> values = [[12, 2], [0, 4], [10, 20], [1, 4], [7, 1], [16, 36]]\\n>>> df = pd.DataFrame(values, columns=[\"max_speed\", \"shield\"], index=index)\\n>>> df\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSingle label. Note this returns a DataFrame with a single index.\\n\\n>>> df.loc[\"cobra\"]\\n         max_speed  shield\\nmark i          12       2\\nmark ii          0       4\\n\\nSingle index tuple. Note this returns a Series.\\n\\n>>> df.loc[(\"cobra\", \"mark ii\")]\\nmax_speed    0\\nshield       4\\nName: (cobra, mark ii), dtype: int64\\n\\nSingle label for row and column. Similar to passing in a tuple, this\\nreturns a Series.\\n\\n>>> df.loc[\"cobra\", \"mark i\"]\\nmax_speed    12\\nshield        2\\nName: (cobra, mark i), dtype: int64\\n\\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[(\"cobra\", \"mark ii\")]]\\n               max_speed  shield\\ncobra mark ii          0       4\\n\\nSingle tuple for the index with a single label for the column\\n\\n>>> df.loc[(\"cobra\", \"mark i\"), \"shield\"]\\n2\\n\\nSlice from index tuple to single label\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : \"viper\"]\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSlice from index tuple to index tuple\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : (\"viper\", \"mark ii\")]\\n                    max_speed  shield\\ncobra      mark i          12       2\\n           mark ii          0       4\\nsidewinder mark i          10      20\\n           mark ii          1       4\\nviper      mark ii          7       1\\n\\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\\nfor more details and explanations of advanced indexing.',\n",
       " 'Test for stalled tqdm instance and monitor deletion',\n",
       " 'extract my config from a global Config object\\n\\nwill construct a Config object of only the config values that apply to me\\nbased on my mro(), as well as those of my parent(s) if they exist.\\n\\nIf I am Bar and my parent is Foo, and their parent is Tim,\\nthis will return merge following config sections, in this order::\\n\\n    [Bar, Foo.Bar, Tim.Foo.Bar]\\n\\nWith the last item being the highest priority.',\n",
       " 'Optional method. If not provided, the interpreter will use\\n``__iter__`` or the old ``__getitem__`` protocol\\nto implement ``in``.',\n",
       " 'Constuct an object holding a date value.\\n\\nThis function is part of the `DBAPI 2.0 specification\\n<http://www.python.org/dev/peps/pep-0249/>`_.\\n\\n:rtype: :class:`datetime.date`',\n",
       " 'Returns the normalized identifier of any currency code.\\n\\nAccepts a ``locale`` parameter for fined-grained validation, working as\\nthe one defined above in ``list_currencies()`` method.\\n\\nReturns None if the currency is unknown to Babel.',\n",
       " 'Append `leaf` to current line or to new line if appending impossible.',\n",
       " \"Parse Prometheus text format from a file descriptor.\\n\\nThis is a laxer parser than the main Go parser,\\nso successful parsing does not imply that the parsed\\ntext meets the specification.\\n\\nYields Metric's.\",\n",
       " ':param string type:\\n:param integer request_seq: Sequence number of the corresponding request.\\n:param boolean success: Outcome of the request.\\nIf true, the request was successful and the `body` attribute may contain the result of the request.\\nIf the value is false, the attribute `message` contains the error in short form and the `body` may contain additional information (see `ErrorResponse.body.error`).\\n:param string command: The command requested.\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.\\n:param string message: Contains the raw error in short form if `success` is false.\\nThis raw error might be interpreted by the client and is not shown in the UI.\\nSome predefined values exist.\\n:param Capabilities body: The capabilities of this debug adapter.',\n",
       " \":param array areas: Set of logical areas that got invalidated. This property has a hint characteristic: a client can only be expected to make a 'best effort' in honoring the areas but there are no guarantees. If this property is missing, empty, or if values are not understood, the client should assume a single value `all`.\\n:param integer threadId: If specified, the client only needs to refetch data related to this thread.\\n:param integer stackFrameId: If specified, the client only needs to refetch data related to this stack frame (and the `threadId` is ignored).\",\n",
       " 'Query nameservers to find the answer to the question.\\n\\nThis is a convenience function that uses the default resolver\\nobject to make the query.\\n\\nSee ``dns.resolver.Resolver.resolve`` for more information on the\\nparameters.',\n",
       " \"Get log stream for a service.\\nNote: This endpoint works only for services with the ``json-file``\\nor ``journald`` logging drivers.\\n\\nArgs:\\n    service (str): ID or name of the service\\n    details (bool): Show extra details provided to logs.\\n        Default: ``False``\\n    follow (bool): Keep connection open to read logs as they are\\n        sent by the Engine. Default: ``False``\\n    stdout (bool): Return logs from ``stdout``. Default: ``False``\\n    stderr (bool): Return logs from ``stderr``. Default: ``False``\\n    since (int): UNIX timestamp for the logs staring point.\\n        Default: 0\\n    timestamps (bool): Add timestamps to every log line.\\n    tail (string or int): Number of log lines to be returned,\\n        counting from the current end of the logs. Specify an\\n        integer or ``'all'`` to output all log lines.\\n        Default: ``all``\\n    is_tty (bool): Whether the service's :py:class:`ContainerSpec`\\n        enables the TTY option. If omitted, the method will query\\n        the Engine for the information, causing an additional\\n        roundtrip.\\n\\nReturns (generator): Logs for the service.\",\n",
       " \"Apply the chain's changes and write the final result using the passed\\nwrite function.\\n:param bbuf: base buffer containing the base of all deltas contained in this\\n    list. It will only be used if the chunk in question does not have a base\\n    chain.\\n:param write: function taking a string of bytes to write to the output\",\n",
       " 'Create a JSON representation of an instance of MediaUpload.\\n\\nReturns:\\n   string, a JSON representation of this instance, suitable to pass to\\n   from_json().',\n",
       " 'Call the create ad sense link method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.CreateAdSenseLinkRequest):\\n        The request object. Request message to be passed to\\n    CreateAdSenseLink method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.resources.AdSenseLink:\\n        A link between a GA4 Property and an\\n    AdSense for Content ad client.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the get project settings method over gRPC.\\n\\nRetrieves the Settings for the Project.\\n\\nReturns:\\n    Callable[[~.GetProjectSettingsRequest],\\n            ~.ProjectSettings]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Sleep *ds* seconds.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    MetastoreServiceAsyncClient: The constructed client.',\n",
       " \"Merges capacity commitments of the same plan into a single\\ncommitment.\\n\\nThe resulting capacity commitment has the greater\\ncommitment_end_time out of the to-be-merged capacity\\ncommitments.\\n\\nAttempting to merge capacity commitments of different plan will\\nfail with the error code\\n``google.rpc.Code.FAILED_PRECONDITION``.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_reservation_v1\\n\\n    def sample_merge_capacity_commitments():\\n        # Create a client\\n        client = bigquery_reservation_v1.ReservationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_reservation_v1.MergeCapacityCommitmentsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.merge_capacity_commitments(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_reservation_v1.types.MergeCapacityCommitmentsRequest, dict]):\\n        The request object. The request for\\n        [ReservationService.MergeCapacityCommitments][google.cloud.bigquery.reservation.v1.ReservationService.MergeCapacityCommitments].\\n    parent (str):\\n        Parent resource that identifies admin project and\\n        location e.g., ``projects/myproject/locations/us``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    capacity_commitment_ids (MutableSequence[str]):\\n        Ids of capacity commitments to merge.\\n        These capacity commitments must exist\\n        under admin project and location\\n        specified in the parent.\\n        ID is the last portion of capacity\\n        commitment name e.g., 'abc' for\\n        projects/myproject/locations/US/capacityCommitments/abc\\n\\n        This corresponds to the ``capacity_commitment_ids`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_reservation_v1.types.CapacityCommitment:\\n        Capacity commitment is a way to\\n        purchase compute capacity for BigQuery\\n        jobs (in the form of slots) with some\\n        committed period of usage. Annual\\n        commitments renew by default.\\n        Commitments can be removed after their\\n        commitment end time passes.\\n\\n        In order to remove annual commitment,\\n        its plan needs to be changed to monthly\\n        or flex first.\\n\\n        A capacity commitment resource exists as\\n        a child resource of the admin project.\",\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.billing_v1.types.ListProjectBillingInfoRequest):\\n        The initial request object.\\n    response (google.cloud.billing_v1.types.ListProjectBillingInfoResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Returns the specified network firewall policy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.NetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetNetworkFirewallPolicyRequest(\\n            firewall_policy=\"firewall_policy_value\",\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        NetworkFirewallPolicies.Get. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy (str):\\n        Name of the firewall policy to get.\\n        This corresponds to the ``firewall_policy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.FirewallPolicy:\\n        Represents a Firewall Policy\\n        resource.',\n",
       " 'Sets the port of this CoreV1EndpointPort.\\n\\nThe port number of the endpoint.  # noqa: E501\\n\\n:param port: The port of this CoreV1EndpointPort.  # noqa: E501\\n:type: int',\n",
       " 'Post-rpc interceptor for export_deployment_statefile\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Config server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the read repository file method over gRPC.\\n\\nReturns the contents of a file (inside a Repository). The\\nRepository must not have a value for\\n``git_remote_settings.url``.\\n\\nReturns:\\n    Callable[[~.ReadRepositoryFileRequest],\\n            ~.ReadRepositoryFileResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Gets the resource representation for an interactive\\nsession.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataproc_v1\\n\\n    def sample_get_session():\\n        # Create a client\\n        client = dataproc_v1.SessionControllerClient()\\n\\n        # Initialize request argument(s)\\n        request = dataproc_v1.GetSessionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_session(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataproc_v1.types.GetSessionRequest, dict]):\\n        The request object. A request to get the resource\\n        representation for a session.\\n    name (str):\\n        Required. The name of the session to\\n        retrieve.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataproc_v1.types.Session:\\n        A representation of a session.',\n",
       " 'Call the create connection method over HTTP.\\n\\nArgs:\\n    request (~.developer_connect.CreateConnectionRequest):\\n        The request object. Message for creating a Connection\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Post-rpc interceptor for list_participants\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the check grounding method over gRPC.\\n\\nPerforms a grounding check.\\n\\nReturns:\\n    Callable[[~.CheckGroundingRequest],\\n            ~.CheckGroundingResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the purge suggestion deny list\\nentries method over gRPC.\\n\\nPermanently deletes all\\n[SuggestionDenyListEntry][google.cloud.discoveryengine.v1beta.SuggestionDenyListEntry]\\nfor a DataStore.\\n\\nReturns:\\n    Callable[[~.PurgeSuggestionDenyListEntriesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Return a callable for the list sites method over gRPC.\\n\\nLists sites in a given project and location.\\n\\nReturns:\\n    Callable[[~.ListSitesRequest],\\n            ~.ListSitesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AwsClustersAsyncClient: The constructed client.',\n",
       " 'Call the create kms config method over HTTP.\\n\\nArgs:\\n    request (~.kms.CreateKmsConfigRequest):\\n        The request object. CreateKmsConfigRequest creates a KMS\\n    Config.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return if pages/frames are currently being cached.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Return a callable for the list related account groups method over gRPC.\\n\\nList groups of related accounts.\\n\\nReturns:\\n    Callable[[~.ListRelatedAccountGroupsRequest],\\n            ~.ListRelatedAccountGroupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Return a callable for the add control method over gRPC.\\n\\nEnables a Control on the specified ServingConfig. The control is\\nadded in the last position of the list of controls it belongs to\\n(e.g. if it's a facet spec control it will be applied in the\\nlast position of servingConfig.facetSpecIds) Returns a\\nALREADY_EXISTS error if the control has already been applied.\\nReturns a FAILED_PRECONDITION error if the addition could exceed\\nmaximum number of control allowed for that type of control.\\n\\nReturns:\\n    Callable[[~.AddControlRequest],\\n            ~.ServingConfig]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'Return a callable for the list accelerator types method over gRPC.\\n\\nLists accelerator types supported by this API.\\n\\nReturns:\\n    Callable[[~.ListAcceleratorTypesRequest],\\n            ~.ListAcceleratorTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Call the update organization\\nsettings method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.UpdateOrganizationSettingsRequest):\\n            The request object. Request message for updating an\\n        organization's settings.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcs_organization_settings.OrganizationSettings:\\n            User specified settings that are\\n        attached to the Security Command Center\\n        organization.\",\n",
       " 'Begins executing a batch create jobs operation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import talent_v4\\n\\n    def sample_batch_create_jobs():\\n        # Create a client\\n        client = talent_v4.JobServiceClient()\\n\\n        # Initialize request argument(s)\\n        jobs = talent_v4.Job()\\n        jobs.company = \"company_value\"\\n        jobs.requisition_id = \"requisition_id_value\"\\n        jobs.title = \"title_value\"\\n        jobs.description = \"description_value\"\\n\\n        request = talent_v4.BatchCreateJobsRequest(\\n            parent=\"parent_value\",\\n            jobs=jobs,\\n        )\\n\\n        # Make the request\\n        operation = client.batch_create_jobs(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.talent_v4.types.BatchCreateJobsRequest, dict]):\\n        The request object. Request to create a batch of jobs.\\n    parent (str):\\n        Required. The resource name of the tenant under which\\n        the job is created.\\n\\n        The format is\\n        \"projects/{project_id}/tenants/{tenant_id}\". For\\n        example, \"projects/foo/tenants/bar\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    jobs (MutableSequence[google.cloud.talent_v4.types.Job]):\\n        Required. The jobs to be created.\\n        A maximum of 200 jobs can be created in\\n        a batch.\\n\\n        This corresponds to the ``jobs`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.talent_v4.types.BatchCreateJobsResponse` The result of\\n           [JobService.BatchCreateJobs][google.cloud.talent.v4.JobService.BatchCreateJobs].\\n           It\\'s used to replace\\n           [google.longrunning.Operation.response][google.longrunning.Operation.response]\\n           in case of success.',\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Pre-rpc interceptor for get_management_dns_zone_binding\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VmwareEngine server.',\n",
       " 'Call the start scan run method over HTTP.\\n\\nArgs:\\n    request (~.web_security_scanner.StartScanRunRequest):\\n        The request object. Request for the ``StartScanRun`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.scan_run.ScanRun:\\n        A ScanRun is a output-only resource\\n    representing an actual run of the scan.\\n    Next id: 12',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'forward latex should really return nothing in either field if nothing is found.',\n",
       " 'Private function that doesn\\'t support extended axis or keepdims.\\nThese methods are extended to this function using _ureduce\\nSee nanpercentile for parameter usage\\nIt computes the quantiles of the array for the given axis.\\nA linear interpolation is performed based on the `interpolation`.\\n\\nBy default, the method is \"linear\" where alpha == beta == 1 which\\nperforms the 7th method of Hyndman&Fan.\\nWith \"median_unbiased\" we get alpha == beta == 1/3\\nthus the 8th method of Hyndman&Fan.',\n",
       " 'Custom memmap constructor compatible with numpy.memmap.\\n\\nThis function:\\n- is a backport the numpy memmap offset fix (See\\n  https://github.com/numpy/numpy/pull/8443 for more details.\\n  The numpy fix is available starting numpy 1.13)\\n- adds ``unlink_on_gc_collect``, which specifies  explicitly whether\\n  the process re-constructing the memmap owns a reference to the\\n  underlying file. If set to True, it adds a finalizer to the\\n  newly-created memmap that sends a maybe_unlink request for the\\n  memmaped file to resource_tracker.',\n",
       " 'If the most relevant error is an anyOf, then we traverse its context\\nand select the otherwise *least* relevant error, since in this case\\nthat means the most specific, deep, error inside the instance.\\n\\nI.e. since only one of the schemas must match, we look for the most\\nrelevant one.',\n",
       " 'Test that a nested `AtomicString` is not parsed. ',\n",
       " 'Recall the first view and position from the stack.',\n",
       " 'Remove this colorbar from the figure.\\n\\nIf the colorbar was created with ``use_gridspec=True`` the previous\\ngridspec is restored.',\n",
       " 'Tests that for loops at deeper levels are picked up',\n",
       " 'Add a colorbar to a plot.\\n\\nParameters\\n----------\\nmappable\\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\\n    mandatory for the `.Figure.colorbar` method but optional for the\\n    `.pyplot.colorbar` function, which sets the default to the current\\n    image.\\n\\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\\n    generate colorbars not attached to a previously drawn artist, e.g.\\n    ::\\n\\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\\n\\ncax : `~matplotlib.axes.Axes`, optional\\n    Axes into which the colorbar will be drawn.  If `None`, then a new\\n    Axes is created and the space for it will be stolen from the Axes(s)\\n    specified in *ax*.\\n\\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\\n    The one or more parent Axes from which space for a new colorbar Axes\\n    will be stolen. This parameter is only used if *cax* is not set.\\n\\n    Defaults to the Axes that contains the mappable used to create the\\n    colorbar.\\n\\nuse_gridspec : bool, optional\\n    If *cax* is ``None``, a new *cax* is created as an instance of\\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\\n    is ``True``, then *cax* is also positioned with a subplotspec.\\n\\nReturns\\n-------\\ncolorbar : `~matplotlib.colorbar.Colorbar`\\n\\nOther Parameters\\n----------------\\n%(_make_axes_kw_doc)s\\n%(_colormap_kw_doc)s\\n\\nNotes\\n-----\\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\\nincluded automatically.\\n\\nThe *shrink* kwarg provides a simple way to scale the colorbar with\\nrespect to the Axes. Note that if *cax* is specified, it determines the\\nsize of the colorbar, and *shrink* and *aspect* are ignored.\\n\\nFor more precise control, you can manually specify the positions of the\\naxes objects in which the mappable and the colorbar are drawn.  In this\\ncase, do not use any of the Axes properties kwargs.\\n\\nIt is known that some vector graphics viewers (svg and pdf) render\\nwhite gaps between segments of the colorbar.  This is due to bugs in\\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\\nrendered with overlapping segments::\\n\\n    cbar = colorbar()\\n    cbar.solids.set_edgecolor(\"face\")\\n    draw()\\n\\nHowever, this has negative consequences in other circumstances, e.g.\\nwith semi-transparent images (alpha < 1) and colorbar extensions;\\ntherefore, this workaround is not used by default (see issue #1188).',\n",
       " 'Set slider value to *val*.\\n\\nParameters\\n----------\\nval : float',\n",
       " 'Return the ``ZAxis`` (`~.axis3d.Axis`) instance.',\n",
       " 'Whether the returned results should be verbose.',\n",
       " 'Test scale function with non-array',\n",
       " \"Verify MAC challenge\\n\\nIf our message did not include a digest_name prefix, the client is allowed\\nto select a stronger digest_name from _ALLOWED_DIGESTS.\\n\\nIn case our message is prefixed, a client cannot downgrade to a weaker\\nalgorithm, because the MAC is calculated over the entire message\\nincluding the '{digest_name}' prefix.\",\n",
       " 'Given a non-complete graph G, returns a missing edge.',\n",
       " 'Make X orthogonal to the nullspace of L.',\n",
       " 'Determine if a provided dtype is of a specified data type ``kind``.\\n\\nThis function only supports built-in NumPy\\'s data types.\\nThird-party dtypes are not yet supported.\\n\\nParameters\\n----------\\ndtype : dtype\\n    The input dtype.\\nkind : dtype or str or tuple of dtypes/strs.\\n    dtype or dtype kind. Allowed dtype kinds are:\\n    * ``\\'bool\\'`` : boolean kind\\n    * ``\\'signed integer\\'`` : signed integer data types\\n    * ``\\'unsigned integer\\'`` : unsigned integer data types\\n    * ``\\'integral\\'`` : integer data types\\n    * ``\\'real floating\\'`` : real-valued floating-point data types\\n    * ``\\'complex floating\\'`` : complex floating-point data types\\n    * ``\\'numeric\\'`` : numeric data types\\n\\nReturns\\n-------\\nout : bool\\n\\nSee Also\\n--------\\nissubdtype\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> np.isdtype(np.float32, np.float64)\\nFalse\\n>>> np.isdtype(np.float32, \"real floating\")\\nTrue\\n>>> np.isdtype(np.complex128, (\"real floating\", \"complex floating\"))\\nTrue',\n",
       " 'Sets the pod_selector of this V1NetworkPolicySpec.\\n\\n\\n:param pod_selector: The pod_selector of this V1NetworkPolicySpec.  # noqa: E501\\n:type: V1LabelSelector',\n",
       " 'Ensure the logged in user has authorized silent OpenID authorization.\\n\\nSilent OpenID authorization allows access tokens and id tokens to be\\ngranted to clients without any user prompt or interaction.\\n\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n:rtype: True or False\\n\\nMethod is used by:\\n    - OpenIDConnectAuthCode\\n    - OpenIDConnectImplicit\\n    - OpenIDConnectHybrid',\n",
       " 'This property can be used as a prefix for any HTTP method call to return the\\nthe raw response object instead of the parsed content.\\n\\nFor more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers',\n",
       " 'Test that the meter provides a function to create a new ObservableCounter',\n",
       " 'Loads an ASN.1 object of an x509 certificate into a Certificate object\\n\\n:param certificate:\\n    An asn1crypto.x509.Certificate object\\n\\n:return:\\n    A Certificate object',\n",
       " 'Convert a native series structure to a Series object.',\n",
       " 'Return the list of thead row elements from the parsed table element.\\n\\nParameters\\n----------\\ntable : a table element that contains zero or more thead elements.\\n\\nReturns\\n-------\\nlist of node-like\\n    These are the <tr> row elements of a table.',\n",
       " 'Try to format axes if they are datelike.',\n",
       " 'return the root node',\n",
       " 'Convert the data from this selection to the appropriate pandas type.\\n\\nParameters\\n----------\\nvalues : np.ndarray\\nnan_rep : str\\nencoding : str\\nerrors : str',\n",
       " 'Return different versions of data for count times',\n",
       " 'Check if we have a not-outdated version loaded already.',\n",
       " 'Remove a number of characters from the end of the text.',\n",
       " 'This method should only be called once, before the connection is used.',\n",
       " 'Negate a polynomial in ``K[x]``.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x = ring(\"x\", ZZ)\\n\\n>>> R.dup_neg(x**2 - 1)\\n-x**2 + 1',\n",
       " 'Test Subversion.get_vcs_version() with previously cached result.',\n",
       " 'Execute a statement using :sql:`VALUES` with a sequence of parameters.\\n\\n:param cur: the cursor to use to execute the query.\\n\\n:param sql: the query to execute. It must contain a single ``%s``\\n    placeholder, which will be replaced by a `VALUES list`__.\\n    Example: ``\"INSERT INTO mytable (id, f1, f2) VALUES %s\"``.\\n\\n:param argslist: sequence of sequences or dictionaries with the arguments\\n    to send to the query. The type and content must be consistent with\\n    *template*.\\n\\n:param template: the snippet to merge to every item in *argslist* to\\n    compose the query.\\n\\n    - If the *argslist* items are sequences it should contain positional\\n      placeholders (e.g. ``\"(%s, %s, %s)\"``, or ``\"(%s, %s, 42)``\" if there\\n      are constants value...).\\n\\n    - If the *argslist* items are mappings it should contain named\\n      placeholders (e.g. ``\"(%(id)s, %(f1)s, 42)\"``).\\n\\n    If not specified, assume the arguments are sequence and use a simple\\n    positional template (i.e.  ``(%s, %s, ...)``), with the number of\\n    placeholders sniffed by the first element in *argslist*.\\n\\n:param page_size: maximum number of *argslist* items to include in every\\n    statement. If there are more items the function will execute more than\\n    one statement.\\n\\n:param fetch: if `!True` return the query results into a list (like in a\\n    `~cursor.fetchall()`).  Useful for queries with :sql:`RETURNING`\\n    clause.\\n\\n.. __: https://www.postgresql.org/docs/current/static/queries-values.html\\n\\nAfter the execution of the function the `cursor.rowcount` property will\\n**not** contain a total result.\\n\\nWhile :sql:`INSERT` is an obvious candidate for this function it is\\npossible to use it with other statements, for example::\\n\\n    >>> cur.execute(\\n    ... \"create table test (id int primary key, v1 int, v2 int)\")\\n\\n    >>> execute_values(cur,\\n    ... \"INSERT INTO test (id, v1, v2) VALUES %s\",\\n    ... [(1, 2, 3), (4, 5, 6), (7, 8, 9)])\\n\\n    >>> execute_values(cur,\\n    ... \"\"\"UPDATE test SET v1 = data.v1 FROM (VALUES %s) AS data (id, v1)\\n    ... WHERE test.id = data.id\"\"\",\\n    ... [(1, 20), (4, 50)])\\n\\n    >>> cur.execute(\"select * from test order by id\")\\n    >>> cur.fetchall()\\n    [(1, 20, 3), (4, 50, 6), (7, 8, 9)])',\n",
       " 'Finalize build system configuration on win32 platform.',\n",
       " 'Instantiate a cipher object that performs ECB encryption/decryption.\\n\\n:Parameters:\\n  factory : module\\n    The underlying block cipher, a module from ``Crypto.Cipher``.\\n\\nAll keywords are passed to the underlying block cipher.\\nSee the relevant documentation for details (at least ``key`` will need\\nto be present',\n",
       " '`version` is the mypy version string.\\n\\nWe might want to use this to print a warning if the mypy version being used is\\nnewer, or especially older, than we expect (or need).\\n\\nArgs:\\n    version: The mypy version string.\\n\\nReturn:\\n    The Pydantic mypy plugin type.',\n",
       " 'Returns a dict of config keys to values.\\n\\nIt reads configs from toml file and returns `None` if the file is not a toml file.',\n",
       " ':calls: `POST /repos/{owner}/{repo}/forks <https://docs.github.com/en/rest/reference/repos#forks>`_\\n:param organization: :class:`github.Organization.Organization` or string\\n:param name: string\\n:param default_branch_only: bool\\n:rtype: :class:`github.Repository.Repository`',\n",
       " 'Issue the warning :param:`message` for the definition of the given :param:`method`\\n\\nthis helps to log warnings for functions defined prior to finding an issue with them\\n(like hook wrappers being marked in a legacy mechanism)',\n",
       " \"Check whether 'value' should be coerced to 'field' type.\",\n",
       " 'Error 301 -- also relocated (permanently).',\n",
       " 'Issue #96',\n",
       " 'Polls a Cloud Pub/Sub subscription for new GCS events for display.',\n",
       " 'Sets the api_server_id of this V1alpha1ServerStorageVersion.\\n\\nThe ID of the reporting API server.  # noqa: E501\\n\\n:param api_server_id: The api_server_id of this V1alpha1ServerStorageVersion.  # noqa: E501\\n:type: str',\n",
       " \"Differentiate polynomials represented with coefficients.\\n\\np must be a 1-D or 2-D array.  In the 2-D case, each column gives\\nthe coefficients of a polynomial; the first row holds the coefficients\\nassociated with the highest power. m must be a nonnegative integer.\\n(numpy.polyder doesn't handle the 2-D case.)\",\n",
       " 'Return a :class:`.MapperOption` that will indicate to the\\n:class:`_query.Query`\\nthat the main table has been aliased.',\n",
       " \"Initialize an ``AlgorithmEstimator`` instance.\\n\\nArgs:\\n    algorithm_arn (str): algorithm arn used for training. Can be just the name if your\\n        account owns the algorithm.\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker\\n        training jobs and APIsthat create Amazon SageMaker endpoints use this role to\\n        access training data and model artifacts. After the endpoint\\n        is created, the inference code might use the IAM role, if it\\n        needs to access an AWS resource.\\n    instance_count (int or PipelineVariable): Number of Amazon EC2 instances to use\\n        for training.\\n    instance_type (str or PipelineVariable): Type of EC2 instance to use for training,\\n        for example, 'ml.c4.xlarge'.\\n    volume_size (int or PipelineVariable): Size in GB of the EBS volume to use for\\n        storing input data during training (default: 30). Must be large enough to store\\n        training data if File Mode is used (which is the default).\\n    volume_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting\\n        EBS volume attached to the training instance (default: None).\\n    max_run (int or PipelineVariable): Timeout in seconds for training\\n        (default: 24 * 60 * 60).\\n        After this amount of time Amazon SageMaker terminates the\\n        job regardless of its current status.\\n    input_mode (str or PipelineVariable): The input mode that the algorithm supports\\n        (default: 'File'). Valid modes:\\n\\n        * 'File' - Amazon SageMaker copies the training dataset from\\n          the S3 location to a local directory.\\n        * 'Pipe' - Amazon SageMaker streams data directly from S3 to\\n          the container via a Unix-named pipe.\\n\\n        This argument can be overriden on a per-channel basis using\\n        ``sagemaker.inputs.TrainingInput.input_mode``.\\n\\n    output_path (str or PipelineVariable): S3 location for saving the training result\\n        (model artifacts and output files). If not specified,\\n        results are stored to a default bucket. If\\n        the bucket with the specific name does not exist, the\\n        estimator creates the bucket during the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit` method\\n        execution.\\n    output_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting the\\n        training output (default: None). base_job_name (str): Prefix for\\n        training job name when the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit`\\n        method launches. If not specified, the estimator generates a\\n        default job name, based on the training image name and\\n        current timestamp.\\n    sagemaker_session (sagemaker.session.Session): Session object which manages\\n        interactions with Amazon SageMaker APIs and any other AWS services needed. If\\n        not specified, the estimator creates one using the default\\n        AWS configuration chain.\\n    tags (Union[Tags]): Tags for\\n        labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    subnets (list[str] or list[PipelineVariable]): List of subnet ids. If not specified\\n        training job will be created without VPC config.\\n        security_group_ids (list[str]): List of security group ids. If\\n        not specified training job will be created without VPC config.\\n    model_uri (str): URI where a pre-trained model is stored, either locally or in S3\\n        (default: None). If specified, the estimator will create a channel pointing to\\n        the model so the training job can download it. This model\\n        can be a 'model.tar.gz' from a previous training job, or\\n        other artifacts coming from a different source.\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization\\n    model_channel_name (str or PipelineVariable): Name of the channel where 'model_uri'\\n        will be downloaded (default: 'model'). metric_definitions\\n        (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for\\n        the name of the metric, and 'Regex' for the regular\\n        expression used to extract the metric from the logs.\\n    encrypt_inter_container_traffic (bool or PipelineVariable): Specifies whether traffic\\n        between training containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool or PipelineVariable): Specifies whether to use SageMaker\\n        Managed Spot instances for training. If enabled then the\\n        `max_wait` arg should also be set.\\n\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\\n        (default: ``False``).\\n    max_wait (int or PipelineVariable): Timeout in seconds waiting for spot training\\n        instances (default: None). After this amount of time Amazon\\n        SageMaker will stop waiting for Spot instances to become\\n        available (default: ``None``).\\n    **kwargs: Additional kwargs. This is unused. It's only added for AlgorithmEstimator\\n        to ignore the irrelevant arguments.\\n\\nRaises:\\n    ValueError:\\n    - If an AWS IAM Role is not provided.\\n    - Bad value for instance type.\\n    RuntimeError:\\n    - When setting up custom VPC, both subnets and security_group_ids are not provided\\n    - If instance_count > 1 (distributed training) with instance type local or local gpu\\n    - If LocalSession is not used with instance type local or local gpu\\n    - file:// output path used outside of local mode\\n    botocore.exceptions.ClientError:\\n    - algorithm arn is incorrect\\n    - insufficient permission to access/ describe algorithm\\n    - algorithm is in a different region\",\n",
       " 'Sets fields in object based on json of header.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of spec.',\n",
       " 'The full SELECT statement represented by this Query.\\n\\nThe statement by default will not have disambiguating labels\\napplied to the construct unless with_labels(True) is called\\nfirst.',\n",
       " 'Set Additional Model Source to ``this`` model.\\n\\nArgs:\\n    speculative_decoding_config (Optional[Dict[str, Any]]): Speculative decoding config.\\n    accept_eula (Optional[bool]): For models that require a Model Access Config.',\n",
       " 'Decorator to emit telemetry logs for SageMaker Python SDK functions',\n",
       " 'Memoized fast path for _get_funcs instances',\n",
       " 'Access nonzero values, possibly after summing duplicates.\\n\\nParameters\\n----------\\ns : sparse array\\n    Input sparse array.\\n\\nReturns\\n-------\\ndata: ndarray\\n  Nonzero values of the array, with shape (s.nnz,)',\n",
       " 'Compute matrices to transform rot. vector derivatives to angular rates.\\n\\nThe matrices depend on the current attitude represented as a rotation\\nvector.\\n\\nParameters\\n----------\\nrotvecs : ndarray, shape (n, 3)\\n    Set of rotation vectors.\\n\\nReturns\\n-------\\nndarray, shape (n, 3, 3)',\n",
       " 'Return a GeoJSON-like mapping of the LineString geometry.',\n",
       " 'Unsupported.',\n",
       " 'Does s3 multipart chunking work correctly?',\n",
       " 'test #6696',\n",
       " 'Gets the value of withCentering or its default value.',\n",
       " 'Read HTML tables into a ``list`` of ``DataFrame`` objects.\\n\\nParameters\\n----------\\nio : str or file-like\\n    A URL, a file-like object, or a raw string containing HTML. Note that\\n    lxml only accepts the http, FTP and file URL protocols. If you have a\\n    URL that starts with ``\\'https\\'`` you might try removing the ``\\'s\\'``.\\n\\n    .. deprecated:: 4.0.0\\n        Passing html literal strings is deprecated.\\n        Wrap literal string/bytes input in io.StringIO/io.BytesIO instead.\\n\\nmatch : str or compiled regular expression, optional\\n    The set of tables containing text matching this regex or string will be\\n    returned. Unless the HTML is extremely simple you will probably need to\\n    pass a non-empty string here. Defaults to \\'.+\\' (match any non-empty\\n    string). The default value will return all tables contained on a page.\\n    This value is converted to a regular expression so that there is\\n    consistent behavior between Beautiful Soup and lxml.\\n\\nflavor : str or None, container of strings\\n    The parsing engine to use. \\'bs4\\' and \\'html5lib\\' are synonymous with\\n    each other, they are both there for backwards compatibility. The\\n    default of ``None`` tries to use ``lxml`` to parse and if that fails it\\n    falls back on ``bs4`` + ``html5lib``.\\n\\nheader : int or list-like or None, optional\\n    The row (or list of rows for a :class:`~ps.MultiIndex`) to use to\\n    make the columns headers.\\n\\nindex_col : int or list-like or None, optional\\n    The column (or list of columns) to use to create the index.\\n\\nskiprows : int or list-like or slice or None, optional\\n    0-based. Number of rows to skip after parsing the column integer. If a\\n    sequence of integers or a slice is given, will skip the rows indexed by\\n    that sequence.  Note that a single element sequence means \\'skip the nth\\n    row\\' whereas an integer means \\'skip n rows\\'.\\n\\nattrs : dict or None, optional\\n    This is a dictionary of attributes that you can pass to use to identify\\n    the table in the HTML. These are not checked for validity before being\\n    passed to lxml or Beautiful Soup. However, these attributes must be\\n    valid HTML table attributes to work correctly. For example, ::\\n\\n        attrs = {\\'id\\': \\'table\\'}\\n\\n    is a valid attribute dictionary because the \\'id\\' HTML tag attribute is\\n    a valid HTML attribute for *any* HTML tag as per `this document\\n    <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\\n\\n        attrs = {\\'asdf\\': \\'table\\'}\\n\\n    is *not* a valid attribute dictionary because \\'asdf\\' is not a valid\\n    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\\n    table attributes can be found `here\\n    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\\n    working draft of the HTML 5 spec can be found `here\\n    <http://www.w3.org/TR/html-markup/table.html>`__. It contains the\\n    latest information on table attributes for the modern web.\\n\\nparse_dates : bool, optional\\n    See :func:`~ps.read_csv` for more details.\\n\\nthousands : str, optional\\n    Separator to use to parse thousands. Defaults to ``\\',\\'``.\\n\\nencoding : str or None, optional\\n    The encoding used to decode the web page. Defaults to ``None``.``None``\\n    preserves the previous encoding behavior, which depends on the\\n    underlying parser library (e.g., the parser library will try to use\\n    the encoding provided by the document).\\n\\ndecimal : str, default \\'.\\'\\n    Character to recognize as decimal point (example: use \\',\\' for European\\n    data).\\n\\nconverters : dict, default None\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\n\\nna_values : iterable, default None\\n    Custom NA values\\n\\nkeep_default_na : bool, default True\\n    If na_values are specified and keep_default_na is False the default NaN\\n    values are overridden, otherwise they\\'re appended to\\n\\ndisplayed_only : bool, default True\\n    Whether elements with \"display: none\" should be parsed\\n\\nReturns\\n-------\\ndfs : list of DataFrames\\n\\nSee Also\\n--------\\nread_csv\\nDataFrame.to_html',\n",
       " 'Truncate the timestamps for nanoseconds.',\n",
       " 'Test the different timestamp, date, and timedelta types.',\n",
       " 'Return the \"comment\" for the table identified by ``table_name``.\\n\\nGiven a string ``table_name`` and an optional string ``schema``, return\\ntable comment information as a dictionary corresponding to the\\n:class:`.ReflectedTableComment` dictionary.\\n\\nThis is an internal dialect method. Applications should use\\n:meth:`.Inspector.get_table_comment`.\\n\\n:raise: ``NotImplementedError`` for dialects that don\\'t support\\n comments.\\n\\n.. versionadded:: 1.2',\n",
       " 'Driver quirk where the cursor.fetchall() will work even if\\nthe connection has been rolled back.\\n\\nThis generally refers to buffered cursors but also seems to work\\nwith cx_oracle, for example.',\n",
       " 'Target must support simultaneous, independent database connections\\nthat will be used in a readonly fashion.',\n",
       " 'Start date column, if not present already.',\n",
       " 'Returns the typecast or ``None`` of this object as a string.',\n",
       " 'Target must support simultaneous, independent database connections.',\n",
       " 'If ``key`` is in ``dictionary``, set the new value of ``key``\\nto be the union between the old value and ``value``.\\nOtherwise, set the value of ``key`` to ``value.\\n\\nReturns ``True`` if the key already was in the dictionary and\\n``False`` otherwise.',\n",
       " 'Return True if expr1 and expr2 are numerically close.\\n\\nThe expressions must have the same structure, but any Rational, Integer, or\\nFloat numbers they contain are compared approximately using rtol and atol.\\nAny other parts of expressions are compared exactly. However, allowance is\\nmade to allow for the additive and multiplicative identities.\\n\\nRelative tolerance is measured with respect to expr2 so when used in\\ntesting expr2 should be the expected correct answer.\\n\\nExamples\\n========\\n\\n>>> from sympy import exp\\n>>> from sympy.abc import x, y\\n>>> from sympy.core.numbers import all_close\\n>>> expr1 = 0.1*exp(x - y)\\n>>> expr2 = exp(x - y)/10\\n>>> expr1\\n0.1*exp(x - y)\\n>>> expr2\\nexp(x - y)/10\\n>>> expr1 == expr2\\nFalse\\n>>> all_close(expr1, expr2)\\nTrue\\n\\nIdentities are automatically supplied:\\n\\n>>> all_close(x, x + 1e-10)\\nTrue\\n>>> all_close(x, 1.0*x)\\nTrue\\n>>> all_close(x, 1.0*x + 1e-10)\\nTrue',\n",
       " 'Turn all numbers in eq into their polar equivalents (under the standard\\nchoice of argument).\\n\\nNote that no attempt is made to guess a formal convention of adding\\npolar numbers, expressions like $1 + x$ will generally not be altered.\\n\\nNote also that this function does not promote ``exp(x)`` to ``exp_polar(x)``.\\n\\nIf ``subs`` is ``True``, all symbols which are not already polar will be\\nsubstituted for polar dummies; in this case the function behaves much\\nlike :func:`~.posify`.\\n\\nIf ``lift`` is ``True``, both addition statements and non-polar symbols are\\nchanged to their ``polar_lift()``ed versions.\\nNote that ``lift=True`` implies ``subs=False``.\\n\\nExamples\\n========\\n\\n>>> from sympy import polarify, sin, I\\n>>> from sympy.abc import x, y\\n>>> expr = (-x)**y\\n>>> expr.expand()\\n(-x)**y\\n>>> polarify(expr)\\n((_x*exp_polar(I*pi))**_y, {_x: x, _y: y})\\n>>> polarify(expr)[0].expand()\\n_x**_y*exp_polar(_y*I*pi)\\n>>> polarify(x, lift=True)\\npolar_lift(x)\\n>>> polarify(x*(1+y), lift=True)\\npolar_lift(x)*polar_lift(y + 1)\\n\\nAdds are treated carefully:\\n\\n>>> polarify(1 + sin((1 + I)*x))\\n(sin(_x*polar_lift(1 + I)) + 1, {_x: x})',\n",
       " 'Front-end function of the inverse Laplace transform. It tries to apply all\\nknown rules recursively.  If everything else fails, it tries to integrate.',\n",
       " 'Returns denominator of ``a``. ',\n",
       " 'Returns a field associated with ``self``. ',\n",
       " 'Return the Smith Normal Form of a matrix `m` over the ring `domain`.\\nThis will only work if the ring is a principal ideal domain.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_form\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> print(smith_normal_form(m).to_Matrix())\\nMatrix([[1, 0, 0], [0, 10, 0], [0, 0, 30]])',\n",
       " 'Returns ``True`` if ``f`` is a cyclotomic polynomial. ',\n",
       " 'Calls the given callback on the next IOLoop iteration.\\n\\nAs of Tornado 6.0, this method is equivalent to `add_callback`.\\n\\n.. versionadded:: 4.0',\n",
       " 'Put a connection back into the pool.\\n\\n:param conn:\\n    Connection object for the current host and port as returned by\\n    :meth:`._new_conn` or :meth:`._get_conn`.\\n\\nIf the pool is already full, the connection is closed and discarded\\nbecause we exceeded maxsize. If connections are discarded frequently,\\nthen maxsize should be increased.\\n\\nIf the pool is closed, then the connection will be closed and discarded.',\n",
       " \"Create a WebSocket server listening on a Unix socket.\\n\\nThis function is identical to :func:`serve`, except the ``host`` and\\n``port`` arguments are replaced by ``path``. It's only available on Unix.\\n\\nIt's useful for deploying a server behind a reverse proxy such as nginx.\\n\\nArgs:\\n    handler: Connection handler. It receives the WebSocket connection,\\n        which is a :class:`ServerConnection`, in argument.\\n    path: File system path to the Unix socket.\",\n",
       " 'Handshake succeeds without subprotocols.',\n",
       " 'Creates datatype test schema, tables, and inserts test data.',\n",
       " 'Write File.',\n",
       " \"Locale display names for months.\\n\\n>>> Locale('de', 'DE').months['format']['wide'][10]\\nu'Oktober'\",\n",
       " 'Return the era names used by the locale for the specified format.\\n\\n>>> get_era_names(\\'wide\\', locale=\\'en_US\\')[1]\\nu\\'Anno Domini\\'\\n>>> get_era_names(\\'abbreviated\\', locale=\\'de_DE\\')[1]\\nu\\'n. Chr.\\'\\n\\n:param width: the width to use, either \"wide\", \"abbreviated\", or \"narrow\"\\n:param locale: the `Locale` object, or a locale string',\n",
       " 'Return a node that represents the (type) result of an indexing operation,\\ne.g. for tuple unpacking or iteration.',\n",
       " 'Compacts the frames to deduplicate recursive calls.',\n",
       " 'Many of these arguments duplicate and override values that can be\\nprovided in a configuration file.  Parameters that are missing here\\nwill use values from the config file.\\n\\n`data_file` is the base name of the data file to use. The config value\\ndefaults to \".coverage\".  None can be provided to prevent writing a data\\nfile.  `data_suffix` is appended (with a dot) to `data_file` to create\\nthe final file name.  If `data_suffix` is simply True, then a suffix is\\ncreated with the machine and process identity included.\\n\\n`cover_pylib` is a boolean determining whether Python code installed\\nwith the Python interpreter is measured.  This includes the Python\\nstandard library and any packages installed with the interpreter.\\n\\nIf `auto_data` is true, then any existing data file will be read when\\ncoverage measurement starts, and data will be saved automatically when\\nmeasurement stops.\\n\\nIf `timid` is true, then a slower and simpler trace function will be\\nused.  This is important for some environments where manipulation of\\ntracing functions breaks the faster trace function.\\n\\nIf `branch` is true, then branch coverage will be measured in addition\\nto the usual statement coverage.\\n\\n`config_file` determines what configuration file to read:\\n\\n    * If it is \".coveragerc\", it is interpreted as if it were True,\\n      for backward compatibility.\\n\\n    * If it is a string, it is the name of the file to read.  If the\\n      file can\\'t be read, it is an error.\\n\\n    * If it is True, then a few standard files names are tried\\n      (\".coveragerc\", \"setup.cfg\", \"tox.ini\").  It is not an error for\\n      these files to not be found.\\n\\n    * If it is False, then no configuration file is read.\\n\\n`source` is a list of file paths or package names.  Only code located\\nin the trees indicated by the file paths or package names will be\\nmeasured.\\n\\n`source_pkgs` is a list of package names. It works the same as\\n`source`, but can be used to name packages where the name can also be\\ninterpreted as a file path.\\n\\n`include` and `omit` are lists of file name patterns. Files that match\\n`include` will be measured, files that match `omit` will not.  Each\\nwill also accept a single string argument.\\n\\n`debug` is a list of strings indicating what debugging information is\\ndesired.\\n\\n`concurrency` is a string indicating the concurrency library being used\\nin the measured code.  Without this, coverage.py will get incorrect\\nresults if these libraries are in use.  Valid strings are \"greenlet\",\\n\"eventlet\", \"gevent\", \"multiprocessing\", or \"thread\" (the default).\\nThis can also be a list of these strings.\\n\\nIf `check_preimported` is true, then when coverage is started, the\\nalready-imported files will be checked to see if they should be\\nmeasured by coverage.  Importing measured files before coverage is\\nstarted can mean that code is missed.\\n\\n`context` is a string to use as the :ref:`static context\\n<static_contexts>` label for collected data.\\n\\nIf `messages` is true, some messages will be printed to stdout\\nindicating what is happening.\\n\\n.. versionadded:: 4.0\\n    The `concurrency` parameter.\\n\\n.. versionadded:: 4.2\\n    The `concurrency` parameter can now be a list of strings.\\n\\n.. versionadded:: 5.0\\n    The `check_preimported` and `context` parameters.\\n\\n.. versionadded:: 5.3\\n    The `source_pkgs` parameter.\\n\\n.. versionadded:: 6.0\\n    The `messages` parameter.',\n",
       " 'Make sure our htmlcov directory exists.',\n",
       " 'Assert that numbits is good.',\n",
       " 'Make a tree of packages.\\n\\nMakes `width` directories, named d0 .. d{width-1}. Each directory has\\n__init__.py, and `width` files, named f0.py .. f{width-1}.py.  Each\\ndirectory also has `width` sub-directories, in the same fashion, until\\na depth of `depth` is reached.',\n",
       " 'Changes the value of a variable',\n",
       " 'Align the given address to the end of the page it occupies.\\nThat is, to point to the start of the next page.\\n\\n@type  address: int\\n@param address: Memory address.\\n\\n@rtype:  int\\n@return: Aligned memory address.',\n",
       " '@rtype:  bool\\n@return: C{True} if the memory in this region belongs to a mapped file.',\n",
       " 'Set a the value for ``key`` to ``value`` inside the ``config``\\ndict.',\n",
       " 'Unpack the crc32 checksum for the ith object from the index file.',\n",
       " 'All refs present in this container.',\n",
       " \"Return a named tuple containing ISO year, week number, and weekday.\\n\\nThe first ISO week of the year is the (Mon-Sun) week\\ncontaining the year's first Thursday; everything else derives\\nfrom that.\\n\\nThe first week is 1; Monday is 1 ... Sunday is 7.\\n\\nISO calendar algorithm taken from\\nhttp://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\\n(used with permission)\",\n",
       " 'Mirror attributes and methods from the given\\norigin_name attribute of the instance to the\\ndecorated class',\n",
       " 'Post-rpc interceptor for create_analytics_account_link\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the MarketingplatformAdminService server but before\\nit is returned to user code.',\n",
       " 'Call the list google ads links method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.ListGoogleAdsLinksRequest):\\n        The request object. Request message for\\n    ListGoogleAdsLinks RPC.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.analytics_admin.ListGoogleAdsLinksResponse:\\n        Response message for\\n    ListGoogleAdsLinks RPC.',\n",
       " 'Returns matching deployments.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import apigee_registry_v1\\n\\n    def sample_list_api_deployments():\\n        # Create a client\\n        client = apigee_registry_v1.RegistryClient()\\n\\n        # Initialize request argument(s)\\n        request = apigee_registry_v1.ListApiDeploymentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_api_deployments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.apigee_registry_v1.types.ListApiDeploymentsRequest, dict]):\\n        The request object. Request message for\\n        ListApiDeployments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        deployments. Format: ``projects/*/locations/*/apis/*``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.apigee_registry_v1.services.registry.pagers.ListApiDeploymentsPager:\\n        Response message for\\n        ListApiDeployments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ApiHubAsyncClient: The constructed client.',\n",
       " 'Call the get api spec method over HTTP.\\n\\nArgs:\\n    request (~.registry_service.GetApiSpecRequest):\\n        The request object. Request message for GetApiSpec.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.registry_models.ApiSpec:\\n        Describes a version of an API in a\\n    structured way. ApiSpecs provide formal\\n    descriptions that consumers can use to\\n    use a version. ApiSpec resources are\\n    intended to be fully-resolved\\n    descriptions of an ApiVersion. When\\n    specs consist of multiple files, these\\n    should be bundled together (e.g., in a\\n    zip archive) and stored as a unit.\\n    Multiple specs can exist to provide\\n    representations in different API\\n    description formats. Synchronization of\\n    these representations would be provided\\n    by tooling and background services.',\n",
       " 'return a boolean if I am possibly a view',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Returns the specified disk type.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.DiskTypesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetDiskTypeRequest(\\n            disk_type=\"disk_type_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetDiskTypeRequest, dict]):\\n        The request object. A request message for DiskTypes.Get.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone for this\\n        request.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk_type (str):\\n        Name of the disk type to return.\\n        This corresponds to the ``disk_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.DiskType:\\n        Represents a Disk Type resource. Google Compute Engine\\n        has two Disk Type resources: \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDiskTypes)\\n        \\\\* [Zonal](/compute/docs/reference/rest/v1/diskTypes)\\n        You can choose from a variety of disk types based on\\n        your needs. For more information, read Storage options.\\n        The diskTypes resource represents disk types for a zonal\\n        persistent disk. For more information, read Zonal\\n        persistent disks. The regionDiskTypes resource\\n        represents disk types for a regional persistent disk.\\n        For more information, read Regional persistent disks.',\n",
       " 'Retrieves a list of resize requests that are\\ncontained in the managed instance group.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InstanceGroupManagerResizeRequestsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInstanceGroupManagerResizeRequestsRequest(\\n            instance_group_manager=\"instance_group_manager_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInstanceGroupManagerResizeRequestsRequest, dict]):\\n        The request object. A request message for\\n        InstanceGroupManagerResizeRequests.List.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone where the\\n        managed instance group is located. The\\n        name should conform to RFC1035.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    instance_group_manager (str):\\n        The name of the managed instance\\n        group. The name should conform to\\n        RFC1035.\\n\\n        This corresponds to the ``instance_group_manager`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.instance_group_manager_resize_requests.pagers.ListPager:\\n        [Output Only] A list of resize requests.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Snapshot the state of a streaming job.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataflow_v1beta3\\n\\n    def sample_snapshot_job():\\n        # Create a client\\n        client = dataflow_v1beta3.JobsV1Beta3Client()\\n\\n        # Initialize request argument(s)\\n        request = dataflow_v1beta3.SnapshotJobRequest(\\n        )\\n\\n        # Make the request\\n        response = client.snapshot_job(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataflow_v1beta3.types.SnapshotJobRequest, dict]):\\n        The request object. Request to create a snapshot of a\\n        job.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataflow_v1beta3.types.Snapshot:\\n        Represents a snapshot of a job.',\n",
       " 'Use this method to delete a private connectivity\\nconfiguration.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datastream_v1alpha1\\n\\n    def sample_delete_private_connection():\\n        # Create a client\\n        client = datastream_v1alpha1.DatastreamClient()\\n\\n        # Initialize request argument(s)\\n        request = datastream_v1alpha1.DeletePrivateConnectionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_private_connection(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datastream_v1alpha1.types.DeletePrivateConnectionRequest, dict]):\\n        The request object.\\n    name (str):\\n        Required. The name of the private\\n        connectivity configuration to delete.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Call the fetch git hub\\ninstallations method over HTTP.\\n\\n    Args:\\n        request (~.developer_connect.FetchGitHubInstallationsRequest):\\n            The request object. Request for fetching github\\n        installations.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.developer_connect.FetchGitHubInstallationsResponse:\\n            Response of fetching github\\n        installations.',\n",
       " 'Call the update conversation\\nprofile method over HTTP.\\n\\n    Args:\\n        request (~.gcd_conversation_profile.UpdateConversationProfileRequest):\\n            The request object. The request message for\\n        [ConversationProfiles.UpdateConversationProfile][google.cloud.dialogflow.v2.ConversationProfiles.UpdateConversationProfile].\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcd_conversation_profile.ConversationProfile:\\n            Defines the services to connect to\\n        incoming Dialogflow conversations.',\n",
       " 'Updates the specified entity type.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflow_v2\\n\\n    def sample_update_entity_type():\\n        # Create a client\\n        client = dialogflow_v2.EntityTypesClient()\\n\\n        # Initialize request argument(s)\\n        entity_type = dialogflow_v2.EntityType()\\n        entity_type.display_name = \"display_name_value\"\\n        entity_type.kind = \"KIND_REGEXP\"\\n\\n        request = dialogflow_v2.UpdateEntityTypeRequest(\\n            entity_type=entity_type,\\n        )\\n\\n        # Make the request\\n        response = client.update_entity_type(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflow_v2.types.UpdateEntityTypeRequest, dict]):\\n        The request object. The request message for\\n        [EntityTypes.UpdateEntityType][google.cloud.dialogflow.v2.EntityTypes.UpdateEntityType].\\n    entity_type (google.cloud.dialogflow_v2.types.EntityType):\\n        Required. The entity type to update.\\n        This corresponds to the ``entity_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    language_code (str):\\n        Optional. The language used to access language-specific\\n        data. If not specified, the agent\\'s default language is\\n        used. For more information, see `Multilingual intent and\\n        entity\\n        data <https://cloud.google.com/dialogflow/docs/agents-multilingual#intent-entity>`__.\\n\\n        This corresponds to the ``language_code`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflow_v2.types.EntityType:\\n        Each intent parameter has a type, called the entity type, which dictates\\n           exactly how data from an end-user expression is\\n           extracted.\\n\\n           Dialogflow provides predefined system entities that\\n           can match many common types of data. For example,\\n           there are system entities for matching dates, times,\\n           colors, email addresses, and so on. You can also\\n           create your own custom entities for matching custom\\n           data. For example, you could define a vegetable\\n           entity that can match the types of vegetables\\n           available for purchase with a grocery store agent.\\n\\n           For more information, see the [Entity\\n           guide](\\\\ https://cloud.google.com/dialogflow/docs/entities-overview).',\n",
       " 'Call the complete query method over HTTP.\\n\\nArgs:\\n    request (~.completion_service.CompleteQueryRequest):\\n        The request object. Request message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.completion_service.CompleteQueryResponse:\\n        Response message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.',\n",
       " 'Deletes a single Subnet.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import edgenetwork_v1\\n\\n    def sample_delete_subnet():\\n        # Create a client\\n        client = edgenetwork_v1.EdgeNetworkClient()\\n\\n        # Initialize request argument(s)\\n        request = edgenetwork_v1.DeleteSubnetRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_subnet(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.edgenetwork_v1.types.DeleteSubnetRequest, dict]):\\n        The request object. Message for deleting a Subnet\\n    name (str):\\n        Required. Name of the resource\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'List OS policies compliance data for all Compute\\nEngine VM instances in the specified zone.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import osconfig_v1alpha\\n\\n    def sample_list_instance_os_policies_compliances():\\n        # Create a client\\n        client = osconfig_v1alpha.OsConfigZonalServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = osconfig_v1alpha.ListInstanceOSPoliciesCompliancesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_instance_os_policies_compliances(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.osconfig_v1alpha.types.ListInstanceOSPoliciesCompliancesRequest, dict]):\\n        The request object. A request message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n    parent (str):\\n        Required. The parent resource name.\\n\\n        Format: ``projects/{project}/locations/{location}``\\n\\n        For ``{project}``, either Compute Engine project-number\\n        or project-id can be provided.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.osconfig_v1alpha.services.os_config_zonal_service.pagers.ListInstanceOSPoliciesCompliancesPager:\\n        A response message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Return GeoTIFF metadata from first page as dict.',\n",
       " 'Call the list crypto keys method over HTTP.\\n\\nArgs:\\n    request (~.service.ListCryptoKeysRequest):\\n        The request object. Request message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.service.ListCryptoKeysResponse:\\n        Response message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].',\n",
       " 'Lists\\n[CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.security import privateca_v1\\n\\n    def sample_list_certificate_templates():\\n        # Create a client\\n        client = privateca_v1.CertificateAuthorityServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = privateca_v1.ListCertificateTemplatesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_certificate_templates(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.security.privateca_v1.types.ListCertificateTemplatesRequest, dict]):\\n        The request object. Request message for\\n        [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n    parent (str):\\n        Required. The resource name of the location associated\\n        with the\\n        [CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate],\\n        in the format ``projects/*/locations/*``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.security.privateca_v1.services.certificate_authority_service.pagers.ListCertificateTemplatesPager:\\n        Response message for\\n           [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Post-rpc interceptor for update_certificate_template\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'Deletes permanently all user events specified by the\\nfilter provided. Depending on the number of events\\nspecified by the filter, this operation could take hours\\nor days to complete. To test a filter, use the list\\ncommand first.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import retail_v2alpha\\n\\n    def sample_purge_user_events():\\n        # Create a client\\n        client = retail_v2alpha.UserEventServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = retail_v2alpha.PurgeUserEventsRequest(\\n            parent=\"parent_value\",\\n            filter=\"filter_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.purge_user_events(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.retail_v2alpha.types.PurgeUserEventsRequest, dict]):\\n        The request object. Request message for PurgeUserEvents\\n        method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.retail_v2alpha.types.PurgeUserEventsResponse` Response of the PurgeUserEventsRequest. If the long running operation is\\n           successfully done, then this message is returned by\\n           the google.longrunning.Operations.response field.',\n",
       " 'Return a callable for the pause job method over gRPC.\\n\\nPauses a job.\\n\\nIf a job is paused then the system will stop executing the job\\nuntil it is re-enabled via\\n[ResumeJob][google.cloud.scheduler.v1.CloudScheduler.ResumeJob].\\nThe state of the job is stored in\\n[state][google.cloud.scheduler.v1.Job.state]; if paused it will\\nbe set to\\n[Job.State.PAUSED][google.cloud.scheduler.v1.Job.State.PAUSED].\\nA job must be in\\n[Job.State.ENABLED][google.cloud.scheduler.v1.Job.State.ENABLED]\\nto be paused.\\n\\nReturns:\\n    Callable[[~.PauseJobRequest],\\n            ~.Job]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the delete series method over HTTP.\\n\\nArgs:\\n    request (~.streams_service.DeleteSeriesRequest):\\n        The request object. Message for deleting a Series.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return whether the supplied file name fn matches pattern filename.',\n",
       " 'Start the main event loop.\\n\\nThis method is called by `.FigureManagerBase.pyplot_show`, which is the\\nimplementation of `.pyplot.show`.  To customize the behavior of\\n`.pyplot.show`, interactive backends should usually override\\n`~.FigureManagerBase.start_main_loop`; if more customized logic is\\nnecessary, `~.FigureManagerBase.pyplot_show` can also be overridden.',\n",
       " 'Process every newly added tool.',\n",
       " \"Enable or disable IPython GUI event loop integration.\\n\\n%gui [GUINAME]\\n\\nThis magic replaces IPython's threaded shells that were activated\\nusing the (pylab/wthread/etc.) command line flags.  GUI toolkits\\ncan now be enabled at runtime and keyboard\\ninterrupts should work without any problems.  The following toolkits\\nare supported:  wxPython, PyQt4, PyGTK, Tk and Cocoa (OSX)::\\n\\n    %gui wx      # enable wxPython event loop integration\\n    %gui qt      # enable PyQt/PySide event loop integration\\n                 # with the latest version available.\\n    %gui qt6     # enable PyQt6/PySide6 event loop integration\\n    %gui qt5     # enable PyQt5/PySide2 event loop integration\\n    %gui gtk     # enable PyGTK event loop integration\\n    %gui gtk3    # enable Gtk3 event loop integration\\n    %gui gtk4    # enable Gtk4 event loop integration\\n    %gui tk      # enable Tk event loop integration\\n    %gui osx     # enable Cocoa event loop integration\\n                 # (requires %matplotlib 1.1)\\n    %gui         # disable all event loop integration\\n\\nWARNING:  after any of these has been called you can simply create\\nan application object, but DO NOT start the event loop yourself, as\\nwe have already handled that.\",\n",
       " 'Run the conda package manager within the current kernel.\\n\\nUsage:\\n  %micromamba install [pkgs]',\n",
       " 'Sagemath use custom prompt and we broke them in 8.19.',\n",
       " 'Create a table of color schemes.\\n\\nThe table can be created empty and manually filled or it can be\\ncreated with a list of valid color schemes AND the specification for\\nthe default active scheme.',\n",
       " 'Some functions use type vars that are not defined by the class, but rather\\nonly defined in the function. See for example `iter`. In those cases we\\nwant to:\\n\\n1. Search for undefined type vars.\\n2. Infer type vars with the execution state we have.\\n3. Return the union of all type vars that have been found.',\n",
       " 'Provided only as a backward-compatible wrapper around `.SSHConfig`.\\n\\n.. deprecated:: 2.7\\n    Use `SSHConfig.from_file` instead.',\n",
       " 'Binds the app context to the current context.',\n",
       " 'Return the corners of the rectangle, moving anti-clockwise from\\n(x0, y0).',\n",
       " 'Check that the animated artists changed in callbacks are updated.',\n",
       " \"Parameters\\n----------\\nnbins : int or 'auto', optional\\n    Number of ticks. Only used if minor is False.\\nminor : bool, default: False\\n    Indicate if this locator is for minor ticks or not.\",\n",
       " 'Set list of module names to try to load in forkserver process.',\n",
       " 'Convert the given BSON value into our own type.',\n",
       " 'Raise a BulkWriteError from the full bulk api result.',\n",
       " 'Discover python modules and packages in sub-directory.\\n\\nReturns iterator of paths to discovered modules and packages.',\n",
       " 'Node betweenness_centrality helper:\\n\\nSee betweenness_centrality for what you probably want.\\nThis actually computes \"load\" and not betweenness.\\nSee https://networkx.lanl.gov/ticket/103\\n\\nThis calculates the load of each node for paths from a single source.\\n(The fraction of number of shortests paths from source that go\\nthrough each node.)\\n\\nTo get the load for a node you need to do all-pairs shortest paths.\\n\\nIf weight is not None then use Dijkstra for finding shortest paths.',\n",
       " 'Tests for providing an alternate distance metric to the generator.',\n",
       " 'Write a graph `G` in GML format to the file or file handle `path`.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n    The graph to be converted to GML.\\n\\npath : filename or filehandle\\n    The filename or filehandle to write. Files whose names end with .gz or\\n    .bz2 will be compressed.\\n\\nstringizer : callable, optional\\n    A `stringizer` which converts non-int/non-float/non-dict values into\\n    strings. If it cannot convert a value into a string, it should raise a\\n    `ValueError` to indicate that. Default value: None.\\n\\nRaises\\n------\\nNetworkXError\\n    If `stringizer` cannot convert a value into a string, or the value to\\n    convert is not a string while `stringizer` is None.\\n\\nSee Also\\n--------\\nread_gml, generate_gml\\nliteral_stringizer\\n\\nNotes\\n-----\\nGraph attributes named \\'directed\\', \\'multigraph\\', \\'node\\' or\\n\\'edge\\', node attributes named \\'id\\' or \\'label\\', edge attributes\\nnamed \\'source\\' or \\'target\\' (or \\'key\\' if `G` is a multigraph)\\nare ignored because these attribute names are used to encode the graph\\nstructure.\\n\\nGML files are stored using a 7-bit ASCII encoding with any extended\\nASCII characters (iso8859-1) appearing as HTML character entities.\\nWithout specifying a `stringizer`/`destringizer`, the code is capable of\\nwriting `int`/`float`/`str`/`dict`/`list` data as required by the GML\\nspecification.  For writing other data types, and for reading data other\\nthan `str` you need to explicitly supply a `stringizer`/`destringizer`.\\n\\nNote that while we allow non-standard GML to be read from a file, we make\\nsure to write GML format. In particular, underscores are not allowed in\\nattribute names.\\nFor additional documentation on the GML file format, please see the\\n`GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.\\n\\nSee the module docstring :mod:`networkx.readwrite.gml` for more details.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(4)\\n>>> nx.write_gml(G, \"test.gml\")\\n\\nFilenames ending in .gz or .bz2 will be compressed.\\n\\n>>> nx.write_gml(G, \"test.gml.gz\")',\n",
       " \"Get information about the arguments accepted by a code object.\\n\\nThree things are returned: (args, varargs, varkw), where 'args' is\\na list of argument names (possibly containing nested lists), and\\n'varargs' and 'varkw' are the names of the * and ** arguments or None.\",\n",
       " 'validate multi targets that defined between parentheses()',\n",
       " 'Ensures that filename is opened with correct encoding parameter.\\n\\nThis function uses charset_normalizer package, when available, for\\ndetermining the encoding of the file to be opened. When charset_normalizer\\nis not available, the function detects only UTF encodings, otherwise, ASCII\\nencoding is used as fallback.',\n",
       " 'Coefficients should be modifiable ',\n",
       " \"Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.\\n\\nThis function returns the values:\\n\\n.. math:: p(a,b,c) = \\\\sum_{i,j,k} c_{i,j,k} * T_i(a) * T_j(b) * T_k(c)\\n\\nwhere the points ``(a, b, c)`` consist of all triples formed by taking\\n`a` from `x`, `b` from `y`, and `c` from `z`. The resulting points form\\na grid with `x` in the first dimension, `y` in the second, and `z` in\\nthe third.\\n\\nThe parameters `x`, `y`, and `z` are converted to arrays only if they\\nare tuples or a lists, otherwise they are treated as a scalars. In\\neither case, either `x`, `y`, and `z` or their elements must support\\nmultiplication and addition both with themselves and with the elements\\nof `c`.\\n\\nIf `c` has fewer than three dimensions, ones are implicitly appended to\\nits shape to make it 3-D. The shape of the result will be c.shape[3:] +\\nx.shape + y.shape + z.shape.\\n\\nParameters\\n----------\\nx, y, z : array_like, compatible objects\\n    The three dimensional series is evaluated at the points in the\\n    Cartesian product of `x`, `y`, and `z`.  If `x`, `y`, or `z` is a\\n    list or tuple, it is first converted to an ndarray, otherwise it is\\n    left unchanged and, if it isn't an ndarray, it is treated as a\\n    scalar.\\nc : array_like\\n    Array of coefficients ordered so that the coefficients for terms of\\n    degree i,j are contained in ``c[i,j]``. If `c` has dimension\\n    greater than two the remaining indices enumerate multiple sets of\\n    coefficients.\\n\\nReturns\\n-------\\nvalues : ndarray, compatible object\\n    The values of the two dimensional polynomial at points in the Cartesian\\n    product of `x` and `y`.\\n\\nSee Also\\n--------\\nchebval, chebval2d, chebgrid2d, chebval3d\\n\\nNotes\\n-----\",\n",
       " 'Forces the mask to soft',\n",
       " 'Initialize the RuntimeContext\\n\\nReturns:\\n    An instance of RuntimeContext.',\n",
       " 'Returns a `Tracer` for use by the given instrumentation library.\\n\\nThis function is a convenience wrapper for\\nopentelemetry.trace.TracerProvider.get_tracer.\\n\\nIf tracer_provider is omitted the current configured one is used.',\n",
       " 'Test `inject()` method for Format.BINARY.',\n",
       " 'Preloads asn1crypto and optionally oscrypto from a local source checkout,\\nor from a normal install\\n\\n:param require_oscrypto:\\n    A bool if oscrypto needs to be preloaded\\n\\n:param print_info:\\n    A bool if info about asn1crypto and oscrypto should be printed',\n",
       " 'Pairwise frames test_pairwise',\n",
       " 'Sends a Unix signal to the subprocess.\\n\\nUse constants from the :mod:`signal` module to specify which signal.',\n",
       " 'Return a comparison of actual and expected hash values.\\n\\nExample::\\n\\n       Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde\\n                    or 123451234512345123451234512345123451234512345\\n            Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef',\n",
       " 'Return True if `name` is a considered as an archive file.',\n",
       " 'Replace the password in a given url with ****.',\n",
       " 'Return all the distribution names known to this locator.',\n",
       " 'Recursive helper for :func:`_rec_strip`.',\n",
       " 'Test that Link.from_json() produces Links with consistent cache\\nlocations',\n",
       " 'Test deprecated default_value=None behavior for Container subclass traits',\n",
       " 'See 2.3.3 in RFC6979',\n",
       " \"Parse the input into a new type, deferring resolution of the type until the current class\\nis fully defined.\\n\\nThis is useful when you need to reference the class in it's own type annotations.\",\n",
       " \"Don't die on unary +.\",\n",
       " \"Called to perform the setup phase for a test item.\\n\\nThe default implementation runs ``setup()`` on ``item`` and all of its\\nparents (which haven't been setup yet). This includes obtaining the\\nvalues of fixtures required by the item (which haven't been obtained\\nyet).\\n\\n:param item:\\n    The item.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest file can implement this hook. For a given item, only conftest\\nfiles in the item's directory and its parent directories are consulted.\",\n",
       " 'Custom .cfg files with [tool:pytest] section are read correctly',\n",
       " '`pythonpath` kicks early enough to load plugins via -p (#11118).',\n",
       " 'Additional properties to set if ``sourceFormat`` is set to AVRO.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration.FIELDS.avro_options',\n",
       " 'Test whether a resource is enabled.  Known resources are set by\\nregrtest.py.',\n",
       " 'Saves the private key in PKCS#1 DER format.\\n\\n:returns: the DER-encoded private key.\\n:rtype: bytes',\n",
       " 'Compute minimum distances between one point and a set of points.\\n\\nThis function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance).\\n\\nThis is mostly equivalent to calling::\\n\\n    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\\n\\nbut uses much less memory, and is faster for large arrays.\\n\\nThis function works with dense 2D arrays only.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\\n    Array containing points.\\n\\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\\n    Arrays containing points.\\n\\naxis : int, default=1\\n    Axis along which the argmin and distances are to be computed.\\n\\nmetric : str or callable, default=\"euclidean\"\\n    Metric to use for distance computation. Any metric from scikit-learn\\n    or scipy.spatial.distance can be used.\\n\\n    If metric is a callable function, it is called on each\\n    pair of instances (rows) and the resulting value recorded. The callable\\n    should take two arrays as input and return one value indicating the\\n    distance between them. This works for Scipy\\'s metrics, but is less\\n    efficient than passing the metric name as a string.\\n\\n    Distance matrices are not supported.\\n\\n    Valid values for metric are:\\n\\n    - from scikit-learn: [\\'cityblock\\', \\'cosine\\', \\'euclidean\\', \\'l1\\', \\'l2\\',\\n      \\'manhattan\\']\\n\\n    - from scipy.spatial.distance: [\\'braycurtis\\', \\'canberra\\', \\'chebyshev\\',\\n      \\'correlation\\', \\'dice\\', \\'hamming\\', \\'jaccard\\', \\'kulsinski\\',\\n      \\'mahalanobis\\', \\'minkowski\\', \\'rogerstanimoto\\', \\'russellrao\\',\\n      \\'seuclidean\\', \\'sokalmichener\\', \\'sokalsneath\\', \\'sqeuclidean\\',\\n      \\'yule\\']\\n\\n    See the documentation for scipy.spatial.distance for details on these\\n    metrics.\\n\\n    .. note::\\n       `\\'kulsinski\\'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n    .. note::\\n       `\\'matching\\'` has been removed in SciPy 1.9 (use `\\'hamming\\'` instead).\\n\\nmetric_kwargs : dict, default=None\\n    Keyword arguments to pass to specified metric function.\\n\\nReturns\\n-------\\nargmin : numpy.ndarray\\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\\n\\nSee Also\\n--------\\npairwise_distances : Distances between every pair of samples of X and Y.\\npairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\\n    returns the distances.\\n\\nExamples\\n--------\\n>>> from sklearn.metrics.pairwise import pairwise_distances_argmin\\n>>> X = [[0, 0, 0], [1, 1, 1]]\\n>>> Y = [[1, 0, 0], [1, 1, 0]]\\n>>> pairwise_distances_argmin(X, Y)\\narray([0, 1])',\n",
       " 'Internal: Align multiline celltext text to bottom',\n",
       " 'delete_collection_namespaced_deployment  # noqa: E501\\n\\ndelete collection of Deployment  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_deployment_with_http_info(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'list_storage_version_migration  # noqa: E501\\n\\nlist or watch objects of kind StorageVersionMigration  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.list_storage_version_migration(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param bool allow_watch_bookmarks: allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server\\'s discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1alpha1StorageVersionMigrationList\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'override if setattr should do something other than call self.set',\n",
       " 'Sets the _continue of this V1ListMeta.\\n\\ncontinue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message.  # noqa: E501\\n\\n:param _continue: The _continue of this V1ListMeta.  # noqa: E501\\n:type: str',\n",
       " 'Returns true if both objects are not equal',\n",
       " \"Gets the max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n\\nMaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | |  P P  |  P P  |   P   | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed.  # noqa: E501\\n\\n:return: The max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n:rtype: int\",\n",
       " 'Append SCORE_FIELD and SCORE.',\n",
       " \"Return a :class:`~sagemaker.amazon.LinearLearnerModel`.\\n\\nIt references the latest s3 model data produced by this Estimator.\\n\\nArgs:\\n    vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on\\n        the model. Default: use subnets and security groups from this Estimator.\\n        * 'Subnets' (list[str]): List of subnet ids.\\n        * 'SecurityGroupIds' (list[str]): List of security group ids.\\n    **kwargs: Additional kwargs passed to the LinearLearnerModel constructor.\",\n",
       " 'Determines if the ``ast.Call`` node points to an ``ast.Name`` node with a matching name.\\n\\nArgs:\\n    node (ast.Call): a node that represents a function call. For more,\\n        see https://docs.python.org/3/library/ast.html#abstract-grammar.\\n    name (str): the function name.\\n\\nReturns:\\n    bool: if ``node.func`` is an ``ast.Name`` node with a matching name.',\n",
       " 'Construct a dictionary based on the attributes.\\n\\nReturns:\\n    dict represents the attributes.',\n",
       " 'Initialize source algorithm object\\n\\nArgs:\\n    algorithm_name (str): The ARN of an algorithm resource that was used to create the model package.\\n    model_data_url (str, optional): The Amazon S3 path where the model artifacts, which result from model training, are stored (default: None).',\n",
       " 'Initialize a TrainingDetails object.\\n\\nArgs:\\n    objective_function (ObjectiveFunction, optional): The objective function that is optimized during training (default: None).\\n    training_observations (str, optional): Any observations about training (default: None).\\n    training_job_details (TrainingJobDetails, optional): Details about any associated training jobs (default: None).',\n",
       " 'Generates indices to split data into training and test set.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    Training data, where `n_samples` is the number of samples\\n    and `n_features` is the number of features.\\n\\ny : array-like of shape (n_samples,)\\n    The target variable for supervised learning problems.\\n\\ngroups : array-like of shape (n_samples,), default=None\\n    Group labels for the samples used while splitting the dataset into\\n    train/test set.\\n\\nYields\\n------\\ntrain : ndarray\\n    The training set indices for that split.\\n\\ntest : ndarray\\n    The testing set indices for that split.',\n",
       " 'Get feature names from X.\\n\\nSupport for other array containers should place its implementation here.\\n\\nParameters\\n----------\\nX : {ndarray, dataframe} of shape (n_samples, n_features)\\n    Array container to extract feature names.\\n\\n    - pandas dataframe : The columns will be considered to be feature\\n      names. If the dataframe contains non-string feature names, `None` is\\n      returned.\\n    - All other array containers will return `None`.\\n\\nReturns\\n-------\\nnames: ndarray or None\\n    Feature names of `X`. Unrecognized array containers will return `None`.',\n",
       " \"Add a new knot.\\n\\n(Approximately) replicate FITPACK's logic:\\n  1. split the `x` array into knot intervals, ``t(j+k) <= x(i) <= t(j+k+1)``\\n  2. find the interval with the maximum sum of residuals\\n  3. insert a new knot into the middle of that interval.\\n\\nNB: a new knot is in fact an `x` value at the middle of the interval.\\nSo *the knots are a subset of `x`*.\\n\\nThis routine is an analog of\\nhttps://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpcurf.f#L190-L215\\n(cf _split function)\\n\\nand https://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpknot.f\",\n",
       " 'Encode bson.objectid.ObjectId.',\n",
       " 'Sets the value of :py:attr:`labelType`.',\n",
       " 'Returns R^2^, the coefficient of determination.',\n",
       " 'Returns a rotation matrix for a rotation of theta (in radians)\\nabout the 3-axis.\\n\\nExplanation\\n===========\\n\\nFor a right-handed coordinate system, this corresponds to a\\nclockwise rotation around the `z`-axis, given by:\\n\\n.. math::\\n\\n    R  = \\\\begin{bmatrix}\\n             \\\\cos(\\\\theta) & \\\\sin(\\\\theta) & 0 \\\\\\\\\\n            -\\\\sin(\\\\theta) & \\\\cos(\\\\theta) & 0 \\\\\\\\\\n                        0 &            0 & 1\\n        \\\\end{bmatrix}\\n\\nExamples\\n========\\n\\n>>> from sympy import pi, rot_axis3\\n\\nA rotation of pi/3 (60 degrees):\\n\\n>>> theta = pi/3\\n>>> rot_axis3(theta)\\nMatrix([\\n[       1/2, sqrt(3)/2, 0],\\n[-sqrt(3)/2,       1/2, 0],\\n[         0,         0, 1]])\\n\\nIf we rotate by pi/2 (90 degrees):\\n\\n>>> rot_axis3(pi/2)\\nMatrix([\\n[ 0, 1, 0],\\n[-1, 0, 0],\\n[ 0, 0, 1]])\\n\\nSee Also\\n========\\n\\nrot_givens: Returns a Givens rotation matrix (generalized rotation for\\n    any number of dimensions)\\nrot_ccw_axis3: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 3-axis (counterclockwise around the z axis)\\nrot_axis1: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 1-axis (clockwise around the x axis)\\nrot_axis2: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 2-axis (clockwise around the y axis)',\n",
       " 'Apply \"render derived\" to this :class:`_sql.TableValuedAlias`.\\n\\nThis has the effect of the individual column names listed out\\nafter the alias name in the \"AS\" sequence, e.g.:\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.unnest(array([\"one\", \"two\", \"three\"])).\\n                table_valued(\"x\", with_ordinality=\"o\").render_derived()\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.x, anon_1.o\\n    FROM unnest(ARRAY[%(param_1)s, %(param_2)s, %(param_3)s]) WITH ORDINALITY AS anon_1(x, o)\\n\\nThe ``with_types`` keyword will render column types inline within\\nthe alias expression (this syntax currently applies to the\\nPostgreSQL database):\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.json_to_recordset(\\n    ...             \\'[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]\\'\\n    ...         )\\n    ...         .table_valued(column(\"a\", Integer), column(\"b\", String))\\n    ...         .render_derived(with_types=True)\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.a, anon_1.b FROM json_to_recordset(:json_to_recordset_1)\\n    AS anon_1(a INTEGER, b VARCHAR)\\n\\n:param name: optional string name that will be applied to the alias\\n generated.  If left as None, a unique anonymizing name will be used.\\n\\n:param with_types: if True, the derived columns will include the\\n datatype specification with each column. This is a special syntax\\n currently known to be required by PostgreSQL for some SQL functions.',\n",
       " 'Matches any differential equation that nth_algebraic can solve. Uses\\n`sympy.solve` but teaches it how to integrate derivatives.\\n\\nThis involves calling `sympy.solve` and does most of the work of finding a\\nsolution (apart from evaluating the integrals).',\n",
       " \"Create a string item.\\n\\nBy default, this function will create *single line basic* strings, but\\nboolean flags (e.g. ``literal=True`` and/or ``multiline=True``)\\ncan be used for personalization.\\n\\nFor more information, please check the spec: `<https://toml.io/en/v1.0.0#string>`__.\\n\\nCommon escaping rules will be applied for basic strings.\\nThis can be controlled by explicitly setting ``escape=False``.\\nPlease note that, if you disable escaping, you will have to make sure that\\nthe given strings don't contain any forbidden character or sequence.\",\n",
       " 'Return a decorated class with a constructor signature that contain Trait names as kwargs.',\n",
       " 'Figures out the full host name for the given domain part.  The\\ndomain part is a subdomain in case host matching is disabled or\\na full host name.',\n",
       " 'Check if the mimetype indicates JSON data, either\\n:mimetype:`application/json` or :mimetype:`application/*+json`.',\n",
       " 'Sets ``Content-Disposition`` header.',\n",
       " 'Determines if the labels in a domain are a match for labels from a\\nwildcard valid domain name\\n\\n:param domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in the domain name to check\\n\\n:param valid_domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in a wildcard domain pattern\\n\\n:return:\\n    A boolean - if the domain matches the valid domain',\n",
       " 'Indicates whether the :class:`Arrow <arrow.arrow.Arrow>` object is a repeated wall time in the current\\ntimezone.',\n",
       " ':return:\\n    Integer',\n",
       " 'Upload Part.',\n",
       " 'Parse the .c file written by pgen.  (Internal)\\n\\nThe file looks as follows.  The first two lines are always this:\\n\\n#include \"pgenheaders.h\"\\n#include \"grammar.h\"\\n\\nAfter that come four blocks:\\n\\n1) one or more state definitions\\n2) a table defining dfas\\n3) a table defining labels\\n4) a struct defining the grammar\\n\\nA state definition has the following form:\\n- one or more arc arrays, each of the form:\\n  static arc arcs_<n>_<m>[<k>] = {\\n          {<i>, <j>},\\n          ...\\n  };\\n- followed by a state array, of the form:\\n  static state states_<s>[<t>] = {\\n          {<k>, arcs_<n>_<m>},\\n          ...\\n  };',\n",
       " \"Check if a stream's encoding and errors attributes are\\ncompatible with the desired values.\",\n",
       " 'Alias for :meth:`main`.',\n",
       " 'Returns the sample value, or None if not found.\\n\\nThis is inefficient, and intended only for use in unittests.',\n",
       " 'Run a benchmarking experiment and print a table of results.\\n\\nArguments:\\n\\n    py_versions: The Python versions to test.\\n    cov_versions: The coverage versions to test.\\n    projects: The projects to run.\\n    rows: A list of strings chosen from `\"pyver\"`, `\"cov\"`, and `\"proj\"`.\\n    column: The remaining dimension not used in `rows`.\\n    ratios: A list of triples: (title, slug1, slug2).\\n    num_runs: The number of times to run each matrix element.',\n",
       " 'Import modules randomly to stress coverage.',\n",
       " 'Yields a sequence of (PyObjectPtr key, PyObjectPtr value) pairs,\\nanalogous to dict.iteritems()',\n",
       " 'Main entry point to process a list of .py files and inject type inferred declarations.',\n",
       " '>>> in_lambda_in_list_comprehension1()\\n[[0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6]]',\n",
       " 'Build an EDNS option object from wire format.\\n\\n*otype*, an ``int``, is the option type.\\n\\n*parser*, a ``dns.wire.Parser``, the parser, which should be\\nrestricted to the option length.\\n\\nReturns an instance of a subclass of ``dns.edns.Option``.',\n",
       " 'Add the specified rdata to the rdataset.\\n\\nIf the optional *ttl* parameter is supplied, then\\n``self.update_ttl(ttl)`` will be called prior to adding the rdata.\\n\\n*rd*, a ``dns.rdata.Rdata``, the rdata\\n\\n*ttl*, an ``int``, the TTL.\\n\\nRaises ``dns.rdataset.IncompatibleTypes`` if the type and class\\ndo not match the type and class of the rdataset.\\n\\nRaises ``dns.rdataset.DifferingCovers`` if the type is a signature\\ntype and the covered type does not match that of the rdataset.',\n",
       " 'Remove a container. Similar to the ``docker rm`` command.\\n\\nArgs:\\n    container (str): The container to remove\\n    v (bool): Remove the volumes associated with the container\\n    link (bool): Remove the specified link and not the underlying\\n        container\\n    force (bool): Force the removal of a running container (uses\\n        ``SIGKILL``)\\n\\nRaises:\\n    :py:class:`docker.errors.APIError`\\n        If the server returns an error.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsFirewallPolicyRequest):\\n        The request object. A request message for\\n    FirewallPolicies.TestIamPermissions. See\\n    the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'Pre-rpc interceptor for list_terraform_versions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Config server.',\n",
       " 'Gets a previously created question.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataqna_v1alpha\\n\\n    def sample_get_question():\\n        # Create a client\\n        client = dataqna_v1alpha.QuestionServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = dataqna_v1alpha.GetQuestionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_question(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataqna_v1alpha.types.GetQuestionRequest, dict]):\\n        The request object. A request to get a previously created\\n        question.\\n    name (str):\\n        Required. The unique identifier for the question.\\n        Example: ``projects/foo/locations/bar/questions/1234``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataqna_v1alpha.types.Question:\\n        The question resource represents a\\n        natural language query, its settings,\\n        understanding generated by the system,\\n        and answer retrieval status. A question\\n        cannot be modified.',\n",
       " 'Call the get job metrics method over HTTP.\\n\\nArgs:\\n    request (~.metrics.GetJobMetricsRequest):\\n        The request object. Request to get job metrics.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.metrics.JobMetrics:\\n        JobMetrics contains a collection of\\n    metrics describing the detailed progress\\n    of a Dataflow job. Metrics correspond to\\n    user-defined and system-defined metrics\\n    in the job.\\n\\n    This resource captures only the most\\n    recent values of each metric;\\n    time-series data can be queried for them\\n    (under the same metric names) from Cloud\\n    Monitoring.',\n",
       " 'Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.metastore.CreateServiceRequest):\\n        The request object. Request message for\\n    [DataprocMetastore.CreateService][google.cloud.metastore.v1.DataprocMetastore.CreateService].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Pre-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CloudDeploy server.',\n",
       " 'Post-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CloudDeploy server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the get validation result method over gRPC.\\n\\nGets agent validation result. Agent validation is\\nperformed during training time and is updated\\nautomatically when training is completed.\\n\\nReturns:\\n    Callable[[~.GetValidationResultRequest],\\n            ~.ValidationResult]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates a\\n[SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1alpha\\n\\n    def sample_create_sample_query():\\n        # Create a client\\n        client = discoveryengine_v1alpha.SampleQueryServiceClient()\\n\\n        # Initialize request argument(s)\\n        sample_query = discoveryengine_v1alpha.SampleQuery()\\n        sample_query.query_entry.query = \"query_value\"\\n\\n        request = discoveryengine_v1alpha.CreateSampleQueryRequest(\\n            parent=\"parent_value\",\\n            sample_query=sample_query,\\n            sample_query_id=\"sample_query_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_sample_query(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1alpha.types.CreateSampleQueryRequest, dict]):\\n        The request object. Request message for\\n        [SampleQueryService.CreateSampleQuery][google.cloud.discoveryengine.v1alpha.SampleQueryService.CreateSampleQuery]\\n        method.\\n    parent (str):\\n        Required. The parent resource name, such as\\n        ``projects/{project}/locations/{location}/sampleQuerySets/{sampleQuerySet}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query (google.cloud.discoveryengine_v1alpha.types.SampleQuery):\\n        Required. The\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n        to create.\\n\\n        This corresponds to the ``sample_query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query_id (str):\\n        Required. The ID to use for the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        which will become the final component of the\\n        [SampleQuery.name][google.cloud.discoveryengine.v1alpha.SampleQuery.name].\\n\\n        If the caller does not have permission to create the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        regardless of whether or not it exists, a\\n        ``PERMISSION_DENIED`` error is returned.\\n\\n        This field must be unique among all\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]s\\n        with the same\\n        [parent][google.cloud.discoveryengine.v1alpha.CreateSampleQueryRequest.parent].\\n        Otherwise, an ``ALREADY_EXISTS`` error is returned.\\n\\n        This field must conform to\\n        `RFC-1034 <https://tools.ietf.org/html/rfc1034>`__\\n        standard with a length limit of 63 characters.\\n        Otherwise, an ``INVALID_ARGUMENT`` error is returned.\\n\\n        This corresponds to the ``sample_query_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.discoveryengine_v1alpha.types.SampleQuery:\\n        Sample Query captures metadata to be\\n        used for evaluation.',\n",
       " 'Pre-rpc interceptor for create_discovery_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DlpService server.',\n",
       " 'Updates metadata associated with a dataset. Note that this\\nmethod requires the\\n``documentai.googleapis.com/datasets.update`` permission on the\\nproject, which is highly privileged. A user or service account\\nwith this permission can create new processors that can interact\\nwith any gcs bucket in your project.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_update_dataset():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        dataset = documentai_v1beta3.Dataset()\\n        dataset.state = \"INITIALIZED\"\\n\\n        request = documentai_v1beta3.UpdateDatasetRequest(\\n            dataset=dataset,\\n        )\\n\\n        # Make the request\\n        operation = client.update_dataset(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.UpdateDatasetRequest, dict]):\\n        The request object.\\n    dataset (google.cloud.documentai_v1beta3.types.Dataset):\\n        Required. The ``name`` field of the ``Dataset`` is used\\n        to identify the resource to be updated.\\n\\n        This corresponds to the ``dataset`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        The update mask applies to the\\n        resource.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.documentai_v1beta3.types.Dataset` A singleton resource under a\\n           [Processor][google.cloud.documentai.v1beta3.Processor]\\n           which configures a collection of documents.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ManagedIdentitiesServiceAsyncClient: The constructed client.',\n",
       " 'Lists the clusters in a given project and location.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import managedkafka_v1\\n\\n    def sample_list_clusters():\\n        # Create a client\\n        client = managedkafka_v1.ManagedKafkaClient()\\n\\n        # Initialize request argument(s)\\n        request = managedkafka_v1.ListClustersRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_clusters(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.managedkafka_v1.types.ListClustersRequest, dict]):\\n        The request object. Request for ListClusters.\\n    parent (str):\\n        Required. The parent location whose clusters are to be\\n        listed. Structured like\\n        ``projects/{project}/locations/{location}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.managedkafka_v1.services.managed_kafka.pagers.ListClustersPager:\\n        Response for ListClusters.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Creates a new GrpcRoute in a given project and\\nlocation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import network_services_v1\\n\\n    def sample_create_grpc_route():\\n        # Create a client\\n        client = network_services_v1.NetworkServicesClient()\\n\\n        # Initialize request argument(s)\\n        grpc_route = network_services_v1.GrpcRoute()\\n        grpc_route.name = \"name_value\"\\n        grpc_route.hostnames = [\\'hostnames_value1\\', \\'hostnames_value2\\']\\n\\n        request = network_services_v1.CreateGrpcRouteRequest(\\n            parent=\"parent_value\",\\n            grpc_route_id=\"grpc_route_id_value\",\\n            grpc_route=grpc_route,\\n        )\\n\\n        # Make the request\\n        operation = client.create_grpc_route(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.network_services_v1.types.CreateGrpcRouteRequest, dict]):\\n        The request object. Request used by the CreateGrpcRoute\\n        method.\\n    parent (str):\\n        Required. The parent resource of the GrpcRoute. Must be\\n        in the format ``projects/*/locations/global``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route (google.cloud.network_services_v1.types.GrpcRoute):\\n        Required. GrpcRoute resource to be\\n        created.\\n\\n        This corresponds to the ``grpc_route`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route_id (str):\\n        Required. Short name of the GrpcRoute\\n        resource to be created.\\n\\n        This corresponds to the ``grpc_route_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.network_services_v1.types.GrpcRoute` GrpcRoute is the resource defining how gRPC traffic routed by a Mesh\\n           or Gateway resource is routed.',\n",
       " 'Quantize the bitmap to make it 8-bit (paletted). Returns a new\\nFIBitmap object.\\nOnly for 24 bit images.',\n",
       " 'Call the list models method over HTTP.\\n\\nArgs:\\n    request (~.model_service.ListModelsRequest):\\n        The request object. Request for listing models associated\\n    with a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.model_service.ListModelsResponse:\\n        Response to a ListModelRequest.',\n",
       " 'Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Performs bidirectional streaming speech synthesis:\\nreceive audio while sending text.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import texttospeech_v1\\n\\n    async def sample_streaming_synthesize():\\n        # Create a client\\n        client = texttospeech_v1.TextToSpeechAsyncClient()\\n\\n        # Initialize request argument(s)\\n        streaming_config = texttospeech_v1.StreamingSynthesizeConfig()\\n        streaming_config.voice.language_code = \"language_code_value\"\\n\\n        request = texttospeech_v1.StreamingSynthesizeRequest(\\n            streaming_config=streaming_config,\\n        )\\n\\n        # This method expects an iterator which contains\\n        # \\'texttospeech_v1.StreamingSynthesizeRequest\\' objects\\n        # Here we create a generator that yields a single `request` for\\n        # demonstrative purposes.\\n        requests = [request]\\n\\n        def request_generator():\\n            for request in requests:\\n                yield request\\n\\n        # Make the request\\n        stream = await client.streaming_synthesize(requests=request_generator())\\n\\n        # Handle the response\\n        async for response in stream:\\n            print(response)\\n\\nArgs:\\n    requests (AsyncIterator[`google.cloud.texttospeech_v1.types.StreamingSynthesizeRequest`]):\\n        The request object AsyncIterator. Request message for the ``StreamingSynthesize`` method.\\n        Multiple ``StreamingSynthesizeRequest`` messages are\\n        sent in one call. The first message must contain a\\n        ``streaming_config`` that fully specifies the request\\n        configuration and must not contain ``input``. All\\n        subsequent messages must only have ``input`` set.\\n    retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    AsyncIterable[google.cloud.texttospeech_v1.types.StreamingSynthesizeResponse]:\\n        StreamingSynthesizeResponse is the only message returned to the\\n           client by StreamingSynthesize method. A series of\\n           zero or more StreamingSynthesizeResponse messages are\\n           streamed back to the client.',\n",
       " 'Pre-rpc interceptor for get_connector\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VpcAccessService server.',\n",
       " 'Returns `True` for 4xx status codes, `False` otherwise.',\n",
       " 'Print the docstring for an object.\\n\\nIf the given object is a class, it will print both the class and the\\nconstructor docstrings.',\n",
       " 'Idle, the editor bundled with python\\n\\nParameters\\n----------\\nexe : str, None\\n    If none, should be pretty smart about finding the executable.',\n",
       " 'Tests for definition_start_position and definition_end_position',\n",
       " \"Configure the null keyring as the default.\\n\\n>>> fs = getfixture('fs')\\n>>> disable()\\n>>> disable()\\nTraceback (most recent call last):\\n...\\nRuntimeError: Refusing to overwrite...\",\n",
       " 'Parameters\\n----------\\nxy : (float, float)\\n  The lower left corner of the box.\\n\\nwidth : float\\n    The width of the box.\\n\\nheight : float\\n    The height of the box.\\n\\nboxstyle : str or `~matplotlib.patches.BoxStyle`\\n    The style of the fancy box. This can either be a `.BoxStyle`\\n    instance or a string of the style name and optionally comma\\n    separated attributes (e.g. \"Round, pad=0.2\"). This string is\\n    passed to `.BoxStyle` to construct a `.BoxStyle` object. See\\n    there for a full documentation.\\n\\n    The following box styles are available:\\n\\n    %(BoxStyle:table)s\\n\\nmutation_scale : float, default: 1\\n    Scaling factor applied to the attributes of the box style\\n    (e.g. pad or rounding_size).\\n\\nmutation_aspect : float, default: 1\\n    The height of the rectangle will be squeezed by this value before\\n    the mutation and the mutated box will be stretched by the inverse\\n    of it. For example, this allows different horizontal and vertical\\n    padding.\\n\\nOther Parameters\\n----------------\\n**kwargs : `~matplotlib.patches.Patch` properties\\n\\n%(Patch:kwdoc)s',\n",
       " 'Test constrained_layout for nested gridspecs',\n",
       " 'Parameters\\n----------\\nh : list of :mod:`~mpl_toolkits.axes_grid1.axes_size`\\n    sizes for horizontal division',\n",
       " 'Parameters\\n----------\\nresult : np.ndarray\\nfill_value : object, default iNaT\\nconvert : str, dtype or None\\n\\nReturns\\n-------\\nresult : ndarray with values replace by the fill_value\\n\\nmask the result if needed, convert to the provided dtype if its not\\nNone\\n\\nThis is an internal routine.',\n",
       " 'Reset the option store to its initial state\\n\\nReturns\\n-------\\nNone',\n",
       " 'Create a new, parsed `SSHConfig` from the file found at ``path``.\\n\\n.. versionadded:: 2.7',\n",
       " 'Blocking expect',\n",
       " \"Recompute this distribution's dependencies.\",\n",
       " 'Test using global distutils options.\\n(In particular those that disable the actual install action)',\n",
       " 'Create a new hash object.\\n\\n:parameter data:\\n    Optional. The very first chunk of the message to hash.\\n    It is equivalent to an early call to :meth:`SHA224Hash.update`.\\n:type data: byte string/byte array/memoryview\\n\\n:Return: A :class:`SHA224Hash` hash object',\n",
       " ':calls: `GET /users/{user}/events/orgs/{org} <http://docs.github.com/en/rest/reference/activity#events>`_',\n",
       " '(Experimental) Rate limit for GraphQL API, use with caution.',\n",
       " 'Registers a new Algorithm for use when creating and verifying tokens.',\n",
       " 'Transforms a yes/no or stringified bool into a bool.',\n",
       " 'Set an attribute.',\n",
       " 'This is a Sphinx docstring.\\n\\n:raises NameError: Never',\n",
       " 'Depends on `undefined1` in function return annotation. ',\n",
       " 'The value of `OpenSSL.SSL.OP_NO_QUERY_MTU` is 0x1000, the value\\nof `SSL_OP_NO_QUERY_MTU` defined by `openssl/ssl.h`.',\n",
       " \"The 'uses no fixture' error tells the user at collection time\\nthat the parametrize data they've set up doesn't correspond to the\\nfixtures in their test function, rather than silently ignoring this\\nand letting the test potentially pass.\\n\\n#714\",\n",
       " '#3498',\n",
       " 'Call the update topic method over HTTP.\\n\\nArgs:\\n    request (~.pubsub.UpdateTopicRequest):\\n        The request object. Request for the UpdateTopic method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.pubsub.Topic:\\n        A topic resource.',\n",
       " 'Mode of the Wishart distribution\\n\\nOnly valid if the degrees of freedom are greater than the dimension of\\nthe scale matrix.\\n\\nParameters\\n----------\\n%(_doc_default_callparams)s\\n\\nReturns\\n-------\\nmode : float or None\\n    The Mode of the distribution',\n",
       " 'Check for non-CSR input to private method `_silhouette_reduce`.',\n",
       " \"create_ip_address  # noqa: E501\\n\\ncreate an IPAddress  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.create_ip_address_with_http_info(body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param V1beta1IPAddress body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1beta1IPAddress, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Return new instance of configuration.\\n\\nThis method returns newly created, based on default constructor,\\nobject of Configuration class or returns a copy of default\\nconfiguration passed by the set_default method.\\n\\n:return: The configuration object.',\n",
       " 'Gets the rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n\\n\\n:return: The rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n:rtype: V1RollingUpdateDaemonSet',\n",
       " \"Sets the propagation_policy of this V1DeleteOptions.\\n\\nWhether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.  # noqa: E501\\n\\n:param propagation_policy: The propagation_policy of this V1DeleteOptions.  # noqa: E501\\n:type: str\",\n",
       " 'Returns true if both objects are equal',\n",
       " \"Returns the raw data of the ``event``'s latency spikes time series.\\n\\nFor more information see https://redis.io/commands/latency-history\",\n",
       " \"AWS4Auth instances can be created by supplying key scope parameters\\ndirectly or by using an AWS4SigningKey instance:\\n\\n>>> auth = AWS4Auth(access_id, secret_key, region, service\\n...                 [, date][, raise_invalid_date=False][, session_token=None])\\n\\n  or\\n\\n>>> auth = AWS4Auth(access_id, signing_key[, raise_invalid_date=False])\\n\\n  or using auto-refreshed STS temporary creds via botocore RefreshableCredentials\\n  (useful for long-running processes):\\n\\n>>> auth = AWS4Auth(refreshable_credentials=botocore.session.Session().get_credentials(),\\n...                 region='eu-west-1', service='es')\\n\\naccess_id   -- This is your AWS access ID\\nsecret_key  -- This is your AWS secret access key\\nregion      -- The region you're connecting to, as per the list at\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\\n               e.g. us-east-1. For services which don't require a region\\n               (e.g. IAM), use us-east-1.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\nservice     -- The name of the service you're connecting to, as per\\n               endpoints at:\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html\\n               e.g. elasticbeanstalk.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\ndate        -- Date this instance is valid for. 8-digit date as str of the\\n               form YYYYMMDD. Key is only valid for requests with a\\n               Date or X-Amz-Date header matching this date. If date is\\n               not supplied the current date is used.\\nsigning_key -- An AWS4SigningKey instance.\\nraise_invalid_date\\n            -- Must be supplied as keyword argument. AWS4Auth tries to\\n               parse a date from the X-Amz-Date and Date headers of the\\n               request, first trying X-Amz-Date, and then Date if\\n               X-Amz-Date is not present or is in an unrecognised\\n               format. If one or both of the two headers are present\\n               yet neither are in a format which AWS4Auth recognises\\n               then it will remove both headers and replace with a new\\n               X-Amz-Date header using the current date.\\n\\n               If this behaviour is not wanted, set the\\n               raise_invalid_date keyword argument to True, and\\n               instead an InvalidDateError will be raised when neither\\n               date is recognised. If neither header is present at all\\n               then an X-Amz-Date header will still be added containing\\n               the current date.\\n\\n               See the AWS4Auth class docstring for supported date\\n               formats.\\nsession_token\\n            -- Must be supplied as keyword argument. If session_token\\n               is set, then it is used for the x-amz-security-token\\n               header, for use with STS temporary credentials.\\nrefreshable_credentials\\n            -- A botocore.credentials.RefreshableCredentials instance.\\n               Must be supplied as keyword argument. This instance is\\n               used to generate valid per-request static credentials,\\n               without needing to re-generate the AWS4Auth instance.                       \\n               If refreshable_credentials is set, the following arguments\\n               are ignored: access_id, secret_key, signing_key,\\n               session_token.\",\n",
       " 'Rename namespace ``session`` to ``inputs``.',\n",
       " 'Instantiates HubNotebookDocument object.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of hub content document.',\n",
       " 'Return a scalar result corresponding to the given\\ncolumn expression.',\n",
       " 'Initializes a ConstraintViolations object from a file path.\\n\\nArgs:\\n    constraint_violations_file_path (str): The path to the constraint violations file.\\n    kms_key (str): The kms_key to use when encrypting the file in S3.\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions (default: None). If not\\n        specified, one is created using the default AWS configuration\\n        chain.\\n\\nReturns:\\n    sagemaker.model_monitor.ConstraintViolations: The instance of ConstraintViolations\\n        generated from the local file path.',\n",
       " \"Save this hyperparameter to be applied to the graph_manager object when\\nit's ready.\",\n",
       " 'Iterate over the points in the grid.\\n\\nReturns\\n-------\\nparams : iterator over dict of str to any\\n    Yields dictionaries mapping each estimator parameter to one of its\\n    allowed values.',\n",
       " \"Find root of a function within an interval using bisection.\\n\\nBasic bisection routine to find a root of the function `f` between the\\narguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\\nSlow but sure.\\n\\nParameters\\n----------\\nf : function\\n    Python function returning a number.  `f` must be continuous, and\\n    f(a) and f(b) must have opposite signs.\\na : scalar\\n    One end of the bracketing interval [a,b].\\nb : scalar\\n    The other end of the bracketing interval [a,b].\\nxtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter must be positive.\\nrtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter cannot be smaller than its default value of\\n    ``4*np.finfo(float).eps``.\\nmaxiter : int, optional\\n    If convergence is not achieved in `maxiter` iterations, an error is\\n    raised. Must be >= 0.\\nargs : tuple, optional\\n    Containing extra arguments for the function `f`.\\n    `f` is called by ``apply(f, (x)+args)``.\\nfull_output : bool, optional\\n    If `full_output` is False, the root is returned. If `full_output` is\\n    True, the return value is ``(x, r)``, where x is the root, and r is\\n    a `RootResults` object.\\ndisp : bool, optional\\n    If True, raise RuntimeError if the algorithm didn't converge.\\n    Otherwise, the convergence status is recorded in a `RootResults`\\n    return object.\\n\\nReturns\\n-------\\nroot : float\\n    Root of `f` between `a` and `b`.\\nr : `RootResults` (present if ``full_output = True``)\\n    Object containing information about the convergence. In particular,\\n    ``r.converged`` is True if the routine converged.\\n\\nExamples\\n--------\\n\\n>>> def f(x):\\n...     return (x**2 - 1)\\n\\n>>> from scipy import optimize\\n\\n>>> root = optimize.bisect(f, 0, 2)\\n>>> root\\n1.0\\n\\n>>> root = optimize.bisect(f, -2, 0)\\n>>> root\\n-1.0\\n\\nSee Also\\n--------\\nbrentq, brenth, bisect, newton\\nfixed_point : scalar fixed-point finder\\nfsolve : n-dimensional root-finding\",\n",
       " 'Checks whether a matrix contains only independent rows of another',\n",
       " \"Compare to output from 'qvoronoi o Fv < data' to Voronoi()\",\n",
       " 'Return True if a Geometry is prepared.\\n\\nNote that it is not necessary to check if a geometry is already prepared\\nbefore preparing it. It is more efficient to call ``prepare`` directly\\nbecause it will skip geometries that are already prepared.\\n\\nThis function will return False for missing geometries (None).\\n\\nParameters\\n----------\\ngeometry : Geometry or array_like\\n    Geometry or geometries to check.\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\nis_valid_input : check if an object is a geometry or None\\nprepare : prepare a geometry\\n\\nExamples\\n--------\\n>>> from shapely import Point, prepare\\n>>> geometry = Point(0, 0)\\n>>> is_prepared(Point(0, 0))\\nFalse\\n>>> prepare(geometry)\\n>>> is_prepared(geometry)\\nTrue\\n>>> is_prepared(None)\\nFalse',\n",
       " 'Is prerelease.',\n",
       " 'Trim values at input threshold(s).\\n\\nAssigns values outside boundary-to-boundary values.\\n\\nParameters\\n----------\\nlower : float or int, default None\\n    Minimum threshold value. All values below this threshold will be set to it.\\nupper : float or int, default None\\n    Maximum threshold value. All values above this threshold will be set to it.\\n\\nReturns\\n-------\\nDataFrame\\n    DataFrame with the values outside the clip boundaries replaced.\\n\\nExamples\\n--------\\n>>> ps.DataFrame({\\'A\\': [0, 2, 4]}).clip(1, 3)\\n   A\\n0  1\\n1  2\\n2  3\\n\\nNotes\\n-----\\nOne difference between this implementation and pandas is that running\\npd.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1) will crash with \"TypeError: \\'<=\\' not supported\\nbetween instances of \\'str\\' and \\'int\\'\" while ps.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1)\\nwill output the original DataFrame, simply ignoring the incompatible types.',\n",
       " 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\nfrom the uniform distribution U(0.0, 1.0).\\n\\n.. versionadded:: 1.1.0\\n\\nParameters\\n----------\\nsc : :py:class:`pyspark.SparkContext`\\n    SparkContext used to create the RDD.\\nnumRows : int\\n    Number of Vectors in the RDD.\\nnumCols : int\\n    Number of elements in each Vector.\\nnumPartitions : int, optional\\n    Number of partitions in the RDD.\\nseed : int, optional\\n    Seed for the RNG that generates the seed for the generator in each partition.\\n\\nReturns\\n-------\\n:py:class:`pyspark.RDD`\\n    RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n>>> mat.shape\\n(10, 10)\\n>>> bool(mat.max() <= 1.0 and mat.min() >= 0.0)\\nTrue\\n>>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n4',\n",
       " \"Return ``eq`` with non-commutative objects replaced with Dummy\\nsymbols. A dictionary that can be used to restore the original\\nvalues is returned: if it is None, the expression is noncommutative\\nand cannot be made commutative. The third value returned is a list\\nof any non-commutative symbols that appear in the returned equation.\\n\\nExplanation\\n===========\\n\\nAll non-commutative objects other than Symbols are replaced with\\na non-commutative Symbol. Identical objects will be identified\\nby identical symbols.\\n\\nIf there is only 1 non-commutative object in an expression it will\\nbe replaced with a commutative symbol. Otherwise, the non-commutative\\nentities are retained and the calling routine should handle\\nreplacements in this case since some care must be taken to keep\\ntrack of the ordering of symbols when they occur within Muls.\\n\\nParameters\\n==========\\n\\nname : str\\n    ``name``, if given, is the name that will be used with numbered Dummy\\n    variables that will replace the non-commutative objects and is mainly\\n    used for doctesting purposes.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.secondquant import Commutator, NO, F, Fd\\n>>> from sympy import symbols\\n>>> from sympy.core.exprtools import _mask_nc\\n>>> from sympy.abc import x, y\\n>>> A, B, C = symbols('A,B,C', commutative=False)\\n\\nOne nc-symbol:\\n\\n>>> _mask_nc(A**2 - x**2, 'd')\\n(_d0**2 - x**2, {_d0: A}, [])\\n\\nMultiple nc-symbols:\\n\\n>>> _mask_nc(A**2 - B**2, 'd')\\n(A**2 - B**2, {}, [A, B])\\n\\nAn nc-object with nc-symbols but no others outside of it:\\n\\n>>> _mask_nc(1 + x*Commutator(A, B), 'd')\\n(_d0*x + 1, {_d0: Commutator(A, B)}, [])\\n>>> _mask_nc(NO(Fd(x)*F(y)), 'd')\\n(_d0, {_d0: NO(CreateFermion(x)*AnnihilateFermion(y))}, [])\\n\\nMultiple nc-objects:\\n\\n>>> eq = x*Commutator(A, B) + x*Commutator(A, C)*Commutator(A, B)\\n>>> _mask_nc(eq, 'd')\\n(x*_d0 + x*_d1*_d0, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1])\\n\\nMultiple nc-objects and nc-symbols:\\n\\n>>> eq = A*Commutator(A, B) + B*Commutator(A, C)\\n>>> _mask_nc(eq, 'd')\\n(A*_d0 + B*_d1, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1, A, B])\",\n",
       " 'Returns the wavefunction psi_{n} for the One-dimensional harmonic oscillator.\\n\\nParameters\\n==========\\n\\nn :\\n    the \"nodal\" quantum number.  Corresponds to the number of nodes in the\\n    wavefunction.  ``n >= 0``\\nx :\\n    x coordinate.\\nm :\\n    Mass of the particle.\\nomega :\\n    Angular frequency of the oscillator.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.qho_1d import psi_n\\n>>> from sympy.abc import m, x, omega\\n>>> psi_n(0, x, m, omega)\\n(m*omega)**(1/4)*exp(-m*omega*x**2/(2*hbar))/(hbar**(1/4)*pi**(1/4))',\n",
       " 'Factor out gcd of the elements of a matrix.\\n\\nRequires ``gcd`` in the ground domain.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DM\\n>>> from sympy import ZZ\\n>>> M = DM([[2, 4], [4, 12]], ZZ)\\n>>> content, M_primitive = M.primitive()\\n>>> content\\n2\\n>>> M_primitive\\nDomainMatrix([[1, 2], [2, 6]], (2, 2), ZZ)\\n>>> content * M_primitive == M\\nTrue\\n>>> M_primitive.content() == ZZ(1)\\nTrue\\n\\nSee Also\\n========\\n\\ncontent\\ncancel_denom',\n",
       " 'Returns\\n=======\\n\\nsize: int\\n    The size of set T. Set T is the set of all possible\\n    monomials of the n variables for degree equal to the\\n    degree_m',\n",
       " 'Each entry fundamental matrix can be interpreted as\\nthe expected number of times the chains is in state j\\nif it started in state i.\\n\\nReferences\\n==========\\n\\n.. [1] https://lips.cs.princeton.edu/the-fundamental-matrix-of-a-finite-markov-chain/',\n",
       " 'Build up the response data into a bytearray.',\n",
       " 'This injects a type check into an assignment expression (a := foo()).',\n",
       " 'Test that bare HTTPSConnection can connect, make requests',\n",
       " 'Create a handshake response to reject the connection.\\n\\nA short plain text response is the best fallback when failing to\\nestablish a WebSocket connection.\\n\\nYou must send the handshake response with :meth:`send_response`.\\n\\nYou may modify the response before sending it, for example by changing\\nHTTP headers.\\n\\nArgs:\\n    status: HTTP status code.\\n    text: HTTP response body; it will be encoded to UTF-8.\\n\\nReturns:\\n    HTTP response to send to the client.',\n",
       " 'Parse upgrade command syntax :target to retrieve the target revision\\nand given the :current_revisions stamp of the database.\\n\\nReturns a tuple of Revision objects which should be iterated/upgraded\\nto. The target may be specified in absolute form, or relative to\\n:current_revisions.',\n",
       " ':return:\\n    A unicode string',\n",
       " \"Upload a file to an S3 object.\\n\\nVariants have also been injected into S3 client, Bucket and Object.\\nYou don't have to use S3Transfer.upload_file() directly.\\n\\n.. seealso::\\n    :py:meth:`S3.Client.upload_file`\\n    :py:meth:`S3.Client.upload_fileobj`\",\n",
       " 'Get the value of a command option.',\n",
       " 'Get a stringified version of the param for use in error messages to\\nindicate which param caused the error.',\n",
       " 'The bulk of the command line interface to coverage.py.\\n\\n`argv` is the argument list to process.\\n\\nReturns 0 if all is well, 1 if something went wrong.',\n",
       " 'Construct and initialize a new FileDisposition object.',\n",
       " 'Find strings in `v`, and replace backslashes with slashes throughout.',\n",
       " 'Update a class object.',\n",
       " 'Notify breakpoints of a single step exception event.\\n\\n@type  event: L{ExceptionEvent}\\n@param event: Single step exception event.\\n\\n@rtype:  bool\\n@return: C{True} to call the user-defined handle, C{False} otherwise.',\n",
       " 'Dump the x86/x64 processor register values.\\nThe output mimics that of the WinDBG debugger.\\n\\n@type  registers: dict( str S{->} int )\\n@param registers: Dictionary mapping register names to their values.\\n\\n@type  arch: str\\n@param arch: Architecture of the machine whose registers were dumped.\\n    Defaults to the current architecture.\\n    Currently only the following architectures are supported:\\n     - L{win32.ARCH_I386}\\n     - L{win32.ARCH_AMD64}\\n\\n@rtype:  str\\n@return: Text suitable for logging.',\n",
       " 'Convert a space-separated list of EDNS flag text values into a EDNS\\nflags value.\\n\\nReturns an ``int``',\n",
       " 'see ``_offset_v2``',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Pre-rpc interceptor for create_rollup_property\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'Return a callable for the list playbooks method over gRPC.\\n\\nReturns a list of playbooks in the specified agent.\\n\\nReturns:\\n    Callable[[~.ListPlaybooksRequest],\\n            ~.ListPlaybooksResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.TestIamPermissionsRequest):\\n        The request object for TestIamPermissions method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    iam_policy_pb2.TestIamPermissionsResponse: Response from TestIamPermissions method.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Deletes an existing catalog specified by the catalog\\nID.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_biglake_v1\\n\\n    def sample_delete_catalog():\\n        # Create a client\\n        client = bigquery_biglake_v1.MetastoreServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_biglake_v1.DeleteCatalogRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete_catalog(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_biglake_v1.types.DeleteCatalogRequest, dict]):\\n        The request object. Request message for the DeleteCatalog\\n        method.\\n    name (str):\\n        Required. The name of the catalog to delete. Format:\\n        projects/{project_id_or_number}/locations/{location_id}/catalogs/{catalog_id}\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_biglake_v1.types.Catalog:\\n        Catalog is the container of\\n        databases.',\n",
       " 'Return a callable for the get attestor method over gRPC.\\n\\nGets an\\n[attestor][google.cloud.binaryauthorization.v1.Attestor].\\nReturns NOT_FOUND if the\\n[attestor][google.cloud.binaryauthorization.v1.Attestor] does\\nnot exist.\\n\\nReturns:\\n    Callable[[~.GetAttestorRequest],\\n            ~.Attestor]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Unassigns a license from a user.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import commerce_consumer_procurement_v1\\n\\n    def sample_unassign():\\n        # Create a client\\n        client = commerce_consumer_procurement_v1.LicenseManagementServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = commerce_consumer_procurement_v1.UnassignRequest(\\n            parent=\"parent_value\",\\n            usernames=[\\'usernames_value1\\', \\'usernames_value2\\'],\\n        )\\n\\n        # Make the request\\n        response = client.unassign(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.commerce_consumer_procurement_v1.types.UnassignRequest, dict]):\\n        The request object. Request message for\\n        [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].\\n    parent (str):\\n        Required. License pool name.\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    usernames (MutableSequence[str]):\\n        Required. Username. Format: ``name@domain.com``.\\n        This corresponds to the ``usernames`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.commerce_consumer_procurement_v1.types.UnassignResponse:\\n        Response message for\\n           [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].',\n",
       " 'Call the list network endpoints method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListNetworkEndpointsGlobalNetworkEndpointGroupsRequest):\\n        The request object. A request message for\\n    GlobalNetworkEndpointGroups.ListNetworkEndpoints.\\n    See the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.NetworkEndpointGroupsListNetworkEndpoints:',\n",
       " 'Retrieves the list of interconnect attachments\\ncontained within the specified region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InterconnectAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInterconnectAttachmentsRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInterconnectAttachmentsRequest, dict]):\\n        The request object. A request message for\\n        InterconnectAttachments.List. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.interconnect_attachments.pagers.ListPager:\\n        Response to the list request, and\\n        contains a list of interconnect\\n        attachments.  Iterating over this object\\n        will yield results and resolve\\n        additional pages automatically.',\n",
       " 'Deletes the specified NetworkAttachment in the given\\nscope\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.NetworkAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteNetworkAttachmentRequest(\\n            network_attachment=\"network_attachment_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteNetworkAttachmentRequest, dict]):\\n        The request object. A request message for\\n        NetworkAttachments.Delete. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region of this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    network_attachment (str):\\n        Name of the NetworkAttachment\\n        resource to delete.\\n\\n        This corresponds to the ``network_attachment`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.compute_v1.types.ListRegionUrlMapsRequest):\\n        The initial request object.\\n    response (google.cloud.compute_v1.types.UrlMapList):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Creates an analysis. The long running operation is\\ndone when the analysis has completed.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contact_center_insights_v1\\n\\n    def sample_create_analysis():\\n        # Create a client\\n        client = contact_center_insights_v1.ContactCenterInsightsClient()\\n\\n        # Initialize request argument(s)\\n        request = contact_center_insights_v1.CreateAnalysisRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.create_analysis(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contact_center_insights_v1.types.CreateAnalysisRequest, dict]):\\n        The request object. The request to create an analysis.\\n    parent (str):\\n        Required. The parent resource of the\\n        analysis.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    analysis (google.cloud.contact_center_insights_v1.types.Analysis):\\n        Required. The analysis to create.\\n        This corresponds to the ``analysis`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.contact_center_insights_v1.types.Analysis`\\n        The analysis resource.',\n",
       " 'Pre-rpc interceptor for get_phrase_matcher\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ContactCenterInsights server.',\n",
       " 'Call the execute question method over HTTP.\\n\\nArgs:\\n    request (~.question_service.ExecuteQuestionRequest):\\n        The request object. Request to execute an interpretation.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.question.Question:\\n        The question resource represents a\\n    natural language query, its settings,\\n    understanding generated by the system,\\n    and answer retrieval status. A question\\n    cannot be modified.',\n",
       " 'Instantiates the job controller client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,JobControllerTransport,Callable[..., JobControllerTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the JobControllerTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'Returns the list of all experiments in the specified\\n[Environment][google.cloud.dialogflow.cx.v3.Environment].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflowcx_v3\\n\\n    def sample_list_experiments():\\n        # Create a client\\n        client = dialogflowcx_v3.ExperimentsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflowcx_v3.ListExperimentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_experiments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflowcx_v3.types.ListExperimentsRequest, dict]):\\n        The request object. The request message for\\n        [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n    parent (str):\\n        Required. The\\n        [Environment][google.cloud.dialogflow.cx.v3.Environment]\\n        to list all environments for. Format:\\n        ``projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/environments/<Environment ID>``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflowcx_v3.services.experiments.pagers.ListExperimentsPager:\\n        The response message for\\n           [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the delete agent method over gRPC.\\n\\nDeletes the specified agent.\\n\\nReturns:\\n    Callable[[~.DeleteAgentRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the start experiment method over gRPC.\\n\\nStarts the specified\\n[Experiment][google.cloud.dialogflow.cx.v3beta1.Experiment].\\nThis rpc only changes the state of experiment from PENDING to\\nRUNNING.\\n\\nReturns:\\n    Callable[[~.StartExperimentRequest],\\n            ~.Experiment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns True if the graph G is a triad, else False.\\n\\nParameters\\n----------\\nG : graph\\n   A NetworkX Graph\\n\\nReturns\\n-------\\nistriad : boolean\\n   Whether G is a valid triad\\n\\nExamples\\n--------\\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\\n>>> nx.is_triad(G)\\nTrue\\n>>> G.add_edge(0, 1)\\n>>> nx.is_triad(G)\\nFalse',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Deletes the processor, unloads all deployed model\\nartifacts if it was enabled and then deletes all\\nartifacts associated with this processor.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_delete_processor():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentProcessorServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = documentai_v1beta3.DeleteProcessorRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_processor(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.DeleteProcessorRequest, dict]):\\n        The request object. Request message for the\\n        [DeleteProcessor][google.cloud.documentai.v1beta3.DocumentProcessorService.DeleteProcessor]\\n        method.\\n    name (str):\\n        Required. The processor resource name\\n        to be deleted.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " \"Publish events to a subscriber's channel.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import eventarc_publishing_v1\\n\\n    def sample_publish_events():\\n        # Create a client\\n        client = eventarc_publishing_v1.PublisherClient()\\n\\n        # Initialize request argument(s)\\n        request = eventarc_publishing_v1.PublishEventsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.publish_events(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.eventarc_publishing_v1.types.PublishEventsRequest, dict]):\\n        The request object. The request message for the\\n        PublishEvents method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.eventarc_publishing_v1.types.PublishEventsResponse:\\n        The response message for the\\n        PublishEvents method.\",\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the create comment method over gRPC.\\n\\nCreates a new comment on an order.\\n\\nReturns:\\n    Callable[[~.CreateCommentRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the delete hardware group method over HTTP.\\n\\nArgs:\\n    request (~.service.DeleteHardwareGroupRequest):\\n        The request object. A request to delete a hardware group.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.network_management_v1.types.ListConnectivityTestsRequest):\\n        The initial request object.\\n    response (google.cloud.network_management_v1.types.ListConnectivityTestsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Return a callable for the get inventory method over gRPC.\\n\\nGet inventory data for the specified VM instance. If the VM has\\nno associated inventory, the message ``NOT_FOUND`` is returned.\\n\\nReturns:\\n    Callable[[~.GetInventoryRequest],\\n            ~.Inventory]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Read image data from file and return as numpy array.\\n\\nRaise ValueError if format is unsupported.\\n\\nParameters\\n----------\\nout : numpy.ndarray, str, or file-like object; optional\\n    Buffer where image data will be saved.\\n    If None (default), a new array will be created.\\n    If numpy.ndarray, a writable array of compatible dtype and shape.\\n    If 'memmap', directly memory-map the image data in the TIFF file\\n    if possible; else create a memory-mapped array in a temporary file.\\n    If str or open file, the file name or file object used to\\n    create a memory-map to an array stored in a binary file on disk.\\nsqueeze : bool\\n    If True, all length-1 dimensions (except X and Y) are\\n    squeezed out from the array.\\n    If False, the shape of the returned array might be different from\\n    the page.shape.\\nlock : {RLock, NullContext}\\n    A reentrant lock used to synchronize reads from file.\\n    If None (default), the lock of the parent's filehandle is used.\\nreopen : bool\\n    If True (default) and the parent file handle is closed, the file\\n    is temporarily re-opened and closed if no exception occurs.\\nmaxsize: int or None\\n    Maximum size of data before a ValueError is raised.\\n    Can be used to catch DOS. Default: 16 TB.\\nvalidate : bool\\n    If True (default), validate various parameters.\\n    If None, only validate parameters and return None.\",\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    RecaptchaEnterpriseServiceAsyncClient: The constructed client.',\n",
       " 'Return a callable for the mark recommendation failed method over gRPC.\\n\\nMarks the Recommendation State as Failed. Users can use this\\nmethod to indicate to the Recommender API that they have applied\\nthe recommendation themselves, and the operation failed. This\\nstops the recommendation content from being updated. Associated\\ninsights are frozen and placed in the ACCEPTED state.\\n\\nMarkRecommendationFailed can be applied to recommendations in\\nACTIVE, CLAIMED, SUCCEEDED, or FAILED state.\\n\\nRequires the recommender.*.update IAM permission for the\\nspecified recommender.\\n\\nReturns:\\n    Callable[[~.MarkRecommendationFailedRequest],\\n            ~.Recommendation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the remove fulfillment places method over gRPC.\\n\\nWe recommend that you use the\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nmethod instead of the\\n[ProductService.RemoveFulfillmentPlaces][google.cloud.retail.v2alpha.ProductService.RemoveFulfillmentPlaces]\\nmethod.\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nachieves the same results but provides more fine-grained control\\nover ingesting local inventory data.\\n\\nIncrementally removes place IDs from a\\n[Product.fulfillment_info.place_ids][google.cloud.retail.v2alpha.FulfillmentInfo.place_ids].\\n\\nThis process is asynchronous and does not require the\\n[Product][google.cloud.retail.v2alpha.Product] to exist before\\nupdating fulfillment information. If the request is valid, the\\nupdate will be enqueued and processed downstream. As a\\nconsequence, when a response is returned, the removed place IDs\\nare not immediately manifested in the\\n[Product][google.cloud.retail.v2alpha.Product] queried by\\n[ProductService.GetProduct][google.cloud.retail.v2alpha.ProductService.GetProduct]\\nor\\n[ProductService.ListProducts][google.cloud.retail.v2alpha.ProductService.ListProducts].\\n\\nThe returned [Operation][google.longrunning.Operation]s will be\\nobsolete after 1 day, and\\n[GetOperation][google.longrunning.Operations.GetOperation] API\\nwill return NOT_FOUND afterwards.\\n\\nIf conflicting updates are issued, the\\n[Operation][google.longrunning.Operation]s associated with the\\nstale updates will not be marked as\\n[done][google.longrunning.Operation.done] until being obsolete.\\n\\nReturns:\\n    Callable[[~.RemoveFulfillmentPlacesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get big query export method over gRPC.\\n\\nGets a BigQuery export.\\n\\nReturns:\\n    Callable[[~.GetBigQueryExportRequest],\\n            ~.BigQueryExport]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list operations method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.ListOperationsRequest):\\n        The request object for ListOperations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    operations_pb2.ListOperationsResponse: Response from ListOperations method.',\n",
       " 'Pre-rpc interceptor for analyze_asset\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Warehouse server.',\n",
       " 'Post-rpc interceptor for create_clone_job\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the VmMigration server but before\\nit is returned to user code.',\n",
       " 'Call the list locations method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.ListLocationsRequest):\\n        The request object for ListLocations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.ListLocationsResponse: Response from ListLocations method.',\n",
       " \"Process the response from an HTTP request that canceled the upload.\\n\\nThis is everything that must be done after a request that doesn't\\nrequire network I/O (or other I/O). This is based on the `sans-I/O`_\\nphilosophy.\\n\\nArgs:\\n    response (object): The HTTP response object.\\n\\nRaises:\\n    ~google.resumable_media.common.InvalidResponse: If the status\\n        code is not 204.\\n\\n.. _sans-I/O: https://sans-io.readthedocs.io/\",\n",
       " 'maybe_int : NUMBER\\n| empty',\n",
       " 'Like os.path.walk, but follows symlinks on POSIX systems.\\n\\nIf the symlinks create a loop, this function will never finish.',\n",
       " 'Run a test only if the client is not connected to a mongos.',\n",
       " 'create a unix-style long description of the file (like ls -l)',\n",
       " \"Set how the Axes adjusts to achieve the required aspect ratio.\\n\\nParameters\\n----------\\nadjustable : {'box', 'datalim'}\\n    If 'box', change the physical dimensions of the Axes.\\n    If 'datalim', change the ``x`` or ``y`` data limits. This\\n    may ignore explicitly defined axis limits.\\n\\nshare : bool, default: False\\n    If ``True``, apply the settings to all shared Axes.\\n\\nSee Also\\n--------\\nmatplotlib.axes.Axes.set_aspect\\n    For a description of aspect handling.\\n\\nNotes\\n-----\\nShared Axes (of which twinned Axes are a special case)\\nimpose restrictions on how aspect ratios can be imposed.\\nFor twinned Axes, use 'datalim'.  For Axes that share both\\nx and y, use 'box'.  Otherwise, either 'datalim' or 'box'\\nmay be used.  These limitations are partly a requirement\\nto avoid over-specification, and partly a result of the\\nparticular implementation we are currently using, in\\nwhich the adjustments for aspect ratios are done sequentially\\nand independently on each Axes as it is drawn.\",\n",
       " 'Return the size of the image as tuple (numrows, numcols).',\n",
       " 'Function used for unpickling proxy objects.',\n",
       " 'Helper function to implement map, starmap and their async counterparts.',\n",
       " 'Create a Timestamp from posix timestamp in nanoseconds.\\n\\n:param int unix_ns: Posix timestamp in nanoseconds.\\n:rtype: Timestamp',\n",
       " 'Compute the Katz centrality for the graph G.\\n\\nKatz centrality computes the centrality for a node based on the centrality\\nof its neighbors. It is a generalization of the eigenvector centrality. The\\nKatz centrality for node $i$ is\\n\\n.. math::\\n\\n    x_i = \\\\alpha \\\\sum_{j} A_{ij} x_j + \\\\beta,\\n\\nwhere $A$ is the adjacency matrix of graph G with eigenvalues $\\\\lambda$.\\n\\nThe parameter $\\\\beta$ controls the initial centrality and\\n\\n.. math::\\n\\n    \\\\alpha < \\\\frac{1}{\\\\lambda_{\\\\max}}.\\n\\nKatz centrality computes the relative influence of a node within a\\nnetwork by measuring the number of the immediate neighbors (first\\ndegree nodes) and also all other nodes in the network that connect\\nto the node under consideration through these immediate neighbors.\\n\\nExtra weight can be provided to immediate neighbors through the\\nparameter $\\\\beta$.  Connections made with distant neighbors\\nare, however, penalized by an attenuation factor $\\\\alpha$ which\\nshould be strictly less than the inverse largest eigenvalue of the\\nadjacency matrix in order for the Katz centrality to be computed\\ncorrectly. More information is provided in [1]_.\\n\\nParameters\\n----------\\nG : graph\\n  A NetworkX graph\\n\\nalpha : float\\n  Attenuation factor\\n\\nbeta : scalar or dictionary, optional (default=1.0)\\n  Weight attributed to the immediate neighborhood. If not a scalar the\\n  dictionary must have an value for every node.\\n\\nnormalized : bool\\n  If True normalize the resulting values.\\n\\nweight : None or string, optional\\n  If None, all edge weights are considered equal.\\n  Otherwise holds the name of the edge attribute used as weight.\\n  In this measure the weight is interpreted as the connection strength.\\n\\nReturns\\n-------\\nnodes : dictionary\\n   Dictionary of nodes with Katz centrality as the value.\\n\\nRaises\\n------\\nNetworkXError\\n   If the parameter `beta` is not a scalar but lacks a value for at least\\n   one node\\n\\nExamples\\n--------\\n>>> import math\\n>>> G = nx.path_graph(4)\\n>>> phi = (1 + math.sqrt(5)) / 2.0  # largest eigenvalue of adj matrix\\n>>> centrality = nx.katz_centrality_numpy(G, 1 / phi)\\n>>> for n, c in sorted(centrality.items()):\\n...     print(f\"{n} {c:.2f}\")\\n0 0.37\\n1 0.60\\n2 0.60\\n3 0.37\\n\\nSee Also\\n--------\\nkatz_centrality\\neigenvector_centrality_numpy\\neigenvector_centrality\\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\\n\\nNotes\\n-----\\nKatz centrality was introduced by [2]_.\\n\\nThis algorithm uses a direct linear solver to solve the above equation.\\nThe parameter ``alpha`` should be strictly less than the inverse of largest\\neigenvalue of the adjacency matrix for there to be a solution.\\nYou can use ``max(nx.adjacency_spectrum(G))`` to get $\\\\lambda_{\\\\max}$ the largest\\neigenvalue of the adjacency matrix.\\n\\nFor strongly connected graphs, as $\\\\alpha \\\\to 1/\\\\lambda_{\\\\max}$, and $\\\\beta > 0$,\\nKatz centrality approaches the results for eigenvector centrality.\\n\\nFor directed graphs this finds \"left\" eigenvectors which corresponds\\nto the in-edges in the graph. For out-edges Katz centrality,\\nfirst reverse the graph with ``G.reverse()``.\\n\\nReferences\\n----------\\n.. [1] Mark E. J. Newman:\\n   Networks: An Introduction.\\n   Oxford University Press, USA, 2010, p. 173.\\n.. [2] Leo Katz:\\n   A New Status Index Derived from Sociometric Index.\\n   Psychometrika 18(1):39â€“43, 1953\\n   https://link.springer.com/content/pdf/10.1007/BF02289026.pdf',\n",
       " 'Remove node attributes from all nodes in the graph.\\n\\nParameters\\n----------\\nG : NetworkX Graph\\n\\n*attr_names : List of Strings\\n    The attribute names to remove from the graph.\\n\\nnbunch : List of Nodes\\n    Remove the node attributes only from the nodes in this list.\\n\\nExamples\\n--------\\n>>> G = nx.Graph()\\n>>> G.add_nodes_from([1, 2, 3], color=\"blue\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{1: \\'blue\\', 2: \\'blue\\', 3: \\'blue\\'}\\n>>> nx.remove_node_attributes(G, \"color\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{}',\n",
       " 'Check the message is formatted correctly for the decimal value.\\nAlso check the message when input includes inf or nan (gh12200)',\n",
       " 'Check that ._metadata attributes are equivalent.',\n",
       " 'Check for the `min_count` keyword. Returns True if below `min_count` (when\\nmissing value should be returned from the reduction).\\n\\nParameters\\n----------\\nshape : tuple\\n    The shape of the values (`values.shape`).\\nmask : ndarray[bool] or None\\n    Boolean numpy array (typically of same shape as `shape`) or None.\\nmin_count : int\\n    Keyword passed through from sum/prod call.\\n\\nReturns\\n-------\\nbool',\n",
       " 'Issue #96 (for newbytes instead of newobject)',\n",
       " \"Convert ``color_spec`` to an openpyxl v2 Color object.\\n\\nParameters\\n----------\\ncolor_spec : str, dict\\n    A 32-bit ARGB hex string, or a dict with zero or more of the\\n    following keys.\\n        'rgb'\\n        'indexed'\\n        'auto'\\n        'theme'\\n        'tint'\\n        'index'\\n        'type'\\n\\nReturns\\n-------\\ncolor : openpyxl.styles.Color\",\n",
       " 'A simplified json_normalize\\n\\nConverts a nested dict into a flat dict (\"record\"), unlike json_normalize,\\nit does not attempt to extract a subset of the data.\\n\\nParameters\\n----------\\nds : dict or list of dicts\\nprefix: the prefix, optional, default: \"\"\\nsep : str, default \\'.\\'\\n    Nested records will generate names separated by sep,\\n    e.g., for sep=\\'.\\', { \\'foo\\' : { \\'bar\\' : 0 } } -> foo.bar\\nlevel: int, optional, default: 0\\n    The number of levels in the json string.\\n\\nmax_level: int, optional, default: None\\n    The max depth to normalize.\\n\\nReturns\\n-------\\nd - dict or list of dicts, matching `ds`\\n\\nExamples\\n--------\\n>>> nested_to_record(\\n...     dict(flat1=1, dict1=dict(c=1, d=2), nested=dict(e=dict(c=1, d=2), d=2))\\n... )\\n{\\'flat1\\': 1, \\'dict1.c\\': 1, \\'dict1.d\\': 2, \\'nested.e.c\\': 1, \\'nested.e.d\\': 2, \\'nested.d\\': 2}',\n",
       " \"Converts lists of lists/tuples into DataFrames with proper type inference\\nand optional (e.g. string to datetime) conversion. Also enables iterating\\nlazily over chunks of large files\\n\\nParameters\\n----------\\ndata : file-like object or list\\ndelimiter : separator character to use\\ndialect : str or csv.Dialect instance, optional\\n    Ignored if delimiter is longer than 1 character\\nnames : sequence, default\\nheader : int, default 0\\n    Row to use to parse column labels. Defaults to the first row. Prior\\n    rows will be discarded\\nindex_col : int or list, optional\\n    Column or columns to use as the (possibly hierarchical) index\\nhas_index_names: bool, default False\\n    True if the cols defined in index_col have an index name and are\\n    not in the header.\\nna_values : scalar, str, list-like, or dict, optional\\n    Additional strings to recognize as NA/NaN.\\nkeep_default_na : bool, default True\\nthousands : str, optional\\n    Thousands separator\\ncomment : str, optional\\n    Comment out remainder of line\\nparse_dates : bool, default False\\ndate_format : str or dict of column -> format, default ``None``\\n\\n    .. versionadded:: 2.0.0\\nskiprows : list of integers\\n    Row numbers to skip\\nskipfooter : int\\n    Number of line at bottom of file to skip\\nconverters : dict, optional\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\nencoding : str, optional\\n    Encoding to use for UTF when reading/writing (ex. 'utf-8')\\nfloat_precision : str, optional\\n    Specifies which converter the C engine should use for floating-point\\n    values. The options are `None` or `high` for the ordinary converter,\\n    `legacy` for the original lower precision pandas converter, and\\n    `round_trip` for the round-trip converter.\",\n",
       " \"This reads until EOF using readline() and returns a list containing\\nthe lines thus read. The optional 'sizehint' argument is ignored.\\nRemember, because this reads until EOF that means the child\\nprocess should have closed its stdout. If you run this method on\\na child that is still running with its stdout open then this\\nmethod will block until it timesout.\",\n",
       " \"Return the address of the remote side of this Channel, if possible.\\n\\nThis simply wraps `.Transport.getpeername`, used to provide enough of a\\nsocket-like interface to allow asyncore to work. (asyncore likes to\\ncall ``'getpeername'``.)\",\n",
       " 'Returns a generator of `funcdef` nodes.',\n",
       " \"Fixture for 'na_action' argument in sort_values/sort_index/rank.\",\n",
       " 'Test the wheel cache filters on wheel name when several wheels\\nfor different package are stored under the same cache directory.',\n",
       " 'Get the sha256 digest of a string\\n\\nSupports the `usedforsecurity` argument for Python 3.9+ to allow running on\\na FIPS-enabled system.',\n",
       " \"create a modified version of this path. A 'rev' argument\\nindicates a new revision.\\nthe following keyword arguments modify various path parts::\\n\\n  http://host.com/repo/path/file.ext\\n  |-----------------------|          dirname\\n                            |------| basename\\n                            |--|     purebasename\\n                                |--| ext\",\n",
       " 'Assign |ASN.1| type component by position.\\n\\nEquivalent to Python sequence item assignment operation (e.g. `[]`).\\n\\nParameters\\n----------\\nidx : :class:`int`\\n    Component index (zero-based). Must either refer to existing\\n    component (if *componentType* is set) or to N+1 component\\n    otherwise. In the latter case a new component of given ASN.1\\n    type gets instantiated and appended to |ASN.1| sequence.\\n\\nKeyword Args\\n------------\\nvalue: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\\n    A Python value to initialize |ASN.1| component with (if *componentType* is set)\\n    or ASN.1 value object to assign to |ASN.1| component.\\n    If `value` is not given, schema object will be set as a component.\\n\\nverifyConstraints : :class:`bool`\\n     If :obj:`False`, skip constraints validation\\n\\nmatchTags: :class:`bool`\\n     If :obj:`False`, skip component tags matching\\n\\nmatchConstraints: :class:`bool`\\n     If :obj:`False`, skip component constraints matching\\n\\nReturns\\n-------\\nself',\n",
       " 'Avoid extraneous whitespace around an operator.\\n\\nOkay: a = 12 + 3\\nE221: a = 4  + 5\\nE222: a = 4 +  5\\nE223: a = 4\\\\t+ 5\\nE224: a = 4 +\\\\t5',\n",
       " 'The error reported for source files which end prematurely causing a\\nsyntax error reflects the cause for the syntax error.',\n",
       " 'Test line referrals.',\n",
       " 'The check',\n",
       " 'Similar, but with a subscript in a key-value pair rather than the test\\nSee https://github.com/pylint-dev/pylint/issues/6069',\n",
       " 'The type name of the exception.',\n",
       " 'Raise pytest.skip() if all examples in the given DocTest have the SKIP\\noption set.',\n",
       " 'See :meth:`Pytester.parseconfigure`.',\n",
       " 'Regarding issue pytest-xdist#241.\\n\\nThis test came originally from test_remote.py in xdist (ca03269).',\n",
       " 'From the backport of the email package',\n",
       " 'patch_service_cidr_status  # noqa: E501\\n\\npartially update status of the specified ServiceCIDR  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.patch_service_cidr_status(name, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ServiceCIDR (required)\\n:param object body: (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param bool force: Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1beta1ServiceCIDR\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'test router-router MQ devices',\n",
       " 'Tests for the regular expressions used in ISO8601Highlighter.',\n",
       " 'Placeholder docstring',\n",
       " 'Update to enable or disable remote debug for a training job.\\n\\nThis method updates the ``_enable_remote_debug`` parameter\\nand enables or disables remote debug for a training job',\n",
       " 'Create an Amazon SageMaker training job.\\n\\nArgs:\\n    input_mode (str): The input mode that the algorithm supports. Valid modes:\\n        * \\'File\\' - Amazon SageMaker copies the training dataset from the S3 location to\\n        a directory in the Docker container.\\n        * \\'Pipe\\' - Amazon SageMaker streams data directly from S3 to the container via a\\n        Unix-named pipe.\\n        * \\'FastFile\\' - Amazon SageMaker streams data from S3 on demand instead of\\n        downloading the entire dataset before training begins.\\n    input_config (list): A list of Channel objects. Each channel is a named input source.\\n        Please refer to the format details described:\\n        https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\\n        jobs and APIs that create Amazon SageMaker endpoints use this role to access\\n        training data and model artifacts. You must grant sufficient permissions to this\\n        role.\\n    job_name (str): Name of the training job being created.\\n    output_config (dict): The S3 URI where you want to store the training results and\\n        optional KMS key ID.\\n    resource_config (dict): Contains values for ResourceConfig:\\n        * instance_count (int): Number of EC2 instances to use for training.\\n        The key in resource_config is \\'InstanceCount\\'.\\n        * instance_type (str): Type of EC2 instance to use for training, for example,\\n        \\'ml.c4.xlarge\\'. The key in resource_config is \\'InstanceType\\'.\\n    vpc_config (dict): Contains values for VpcConfig:\\n        * subnets (list[str]): List of subnet ids.\\n        The key in vpc_config is \\'Subnets\\'.\\n        * security_group_ids (list[str]): List of security group ids.\\n        The key in vpc_config is \\'SecurityGroupIds\\'.\\n    hyperparameters (dict): Hyperparameters for model training. The hyperparameters are\\n        made accessible as a dict[str, str] to the training code on SageMaker. For\\n        convenience, this accepts other types for keys and values, but ``str()`` will be\\n        called to convert them before training.\\n    stop_condition (dict): Defines when training shall finish. Contains entries that can\\n        be understood by the service like ``MaxRuntimeInSeconds``.\\n    tags (Optional[Tags]): Tags for labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    metric_definitions (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: \\'Name\\' for\\n        the name of the metric, and \\'Regex\\' for the regular expression used to extract the\\n        metric from the logs.\\n    enable_network_isolation (bool): Whether to request for the training job to run with\\n        network isolation or not.\\n    image_uri (str): Docker image containing training code.\\n    training_image_config(dict): Training image configuration.\\n        Optionally, the dict can contain \\'TrainingRepositoryAccessMode\\' and\\n        \\'TrainingRepositoryCredentialsProviderArn\\' (under \\'TrainingRepositoryAuthConfig\\').\\n        For example,\\n\\n        .. code:: python\\n\\n            training_image_config = {\\n                \"TrainingRepositoryAccessMode\": \"Vpc\",\\n                \"TrainingRepositoryAuthConfig\": {\\n                    \"TrainingRepositoryCredentialsProviderArn\":\\n                      \"arn:aws:lambda:us-west-2:1234567890:function:test\"\\n                },\\n            }\\n\\n        If TrainingRepositoryAccessMode is set to Vpc, the training image is accessed\\n        through a private Docker registry in customer Vpc. If it\\'s set to Platform or None,\\n        the training image is accessed through ECR.\\n        If TrainingRepositoryCredentialsProviderArn is provided, the credentials to\\n        authenticate to the private Docker registry will be retrieved from this AWS Lambda\\n        function. (default: ``None``). When it\\'s set to None, SageMaker will not do\\n        authentication before pulling the image in the private Docker registry.\\n    container_entry_point (List[str]): Optional. The entrypoint script for a Docker\\n        container used to run a training job. This script takes precedence over\\n        the default train processing instructions.\\n    container_arguments (List[str]): Optional. The arguments for a container used to run\\n        a training job.\\n    algorithm_arn (str): Algorithm Arn from Marketplace.\\n    encrypt_inter_container_traffic (bool): Specifies whether traffic between training\\n        containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool): whether to use spot instances for training.\\n    checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\\n        that the algorithm persists (if any) during training. (default:\\n        ``None``).\\n    checkpoint_local_path (str): The local path that the algorithm\\n        writes its checkpoints to. SageMaker will persist all files\\n        under this path to `checkpoint_s3_uri` continually during\\n        training. On job startup the reverse happens - data from the\\n        s3 location is downloaded to this path before the algorithm is\\n        started. If the path is unset then SageMaker assumes the\\n        checkpoints will be provided under `/opt/ml/checkpoints/`.\\n        (default: ``None``).\\n    experiment_config (dict[str, str]): Experiment management configuration.\\n        Optionally, the dict can contain four keys:\\n        \\'ExperimentName\\', \\'TrialName\\',  \\'TrialComponentDisplayName\\' and \\'RunName\\'.\\n        The behavior of setting these keys is as follows:\\n        * If `ExperimentName` is supplied but `TrialName` is not a Trial will be\\n        automatically created and the job\\'s Trial Component associated with the Trial.\\n        * If `TrialName` is supplied and the Trial already exists the job\\'s Trial Component\\n        will be associated with the Trial.\\n        * If both `ExperimentName` and `TrialName` are not supplied the trial component\\n        will be unassociated.\\n        * `TrialComponentDisplayName` is used for display in Studio.\\n        * `RunName` is used to record an experiment run.\\n    enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\\n        Series. For more information see:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html\\n        #SageMaker-Type\\n        -AlgorithmSpecification-EnableSageMakerMetricsTimeSeries\\n        (default: ``None``).\\n    profiler_rule_configs (list[dict]): A list of profiler rule\\n        configurations.src/sagemaker/lineage/artifact.py:285\\n    profiler_config (dict): Configuration for how profiling information is emitted\\n        with SageMaker Profiler. (default: ``None``).\\n    remote_debug_config(dict): Configuration for RemoteDebug. (default: ``None``)\\n        The dict can contain \\'EnableRemoteDebug\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            remote_debug_config = {\\n                \"EnableRemoteDebug\": True,\\n            }\\n    session_chaining_config(dict): Configuration for SessionChaining. (default: ``None``)\\n        The dict can contain \\'EnableSessionTagChaining\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            session_chaining_config = {\\n                \"EnableSessionTagChaining\": True,\\n            }\\n    environment (dict[str, str]) : Environment variables to be set for\\n        use during training job (default: ``None``)\\n    retry_strategy(dict): Defines RetryStrategy for InternalServerFailures.\\n        * max_retry_attsmpts (int): Number of times a job should be retried.\\n        The key in RetryStrategy is \\'MaxRetryAttempts\\'.\\n    infra_check_config(dict): Infra check configuration.\\n        Optionally, the dict can contain \\'EnableInfraCheck\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            infra_check_config = {\\n                \"EnableInfraCheck\": True,\\n            }\\nReturns:\\n    str: ARN of the training job, if it is created.\\n\\nRaises:\\n    - botocore.exceptions.ClientError: If Sagemaker throws an exception while creating\\n    training job.\\n    - ValueError: If both image_uri and algorithm are provided, or if neither is provided.',\n",
       " 'Convenience method for accessing the SageMaker session.\\n\\nIt access :class:`~sagemaker.session.Session` object associated with the estimator\\nfor the ``HyperparameterTuner``.',\n",
       " 'Check for correspondence between linkage and condensed distance matrices.\\n\\nThey must have the same number of original observations for\\nthe check to succeed.\\n\\nThis function is useful as a sanity check in algorithms that make\\nextensive use of linkage and distance matrices that must\\ncorrespond to the same set of original observations.\\n\\nParameters\\n----------\\nZ : array_like\\n    The linkage matrix to check for correspondence.\\nY : array_like\\n    The condensed distance matrix to check for correspondence.\\n\\nReturns\\n-------\\nb : bool\\n    A boolean indicating whether the linkage matrix and distance\\n    matrix could possibly correspond to one another.\\n\\nSee Also\\n--------\\nlinkage : for a description of what a linkage matrix is.\\n\\nExamples\\n--------\\n>>> from scipy.cluster.hierarchy import ward, correspond\\n>>> from scipy.spatial.distance import pdist\\n\\nThis method can be used to check if a given linkage matrix ``Z`` has been\\nobtained from the application of a cluster method over a dataset ``X``:\\n\\n>>> X = [[0, 0], [0, 1], [1, 0],\\n...      [0, 4], [0, 3], [1, 4],\\n...      [4, 0], [3, 0], [4, 1],\\n...      [4, 4], [3, 4], [4, 3]]\\n>>> X_condensed = pdist(X)\\n>>> Z = ward(X_condensed)\\n\\nHere, we can compare ``Z`` and ``X`` (in condensed form):\\n\\n>>> correspond(Z, X_condensed)\\nTrue',\n",
       " 'Fit the gradient boosting model.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The input samples.\\n\\ny : array-like of shape (n_samples,)\\n    Target values.\\n\\nsample_weight : array-like of shape (n_samples,) default=None\\n    Weights of training data.\\n\\n    .. versionadded:: 0.23\\n\\nReturns\\n-------\\nself : object\\n    Fitted estimator.',\n",
       " 'Informative warnings should be raised when mixing sklearn and joblib API',\n",
       " \"Options\\n-------\\nnit : int, optional\\n    Number of iterations to make. If omitted (default), make as many\\n    as required to meet tolerances.\\ndisp : bool, optional\\n    Print status to stdout on every iteration.\\nmaxiter : int, optional\\n    Maximum number of iterations to make.\\nftol : float, optional\\n    Relative tolerance for the residual. If omitted, not used.\\nfatol : float, optional\\n    Absolute tolerance (in max-norm) for the residual.\\n    If omitted, default is 6e-6.\\nxtol : float, optional\\n    Relative minimum step size. If omitted, not used.\\nxatol : float, optional\\n    Absolute minimum step size, as determined from the Jacobian\\n    approximation. If the step size is smaller than this, optimization\\n    is terminated as successful. If omitted, not used.\\ntol_norm : function(vector) -> scalar, optional\\n    Norm to use in convergence check. Default is the maximum norm.\\nline_search : {None, 'armijo' (default), 'wolfe'}, optional\\n    Which type of a line search to use to determine the step size in\\n    the direction given by the Jacobian approximation. Defaults to\\n    'armijo'.\\njac_options : dict, optional\\n    Options for the respective Jacobian approximation.\\n\\n    alpha : float, optional\\n        Initial guess for the Jacobian is (-1/alpha).\\n    reduction_method : str or tuple, optional\\n        Method used in ensuring that the rank of the Broyden\\n        matrix stays low. Can either be a string giving the\\n        name of the method, or a tuple of the form ``(method,\\n        param1, param2, ...)`` that gives the name of the\\n        method and values for additional parameters.\\n\\n        Methods available:\\n\\n        - ``restart``: drop all matrix columns. Has no extra parameters.\\n        - ``simple``: drop oldest matrix column. Has no extra parameters.\\n        - ``svd``: keep only the most significant SVD components.\\n          Takes an extra parameter, ``to_retain``, which determines the\\n          number of SVD components to retain when rank reduction is done.\\n          Default is ``max_rank - 2``.\\n\\n    max_rank : int, optional\\n        Maximum rank for the Broyden matrix.\\n        Default is infinity (i.e., no rank reduction).\\n\\nExamples\\n--------\\n>>> def func(x):\\n...     return np.cos(x) + x[::-1] - [1, 2, 3, 4]\\n...\\n>>> from scipy import optimize\\n>>> res = optimize.root(func, [1, 1, 1, 1], method='broyden1', tol=1e-14)\\n>>> x = res.x\\n>>> x\\narray([4.04674914, 3.91158389, 2.71791677, 1.61756251])\\n>>> np.cos(x) + x[::-1]\\narray([1., 2., 3., 4.])\",\n",
       " 'Check stats.rankdata with an array of length 1.',\n",
       " 'List commits in reverse chronological order.\\n\\nOnly the first `num_commits` are shown.',\n",
       " 'Returns UNIX timestamp (integer) representing the time\\nwhen the item was created.\\n\\n.. versionadded:: 1.1',\n",
       " 'Return a buffer object which allows access to our memory region from our offset\\nto the window size. Please note that it might be smaller than you requested when calling use_region()\\n\\n**Note:** You can only obtain a buffer if this instance is_valid() !\\n\\n**Note:** buffers should not be cached passed the duration of your access as it will\\nprevent resources from being freed even though they might not be accounted for anymore !',\n",
       " 'Test focus within.',\n",
       " \"Round each value in a Series to the given number of decimals.\\n\\nParameters\\n----------\\ndecimals : int\\n    Number of decimal places to round to (default: 0).\\n    If decimals are negative, it specifies the number of\\n    positions to the left of the decimal point.\\n\\nReturns\\n-------\\nSeries object\\n\\nSee Also\\n--------\\nDataFrame.round\\n\\nExamples\\n--------\\n>>> df = ps.Series([0.028208, 0.038683, 0.877076], name='x')\\n>>> df\\n0    0.028208\\n1    0.038683\\n2    0.877076\\nName: x, dtype: float64\\n\\n>>> df.round(2)\\n0    0.03\\n1    0.04\\n2    0.88\\nName: x, dtype: float64\",\n",
       " 'For python2 compatibility.',\n",
       " \"Generates data for a given partition and returns an iterator of tuples or rows.\\n\\nThis method is invoked once per partition to read the data. Implementing\\nthis method is required for stream reader. You can initialize any\\nnon-serializable resources required for reading data from the data source\\nwithin this method.\\n\\nNotes\\n-----\\nThis method is static and stateless. You shouldn't access mutable class member\\nor keep in memory state between different invocations of read().\\n\\nParameters\\n----------\\npartition : :class:`InputPartition`\\n    The partition to read. It must be one of the partition values returned by\\n    :meth:`DataSourceStreamReader.partitions`.\\n\\nReturns\\n-------\\niterator of tuples or PyArrow's `RecordBatch`\\n    An iterator of tuples or rows. Each tuple or row will be converted to a row\\n    in the final DataFrame.\\n    It can also return an iterator of PyArrow's `RecordBatch` if the data source\\n    supports it.\",\n",
       " 'test #9635',\n",
       " 'Just parse the simple ones.',\n",
       " \"test that 'literal binds' mode works - no bound params.\",\n",
       " 'Clear the current scope, if any.',\n",
       " 'Target must support simultaneous, independent database cursors\\non a single connection.',\n",
       " \"Encodes a plaintext into popular Morse Code with letters\\nseparated by ``sep`` and words by a double ``sep``.\\n\\nExamples\\n========\\n\\n>>> from sympy.crypto.crypto import encode_morse\\n>>> msg = 'ATTACK RIGHT FLANK'\\n>>> encode_morse(msg)\\n'.-|-|-|.-|-.-.|-.-||.-.|..|--.|....|-||..-.|.-..|.-|-.|-.-'\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Morse_code\",\n",
       " 'Returns True if the axis of the pure quaternions seen as 3D vectors\\n``q1``, ``q2``, and ``q3`` are coplanar.\\n\\nExplanation\\n===========\\n\\nThree pure quaternions are vector coplanar if the quaternions seen as 3D vectors are coplanar.\\n\\nParameters\\n==========\\n\\nq1\\n    A pure Quaternion.\\nq2\\n    A pure Quaternion.\\nq3\\n    A pure Quaternion.\\n\\nReturns\\n=======\\n\\nTrue : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar.\\nFalse : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are not coplanar.\\nNone : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar is unknown.\\n\\nExamples\\n========\\n\\n>>> from sympy.algebras.quaternion import Quaternion\\n>>> q1 = Quaternion(0, 4, 4, 4)\\n>>> q2 = Quaternion(0, 8, 8, 8)\\n>>> q3 = Quaternion(0, 24, 24, 24)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nTrue\\n\\n>>> q1 = Quaternion(0, 8, 16, 8)\\n>>> q2 = Quaternion(0, 8, 3, 12)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nFalse\\n\\nSee Also\\n========\\n\\naxis\\nis_pure',\n",
       " \"Convert SymPy's expression to ``dtype``. \",\n",
       " 'Construct new raw ``RootSum`` instance. ',\n",
       " 'Prepend a minus sign to a pretty form. ',\n",
       " 'Equality as defined by https://tools.ietf.org/html/rfc5280#section-7.1\\n\\n:param other:\\n    Another RDNSequence object\\n\\n:return:\\n    A boolean',\n",
       " 'This extension is used to prevent mapping of the any policy to\\nspecific requirements\\n\\n:return:\\n    None or a Integer object',\n",
       " 'Builds a scope and request body into a WSGI environ object.',\n",
       " 'Dispatch on engine function decorator.',\n",
       " \"Get Pandas DataFrame of tables filtered by a search string.\\n\\nParameters\\n----------\\ntext\\n    Select only tables with the given string in table's properties.\\ncatalog_id\\n    The ID of the Data Catalog from which to retrieve Databases.\\n    If ``None`` is provided, the AWS account ID is used by default.\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** receive ``None``.\\n\\nReturns\\n-------\\n    Iterator of tables.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> df_tables = wr.catalog.search_tables(text='my_property')\",\n",
       " 'Write records stored in a DataFrame into Microsoft SQL Server.\\n\\nParameters\\n----------\\ndf\\n    Pandas DataFrame https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\\ncon\\n    Use pyodbc.connect() to use credentials directly or wr.sqlserver.connect() to fetch it from the Glue Catalog.\\ntable\\n    Table name\\nschema\\n    Schema name\\nmode\\n    Append, overwrite or upsert.\\n\\n    - append: Inserts new records into table.\\n    - overwrite: Drops table and recreates.\\n    - upsert: Perform an upsert which checks for conflicts on columns given by ``upsert_conflict_columns`` and sets the new values on conflicts. Note that column names of the Dataframe will be used for this operation, as if ``use_column_names`` was set to True.\\n\\nindex\\n    True to store the DataFrame index as a column in the table,\\n    otherwise False to ignore it.\\ndtype\\n    Dictionary of columns names and Microsoft SQL Server types to be casted.\\n    Useful when you have columns with undetermined or mixed data types.\\n    (e.g. {\\'col name\\': \\'TEXT\\', \\'col2 name\\': \\'FLOAT\\'})\\nvarchar_lengths\\n    Dict of VARCHAR length by columns. (e.g. {\"col1\": 10, \"col5\": 200}).\\nuse_column_names\\n    If set to True, will use the column names of the DataFrame for generating the INSERT SQL Query.\\n    E.g. If the DataFrame has two columns `col1` and `col3` and `use_column_names` is True, data will only be\\n    inserted into the database columns `col1` and `col3`.\\nuspert_conflict_columns\\n    List of columns to be used as conflict columns in the upsert operation.\\nchunksize\\n    Number of rows which are inserted with each SQL query. Defaults to inserting 200 rows per query.\\nfast_executemany\\n    Mode of execution which greatly reduces round trips for a DBAPI executemany() call when using\\n    Microsoft ODBC drivers, for limited size batches that fit in memory. `False` by default.\\n\\n    https://github.com/mkleehammer/pyodbc/wiki/Cursor#executemanysql-params-with-fast_executemanytrue\\n\\n    Note: when using this mode, pyodbc converts the Python parameter values to their ODBC \"C\" equivalents,\\n    based on the target column types in the database which may lead to subtle data type conversion\\n    differences depending on whether fast_executemany is True or False.\\n\\nExamples\\n--------\\nWriting to Microsoft SQL Server using a Glue Catalog Connections\\n\\n>>> import awswrangler as wr\\n>>> with wr.sqlserver.connect(connection=\"MY_GLUE_CONNECTION\", odbc_driver_version=17) as con:\\n...     wr.sqlserver.to_sql(\\n...         df=df,\\n...         table=\"table\",\\n...         schema=\"dbo\",\\n...         con=con\\n...     )',\n",
       " 'Get a list of available services that can be loaded as resource\\nclients via :py:meth:`Session.resource`.\\n\\n:rtype: list\\n:return: List of service names',\n",
       " \"Convert and validate a value against the option's\\n:attr:`type`, :attr:`multiple`, and :attr:`nargs`.\",\n",
       " 'Reset all resolver configuration to the defaults.',\n",
       " 'The coefficient of kinetic friction.',\n",
       " 'Call the optimize tours method over HTTP.\\n\\nArgs:\\n    request (~.route_optimization_service.OptimizeToursRequest):\\n        The request object. Request to be given to a tour\\n    optimization solver which defines the\\n    shipment model to solve as well as\\n    optimization parameters.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.route_optimization_service.OptimizeToursResponse:\\n        Response after solving a tour\\n    optimization problem containing the\\n    routes followed by each vehicle, the\\n    shipments which have been skipped and\\n    the overall cost of the solution.',\n",
       " 'Tests against python glob to check if our posix tests are accurate.',\n",
       " 'Make a blank PluginOptions, mostly used for tests.',\n",
       " 'Handles an HTTP exception.  By default this will invoke the\\nregistered error handlers and fall back to returning the\\nexception as response.\\n\\n.. versionchanged:: 1.0.3\\n    ``RoutingException``, used internally for actions such as\\n     slash redirects during routing, is not passed to error\\n     handlers.\\n\\n.. versionchanged:: 1.0\\n    Exceptions are looked up by code *and* by MRO, so\\n    ``HTTPException`` subclasses can be handled with a catch-all\\n    handler for the base ``HTTPException``.\\n\\n.. versionadded:: 0.3',\n",
       " 'Check a ref name for validity.\\n\\nThis is based on the rules described in :manpage:`git-check-ref-format(1)`.',\n",
       " 'API to retrieve a list of ``TaxonomyCategory`` objects.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.ads import admanager_v1\\n\\n    def sample_list_taxonomy_categories():\\n        # Create a client\\n        client = admanager_v1.TaxonomyCategoryServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = admanager_v1.ListTaxonomyCategoriesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_taxonomy_categories(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.ads.admanager_v1.types.ListTaxonomyCategoriesRequest, dict]):\\n        The request object. Request object for ``ListTaxonomyCategories`` method.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        TaxonomyCategories. Format: ``networks/{network_code}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.ads.admanager_v1.services.taxonomy_category_service.pagers.ListTaxonomyCategoriesPager:\\n        Response object for ListTaxonomyCategoriesRequest containing matching\\n           TaxonomyCategory objects.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the update google ads link method over gRPC.\\n\\nUpdates a GoogleAdsLink on a property\\n\\nReturns:\\n    Callable[[~.UpdateGoogleAdsLinkRequest],\\n            ~.GoogleAdsLink]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the update channel group method over gRPC.\\n\\nUpdates a ChannelGroup.\\n\\nReturns:\\n    Callable[[~.UpdateChannelGroupRequest],\\n            ~.ChannelGroup]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list display video360\\nadvertiser links method over HTTP.\\n\\n    Args:\\n        request (~.analytics_admin.ListDisplayVideo360AdvertiserLinksRequest):\\n            The request object. Request message for\\n        ListDisplayVideo360AdvertiserLinks RPC.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.analytics_admin.ListDisplayVideo360AdvertiserLinksResponse:\\n            Response message for\\n        ListDisplayVideo360AdvertiserLinks RPC.',\n",
       " 'Pre-rpc interceptor for list_participant_sessions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ConferenceRecordsService server.',\n",
       " 'Return a callable for the list versions method over gRPC.\\n\\nList API versions of an API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.ListVersionsRequest],\\n            ~.ListVersionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the create external api method over gRPC.\\n\\nCreate an External API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.CreateExternalApiRequest],\\n            ~.ExternalApi]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.apphub_service.CreateServiceRequest):\\n        The request object. Request for CreateService.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsStoragePoolRequest):\\n        The request object. A request message for\\n    StoragePools.TestIamPermissions. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'Searches for documents using provided\\n[SearchDocumentsRequest][google.cloud.contentwarehouse.v1.SearchDocumentsRequest].\\nThis call only returns documents that the caller has permission\\nto search against.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contentwarehouse_v1\\n\\n    def sample_search_documents():\\n        # Create a client\\n        client = contentwarehouse_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = contentwarehouse_v1.SearchDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_documents(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contentwarehouse_v1.types.SearchDocumentsRequest, dict]):\\n        The request object. Request message for\\n        DocumentService.SearchDocuments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        documents. Format:\\n        projects/{project_number}/locations/{location}.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.contentwarehouse_v1.services.document_service.pagers.SearchDocumentsPager:\\n        Response message for\\n        DocumentService.SearchDocuments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Pre-rpc interceptor for batch_update_entities\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the EntityTypes server.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Parses a session path into its component segments.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Return a callable for the initialize encryption spec method over gRPC.\\n\\nInitializes a location-level encryption key\\nspecification.  An error will be thrown if the location\\nhas resources already created before the initialization.\\nOnce the encryption specification is initialized at a\\nlocation, it is immutable and all newly created\\nresources under the location will be encrypted with the\\nexisting specification.\\n\\nReturns:\\n    Callable[[~.InitializeEncryptionSpecRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the update intent method over gRPC.\\n\\nUpdates the specified intent.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\nReturns:\\n    Callable[[~.UpdateIntentRequest],\\n            ~.Intent]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the streaming analyze content method over gRPC.\\n\\nAdds a text (e.g., chat) or audio (e.g., phone recording)\\nmessage from a participant into the conversation. Note: This\\nmethod is only available through the gRPC API (not REST).\\n\\nThe top-level message sent to the client by the server is\\n``StreamingAnalyzeContentResponse``. Multiple response messages\\ncan be returned in order. The first one or more messages contain\\nthe ``recognition_result`` field. Each result represents a more\\ncomplete transcript of what the user said. The next message\\ncontains the ``reply_text`` field, and potentially the\\n``reply_audio`` and/or the ``automated_agent_reply`` fields.\\n\\nNote: Always use agent versions for production traffic sent to\\nvirtual agents. See `Versions and\\nenvironments <https://cloud.google.com/dialogflow/es/docs/agents-versions>`__.\\n\\nReturns:\\n    Callable[[~.StreamingAnalyzeContentRequest],\\n            ~.StreamingAnalyzeContentResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    EventarcAsyncClient: The constructed client.',\n",
       " 'jsonpath : fields_or_any',\n",
       " 'Returns a list of services that allow you to opt into audit logs\\nthat are not generated by default.\\n\\nTo learn more about audit logs, see the `Logging\\ndocumentation <https://cloud.google.com/logging/docs/audit>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iam_admin_v1\\n\\n    def sample_query_auditable_services():\\n        # Create a client\\n        client = iam_admin_v1.IAMClient()\\n\\n        # Initialize request argument(s)\\n        request = iam_admin_v1.QueryAuditableServicesRequest(\\n        )\\n\\n        # Make the request\\n        response = client.query_auditable_services(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iam_admin_v1.types.QueryAuditableServicesRequest, dict]):\\n        The request object. A request to get the list of\\n        auditable services for a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iam_admin_v1.types.QueryAuditableServicesResponse:\\n        A response containing a list of\\n        auditable services for a resource.',\n",
       " 'Post-rpc interceptor for delete_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the IDS server but before\\nit is returned to user code.',\n",
       " 'Call the create ekm connection method over HTTP.\\n\\nArgs:\\n    request (~.ekm_service.CreateEkmConnectionRequest):\\n        The request object. Request message for\\n    [EkmService.CreateEkmConnection][google.cloud.kms.v1.EkmService.CreateEkmConnection].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.ekm_service.EkmConnection:\\n        An [EkmConnection][google.cloud.kms.v1.EkmConnection]\\n    represents an individual EKM connection. It can be used\\n    for creating [CryptoKeys][google.cloud.kms.v1.CryptoKey]\\n    and\\n    [CryptoKeyVersions][google.cloud.kms.v1.CryptoKeyVersion]\\n    with a\\n    [ProtectionLevel][google.cloud.kms.v1.ProtectionLevel]\\n    of\\n    [EXTERNAL_VPC][CryptoKeyVersion.ProtectionLevel.EXTERNAL_VPC],\\n    as well as performing cryptographic operations using\\n    keys created within the\\n    [EkmConnection][google.cloud.kms.v1.EkmConnection].',\n",
       " 'Pre-rpc interceptor for get_crypto_key\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the KeyManagementService server.',\n",
       " 'Post-rpc interceptor for revoke_certificate\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'Post-rpc interceptor for delete_collector\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the RapidMigrationAssessment server but before\\nit is returned to user code.',\n",
       " 'Initiates a failover of the primary node to current\\nreplica node for a specific STANDARD tier Cloud\\nMemorystore for Redis instance.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import redis_v1beta1\\n\\n    def sample_failover_instance():\\n        # Create a client\\n        client = redis_v1beta1.CloudRedisClient()\\n\\n        # Initialize request argument(s)\\n        request = redis_v1beta1.FailoverInstanceRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.failover_instance(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.redis_v1beta1.types.FailoverInstanceRequest, dict]):\\n        The request object. Request for\\n        [Failover][google.cloud.redis.v1beta1.CloudRedis.FailoverInstance].\\n    name (str):\\n        Required. Redis instance resource name using the form:\\n        ``projects/{project_id}/locations/{location_id}/instances/{instance_id}``\\n        where ``location_id`` refers to a GCP region.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    data_protection_mode (google.cloud.redis_v1beta1.types.FailoverInstanceRequest.DataProtectionMode):\\n        Optional. Available data protection modes that the user\\n        can choose. If it\\'s unspecified, data protection mode\\n        will be LIMITED_DATA_LOSS by default.\\n\\n        This corresponds to the ``data_protection_mode`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.redis_v1beta1.types.Instance` A\\n        Memorystore for Redis instance.',\n",
       " 'Pre-rpc interceptor for set_default_branch\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CatalogService server.',\n",
       " 'Gets metadata of a repository.\\n\\n**Host: Data Plane**\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securesourcemanager_v1\\n\\n    def sample_get_repository():\\n        # Create a client\\n        client = securesourcemanager_v1.SecureSourceManagerClient()\\n\\n        # Initialize request argument(s)\\n        request = securesourcemanager_v1.GetRepositoryRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_repository(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securesourcemanager_v1.types.GetRepositoryRequest, dict]):\\n        The request object. GetRepositoryRequest is the request\\n        for getting a repository.\\n    name (str):\\n        Required. Name of the repository to retrieve. The format\\n        is\\n        ``projects/{project_number}/locations/{location_id}/repositories/{repository_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securesourcemanager_v1.types.Repository:\\n        Metadata of a Secure Source Manager\\n        repository.',\n",
       " 'Return a callable for the delete service method over gRPC.\\n\\nDeletes a service. This also deletes all endpoints\\nassociated with the service.\\n\\nReturns:\\n    Callable[[~.DeleteServiceRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns a fully-qualified folder string.',\n",
       " 'Return a callable for the list queues method over gRPC.\\n\\nLists queues.\\n\\nQueues are returned in lexicographical order.\\n\\nReturns:\\n    Callable[[~.ListQueuesRequest],\\n            ~.ListQueuesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " \"Call the update migrating vm method over HTTP.\\n\\nArgs:\\n    request (~.vmmigration.UpdateMigratingVmRequest):\\n        The request object. Request message for\\n    'UpdateMigratingVm' request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.\",\n",
       " 'Return a callable for the update vmware engine network method over gRPC.\\n\\nModifies a VMware Engine network resource. Only the following\\nfields can be updated: ``description``. Only fields specified in\\n``updateMask`` are applied.\\n\\nReturns:\\n    Callable[[~.UpdateVmwareEngineNetworkRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Sends a single request, without handling any redirections.',\n",
       " 'Yield Traversable objects in self',\n",
       " 'The __init__ file can be searched in a directory. If found return it, else\\nNone.',\n",
       " 'Returns a checksum for the source.',\n",
       " 'Parse multiple statements into a list until one of the end tokens\\nis reached.  This is used to parse the body of statements as it also\\nparses template data if appropriate.  The parser checks first if the\\ncurrent token is a colon and skips it if there is one.  Then it checks\\nfor the block end and parses until if one of the `end_tokens` is\\nreached.  Per default the active token in the stream at the end of\\nthe call is the matched end token.  If this is not wanted `drop_needle`\\ncan be set to `True` and the end token is removed.',\n",
       " 'Get items to delete to keep the store under size, file, & age limits.',\n",
       " 'Schedule a func to be run',\n",
       " 'Specifically, `const` validation applies for Draft 7.',\n",
       " 'Executes core chain rules.',\n",
       " 'Create an instant of an simple tag processor.\\n\\nArguments:\\n    pattern: A regular expression that matches a pattern.\\n    tag: Tag of element.',\n",
       " 'Return the axis label as a Text instance.\\n\\n.. admonition:: Discouraged\\n\\n   This overrides `.Artist.get_label`, which is for legend labels, with a new\\n   semantic. It is recommended to use the attribute ``Axis.label`` instead.',\n",
       " \"Add *tool* to `ToolManager`.\\n\\nIf successful, adds a new event ``tool_trigger_{name}`` where\\n``{name}`` is the *name* of the tool; the event is fired every time the\\ntool is triggered.\\n\\nParameters\\n----------\\nname : str\\n    Name of the tool, treated as the ID, has to be unique.\\ntool : type\\n    Class of the tool to be added.  A subclass will be used\\n    instead if one was registered for the current canvas class.\\n*args, **kwargs\\n    Passed to the *tool*'s constructor.\\n\\nSee Also\\n--------\\nmatplotlib.backend_tools.ToolBase : The base class for tools.\",\n",
       " 'Convert a premultiplied ARGB32 buffer to an unmultiplied RGBA8888 buffer.',\n",
       " \"Set the rotation of the text.\\n\\nParameters\\n----------\\ns : float or {'vertical', 'horizontal'}\\n    The rotation angle in degrees in mathematically positive direction\\n    (counterclockwise). 'horizontal' equals 0, 'vertical' equals 90.\",\n",
       " 'Button release event handler.',\n",
       " 'Encode bson.decimal128.Decimal128.',\n",
       " 'Return internal buffer contents as bytes object',\n",
       " 'Tests for conditions specific to\\nparse_edge_list method',\n",
       " 'Find shortest weighted paths and lengths between all nodes.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\ncutoff : integer or float, optional\\n    Length (sum of edge weights) at which the search is stopped.\\n    If cutoff is provided, only return paths with summed weight <= cutoff.\\n\\nweight : string or function\\n    If this is a string, then edge weights will be accessed via the\\n    edge attribute with this key (that is, the weight of the edge\\n    joining `u` to `v` will be ``G.edge[u][v][weight]``). If no\\n    such edge attribute exists, the weight of the edge is assumed to\\n    be one.\\n\\n    If this is a function, the weight of an edge is the value\\n    returned by the function. The function must accept exactly three\\n    positional arguments: the two endpoints of an edge and the\\n    dictionary of edge attributes for that edge. The function must\\n    return a number or None to indicate a hidden edge.\\n\\nYields\\n------\\n(node, (distance, path)) : (node obj, (dict, dict))\\n    Each source node has two associated dicts. The first holds distance\\n    keyed by target and the second holds paths keyed by target.\\n    (See single_source_dijkstra for the source/target node terminology.)\\n    If desired you can apply `dict()` to this function to create a dict\\n    keyed by source node to the two dicts.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(5)\\n>>> len_path = dict(nx.all_pairs_dijkstra(G))\\n>>> len_path[3][0][1]\\n2\\n>>> for node in [0, 1, 2, 3, 4]:\\n...     print(f\"3 - {node}: {len_path[3][0][node]}\")\\n3 - 0: 3\\n3 - 1: 2\\n3 - 2: 1\\n3 - 3: 0\\n3 - 4: 1\\n>>> len_path[3][1][1]\\n[3, 2, 1]\\n>>> for n, (dist, path) in nx.all_pairs_dijkstra(G):\\n...     print(path[1])\\n[0, 1]\\n[1]\\n[2, 1]\\n[3, 2, 1]\\n[4, 3, 2, 1]\\n\\nNotes\\n-----\\nEdge weight attributes must be numerical.\\nDistances are calculated as sums of weighted edges traversed.\\n\\nThe yielded dicts only have keys for reachable nodes.',\n",
       " 'Converts a SciPy sparse array in **Coordinate** format to an iterable\\nof weighted edge triples.',\n",
       " 'get_dirs_from_args() skips over non-existing directories and files',\n",
       " 'Check if file is in free format Fortran.',\n",
       " 'Returns the dtype unchanged if it contained no metadata or a copy of the\\ndtype if it (or any of its structure dtypes) contained metadata.\\n\\nThis utility is used by `np.save` and `np.savez` to drop metadata before\\nsaving.\\n\\n.. note::\\n\\n    Due to its limitation this function may move to a more appropriate\\n    home or change in the future and is considered semi-public API only.\\n\\n.. warning::\\n\\n    This function does not preserve more strange things like record dtypes\\n    and user dtypes may simply return the wrong thing.  If you need to be\\n    sure about the latter, check the result with:\\n    ``np.can_cast(new_dtype, dtype, casting=\"no\")``.',\n",
       " 'Decrypts AES/RC4/RC2/3DES/DES ciphertext via CryptoAPI\\n\\n:param cipher:\\n    A unicode string of \"aes\", \"des\", \"tripledes_2key\", \"tripledes_3key\",\\n    \"rc2\", \"rc4\"\\n\\n:param key:\\n    The encryption key - a byte string 5-16 bytes long\\n\\n:param data:\\n    The ciphertext - a byte string\\n\\n:param iv:\\n    The initialization vector - a byte string - unused for RC4\\n\\n:param padding:\\n    Boolean, if padding should be used - unused for RC4\\n\\n:raises:\\n    ValueError - when any of the parameters contain an invalid value\\n    TypeError - when any of the parameters are of the wrong type\\n    OSError - when an error is returned by the OS crypto library\\n\\n:return:\\n    A byte string of the plaintext',\n",
       " 'Numpy version of itertools.product.\\nSometimes faster (for large inputs)...\\n\\nParameters\\n----------\\nX : list-like of list-likes\\n\\nReturns\\n-------\\nproduct : list of ndarrays\\n\\nExamples\\n--------\\n>>> cartesian_product([list(\"ABC\"), [1, 2]])\\n[array([\\'A\\', \\'A\\', \\'B\\', \\'B\\', \\'C\\', \\'C\\'], dtype=\\'<U1\\'), array([1, 2, 1, 2, 1, 2])]\\n\\nSee Also\\n--------\\nitertools.product : Cartesian product of input iterables.  Equivalent to\\n    nested for-loops.',\n",
       " 'compute and set our version',\n",
       " 'Return a Timezone instance given its name.',\n",
       " 'specifier_qualifier_list    : specifier_qualifier_list type_specifier_no_typeid\\n        ',\n",
       " 'struct_declaration : pppragma_directive\\n        ',\n",
       " ':param e: key or index of element on value\\n:return: raw values for element if self._items is dict and contain needed element',\n",
       " ':calls: `GET /projects/columns/{column_id} <https://docs.github.com/en/rest/reference/projects#get-a-project-column>`_',\n",
       " ':type: string',\n",
       " 'Parse and handle errors of a toml configuration file.\\n\\nRaises ``tomllib.TOMLDecodeError``.',\n",
       " 'This is a numpy docstring.\\n\\nRaises\\n------\\n~re.error\\n    Sometimes',\n",
       " 'Name used safely inside the loop.',\n",
       " 'Build an API representation of this object.\\n\\nReturns:\\n    Dict[str, Any]:\\n        A dictionary in the format used by the BigQuery API.',\n",
       " 'Read everything up to one of the chars in endchars.\\n\\nThis is outside the formal grammar.  The InvalidMailbox TokenList that is\\nreturned acts like a Mailbox, but the data attributes are None.',\n",
       " 'Reads the robots.txt URL and feeds it to the parser.',\n",
       " 'Tests whether the fixer_util.is_encoding_comment() function is working.',\n",
       " 'Issue #43: Is shebang line preserved as the first\\nline by futurize when followed by a docstring?',\n",
       " 'Tests whether itertools.zip_longest is available.',\n",
       " \"Retrieve timestamp at which the object's retention period expires.\\n\\nSee https://cloud.google.com/storage/docs/json_api/v1/objects\\n\\n:rtype: :class:`datetime.datetime` or ``NoneType``\\n:returns: Datetime object parsed from RFC3339 valid timestamp, or\\n          ``None`` if the property is not set locally.\",\n",
       " \"read_namespaced_network_policy  # noqa: E501\\n\\nread the specified NetworkPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.read_namespaced_network_policy_with_http_info(name, namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the NetworkPolicy (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1NetworkPolicy, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Sets the resource_rules of this V1MatchResources.\\n\\nResourceRules describes what operations on what resources/subresources the ValidatingAdmissionPolicy matches. The policy cares about an operation if it matches _any_ Rule.  # noqa: E501\\n\\n:param resource_rules: The resource_rules of this V1MatchResources.  # noqa: E501\\n:type: list[V1NamedRuleWithOperations]',\n",
       " 'Sets the config_source of this V1NodeSpec.\\n\\n\\n:param config_source: The config_source of this V1NodeSpec.  # noqa: E501\\n:type: V1NodeConfigSource',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Sets the max_surge of this V1RollingUpdateDaemonSet.\\n\\nThe maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up to a minimum of 1. Default value is 0. Example: when this is set to 30%, at most 30% of the total number of nodes that should be running the daemon pod (i.e. status.desiredNumberScheduled) can have their a new pod created before the old pod is marked as deleted. The update starts by launching new pods on 30% of nodes. Once an updated pod is available (Ready for at least minReadySeconds) the old DaemonSet pod on that node is marked deleted. If the old pod becomes unavailable for any reason (Ready transitions to false, is evicted, or is drained) an updated pod is immediatedly created on that node without considering surge limits. Allowing surge implies the possibility that the resources consumed by the daemonset on any given node can double if the readiness check fails, and so resource intensive daemonsets should take into account that they may cause evictions during disruption.  # noqa: E501\\n\\n:param max_surge: The max_surge of this V1RollingUpdateDaemonSet.  # noqa: E501\\n:type: object',\n",
       " 'Return a dictionary of memory stats\\n\\nFor more information see https://redis.io/commands/memory-stats',\n",
       " \"Increment a bitfield by a given amount.\\n:param fmt: format-string for the bitfield being updated, e.g. 'u8'\\n    for an unsigned 8-bit integer.\\n:param offset: offset (in number of bits). If prefixed with a\\n    '#', this is an offset multiplier, e.g. given the arguments\\n    fmt='u8', offset='#2', the offset will be 16.\\n:param int increment: value to increment the bitfield by.\\n:param str overflow: overflow algorithm. Defaults to WRAP, but other\\n    acceptable values are SAT and FAIL. See the Redis docs for\\n    descriptions of these algorithms.\\n:returns: a :py:class:`BitFieldOperation` instance.\",\n",
       " 'Placeholder docstring',\n",
       " 'Ingest a single Dataframe row into FeatureStore.\\n\\nArgs:\\n    data_frame (DataFrame): source DataFrame to be ingested.\\n    row (Iterable[tuple[Any, ...]]): current row that is being ingested\\n    feature_group_name (str): name of the Feature Group.\\n    feature_definitions (Dict[str, Dict[Any, Any]]):  dictionary of feature definitions.\\n        where the key is the feature name and the value is the FeatureDefinition.\\n        The FeatureDefinition contains the data type of the feature.\\n    sagemaker_fs_runtime_client (Session): session instance to perform boto calls.\\n    failed_rows (List[int]): list of indices from the data frame for which ingestion failed.\\n    target_stores (Sequence[TargetStoreEnum]): stores to be used for ingestion.\\n\\n\\nReturns:\\n    int of row indices that failed to be ingested.',\n",
       " 'Retrieves instance types GPU info of the given region.\\n\\nArgs:\\n    region (str): The AWS region.\\n\\nReturns:\\n    dict[str, dict[str, int]]: A dictionary that contains instance types as keys\\n                               and GPU info as values or empty dictionary if the\\n                               config for the given region is not found.\\n\\nRaises:\\n    ValueError: If no config found.',\n",
       " 'Return ``True`` if the given instance has locally\\nmodified attributes.\\n\\n.. container:: class_bases\\n\\n    Proxied for the :class:`_orm.Session` class on\\n    behalf of the :class:`_orm.scoping.scoped_session` class.\\n\\nThis method retrieves the history for each instrumented\\nattribute on the instance and performs a comparison of the current\\nvalue to its previously flushed or committed value, if any.\\n\\nIt is in effect a more expensive and accurate\\nversion of checking for the given instance in the\\n:attr:`.Session.dirty` collection; a full test for\\neach attribute\\'s net \"dirty\" status is performed.\\n\\nE.g.::\\n\\n    return session.is_modified(someobject)\\n\\nA few caveats to this method apply:\\n\\n* Instances present in the :attr:`.Session.dirty` collection may\\n  report ``False`` when tested with this method.  This is because\\n  the object may have received change events via attribute mutation,\\n  thus placing it in :attr:`.Session.dirty`, but ultimately the state\\n  is the same as that loaded from the database, resulting in no net\\n  change here.\\n* Scalar attributes may not have recorded the previously set\\n  value when a new value was applied, if the attribute was not loaded,\\n  or was expired, at the time the new value was received - in these\\n  cases, the attribute is assumed to have a change, even if there is\\n  ultimately no net change against its database value. SQLAlchemy in\\n  most cases does not need the \"old\" value when a set event occurs, so\\n  it skips the expense of a SQL call if the old value isn\\'t present,\\n  based on the assumption that an UPDATE of the scalar value is\\n  usually needed, and in those few cases where it isn\\'t, is less\\n  expensive on average than issuing a defensive SELECT.\\n\\n  The \"old\" value is fetched unconditionally upon set only if the\\n  attribute container has the ``active_history`` flag set to ``True``.\\n  This flag is set typically for primary key attributes and scalar\\n  object references that are not a simple many-to-one.  To set this\\n  flag for any arbitrary mapped column, use the ``active_history``\\n  argument with :func:`.column_property`.\\n\\n:param instance: mapped instance to be tested for pending changes.\\n:param include_collections: Indicates if multivalued collections\\n should be included in the operation.  Setting this to ``False`` is a\\n way to detect only local-column based properties (i.e. scalar columns\\n or many-to-one foreign keys) that would result in an UPDATE for this\\n instance upon flush.',\n",
       " 'Initializes a `DelayedReturn` object.\\n\\nArgs:\\n    function_step: A `sagemaker.workflow.step._FunctionStep` instance.\\n    reference_path: A tuple that represents the path to the child member.',\n",
       " 'For one-to-many collections, produce a :class:`_dml.Insert` which\\nwill insert new rows in terms of this this instance-local\\n:class:`_orm.WriteOnlyCollection`.\\n\\nThis construct is only supported for a :class:`_orm.Relationship`\\nthat does **not** include the :paramref:`_orm.relationship.secondary`\\nparameter.  For relationships that refer to a many-to-many table,\\nuse ordinary bulk insert techniques to produce new objects, then\\nuse :meth:`_orm.AbstractCollectionWriter.add_all` to associate them\\nwith the collection.',\n",
       " 'Check scores attribute shape',\n",
       " 'Generate (train, test) indices',\n",
       " 'Perform spherical Voronoi calculation, but not the sorting of\\nvertices in the Voronoi polygons.',\n",
       " \"Compute the determinant of a matrix\\n\\nThe determinant is a scalar that is a function of the associated square\\nmatrix coefficients. The determinant value is zero for singular matrices.\\n\\nParameters\\n----------\\na : (..., M, M) array_like\\n    Input array to compute determinants for.\\noverwrite_a : bool, optional\\n    Allow overwriting data in a (may enhance performance).\\ncheck_finite : bool, optional\\n    Whether to check that the input matrix contains only finite numbers.\\n    Disabling may give a performance gain, but may result in problems\\n    (crashes, non-termination) if the inputs do contain infinities or NaNs.\\n\\nReturns\\n-------\\ndet : (...) float or complex\\n    Determinant of `a`. For stacked arrays, a scalar is returned for each\\n    (m, m) slice in the last two dimensions of the input. For example, an\\n    input of shape (p, q, m, m) will produce a result of shape (p, q). If\\n    all dimensions are 1 a scalar is returned regardless of ndim.\\n\\nNotes\\n-----\\nThe determinant is computed by performing an LU factorization of the\\ninput with LAPACK routine 'getrf', and then calculating the product of\\ndiagonal entries of the U factor.\\n\\nEven if the input array is single precision (float32 or complex64), the\\nresult will be returned in double precision (float64 or complex128) to\\nprevent overflows.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy import linalg\\n>>> a = np.array([[1,2,3], [4,5,6], [7,8,9]])  # A singular matrix\\n>>> linalg.det(a)\\n0.0\\n>>> b = np.array([[0,2,3], [4,5,6], [7,8,9]])\\n>>> linalg.det(b)\\n3.0\\n>>> # An array with the shape (3, 2, 2, 2)\\n>>> c = np.array([[[[1., 2.], [3., 4.]],\\n...                [[5., 6.], [7., 8.]]],\\n...               [[[9., 10.], [11., 12.]],\\n...                [[13., 14.], [15., 16.]]],\\n...               [[[17., 18.], [19., 20.]],\\n...                [[21., 22.], [23., 24.]]]])\\n>>> linalg.det(c)  # The resulting shape is (3, 2)\\narray([[-2., -2.],\\n       [-2., -2.],\\n       [-2., -2.]])\\n>>> linalg.det(c[0, 0])  # Confirm the (0, 0) slice, [[1, 2], [3, 4]]\\n-2.0\",\n",
       " 'Butterworth filter order selection.\\n\\nReturn the order of the lowest order digital or analog Butterworth filter\\nthat loses no more than `gpass` dB in the passband and has at least\\n`gstop` dB attenuation in the stopband.\\n\\nParameters\\n----------\\nwp, ws : float\\n    Passband and stopband edge frequencies.\\n\\n    For digital filters, these are in the same units as `fs`. By default,\\n    `fs` is 2 half-cycles/sample, so these are normalized from 0 to 1,\\n    where 1 is the Nyquist frequency. (`wp` and `ws` are thus in\\n    half-cycles / sample.) For example:\\n\\n        - Lowpass:   wp = 0.2,          ws = 0.3\\n        - Highpass:  wp = 0.3,          ws = 0.2\\n        - Bandpass:  wp = [0.2, 0.5],   ws = [0.1, 0.6]\\n        - Bandstop:  wp = [0.1, 0.6],   ws = [0.2, 0.5]\\n\\n    For analog filters, `wp` and `ws` are angular frequencies (e.g., rad/s).\\ngpass : float\\n    The maximum loss in the passband (dB).\\ngstop : float\\n    The minimum attenuation in the stopband (dB).\\nanalog : bool, optional\\n    When True, return an analog filter, otherwise a digital filter is\\n    returned.\\nfs : float, optional\\n    The sampling frequency of the digital system.\\n\\n    .. versionadded:: 1.2.0\\n\\nReturns\\n-------\\nord : int\\n    The lowest order for a Butterworth filter which meets specs.\\nwn : ndarray or float\\n    The Butterworth natural frequency (i.e. the \"3dB frequency\"). Should\\n    be used with `butter` to give filter results. If `fs` is specified,\\n    this is in the same units, and `fs` must also be passed to `butter`.\\n\\nSee Also\\n--------\\nbutter : Filter design using order and critical points\\ncheb1ord : Find order and critical points from passband and stopband spec\\ncheb2ord, ellipord\\niirfilter : General filter design using order and critical frequencies\\niirdesign : General filter design using passband and stopband spec\\n\\nExamples\\n--------\\nDesign an analog bandpass filter with passband within 3 dB from 20 to\\n50 rad/s, while rejecting at least -40 dB below 14 and above 60 rad/s.\\nPlot its frequency response, showing the passband and stopband\\nconstraints in gray.\\n\\n>>> from scipy import signal\\n>>> import matplotlib.pyplot as plt\\n>>> import numpy as np\\n\\n>>> N, Wn = signal.buttord([20, 50], [14, 60], 3, 40, True)\\n>>> b, a = signal.butter(N, Wn, \\'band\\', True)\\n>>> w, h = signal.freqs(b, a, np.logspace(1, 2, 500))\\n>>> plt.semilogx(w, 20 * np.log10(abs(h)))\\n>>> plt.title(\\'Butterworth bandpass filter fit to constraints\\')\\n>>> plt.xlabel(\\'Frequency [rad/s]\\')\\n>>> plt.ylabel(\\'Amplitude [dB]\\')\\n>>> plt.grid(which=\\'both\\', axis=\\'both\\')\\n>>> plt.fill([1,  14,  14,   1], [-40, -40, 99, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.fill([20, 20,  50,  50], [-99, -3, -3, -99], \\'0.9\\', lw=0) # pass\\n>>> plt.fill([60, 60, 1e9, 1e9], [99, -40, -40, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.axis([10, 100, -60, 3])\\n>>> plt.show()',\n",
       " 'Identity matrix in sparse format\\n\\nReturns an identity matrix with shape (n,n) using a given\\nsparse format and dtype. This differs from `eye_array` in\\nthat it has a square shape with ones only on the main diagonal.\\nIt is thus the multiplicative identity. `eye_array` allows\\nrectangular shapes and the diagonal can be offset from the main one.\\n\\n.. warning::\\n\\n    This function returns a sparse matrix -- not a sparse array.\\n    You are encouraged to use ``eye_array`` to take advantage\\n    of the sparse array functionality.\\n\\nParameters\\n----------\\nn : int\\n    Shape of the identity matrix.\\ndtype : dtype, optional\\n    Data type of the matrix\\nformat : str, optional\\n    Sparse format of the result, e.g., format=\"csr\", etc.\\n\\nExamples\\n--------\\n>>> import scipy as sp\\n>>> sp.sparse.identity(3).toarray()\\narray([[ 1.,  0.,  0.],\\n       [ 0.,  1.,  0.],\\n       [ 0.,  0.,  1.]])\\n>>> sp.sparse.identity(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse matrix of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>\\n>>> sp.sparse.eye_array(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse array of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>',\n",
       " 'Partial singular value decomposition of a sparse matrix using ARPACK.\\n\\nCompute the largest or smallest `k` singular values and corresponding\\nsingular vectors of a sparse matrix `A`. The order in which the singular\\nvalues are returned is not guaranteed.\\n\\nIn the descriptions below, let ``M, N = A.shape``.\\n\\nParameters\\n----------\\nA : sparse matrix or LinearOperator\\n    Matrix to decompose.\\nk : int, optional\\n    Number of singular values and singular vectors to compute.\\n    Must satisfy ``1 <= k <= min(M, N) - 1``.\\n    Default is 6.\\nncv : int, optional\\n    The number of Lanczos vectors generated.\\n    The default is ``min(n, max(2*k + 1, 20))``.\\n    If specified, must satisfy ``k + 1 < ncv < min(M, N)``; ``ncv > 2*k``\\n    is recommended.\\ntol : float, optional\\n    Tolerance for singular values. Zero (default) means machine precision.\\nwhich : {\\'LM\\', \\'SM\\'}\\n    Which `k` singular values to find: either the largest magnitude (\\'LM\\')\\n    or smallest magnitude (\\'SM\\') singular values.\\nv0 : ndarray, optional\\n    The starting vector for iteration:\\n    an (approximate) left singular vector if ``N > M`` and a right singular\\n    vector otherwise. Must be of length ``min(M, N)``.\\n    Default: random\\nmaxiter : int, optional\\n    Maximum number of Arnoldi update iterations allowed;\\n    default is ``min(M, N) * 10``.\\nreturn_singular_vectors : {True, False, \"u\", \"vh\"}\\n    Singular values are always computed and returned; this parameter\\n    controls the computation and return of singular vectors.\\n\\n    - ``True``: return singular vectors.\\n    - ``False``: do not return singular vectors.\\n    - ``\"u\"``: if ``M <= N``, compute only the left singular vectors and\\n      return ``None`` for the right singular vectors. Otherwise, compute\\n      all singular vectors.\\n    - ``\"vh\"``: if ``M > N``, compute only the right singular vectors and\\n      return ``None`` for the left singular vectors. Otherwise, compute\\n      all singular vectors.\\n\\nsolver :  {\\'arpack\\', \\'propack\\', \\'lobpcg\\'}, optional\\n        This is the solver-specific documentation for ``solver=\\'arpack\\'``.\\n        :ref:`\\'lobpcg\\' <sparse.linalg.svds-lobpcg>` and\\n        :ref:`\\'propack\\' <sparse.linalg.svds-propack>`\\n        are also supported.\\nrandom_state : {None, int, `numpy.random.Generator`,\\n                `numpy.random.RandomState`}, optional\\n\\n    Pseudorandom number generator state used to generate resamples.\\n\\n    If `random_state` is ``None`` (or `np.random`), the\\n    `numpy.random.RandomState` singleton is used.\\n    If `random_state` is an int, a new ``RandomState`` instance is used,\\n    seeded with `random_state`.\\n    If `random_state` is already a ``Generator`` or ``RandomState``\\n    instance then that instance is used.\\noptions : dict, optional\\n    A dictionary of solver-specific options. No solver-specific options\\n    are currently supported; this parameter is reserved for future use.\\n\\nReturns\\n-------\\nu : ndarray, shape=(M, k)\\n    Unitary matrix having left singular vectors as columns.\\ns : ndarray, shape=(k,)\\n    The singular values.\\nvh : ndarray, shape=(k, N)\\n    Unitary matrix having right singular vectors as rows.\\n\\nNotes\\n-----\\nThis is a naive implementation using ARPACK as an eigensolver\\non ``A.conj().T @ A`` or ``A @ A.conj().T``, depending on which one is more\\nefficient.\\n\\nExamples\\n--------\\nConstruct a matrix ``A`` from singular values and vectors.\\n\\n>>> import numpy as np\\n>>> from scipy.stats import ortho_group\\n>>> from scipy.sparse import csc_matrix, diags\\n>>> from scipy.sparse.linalg import svds\\n>>> rng = np.random.default_rng()\\n>>> orthogonal = csc_matrix(ortho_group.rvs(10, random_state=rng))\\n>>> s = [0.0001, 0.001, 3, 4, 5]  # singular values\\n>>> u = orthogonal[:, :5]         # left singular vectors\\n>>> vT = orthogonal[:, 5:].T      # right singular vectors\\n>>> A = u @ diags(s) @ vT\\n\\nWith only three singular values/vectors, the SVD approximates the original\\nmatrix.\\n\\n>>> u2, s2, vT2 = svds(A, k=3, solver=\\'arpack\\')\\n>>> A2 = u2 @ np.diag(s2) @ vT2\\n>>> np.allclose(A2, A.toarray(), atol=1e-3)\\nTrue\\n\\nWith all five singular values/vectors, we can reproduce the original\\nmatrix.\\n\\n>>> u3, s3, vT3 = svds(A, k=5, solver=\\'arpack\\')\\n>>> A3 = u3 @ np.diag(s3) @ vT3\\n>>> np.allclose(A3, A.toarray())\\nTrue\\n\\nThe singular values match the expected singular values, and the singular\\nvectors are as expected up to a difference in sign.\\n\\n>>> (np.allclose(s3, s) and\\n...  np.allclose(np.abs(u3), np.abs(u.toarray())) and\\n...  np.allclose(np.abs(vT3), np.abs(vT.toarray())))\\nTrue\\n\\nThe singular vectors are also orthogonal.\\n\\n>>> (np.allclose(u3.T @ u3, np.eye(5)) and\\n...  np.allclose(vT3 @ vT3.T, np.eye(5)))\\nTrue',\n",
       " 'Gauss-Jacobi (shifted) quadrature.\\n\\nCompute the sample points and weights for Gauss-Jacobi (shifted)\\nquadrature. The sample points are the roots of the nth degree\\nshifted Jacobi polynomial, :math:`G^{p,q}_n(x)`. These sample\\npoints and weights correctly integrate polynomials of degree\\n:math:`2n - 1` or less over the interval :math:`[0, 1]` with\\nweight function :math:`w(x) = (1 - x)^{p-q} x^{q-1}`. See 22.2.2\\nin [AS]_ for details.\\n\\nParameters\\n----------\\nn : int\\n    quadrature order\\np1 : float\\n    (p1 - q1) must be > -1\\nq1 : float\\n    q1 must be > 0\\nmu : bool, optional\\n    If True, return the sum of the weights, optional.\\n\\nReturns\\n-------\\nx : ndarray\\n    Sample points\\nw : ndarray\\n    Weights\\nmu : float\\n    Sum of the weights\\n\\nSee Also\\n--------\\nscipy.integrate.fixed_quad\\n\\nReferences\\n----------\\n.. [AS] Milton Abramowitz and Irene A. Stegun, eds.\\n    Handbook of Mathematical Functions with Formulas,\\n    Graphs, and Mathematical Tables. New York: Dover, 1972.',\n",
       " 'Type is treated as case insensitive in HTML.',\n",
       " 'Test direction in `iframe`.',\n",
       " 'Model intercept.',\n",
       " 'Standard deviation of the StandardScalerModel.',\n",
       " 'Gets the value of fitLinear or its default value.',\n",
       " 'Returns registered Documenter classes',\n",
       " 'Invalidate mocked modules on sys.modules.',\n",
       " 'Target database must support EXCEPT or equivalent (i.e. MINUS).',\n",
       " \"FastIntFlag still causes elements to be global symbols.\\n\\nWhile we do this and haven't yet changed it, make sure conflicting\\nint values for the same name don't come in.\",\n",
       " 'Inserts *token* before *where*.',\n",
       " 'Returns the class of objects of this category.\\n\\nExamples\\n========\\n\\n>>> from sympy.categories import Object, Category\\n>>> from sympy import FiniteSet\\n>>> A = Object(\"A\")\\n>>> B = Object(\"B\")\\n>>> K = Category(\"K\", FiniteSet(A, B))\\n>>> K.objects\\nClass({Object(\"A\"), Object(\"B\")})',\n",
       " 'Check if ``a`` belongs to this domain. ',\n",
       " \"Convert GMPY's ``mpz`` to ``dtype``. \",\n",
       " \"Convert ``ModularInteger(int)`` to GMPY's ``mpz``. \",\n",
       " 'Return a sparse DomainMatrix representation of *self*.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy import QQ\\n>>> A = DomainMatrix([[1, 0],[0, 2]], (2, 2), QQ)\\n>>> A.rep\\n[[1, 0], [0, 2]]\\n>>> B = A.to_sparse()\\n>>> B.rep\\n{0: {0: 1}, 1: {1: 2}}',\n",
       " 'Return the Smith-Normal form decomposition of matrix `m`.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_decomp\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> a, s, t = smith_normal_decomp(m)\\n>>> assert a == s * m * t',\n",
       " 'Computes the ``m``-th order indefinite integral of ``f`` in ``x_j``. ',\n",
       " 'Return height of the complex isolating interval. ',\n",
       " \"Fateman's GCD benchmark: sparse inputs (deg f ~ vars f) \",\n",
       " 'Returns the roots of characteristic equation of constant coefficient\\nlinear ODE and list of collectterms which is later on used by simplification\\nto use collect on solution.\\n\\nThe parameter `r` is a dict of order:coeff terms, where order is the order of the\\nderivative on each term, and coeff is the coefficient of that derivative.',\n",
       " 'Returns a fully-qualified backup string.',\n",
       " \"Return maximum delimiter priority inside `node`.\\n\\nThis is specific to atoms with contents contained in a pair of parentheses.\\nIf `node` isn't an atom or there are no enclosing parentheses, returns 0.\",\n",
       " 'a => b',\n",
       " 'Returns the default stream encoding if not found.',\n",
       " 'Add a new state to the machine and return it.',\n",
       " 'Classify a node.\\n\\nA node which contains a CNAME or RRSIG(CNAME) is a\\n``NodeKind.CNAME`` node.\\n\\nA node which contains only \"neutral\" types, i.e. types allowed to\\nco-exist with a CNAME, is a ``NodeKind.NEUTRAL`` node.  The neutral\\ntypes are NSEC, NSEC3, KEY, and their associated RRSIGS.  An empty node\\nis also considered neutral.\\n\\nA node which contains some rdataset which is not a CNAME, RRSIG(CNAME),\\nor a neutral type is a a ``NodeKind.REGULAR`` node.  Regular nodes are\\nalso commonly referred to as \"other data\".',\n",
       " 'Register a URL value preprocessor function for all view\\nfunctions in the application. These functions will be called before the\\n:meth:`before_request` functions.\\n\\nThe function can modify the values captured from the matched url before\\nthey are passed to the view. For example, this can be used to pop a\\ncommon language code value and place it in ``g`` rather than pass it to\\nevery view.\\n\\nThe function is passed the endpoint name and values dict. The return\\nvalue is ignored.\\n\\nThis is available on both app and blueprint objects. When used on an app, this\\nis called for every request. When used on a blueprint, this is called for\\nrequests that the blueprint handles. To register with a blueprint and affect\\nevery request, use :meth:`.Blueprint.app_url_value_preprocessor`.',\n",
       " 'Gets configuration metadata about a specific audience list. This\\nmethod can be used to understand an audience list after it has\\nbeen created.\\n\\nSee `Creating an Audience\\nList <https://developers.google.com/analytics/devguides/reporting/data/v1/audience-list-basics>`__\\nfor an introduction to Audience Lists with examples.\\n\\nThis method is available at beta stability at\\n`audienceExports.get <https://developers.google.com/analytics/devguides/reporting/data/v1/rest/v1beta/properties.audienceExports/get>`__.\\nTo give your feedback on this API, complete the `Google\\nAnalytics Audience Export API\\nFeedback <https://forms.gle/EeA5u5LW6PEggtCEA>`__ form.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.analytics import data_v1alpha\\n\\n    def sample_get_audience_list():\\n        # Create a client\\n        client = data_v1alpha.AlphaAnalyticsDataClient()\\n\\n        # Initialize request argument(s)\\n        request = data_v1alpha.GetAudienceListRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_audience_list(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.analytics.data_v1alpha.types.GetAudienceListRequest, dict]):\\n        The request object. A request to retrieve configuration\\n        metadata about a specific audience list.\\n    name (str):\\n        Required. The audience list resource name. Format:\\n        ``properties/{property}/audienceLists/{audience_list}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.analytics.data_v1alpha.types.AudienceList:\\n        An audience list is a list of users\\n        in an audience at the time of the list\\'s\\n        creation. One audience may have multiple\\n        audience lists created for different\\n        days.',\n",
       " '@rtype:  int\\n@return: Process global ID.',\n",
       " 'Return a callable for the create build trigger method over gRPC.\\n\\nCreates a new ``BuildTrigger``.\\n\\nThis API is experimental.\\n\\nReturns:\\n    Callable[[~.CreateBuildTriggerRequest],\\n            ~.BuildTrigger]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Transfers customer entitlements from their current reseller to\\nGoogle.\\n\\nPossible error codes:\\n\\n-  PERMISSION_DENIED: The customer doesn\\'t belong to the\\n   reseller.\\n-  INVALID_ARGUMENT: Required request parameters are missing or\\n   invalid.\\n-  NOT_FOUND: The customer or offer resource was not found.\\n-  ALREADY_EXISTS: The SKU was already transferred for the\\n   customer.\\n-  CONDITION_NOT_MET or FAILED_PRECONDITION:\\n\\n   -  The SKU requires domain verification to transfer, but the\\n      domain is not verified.\\n   -  An Add-On SKU (example, Vault or Drive) is missing the\\n      pre-requisite SKU (example, G Suite Basic).\\n   -  (Developer accounts only) Reseller and resold domain must\\n      meet the following naming requirements:\\n\\n      -  Domain names must start with goog-test.\\n      -  Domain names must include the reseller domain.\\n\\n-  INTERNAL: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n-  UNKNOWN: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n\\nReturn value: The ID of a long-running operation.\\n\\nTo get the results of the operation, call the GetOperation\\nmethod of CloudChannelOperationsService. The response will\\ncontain google.protobuf.Empty on success. The Operation metadata\\nwill contain an instance of\\n[OperationMetadata][google.cloud.channel.v1.OperationMetadata].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import channel_v1\\n\\n    def sample_transfer_entitlements_to_google():\\n        # Create a client\\n        client = channel_v1.CloudChannelServiceClient()\\n\\n        # Initialize request argument(s)\\n        entitlements = channel_v1.Entitlement()\\n        entitlements.offer = \"offer_value\"\\n\\n        request = channel_v1.TransferEntitlementsToGoogleRequest(\\n            parent=\"parent_value\",\\n            entitlements=entitlements,\\n        )\\n\\n        # Make the request\\n        operation = client.transfer_entitlements_to_google(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.channel_v1.types.TransferEntitlementsToGoogleRequest, dict]):\\n        The request object. Request message for\\n        [CloudChannelService.TransferEntitlementsToGoogle][google.cloud.channel.v1.CloudChannelService.TransferEntitlementsToGoogle].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Returns a specified regional persistent disk.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.RegionDisksClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetRegionDiskRequest(\\n            disk=\"disk_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetRegionDiskRequest, dict]):\\n        The request object. A request message for\\n        RegionDisks.Get. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk (str):\\n        Name of the regional persistent disk\\n        to return.\\n\\n        This corresponds to the ``disk`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.Disk:\\n        Represents a Persistent Disk resource. Google Compute\\n        Engine has two Disk resources: \\\\*\\n        [Zonal](/compute/docs/reference/rest/v1/disks) \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDisks)\\n        Persistent disks are required for running your VM\\n        instances. Create both boot and non-boot (data)\\n        persistent disks. For more information, read Persistent\\n        Disks. For more storage options, read Storage options.\\n        The disks resource represents a zonal persistent disk.\\n        For more information, read Zonal persistent disks. The\\n        regionDisks resource represents a regional persistent\\n        disk. For more information, read Regional resources.',\n",
       " 'Creates a new network firewall policy in the\\nspecified project and region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_insert():\\n        # Create a client\\n        client = compute_v1.RegionNetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.InsertRegionNetworkFirewallPolicyRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.insert(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.InsertRegionNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        RegionNetworkFirewallPolicies.Insert.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy_resource (google.cloud.compute_v1.types.FirewallPolicy):\\n        The body resource for this request\\n        This corresponds to the ``firewall_policy_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Call the list document schemas method over HTTP.\\n\\nArgs:\\n    request (~.document_schema_service.ListDocumentSchemasRequest):\\n        The request object. Request message for\\n    DocumentSchemaService.ListDocumentSchemas.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_schema_service.ListDocumentSchemasResponse:\\n        Response message for\\n    DocumentSchemaService.ListDocumentSchemas.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TemplatesServiceAsyncClient: The constructed client.',\n",
       " 'Pre-rpc interceptor for batch_delete_test_cases\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the TestCases server.',\n",
       " 'Return a callable for the update participant method over gRPC.\\n\\nUpdates the specified participant.\\n\\nReturns:\\n    Callable[[~.UpdateParticipantRequest],\\n            ~.Participant]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a asset path into its component segments.',\n",
       " 'Return a callable for the list preference sets method over gRPC.\\n\\nLists all the preference sets in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListPreferenceSetsRequest],\\n            ~.ListPreferenceSetsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the create grpc route method over gRPC.\\n\\nCreates a new GrpcRoute in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateGrpcRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return string with information about image sequence.',\n",
       " 'Post-rpc interceptor for get_replay\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Simulator server but before\\nit is returned to user code.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TagKeysAsyncClient: The constructed client.',\n",
       " 'Updates the mute state of a finding.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securitycenter_v1\\n\\n    def sample_set_mute():\\n        # Create a client\\n        client = securitycenter_v1.SecurityCenterClient()\\n\\n        # Initialize request argument(s)\\n        request = securitycenter_v1.SetMuteRequest(\\n            name=\"name_value\",\\n            mute=\"UNDEFINED\",\\n        )\\n\\n        # Make the request\\n        response = client.set_mute(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securitycenter_v1.types.SetMuteRequest, dict]):\\n        The request object. Request message for updating a\\n        finding\\'s mute status.\\n    name (str):\\n        Required. The `relative resource\\n        name <https://cloud.google.com/apis/design/resource_names#relative_resource_name>`__\\n        of the finding. Example:\\n        ``organizations/{organization_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``folders/{folder_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``projects/{project_id}/sources/{source_id}/findings/{finding_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    mute (google.cloud.securitycenter_v1.types.Finding.Mute):\\n        Required. The desired state of the\\n        Mute.\\n\\n        This corresponds to the ``mute`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securitycenter_v1.types.Finding:\\n        Security Command Center finding.\\n\\n        A finding is a record of assessment data\\n        like security, risk, health, or privacy,\\n        that is ingested into Security Command\\n        Center for presentation, notification,\\n        analysis, policy testing, and\\n        enforcement. For example, a cross-site\\n        scripting (XSS) vulnerability in an App\\n        Engine application is a finding.',\n",
       " 'Return a callable for the update deployment method over gRPC.\\n\\nUpdates a deployment.\\n\\nReturns:\\n    Callable[[~.UpdateDeploymentRequest],\\n            ~.Deployment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'texttospeech.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'Return a callable for the delete product method over gRPC.\\n\\nPermanently deletes a product and its reference\\nimages.\\nMetadata of the product and all its images will be\\ndeleted right away, but search queries against\\nProductSets containing the product may still work until\\nall related caches are refreshed.\\n\\nReturns:\\n    Callable[[~.DeleteProductRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for search_index_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Warehouse server but before\\nit is returned to user code.',\n",
       " 'Returns a fully-qualified account string.',\n",
       " 'Call the list data sources method over HTTP.\\n\\nArgs:\\n    request (~.datasources.ListDataSourcesRequest):\\n        The request object. Request message for the\\n    ListDataSources method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.datasources.ListDataSourcesResponse:\\n        Response message for the\\n    ListDataSources method.',\n",
       " 'SPE specific metadata.\\n\\nParameters\\n----------\\nindex : int\\n    Ignored as SPE files only store global metadata.\\nexclude_applied : bool\\n    Ignored. Exists for API compatibility.\\nchar_encoding : str\\n    The encoding to use when parsing strings.\\nsdt_control : bool\\n    If `True`, decode special metadata written by the\\n    SDT-control software if present.\\n\\nReturns\\n-------\\nmetadata : dict\\n    Key-value pairs of metadata.\\n\\nNotes\\n-----\\nSPE v3 stores metadata as XML, whereas SPE v2 uses a binary format.\\n\\n.. rubric:: Supported SPE v2 Metadata fields\\n\\nROIs : list of dict\\n    Regions of interest used for recording images. Each dict has the\\n    \"top_left\" key containing x and y coordinates of the top left corner,\\n    the \"bottom_right\" key with x and y coordinates of the bottom right\\n    corner, and the \"bin\" key with number of binned pixels in x and y\\n    directions.\\ncomments : list of str\\n    The SPE format allows for 5 comment strings of 80 characters each.\\ncontroller_version : int\\n    Hardware version\\nlogic_output : int\\n    Definition of output BNC\\namp_hi_cap_low_noise : int\\n    Amp switching mode\\nmode : int\\n    Timing mode\\nexp_sec : float\\n    Alternative exposure in seconds\\ndate : str\\n    Date string\\ndetector_temp : float\\n    Detector temperature\\ndetector_type : int\\n    CCD / diode array type\\nst_diode : int\\n    Trigger diode\\ndelay_time : float\\n    Used with async mode\\nshutter_control : int\\n    Normal, disabled open, or disabled closed\\nabsorb_live : bool\\n    on / off\\nabsorb_mode : int\\n    Reference strip or file\\ncan_do_virtual_chip : bool\\n    True or False whether chip can do virtual chip\\nthreshold_min_live : bool\\n    on / off\\nthreshold_min_val : float\\n    Threshold minimum value\\nthreshold_max_live : bool\\n    on / off\\nthreshold_max_val : float\\n    Threshold maximum value\\ntime_local : str\\n    Experiment local time\\ntime_utc : str\\n    Experiment UTC time\\nadc_offset : int\\n    ADC offset\\nadc_rate : int\\n    ADC rate\\nadc_type : int\\n    ADC type\\nadc_resolution : int\\n    ADC resolution\\nadc_bit_adjust : int\\n    ADC bit adjust\\ngain : int\\n    gain\\nsw_version : str\\n    Version of software which created this file\\nspare_4 : bytes\\n    Reserved space\\nreadout_time : float\\n    Experiment readout time\\ntype : str\\n    Controller type\\nclockspeed_us : float\\n    Vertical clock speed in microseconds\\nreadout_mode : [\"full frame\", \"frame transfer\", \"kinetics\", \"\"]\\n    Readout mode. Empty string means that this was not set by the\\n    Software.\\nwindow_size : int\\n    Window size for Kinetics mode\\nfile_header_ver : float\\n    File header version\\nchip_size : [int, int]\\n    x and y dimensions of the camera chip\\nvirt_chip_size : [int, int]\\n    Virtual chip x and y dimensions\\npre_pixels : [int, int]\\n    Pre pixels in x and y dimensions\\npost_pixels : [int, int],\\n    Post pixels in x and y dimensions\\ngeometric : list of {\"rotate\", \"reverse\", \"flip\"}\\n    Geometric operations\\nsdt_major_version : int\\n    (only for files created by SDT-control)\\n    Major version of SDT-control software\\nsdt_minor_version : int\\n    (only for files created by SDT-control)\\n    Minor version of SDT-control software\\nsdt_controller_name : str\\n    (only for files created by SDT-control)\\n    Controller name\\nexposure_time : float\\n    (only for files created by SDT-control)\\n    Exposure time in seconds\\ncolor_code : str\\n    (only for files created by SDT-control)\\n    Color channels used\\ndetection_channels : int\\n    (only for files created by SDT-control)\\n    Number of channels\\nbackground_subtraction : bool\\n    (only for files created by SDT-control)\\n    Whether background subtraction war turned on\\nem_active : bool\\n    (only for files created by SDT-control)\\n    Whether EM was turned on\\nem_gain : int\\n    (only for files created by SDT-control)\\n    EM gain\\nmodulation_active : bool\\n    (only for files created by SDT-control)\\n    Whether laser modulation (â€œattenuateâ€) was turned on\\npixel_size : float\\n    (only for files created by SDT-control)\\n    Camera pixel size\\nsequence_type : str\\n    (only for files created by SDT-control)\\n    Type of sequnce (standard, TOCCSL, arbitrary, â€¦)\\ngrid : float\\n    (only for files created by SDT-control)\\n    Sequence time unit (â€œgrid sizeâ€) in seconds\\nn_macro : int\\n    (only for files created by SDT-control)\\n    Number of macro loops\\ndelay_macro : float\\n    (only for files created by SDT-control)\\n    Time between macro loops in seconds\\nn_mini : int\\n    (only for files created by SDT-control)\\n    Number of mini loops\\ndelay_mini : float\\n    (only for files created by SDT-control)\\n    Time between mini loops in seconds\\nn_micro : int (only for files created by SDT-control)\\n    Number of micro loops\\ndelay_micro : float (only for files created by SDT-control)\\n    Time between micro loops in seconds\\nn_subpics : int\\n    (only for files created by SDT-control)\\n    Number of sub-pictures\\ndelay_shutter : float\\n    (only for files created by SDT-control)\\n    Camera shutter delay in seconds\\ndelay_prebleach : float\\n    (only for files created by SDT-control)\\n    Pre-bleach delay in seconds\\nbleach_time : float\\n    (only for files created by SDT-control)\\n    Bleaching time in seconds\\nrecovery_time : float\\n    (only for files created by SDT-control)\\n    Recovery time in seconds\\ncomment : str\\n    (only for files created by SDT-control)\\n    User-entered comment. This replaces the \"comments\" field.\\ndatetime : datetime.datetime\\n    (only for files created by SDT-control)\\n    Combines the \"date\" and \"time_local\" keys. The latter two plus\\n    \"time_utc\" are removed.\\nmodulation_script : str\\n    (only for files created by SDT-control)\\n    Laser modulation script. Replaces the \"spare_4\" key.\\nbleach_piezo_active : bool\\n    (only for files created by SDT-control)\\n    Whether piezo for bleaching was enabled',\n",
       " 'Get the global InteractiveShell instance.\\n\\nReturns None if no InteractiveShell instance is registered.',\n",
       " 'Activate the interactive debugger.\\n\\nThis magic command support two ways of activating debugger.\\nOne is to activate debugger before executing code.  This way, you\\ncan set a break point, to step through the code from the point.\\nYou can use this mode by giving statements to execute and optionally\\na breakpoint.\\n\\nThe other one is to activate debugger in post-mortem mode.  You can\\nactivate this mode simply running %debug without any argument.\\nIf an exception has just occurred, this lets you inspect its stack\\nframes interactively.  Note that this will always work only on the last\\ntraceback that occurred, so you must call this quickly after an\\nexception that you wish to inspect has fired, because if another one\\noccurs, it clobbers the previous one.\\n\\nIf you want IPython to automatically do this on every exception, see\\nthe %pdb magic for more details.\\n\\n.. versionchanged:: 7.3\\n    When running code, user variables are no longer expanded,\\n    the magic line is always left unmodified.',\n",
       " \"var_expand on invalid formats shouldn't raise\",\n",
       " 'Check whether the instance conforms to the given format.\\n\\nArguments:\\n\\n    instance (*any primitive type*, i.e. str, number, bool):\\n\\n        The instance to check\\n\\n    format:\\n\\n        The format that instance should conform to\\n\\nReturns:\\n\\n    bool: whether it conformed',\n",
       " 'Return the x-axis view limits.\\n\\nReturns\\n-------\\nleft, right : (float, float)\\n    The current x-axis limits in data coordinates.\\n\\nSee Also\\n--------\\n.Axes.set_xlim\\n.Axes.set_xbound, .Axes.get_xbound\\n.Axes.invert_xaxis, .Axes.xaxis_inverted\\n\\nNotes\\n-----\\nThe x-axis may be inverted, in which case the *left* value will\\nbe greater than the *right* value.',\n",
       " 'Draw a multibyte character from a Type 3 font as an XObject.',\n",
       " 'Return a list of (ind0, ind1) such that ``mask[ind0:ind1].all()`` is\\nTrue and we cover all such regions.',\n",
       " 'You cannot define methods out of the spec',\n",
       " 'Send a batch of write operations to the server.\\n\\nRequests are passed as a list of write operation instances (\\n:class:`~pymongo.operations.InsertOne`,\\n:class:`~pymongo.operations.UpdateOne`,\\n:class:`~pymongo.operations.UpdateMany`,\\n:class:`~pymongo.operations.ReplaceOne`,\\n:class:`~pymongo.operations.DeleteOne`, or\\n:class:`~pymongo.operations.DeleteMany`).\\n\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634ef\\')}\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  >>> # DeleteMany, UpdateOne, and UpdateMany are also available.\\n  ...\\n  >>> from pymongo import InsertOne, DeleteOne, ReplaceOne\\n  >>> requests = [InsertOne({\\'y\\': 1}), DeleteOne({\\'x\\': 1}),\\n  ...             ReplaceOne({\\'w\\': 1}, {\\'z\\': 1}, upsert=True)]\\n  >>> result = db.test.bulk_write(requests)\\n  >>> result.inserted_count\\n  1\\n  >>> result.deleted_count\\n  1\\n  >>> result.modified_count\\n  0\\n  >>> result.upserted_ids\\n  {2: ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  {\\'y\\': 1, \\'_id\\': ObjectId(\\'54f62ee2fba5226811f634f1\\')}\\n  {\\'z\\': 1, \\'_id\\': ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n\\n:param requests: A list of write operations (see examples above).\\n:param ordered: If ``True`` (the default) requests will be\\n    performed on the server serially, in the order provided. If an error\\n    occurs all remaining operations are aborted. If ``False`` requests\\n    will be performed on the server in arbitrary order, possibly in\\n    parallel, and all operations will be attempted.\\n:param bypass_document_validation: (optional) If ``True``, allows the\\n    write to opt-out of document level validation. Default is\\n    ``False``.\\n:param session: a\\n    :class:`~pymongo.client_session.ClientSession`.\\n:param comment: A user-provided comment to attach to this\\n    command.\\n:param let: Map of parameter names and values. Values must be\\n    constant or closed expressions that do not reference document\\n    fields. Parameters can then be accessed as variables in an\\n    aggregate expression context (e.g. \"$$var\").\\n\\n:return: An instance of :class:`~pymongo.results.BulkWriteResult`.\\n\\n.. seealso:: :ref:`writes-and-ids`\\n\\n.. note:: `bypass_document_validation` requires server version\\n  **>= 3.2**\\n\\n.. versionchanged:: 4.1\\n   Added ``comment`` parameter.\\n   Added ``let`` parameter.\\n\\n.. versionchanged:: 3.6\\n   Added ``session`` parameter.\\n\\n.. versionchanged:: 3.2\\n  Added bypass_document_validation support\\n\\n.. versionadded:: 3.0',\n",
       " 'Test a complete undirected graph.',\n",
       " 'Test that the new (>=v1.15) implementation (see #10073) is equal to the original (<=v1.14) ',\n",
       " 'Load data from a text file.\\n\\nParameters\\n----------\\nfname : file, str, pathlib.Path, list of str, generator\\n    File, filename, list, or generator to read.  If the filename\\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\\n    that generators must return bytes or strings. The strings\\n    in a list or produced by a generator are treated as lines.\\ndtype : data-type, optional\\n    Data-type of the resulting array; default: float.  If this is a\\n    structured data-type, the resulting array will be 1-dimensional, and\\n    each row will be interpreted as an element of the array.  In this\\n    case, the number of columns used must match the number of fields in\\n    the data-type.\\ncomments : str or sequence of str or None, optional\\n    The characters or list of characters used to indicate the start of a\\n    comment. None implies no comments. For backwards compatibility, byte\\n    strings will be decoded as \\'latin1\\'. The default is \\'#\\'.\\ndelimiter : str, optional\\n    The character used to separate the values. For backwards compatibility,\\n    byte strings will be decoded as \\'latin1\\'. The default is whitespace.\\n\\n    .. versionchanged:: 1.23.0\\n       Only single character delimiters are supported. Newline characters\\n       cannot be used as the delimiter.\\n\\nconverters : dict or callable, optional\\n    Converter functions to customize value parsing. If `converters` is\\n    callable, the function is applied to all columns, else it must be a\\n    dict that maps column number to a parser function.\\n    See examples for further details.\\n    Default: None.\\n\\n    .. versionchanged:: 1.23.0\\n       The ability to pass a single callable to be applied to all columns\\n       was added.\\n\\nskiprows : int, optional\\n    Skip the first `skiprows` lines, including comments; default: 0.\\nusecols : int or sequence, optional\\n    Which columns to read, with 0 being the first. For example,\\n    ``usecols = (1,4,5)`` will extract the 2nd, 5th and 6th columns.\\n    The default, None, results in all columns being read.\\nunpack : bool, optional\\n    If True, the returned array is transposed, so that arguments may be\\n    unpacked using ``x, y, z = loadtxt(...)``.  When used with a\\n    structured data-type, arrays are returned for each field.\\n    Default is False.\\nndmin : int, optional\\n    The returned array will have at least `ndmin` dimensions.\\n    Otherwise mono-dimensional axes will be squeezed.\\n    Legal values: 0 (default), 1 or 2.\\nencoding : str, optional\\n    Encoding used to decode the inputfile. Does not apply to input streams.\\n    The special value \\'bytes\\' enables backward compatibility workarounds\\n    that ensures you receive byte arrays as results if possible and passes\\n    \\'latin1\\' encoded strings to converters. Override this value to receive\\n    unicode arrays and pass strings as input to converters.  If set to None\\n    the system default is used. The default value is \\'bytes\\'.\\n\\n    .. versionchanged:: 2.0\\n        Before NumPy 2, the default was ``\\'bytes\\'`` for Python 2\\n        compatibility. The default is now ``None``.\\n\\nmax_rows : int, optional\\n    Read `max_rows` rows of content after `skiprows` lines. The default is\\n    to read all the rows. Note that empty rows containing no data such as\\n    empty lines and comment lines are not counted towards `max_rows`,\\n    while such lines are counted in `skiprows`.\\n\\n    .. versionchanged:: 1.23.0\\n        Lines containing no data, including comment lines (e.g., lines\\n        starting with \\'#\\' or as specified via `comments`) are not counted\\n        towards `max_rows`.\\nquotechar : unicode character or None, optional\\n    The character used to denote the start and end of a quoted item.\\n    Occurrences of the delimiter or comment characters are ignored within\\n    a quoted item. The default value is ``quotechar=None``, which means\\n    quoting support is disabled.\\n\\n    If two consecutive instances of `quotechar` are found within a quoted\\n    field, the first is treated as an escape character. See examples.\\n\\n    .. versionadded:: 1.23.0\\n${ARRAY_FUNCTION_LIKE}\\n\\n    .. versionadded:: 1.20.0\\n\\nReturns\\n-------\\nout : ndarray\\n    Data read from the text file.\\n\\nSee Also\\n--------\\nload, fromstring, fromregex\\ngenfromtxt : Load data with missing values handled as specified.\\nscipy.io.loadmat : reads MATLAB data files\\n\\nNotes\\n-----\\nThis function aims to be a fast reader for simply formatted files.  The\\n`genfromtxt` function provides more sophisticated handling of, e.g.,\\nlines with missing values.\\n\\nEach row in the input text file must have the same number of values to be\\nable to read all values. If all rows do not have same number of values, a\\nsubset of up to n columns (where n is the least number of values present\\nin all rows) can be read by specifying the columns via `usecols`.\\n\\nThe strings produced by the Python float.hex method can be used as\\ninput for floats.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from io import StringIO   # StringIO behaves like a file object\\n>>> c = StringIO(\"0 1\\\\n2 3\")\\n>>> np.loadtxt(c)\\narray([[0., 1.],\\n       [2., 3.]])\\n\\n>>> d = StringIO(\"M 21 72\\\\nF 35 58\")\\n>>> np.loadtxt(d, dtype={\\'names\\': (\\'gender\\', \\'age\\', \\'weight\\'),\\n...                      \\'formats\\': (\\'S1\\', \\'i4\\', \\'f4\\')})\\narray([(b\\'M\\', 21, 72.), (b\\'F\\', 35, 58.)],\\n      dtype=[(\\'gender\\', \\'S1\\'), (\\'age\\', \\'<i4\\'), (\\'weight\\', \\'<f4\\')])\\n\\n>>> c = StringIO(\"1,0,2\\\\n3,0,4\")\\n>>> x, y = np.loadtxt(c, delimiter=\\',\\', usecols=(0, 2), unpack=True)\\n>>> x\\narray([1., 3.])\\n>>> y\\narray([2., 4.])\\n\\nThe `converters` argument is used to specify functions to preprocess the\\ntext prior to parsing. `converters` can be a dictionary that maps\\npreprocessing functions to each column:\\n\\n>>> s = StringIO(\"1.618, 2.296\\\\n3.141, 4.669\\\\n\")\\n>>> conv = {\\n...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\\n...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\\n... }\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([[1., 3.],\\n       [3., 5.]])\\n\\n`converters` can be a callable instead of a dictionary, in which case it\\nis applied to all columns:\\n\\n>>> s = StringIO(\"0xDE 0xAD\\\\n0xC0 0xDE\")\\n>>> import functools\\n>>> conv = functools.partial(int, base=16)\\n>>> np.loadtxt(s, converters=conv)\\narray([[222., 173.],\\n       [192., 222.]])\\n\\nThis example shows how `converters` can be used to convert a field\\nwith a trailing minus sign into a negative number.\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> def conv(fld):\\n...     return -float(fld[:-1]) if fld.endswith(\"-\") else float(fld)\\n...\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nUsing a callable as the converter can be particularly useful for handling\\nvalues with different formatting, e.g. floats with underscores:\\n\\n>>> s = StringIO(\"1 2.7 100_000\")\\n>>> np.loadtxt(s, converters=float)\\narray([1.e+00, 2.7e+00, 1.e+05])\\n\\nThis idea can be extended to automatically handle values specified in\\nmany different formats, such as hex values:\\n\\n>>> def conv(val):\\n...     try:\\n...         return float(val)\\n...     except ValueError:\\n...         return float.fromhex(val)\\n>>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\\n\\nOr a format where the ``-`` sign comes after the number:\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> conv = lambda x: -float(x[:-1]) if x.endswith(\"-\") else float(x)\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nSupport for quoted fields is enabled with the `quotechar` parameter.\\nComment and delimiter characters are ignored when they appear within a\\nquoted item delineated by `quotechar`:\\n\\n>>> s = StringIO(\\'\"alpha, #42\", 10.0\\\\n\"beta, #64\", 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nQuoted fields can be separated by multiple whitespace characters:\\n\\n>>> s = StringIO(\\'\"alpha, #42\"       10.0\\\\n\"beta, #64\" 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nTwo consecutive quote characters within a quoted field are treated as a\\nsingle escaped character:\\n\\n>>> s = StringIO(\\'\"Hello, my name is \"\"Monty\"\"!\"\\')\\n>>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar=\\'\"\\')\\narray(\\'Hello, my name is \"Monty\"!\\', dtype=\\'<U26\\')\\n\\nRead subset of columns when all rows do not contain equal number of values:\\n\\n>>> d = StringIO(\"1 2\\\\n2 4\\\\n3 9 12\\\\n4 16 20\")\\n>>> np.loadtxt(d, usecols=(0, 1))\\narray([[ 1.,  2.],\\n       [ 2.,  4.],\\n       [ 3.,  9.],\\n       [ 4., 16.]])',\n",
       " 'Re-pack the fields of a structured array or dtype in memory.\\n\\nThe memory layout of structured datatypes allows fields at arbitrary\\nbyte offsets. This means the fields can be separated by padding bytes,\\ntheir offsets can be non-monotonically increasing, and they can overlap.\\n\\nThis method removes any overlaps and reorders the fields in memory so they\\nhave increasing byte offsets, and adds or removes padding bytes depending\\non the `align` option, which behaves like the `align` option to\\n`numpy.dtype`.\\n\\nIf `align=False`, this method produces a \"packed\" memory layout in which\\neach field starts at the byte the previous field ended, and any padding\\nbytes are removed.\\n\\nIf `align=True`, this methods produces an \"aligned\" memory layout in which\\neach field\\'s offset is a multiple of its alignment, and the total itemsize\\nis a multiple of the largest alignment, by adding padding bytes as needed.\\n\\nParameters\\n----------\\na : ndarray or dtype\\n   array or dtype for which to repack the fields.\\nalign : boolean\\n   If true, use an \"aligned\" memory layout, otherwise use a \"packed\" layout.\\nrecurse : boolean\\n   If True, also repack nested structures.\\n\\nReturns\\n-------\\nrepacked : ndarray or dtype\\n   Copy of `a` with fields repacked, or `a` itself if no repacking was\\n   needed.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n\\n>>> from numpy.lib import recfunctions as rfn\\n>>> def print_offsets(d):\\n...     print(\"offsets:\", [d.fields[name][1] for name in d.names])\\n...     print(\"itemsize:\", d.itemsize)\\n...\\n>>> dt = np.dtype(\\'u1, <i8, <f8\\', align=True)\\n>>> dt\\ndtype({\\'names\\': [\\'f0\\', \\'f1\\', \\'f2\\'], \\'formats\\': [\\'u1\\', \\'<i8\\', \\'<f8\\'], \\'offsets\\': [0, 8, 16], \\'itemsize\\': 24}, align=True)\\n>>> print_offsets(dt)\\noffsets: [0, 8, 16]\\nitemsize: 24\\n>>> packed_dt = rfn.repack_fields(dt)\\n>>> packed_dt\\ndtype([(\\'f0\\', \\'u1\\'), (\\'f1\\', \\'<i8\\'), (\\'f2\\', \\'<f8\\')])\\n>>> print_offsets(packed_dt)\\noffsets: [0, 1, 9]\\nitemsize: 17',\n",
       " 'Test that bin width for integer data is at least 1.',\n",
       " 'Returns the data as a recarray.',\n",
       " 'Invalidate an authorization code after use.\\n\\n:param client_id: Unicode client identifier.\\n:param code: The authorization code grant (request.code).\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n\\nMethod is used by:\\n    - Authorization Code Grant',\n",
       " 'Write an object to file specified by a pathlib.Path and read it back\\n\\nParameters\\n----------\\nwriter : callable bound to pandas object\\n    IO writing function (e.g. DataFrame.to_csv )\\nreader : callable\\n    IO reading function (e.g. pd.read_csv )\\npath : str, default None\\n    The path where the object is written and then read.\\n\\nReturns\\n-------\\npandas object\\n    The original object that was serialized and then re-read.',\n",
       " 'For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results\\nin PeriodArray.  We cannot use ._ndarray directly for the affected\\nmethods because the i8 data has different semantics on NaT values.',\n",
       " ':calls: `POST /gists <http://docs.github.com/en/rest/reference/gists>`_',\n",
       " \"`argslist_list` is a list that can contain an argslist as a first item, but\\nmost not. It's basically the items between the parameter brackets (which is\\nat most one item).\\nThis function modifies the parser structure. It generates `Param` objects\\nfrom the normal ast. Those param objects do not exist in a normal ast, but\\nmake the evaluation of the ast tree so much easier.\\nYou could also say that this function replaces the argslist node with a\\nlist of Param objects.\",\n",
       " 'Erases from the current cursor position to the end of the current\\nline.',\n",
       " 'Apply reduction rules to `word` excluding the reduction rule\\nfor the lhs equal to `exclude`',\n",
       " 'Initialize a SpecifierSet instance.\\n\\n:param specifiers:\\n    The string representation of a specifier or a comma-separated list of\\n    specifiers which will be parsed and normalized before use.\\n:param prereleases:\\n    This tells the SpecifierSet if it should accept prerelease versions if\\n    applicable or not. The default of ``None`` will autodetect it from the\\n    given specifiers.\\n\\n:raises InvalidSpecifier:\\n    If the given ``specifiers`` are not parseable than this exception will be\\n    raised.',\n",
       " 'Register `provider_factory` to make providers for `loader_type`\\n\\n`loader_type` is the type or class of a PEP 302 ``module.__loader__``,\\nand `provider_factory` is a function that, passed a *module* object,\\nreturns an ``IResourceProvider`` for that module.',\n",
       " ':return: runtime directory tied to the user, e.g. ``~/Library/Caches/TemporaryItems/$appname/$version``',\n",
       " \"Connect to *address* and return the socket object.\\n\\nConvenience function.  Connect to *address* (a 2-tuple ``(host,\\nport)``) and return the socket object.  Passing the optional\\n*timeout* parameter will set the timeout on the socket instance\\nbefore attempting to connect.  If no *timeout* is supplied, the\\nglobal default timeout setting returned by :func:`socket.getdefaulttimeout`\\nis used.  If *source_address* is set it must be a tuple of (host, port)\\nfor the socket to bind as a source address before making the connection.\\nAn host of '' or port 0 tells the OS to use the default.\",\n",
       " 'return a Traceback instance wrapping part of this Traceback\\n\\nby provding any combination of path, lineno and firstlineno, the\\nfirst frame to start the to-be-returned traceback is determined\\n\\nthis allows cutting the first part of a Traceback instance e.g.\\nfor formatting reasons (removing some uninteresting bits that deal\\nwith handling of the exception/traceback)',\n",
       " 'A wrapper around typing.List that adds validation.\\n\\nArgs:\\n    item_type: The type of the items in the list.\\n    min_length: The minimum length of the list. Defaults to None.\\n    max_length: The maximum length of the list. Defaults to None.\\n    unique_items: Whether the items in the list must be unique. Defaults to None.\\n        !!! warning Deprecated\\n            The `unique_items` parameter is deprecated, use `Set` instead.\\n            See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.\\n\\nReturns:\\n    The wrapped list type.',\n",
       " 'Check if nodes.Name corresponds to first attribute variable name.\\n\\nName is `self` for method, `cls` for classmethod and `mcs` for metaclass.\\nStatic methods return False.',\n",
       " 'Format and print messages in the context of the path.',\n",
       " 'Return triangle having side of length l1 on the x-axis.',\n",
       " \"Checks that base_var is not seen as defined outsite '__init__'\\n        \",\n",
       " 'https://github.com/pylint-dev/pylint/issues/7131',\n",
       " 'The path from which pytest was invoked.\\n\\n.. versionadded:: 7.0.0',\n",
       " 'Ensure we can collect files with weird file extensions as Python\\nmodules (#2369)',\n",
       " 'Union[google.cloud.bigquery.ExternalConfig, None]: Configuration for\\nan external data source (defaults to :data:`None`).\\n\\nRaises:\\n    ValueError: For invalid value types.',\n",
       " \"Return items as ``(key, value)`` pairs.\\n\\nReturns:\\n    Iterable[Tuple[str, object]]:\\n        The ``(key, value)`` pairs representing this row.\\n\\nExamples:\\n\\n    >>> list(Row(('a', 'b'), {'x': 0, 'y': 1}).items())\\n    [('x', 'a'), ('y', 'b')]\",\n",
       " \"Coerce 'value' to a datetime, if set or not nullable.\\n\\nArgs:\\n    value (str): The timestamp.\\n\\n    field (google.cloud.bigquery.schema.SchemaField):\\n        The field corresponding to the value.\\n\\nReturns:\\n    Optional[datetime.datetime]:\\n        The parsed datetime object from\\n        ``value`` if the ``field`` is not null (otherwise it is\\n        :data:`None`).\",\n",
       " 'Return the estimated number of bytes processed by the query.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobStatistics2.FIELDS.estimated_bytes_processed\\n\\nReturns:\\n    Optional[int]:\\n        number of DML rows affected by the job, or None if job is not\\n        yet complete.',\n",
       " 'Move an existing element to the end (or beginning if last==False).\\n\\nRaises KeyError if the element does not exist.\\nWhen last=True, acts like a fast version of self[key]=self.pop(key).',\n",
       " 'Call finish_request.\\n\\nOverridden by ForkingMixIn and ThreadingMixIn.',\n",
       " 'Note the case normalization of header names here, to\\n.capitalize()-case.  This should be preserved for\\nbackwards-compatibility.  (In the HTTP case, normalization to\\n.title()-case is done by urllib2 before sending headers to\\nhttp.client).\\n\\nNote that e.g. r.has_header(\"spam-EggS\") is currently False, and\\nr.get_header(\"spam-EggS\") returns None, but that could be changed in\\nfuture.\\n\\nMethod r.remove_header should remove items both from r.headers and\\nr.unredirected_hdrs dictionaries',\n",
       " 'Prepending the signature with zeroes should be detected.',\n",
       " 'Input: a list of UserDicts.',\n",
       " 'Output: Using maxcolwidth in conjunction with disable_parsenum is honored',\n",
       " 'Sets the service of this AdmissionregistrationV1WebhookClientConfig.\\n\\n\\n:param service: The service of this AdmissionregistrationV1WebhookClientConfig.  # noqa: E501\\n:type: AdmissionregistrationV1ServiceReference',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Gets the azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n\\n\\n:return: The azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n:rtype: V1AzureDiskVolumeSource',\n",
       " 'Gets the number of this V1ServiceBackendPort.  # noqa: E501\\n\\nnumber is the numerical port number (e.g. 80) on the Service. This is a mutually exclusive setting with \"Name\".  # noqa: E501\\n\\n:return: The number of this V1ServiceBackendPort.  # noqa: E501\\n:rtype: int',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Compute and set the OOB score and attributes.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The data matrix.\\ny : ndarray of shape (n_samples, n_outputs)\\n    The target matrix.\\nscoring_function : callable, default=None\\n    Scoring function for OOB score. Default depends on whether\\n    this is a regression (R2 score) or classification problem\\n    (accuracy score).',\n",
       " 'This provides dict.get method functionality with type checking',\n",
       " \"Ellipsoidal harmonic functions F^p_n(l)\\n\\nThese are also known as Lame functions of the second kind, and are\\nsolutions to the Lame equation:\\n\\n.. math:: (s^2 - h^2)(s^2 - k^2)F''(s)\\n          + s(2s^2 - h^2 - k^2)F'(s) + (a - q s^2)F(s) = 0\\n\\nwhere :math:`q = (n+1)n` and :math:`a` is the eigenvalue (not\\nreturned) corresponding to the solutions.\\n\\nParameters\\n----------\\nh2 : float\\n    ``h**2``\\nk2 : float\\n    ``k**2``; should be larger than ``h**2``\\nn : int\\n    Degree.\\np : int\\n    Order, can range between [1,2n+1].\\ns : float\\n    Coordinate\\n\\nReturns\\n-------\\nF : float\\n    The harmonic :math:`F^p_n(s)`\\n\\nSee Also\\n--------\\nellip_harm, ellip_normal\\n\\nNotes\\n-----\\nLame functions of the second kind are related to the functions of the first kind:\\n\\n.. math::\\n\\n   F^p_n(s)=(2n + 1)E^p_n(s)\\\\int_{0}^{1/s}\\n   \\\\frac{du}{(E^p_n(1/u))^2\\\\sqrt{(1-u^2k^2)(1-u^2h^2)}}\\n\\n.. versionadded:: 0.15.0\\n\\nExamples\\n--------\\n>>> from scipy.special import ellip_harm_2\\n>>> w = ellip_harm_2(5,8,2,1,10)\\n>>> w\\n0.00108056853382\",\n",
       " 'Results above from SAS PROC NPAR1WAY, e.g.\\n\\nDATA myData;\\nINPUT X Y;\\nCARDS;\\n1 1\\n1 2\\n1 3\\n1 4\\n2 1.5\\n2 2\\n2 2.5\\nods graphics on;\\nproc npar1way AB data=myData;\\n    class X;\\n    EXACT;\\nrun;\\nods graphics off;\\n\\nNote: SAS provides Pr >= |S-Mean|, which is different from our\\ndefinition of a two-sided p-value.',\n",
       " 'Match default.',\n",
       " 'See the cancel function in sympy.polys',\n",
       " 'Format the argument signature of *self.object*.\\n\\nShould return None if the object does not have a signature.',\n",
       " 'Outputs the table of ``op``.',\n",
       " 'extension version of the same test in test_mapper.\\n\\nfixes #3408',\n",
       " \"Return a bool indicating whether the error between z1 and z2\\nis $\\\\le$ ``tol``.\\n\\nExamples\\n========\\n\\nIf ``tol`` is ``None`` then ``True`` will be returned if\\n:math:`|z1 - z2|\\\\times 10^p \\\\le 5` where $p$ is minimum value of the\\ndecimal precision of each value.\\n\\n>>> from sympy import comp, pi\\n>>> pi4 = pi.n(4); pi4\\n3.142\\n>>> comp(_, 3.142)\\nTrue\\n>>> comp(pi4, 3.141)\\nFalse\\n>>> comp(pi4, 3.143)\\nFalse\\n\\nA comparison of strings will be made\\nif ``z1`` is a Number and ``z2`` is a string or ``tol`` is ''.\\n\\n>>> comp(pi4, 3.1415)\\nTrue\\n>>> comp(pi4, 3.1415, '')\\nFalse\\n\\nWhen ``tol`` is provided and $z2$ is non-zero and\\n:math:`|z1| > 1` the error is normalized by :math:`|z1|`:\\n\\n>>> abs(pi4 - 3.14)/pi4\\n0.000509791731426756\\n>>> comp(pi4, 3.14, .001)  # difference less than 0.1%\\nTrue\\n>>> comp(pi4, 3.14, .0005)  # difference less than 0.1%\\nFalse\\n\\nWhen :math:`|z1| \\\\le 1` the absolute error is used:\\n\\n>>> 1/pi4\\n0.3183\\n>>> abs(1/pi4 - 0.3183)/(1/pi4)\\n3.07371499106316e-5\\n>>> abs(1/pi4 - 0.3183)\\n9.78393554684764e-6\\n>>> comp(1/pi4, 0.3183, 1e-5)\\nTrue\\n\\nTo see if the absolute error between ``z1`` and ``z2`` is less\\nthan or equal to ``tol``, call this as ``comp(z1 - z2, 0, tol)``\\nor ``comp(z1 - z2, tol=tol)``:\\n\\n>>> abs(pi4 - 3.14)\\n0.00160156249999988\\n>>> comp(pi4 - 3.14, 0, .002)\\nTrue\\n>>> comp(pi4 - 3.14, 0, .001)\\nFalse\",\n",
       " 'Converts a ``Feedback`` object to SymPy Expr.\\n\\nExamples\\n========\\n\\n>>> from sympy.abc import s, a, b\\n>>> from sympy.physics.control.lti import TransferFunction, Feedback\\n>>> from sympy import Expr\\n>>> tf1 = TransferFunction(a+s, 1, s)\\n>>> tf2 = TransferFunction(b+s, 1, s)\\n>>> fd1 = Feedback(tf1, tf2)\\n>>> fd1.to_expr()\\n(a + s)/((a + s)*(b + s) + 1)\\n>>> isinstance(_, Expr)\\nTrue',\n",
       " 'Returns the number of inputs of the system.',\n",
       " 'Returns state space model where numerical expressions are evaluated into floating point numbers.',\n",
       " 'Convert a mpmath ``mpf`` object to ``dtype``. ',\n",
       " 'This supports the functions that compute Hermite Normal Form.\\n\\nExplanation\\n===========\\n\\nLet x, y be the coefficients returned by the extended Euclidean\\nAlgorithm, so that x*a + y*b = g. In the algorithms for computing HNF,\\nit is critical that x, y not only satisfy the condition of being small\\nin magnitude -- namely that |x| <= |b|/g, |y| <- |a|/g -- but also that\\ny == 0 when a | b.',\n",
       " \"Reduce a system of inequalities with nested absolute values.\\n\\nExamples\\n========\\n\\n>>> from sympy import reduce_abs_inequalities, Abs, Symbol\\n>>> x = Symbol('x', extended_real=True)\\n\\n>>> reduce_abs_inequalities([(Abs(3*x - 5) - 7, '<'),\\n... (Abs(x + 25) - 13, '>')], x)\\n(-2/3 < x) & (x < 4) & (((-oo < x) & (x < -38)) | ((-12 < x) & (x < oo)))\\n\\n>>> reduce_abs_inequalities([(Abs(x - 4) + Abs(3*x - 5) - 7, '<')], x)\\n(1/2 < x) & (x < 4)\\n\\nSee Also\\n========\\n\\nreduce_abs_inequality\",\n",
       " 'The usage is simple: you just pass the match method the current\\npath info as well as the method (which defaults to `GET`).  The\\nfollowing things can then happen:\\n\\n- you receive a `NotFound` exception that indicates that no URL is\\n  matching.  A `NotFound` exception is also a WSGI application you\\n  can call to get a default page not found page (happens to be the\\n  same object as `werkzeug.exceptions.NotFound`)\\n\\n- you receive a `MethodNotAllowed` exception that indicates that there\\n  is a match for this URL but not for the current request method.\\n  This is useful for RESTful applications.\\n\\n- you receive a `RequestRedirect` exception with a `new_url`\\n  attribute.  This exception is used to notify you about a request\\n  Werkzeug requests from your WSGI application.  This is for example the\\n  case if you request ``/foo`` although the correct URL is ``/foo/``\\n  You can use the `RequestRedirect` instance as response-like object\\n  similar to all other subclasses of `HTTPException`.\\n\\n- you receive a ``WebsocketMismatch`` exception if the only\\n  match is a WebSocket rule but the bind is an HTTP request, or\\n  if the match is an HTTP rule but the bind is a WebSocket\\n  request.\\n\\n- you get a tuple in the form ``(endpoint, arguments)`` if there is\\n  a match (unless `return_rule` is True, in which case you get a tuple\\n  in the form ``(rule, arguments)``)\\n\\nIf the path info is not passed to the match method the default path\\ninfo of the map is used (defaults to the root URL if not defined\\nexplicitly).\\n\\nAll of the exceptions raised are subclasses of `HTTPException` so they\\ncan be used as WSGI responses. They will all render generic error or\\nredirect pages.\\n\\nHere is a small example for matching:\\n\\n>>> m = Map([\\n...     Rule(\\'/\\', endpoint=\\'index\\'),\\n...     Rule(\\'/downloads/\\', endpoint=\\'downloads/index\\'),\\n...     Rule(\\'/downloads/<int:id>\\', endpoint=\\'downloads/show\\')\\n... ])\\n>>> urls = m.bind(\"example.com\", \"/\")\\n>>> urls.match(\"/\", \"GET\")\\n(\\'index\\', {})\\n>>> urls.match(\"/downloads/42\")\\n(\\'downloads/show\\', {\\'id\\': 42})\\n\\nAnd here is what happens on redirect and missing URLs:\\n\\n>>> urls.match(\"/downloads\")\\nTraceback (most recent call last):\\n  ...\\nRequestRedirect: http://example.com/downloads/\\n>>> urls.match(\"/missing\")\\nTraceback (most recent call last):\\n  ...\\nNotFound: 404 Not Found\\n\\n:param path_info: the path info to use for matching.  Overrides the\\n                  path info specified on binding.\\n:param method: the HTTP method used for matching.  Overrides the\\n               method specified on binding.\\n:param return_rule: return the rule that matched instead of just the\\n                    endpoint (defaults to `False`).\\n:param query_args: optional query arguments that are used for\\n                   automatic redirects as string or dictionary.  It\\'s\\n                   currently not possible to use the query arguments\\n                   for URL matching.\\n:param websocket: Match WebSocket instead of HTTP requests. A\\n    websocket request has a ``ws`` or ``wss``\\n    :attr:`url_scheme`. This overrides that detection.\\n\\n.. versionadded:: 1.0\\n    Added ``websocket``.\\n\\n.. versionchanged:: 0.8\\n    ``query_args`` can be a string.\\n\\n.. versionadded:: 0.7\\n    Added ``query_args``.\\n\\n.. versionadded:: 0.6\\n    Added ``return_rule``.',\n",
       " 'Sets vertical border characters.\\n\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\nâ•‘ ISBN          â”‚ Title                    â”‚ Author           â•‘\\nâ• â•â•â•â•â•â•â•1â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\\nâ•‘ 99921-58-10-7 â”‚ Divine Comedy            â”‚ Dante Alighieri  â•‘\\nâ•‘ 9971-5-0210-0 â”‚ A Tale of Two Cities     â”‚ Charles Dickens  â•‘\\nâ•Ÿâ”€â”€â”€â”€â”€â”€â”€2â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢\\nâ•‘ 960-425-059-0 â”‚ The Lord of the Rings    â”‚ J. R. R. Tolkien â•‘\\nâ•‘ 80-902734-1-6 â”‚ And Then There Were None â”‚ Agatha Christie  â•‘\\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•',\n",
       " 'Initialize the metric object as a child, i.e. when it has labels (if any) set.\\n\\nThis is factored as a separate function to allow for deferred initialization.',\n",
       " 'Concatenate a list of points or None into a single point array or None, with NaNs used to\\nseparate each line.',\n",
       " 'How much RAM is this process using? (Windows)',\n",
       " \"Returns the source and destination addresses of the last taken branch.\\n\\n@rtype: tuple( int, int )\\n@return: Source and destination addresses of the last taken branch.\\n\\n@raise WindowsError:\\n    Raises an exception on error.\\n\\n@raise NotImplementedError:\\n    Current architecture is not C{i386} or C{amd64}.\\n\\n@warning:\\n    This method uses the processor's machine specific registers (MSR).\\n    It could potentially brick your machine.\\n    It works on my machine, but your mileage may vary.\\n\\n@note:\\n    It doesn't seem to work in VMWare or VirtualBox machines.\\n    Maybe it fails in other virtualization/emulation environments,\\n    no extensive testing was made so far.\",\n",
       " 'Disassemble instructions from the address space of the process.\\n\\n@type  lpAddress: int\\n@param lpAddress: Memory address where to read the code from.\\n\\n@type  dwSize: int\\n@param dwSize: Size of binary code to disassemble.\\n\\n@rtype:  list of tuple( long, int, str, str )\\n@return: List of tuples. Each tuple represents an assembly instruction\\n    and contains:\\n     - Memory address of instruction.\\n     - Size of instruction in bytes.\\n     - Disassembly line of instruction.\\n     - Hexadecimal dump of instruction.',\n",
       " 'Return a tuple of arguments that must be passed to __new__ in order to support pickling this object.',\n",
       " 'Extract the URL prefix from a regex match.\\n\\nArgs:\\n  mat: A regex match object.\\nReturns: The URL prefix, defined as the text before the match in the\\n    original string. Normalized to start with one leading slash and end\\n    with zero.',\n",
       " 'Decorate a WebSocket function.\\n\\nRead more about it in the\\n[FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\\n\\n**Example**\\n\\n```python\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```',\n",
       " 'Check that /items/{item_id} returns expected data',\n",
       " 'Post-rpc interceptor for list_api_operations\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ApiHub server but before\\nit is returned to user code.',\n",
       " 'If the given type is `typing.Iterable[T]`',\n",
       " 'Call the update backup method over HTTP.\\n\\nArgs:\\n    request (~.backupvault.UpdateBackupRequest):\\n        The request object. Request message for updating a\\n    Backup.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return a single Task.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import batch_v1\\n\\n    def sample_get_task():\\n        # Create a client\\n        client = batch_v1.BatchServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = batch_v1.GetTaskRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_task(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.batch_v1.types.GetTaskRequest, dict]):\\n        The request object. Request for a single Task by name.\\n    name (str):\\n        Required. Task name.\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.batch_v1.types.Task:\\n        A Cloud Batch task.',\n",
       " 'Sets the default network tier of the project. The\\ndefault network tier is used when an\\naddress/forwardingRule/instance is created without\\nspecifying the network tier field.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_default_network_tier():\\n        # Create a client\\n        client = compute_v1.ProjectsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetDefaultNetworkTierProjectRequest(\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_default_network_tier(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetDefaultNetworkTierProjectRequest, dict]):\\n        The request object. A request message for\\n        Projects.SetDefaultNetworkTier. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    projects_set_default_network_tier_request_resource (google.cloud.compute_v1.types.ProjectsSetDefaultNetworkTierRequest):\\n        The body resource for this request\\n        This corresponds to the ``projects_set_default_network_tier_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Post-rpc interceptor for get_participant\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'Call the get method over HTTP.\\n\\nArgs:\\n    request (~.compute.GetPublicDelegatedPrefixeRequest):\\n        The request object. A request message for\\n    PublicDelegatedPrefixes.Get. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.PublicDelegatedPrefix:\\n        A PublicDelegatedPrefix resource\\n    represents an IP block within a\\n    PublicAdvertisedPrefix that is\\n    configured within a single cloud scope\\n    (global or region). IPs in the block can\\n    be allocated to resources within that\\n    scope. Public delegated prefixes may be\\n    further broken up into smaller IP blocks\\n    in the same scope as the parent block.',\n",
       " 'Post-rpc interceptor for compute_repository_access_token_status\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Dataform server but before\\nit is returned to user code.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsRequest):\\n        The initial request object.\\n    response (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Pre-rpc interceptor for alter_metadata_resource_location\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DataprocMetastore server.',\n",
       " 'Post-rpc interceptor for submit_answer_feedback\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Sessions server but before\\nit is returned to user code.',\n",
       " 'Call the create discovery config method over HTTP.\\n\\nArgs:\\n    request (~.dlp.CreateDiscoveryConfigRequest):\\n        The request object. Request message for\\n    CreateDiscoveryConfig.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.dlp.DiscoveryConfig:\\n        Configuration for discovery to scan resources for\\n    profile generation. Only one discovery configuration may\\n    exist per organization, folder, or project.\\n\\n    The generated data profiles are retained according to\\n    the [data retention policy]\\n    (https://cloud.google.com/sensitive-data-protection/docs/data-profiles#retention).',\n",
       " 'Set the session to be used when the TLS/SSL connection is established.\\n\\n:param session: A Session instance representing the session to use.\\n:returns: None\\n\\n.. versionadded:: 0.14',\n",
       " 'Call the list processor types method over HTTP.\\n\\nArgs:\\n    request (~.document_processor_service.ListProcessorTypesRequest):\\n        The request object. Request message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method. Some processor types may require the project be\\n    added to an allowlist.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_processor_service.ListProcessorTypesResponse:\\n        Response message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method.',\n",
       " 'Return a callable for the delete http route method over gRPC.\\n\\nDeletes a single HttpRoute.\\n\\nReturns:\\n    Callable[[~.DeleteHttpRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a collector path into its component segments.',\n",
       " 'Returns the specified key.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import recaptchaenterprise_v1\\n\\n    def sample_get_key():\\n        # Create a client\\n        client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = recaptchaenterprise_v1.GetKeyRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_key(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.recaptchaenterprise_v1.types.GetKeyRequest, dict]):\\n        The request object. The get key request message.\\n    name (str):\\n        Required. The name of the requested key, in the format\\n        ``projects/{project}/keys/{key}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.recaptchaenterprise_v1.types.Key:\\n        A key used to identify and configure\\n        applications (web and/or mobile) that\\n        use reCAPTCHA Enterprise.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Returns a fully-qualified mute_config string.',\n",
       " 'Post-rpc interceptor for list_effective_event_threat_detection_custom_modules\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SecurityCenter server but before\\nit is returned to user code.',\n",
       " '`Connection.bio_read` raises `TypeError` if passed a non-integer\\nargument.',\n",
       " 'Lists all AdaptiveMtFiles associated to an\\nAdaptiveMtDataset.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import translate_v3\\n\\n    def sample_list_adaptive_mt_files():\\n        # Create a client\\n        client = translate_v3.TranslationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = translate_v3.ListAdaptiveMtFilesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_adaptive_mt_files(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.translate_v3.types.ListAdaptiveMtFilesRequest, dict]):\\n        The request object. The request to list all AdaptiveMt\\n        files under a given dataset.\\n    parent (str):\\n        Required. The resource name of the project from which to\\n        list the Adaptive MT files.\\n        ``projects/{project}/locations/{location}/adaptiveMtDatasets/{dataset}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.translate_v3.services.translation_service.pagers.ListAdaptiveMtFilesPager:\\n        The response for listing all\\n        AdaptiveMt files under a given dataset.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Call the delete application method over HTTP.\\n\\nArgs:\\n    request (~.platform.DeleteApplicationRequest):\\n        The request object. Message for deleting an Application.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return a callable for the create private connection method over gRPC.\\n\\nCreates a new private connection that can be used for\\naccessing private Clouds.\\n\\nReturns:\\n    Callable[[~.CreatePrivateConnectionRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Convert a av.VideoFrame into a ndarray\\n\\nParameters\\n----------\\nframe : av.VideoFrame\\n    The frame to unpack.\\nformat : str\\n    If not None, convert the frame to the given format before unpacking.',\n",
       " 'Loads the encoded object. This function raises\\n:class:`.BadPayload` if the payload is not valid. The\\n``serializer`` parameter can be used to override the serializer\\nstored on the class. The encoded ``payload`` should always be\\nbytes.',\n",
       " 'Call the callable with the arguments and keyword arguments\\nprovided but inject the active context or environment as first\\nargument if the callable has :func:`pass_context` or\\n:func:`pass_environment`.',\n",
       " 'Format a date the way Atom likes it (RFC3339?)',\n",
       " 'Removes PEM-encoding from a public key, private key or certificate. If the\\nprivate key is encrypted, the password will be used to decrypt it.\\n\\n:param data:\\n    A byte string of the PEM-encoded data\\n\\n:param password:\\n    A byte string of the encryption password, or None\\n\\n:return:\\n    A 3-element tuple in the format: (key_type, algorithm, der_bytes). The\\n    key_type will be a unicode string of \"public key\", \"private key\" or\\n    \"certificate\". The algorithm will be a unicode string of \"rsa\", \"dsa\"\\n    or \"ec\".',\n",
       " 'Calculate any free parameters based on the current cmap and norm,\\nand do all the drawing.',\n",
       " 'Generate a new value for this ObjectId.',\n",
       " 'GH 43787\\n\\nTest correct handling of UTF-8 chars when memory_map=True and encoding is UTF-8',\n",
       " \"Returns a bipartite graph from two given degree sequences using a\\nHavel-Hakimi style construction.\\n\\nThe graph is composed of two partitions. Set A has nodes 0 to\\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\\nNodes from the set A are connected to nodes in the set B by\\nconnecting the highest degree nodes in set A to the highest degree\\nnodes in set B until all stubs are connected.\\n\\nParameters\\n----------\\naseq : list\\n   Degree sequence for node set A.\\nbseq : list\\n   Degree sequence for node set B.\\ncreate_using : NetworkX graph instance, optional\\n   Return graph of this type.\\n\\nNotes\\n-----\\nThe sum of the two sequences must be equal: sum(aseq)=sum(bseq)\\nIf no graph type is specified use MultiGraph with parallel edges.\\nIf you want a graph with no parallel edges use create_using=Graph()\\nbut then the resulting degree sequences might not be exact.\\n\\nThe nodes are assigned the attribute 'bipartite' with the value 0 or 1\\nto indicate which bipartite set the node belongs to.\\n\\nThis function is not imported in the main namespace.\\nTo use it use nx.bipartite.havel_hakimi_graph\",\n",
       " 'Tests for forbidding self-loops.',\n",
       " 'Return a list of -arch flags for every supported architecture.',\n",
       " 'Tests that it is possible to return to file processing mode\\nCLI :: :\\nBUG: numpy-gh #20520',\n",
       " 'Test that the API Counter is an abstract class.',\n",
       " 'The time the process has been running',\n",
       " 'TAG[.postDISTANCE[.dev0]+gHEX] .\\n\\nThe \".dev0\" means dirty. Note that .dev0 sorts backwards\\n(a dirty tree will appear \"older\" than the corresponding clean one),\\nbut you shouldn\\'t be releasing software with -dirty anyways.\\n\\nExceptions:\\n1: no tags. 0.postDISTANCE[.dev0]',\n",
       " 'Create a new DataFrame by selecting a subset of columns by index.',\n",
       " 'Highlight the minimum with a style.\\n\\nParameters\\n----------\\n%(subset)s\\n%(color)s\\naxis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n    Apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n    (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n    with ``axis=None``.\\n%(props)s\\n\\n    .. versionadded:: 1.3.0\\n\\nReturns\\n-------\\nStyler\\n    Instance of class where min value is highlighted in given style.\\n\\nSee Also\\n--------\\nStyler.highlight_null: Highlight missing values with a style.\\nStyler.highlight_max: Highlight the maximum with a style.\\nStyler.highlight_between: Highlight a defined range with a style.\\nStyler.highlight_quantile: Highlight values defined by a quantile with a style.\\n\\nExamples\\n--------\\n>>> df = pd.DataFrame({\"A\": [2, 1], \"B\": [3, 4]})\\n>>> df.style.highlight_min(color=\"yellow\")  # doctest: +SKIP\\n\\nPlease see:\\n`Table Visualization <../../user_guide/style.ipynb>`_ for more examples.',\n",
       " \"Convert css-string to sequence of tuples format if needed.\\n'color:red; border:1px solid black;' -> [('color', 'red'),\\n                                         ('border','1px solid red')]\",\n",
       " 'verify that select works when a channel is already closed.',\n",
       " 'Normalize the given rows of a RECORD file.\\n\\nItems in each row are converted into str. Rows are then sorted to make\\nthe value more predictable for tests.\\n\\nEach row is a 3-tuple (path, hash, size) and corresponds to a record of\\na RECORD file (see PEP 376 and PEP 427 for details).  For the rows\\npassed to this function, the size can be an integer as an int or string,\\nor the empty string.',\n",
       " 'Return our best guess of encoding for the given *term*.',\n",
       " \"Verifies that header parts don't contain leading whitespace\\nreserved characters, or return characters.\\n\\n:param header: tuple, in the format (name, value).\",\n",
       " 'Get the current row style.',\n",
       " 'Test that wrong hash in direct URL dependency stops installation.',\n",
       " 'Test that we fall back to setuptools develop when using a backend that\\ndoes not support build_editable. Since there is a pyproject.toml,\\nthe prepare_metadata_for_build_wheel hook is called.',\n",
       " 'Test the log message for an invalid Requires-Python.',\n",
       " 'Evaluate a polynomial at ``x_0 = a`` in ``K[X]`` using the Horner scheme.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x,y = ring(\"x,y\", ZZ)\\n\\n>>> R.dmp_eval(2*x*y + 3*x + y + 2, 2)\\n5*y + 8',\n",
       " 'The source type of system site packages\\nmust not be falsely identified as \"directory\".',\n",
       " 'commit with support for non-recursive commits ',\n",
       " 'Builds a function definition.\\n        ',\n",
       " 'struct_or_union : STRUCT\\n| UNION',\n",
       " 'Check if field name is in validator fields.\\n\\nArgs:\\n    info: The field info.\\n    field: The field name to check.\\n\\nReturns:\\n    `True` if field name is in validator fields, `False` otherwise.',\n",
       " 'Tests that a SymilarChecker can return and reduce mapped data.',\n",
       " 'list index is a str constant',\n",
       " 'Say it load',\n",
       " 'An assignment assumed to execute in one Try should continue to be\\nassumed to execute in a consecutive Try.',\n",
       " 'https://github.com/pylint-dev/pylint/issues/5965',\n",
       " \"Tests that an error in the initializer for the parallel jobs doesn't\\nlead to a deadlock.\",\n",
       " 'Import and return a module from the given path, which can be a file (a module) or\\na directory (a package).\\n\\n:param path:\\n    Path to the file to import.\\n\\n:param mode:\\n    Controls the underlying import mechanism that will be used:\\n\\n    * ImportMode.prepend: the directory containing the module (or package, taking\\n      `__init__.py` files into account) will be put at the *start* of `sys.path` before\\n      being imported with `importlib.import_module`.\\n\\n    * ImportMode.append: same as `prepend`, but the directory will be appended\\n      to the end of `sys.path`, if not already in `sys.path`.\\n\\n    * ImportMode.importlib: uses more fine control mechanisms provided by `importlib`\\n      to import the module, which avoids having to muck with `sys.path` at all. It effectively\\n      allows having same-named test modules in different places.\\n\\n:param root:\\n    Used as an anchor when mode == ImportMode.importlib to obtain\\n    a unique name for the module being imported so it can safely be stored\\n    into ``sys.modules``.\\n\\n:param consider_namespace_packages:\\n    If True, consider namespace packages when resolving module names.\\n\\n:raises ImportPathMismatchError:\\n    If after importing the given `path` and the module `__file__`\\n    are different. Only raised in `prepend` and `append` modes.',\n",
       " 'Shortcut for .makefile() with a .txt extension.\\n\\nDefaults to the test name with a \\'.txt\\' extension, e.g test_foobar.txt, overwriting\\nexisting files.\\n\\nExamples:\\n\\n.. code-block:: python\\n\\n    def test_something(pytester):\\n        # Initial file is created test_something.txt.\\n        pytester.maketxtfile(\"foobar\")\\n        # To create multiple files, pass kwargs accordingly.\\n        pytester.maketxtfile(custom=\"foobar\")\\n        # At this point, both \\'test_something.txt\\' & \\'custom.txt\\' exist in the test directory.',\n",
       " 'Check asynctest support (#7110)',\n",
       " 'As above, but explicitly feeding in a long on Py2. Note that\\nchecks like:\\n    isinstance(n, int)\\nare fragile on Py2, because isinstance(10L, int) is False.',\n",
       " \"It would nice if the b'' literal syntax could be coaxed into producing\\nbytes objects somehow ... ;)\",\n",
       " 'Test spec addition using :data:`+=` operator.',\n",
       " 'delete_validating_admission_policy  # noqa: E501\\n\\ndelete a ValidatingAdmissionPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_validating_admission_policy_with_http_info(name, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ValidatingAdmissionPolicy (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'get_api_versions  # noqa: E501\\n\\nget available API versions  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.get_api_versions_with_http_info(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1APIGroupList, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Gets the git_repo of this V1Volume.  # noqa: E501\\n\\n\\n:return: The git_repo of this V1Volume.  # noqa: E501\\n:rtype: V1GitRepoVolumeSource',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Re-subscribe to any channels and patterns previously subscribed to',\n",
       " 'When out of connections, block until another connection is released\\nto the pool',\n",
       " 'Appends an arbitrary number of strings to use as path constants',\n",
       " 'Search recommendation based on recommendation id',\n",
       " 'Get a metric group.\\n\\nArgs:\\n    group_name (str): The metric group name.',\n",
       " 'Check that check_is_fitted passes for stateless estimators.',\n",
       " 'Entry point for bootstrap script',\n",
       " 'Whitespace sensitive char-n-gram tokenization.\\n\\nTokenize text_document into a sequence of character n-grams\\noperating only inside word boundaries. n-grams at the edges\\nof words are padded with space.',\n",
       " 'Returns whether the kernel is stationary.',\n",
       " 'Is there a file at the given path',\n",
       " 'Objective function',\n",
       " 'Generate argument list for calling the C++ function',\n",
       " 'Merge multiple polygons into one.\\n\\nThis is an optimized version of union which assumes the polygons to be\\nnon-overlapping.\\n\\nParameters\\n----------\\na, b : Geometry or array_like\\n    Geometry or geometries to merge (union).\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\ncoverage_union_all\\n\\nExamples\\n--------\\n>>> from shapely import normalize, Polygon\\n>>> polygon = Polygon([(0, 0), (0, 1), (1, 1), (1, 0), (0, 0)])\\n>>> normalize(coverage_union(polygon, Polygon([(1, 0), (1, 1), (2, 1), (2, 0), (1, 0)])))\\n<POLYGON ((0 0, 0 1, 1 1, 2 1, 2 0, 1 0, 0 0))>\\n\\nUnion with None returns same polygon\\n\\n>>> normalize(coverage_union(polygon, None))\\n<POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))>',\n",
       " 'Test when there is no root due to double root tags.',\n",
       " 'Return any actually-specified arguments.',\n",
       " 'Check if a file uses PySpark-specific errors correctly.\\n\\nParameters\\n----------\\nfile_path : str\\n    Path to the file to check.\\npyspark_error_list : list of str\\n    List of PySpark-specific error names.\\n\\nReturns\\n-------\\nlist of str\\n    A list of strings describing the errors found in the file, with line numbers.',\n",
       " 'Gets the value of numPartitions or its default value.',\n",
       " 'Saves metadata + Params to: path + \"/metadata\"\\n\\n- class\\n- timestamp\\n- sparkVersion\\n- uid\\n- paramMap\\n- defaultParamMap (since 2.4.0)\\n- (optionally, extra metadata)\\n\\nParameters\\n----------\\nextraMetadata : dict, optional\\n    Extra metadata to be saved at same level as uid, paramMap, etc.\\nparamMap : dict, optional\\n    If given, this is saved in the \"paramMap\" field.',\n",
       " 'Convert string styled html_css_files to tuple styled one.',\n",
       " 'this is func1',\n",
       " 'Remove the remote with the given name.\\n\\n:return:\\n    The passed remote name to remove',\n",
       " 'Add one or more :class:`_sql.CTE` constructs to this statement.\\n\\nThis method will associate the given :class:`_sql.CTE` constructs with\\nthe parent statement such that they will each be unconditionally\\nrendered in the WITH clause of the final statement, even if not\\nreferenced elsewhere within the statement or any sub-selects.\\n\\nThe optional :paramref:`.HasCTE.add_cte.nest_here` parameter when set\\nto True will have the effect that each given :class:`_sql.CTE` will\\nrender in a WITH clause rendered directly along with this statement,\\nrather than being moved to the top of the ultimate rendered statement,\\neven if this statement is rendered as a subquery within a larger\\nstatement.\\n\\nThis method has two general uses. One is to embed CTE statements that\\nserve some purpose without being referenced explicitly, such as the use\\ncase of embedding a DML statement such as an INSERT or UPDATE as a CTE\\ninline with a primary statement that may draw from its results\\nindirectly.  The other is to provide control over the exact placement\\nof a particular series of CTE constructs that should remain rendered\\ndirectly in terms of a particular statement that may be nested in a\\nlarger statement.\\n\\nE.g.::\\n\\n    from sqlalchemy import table, column, select\\n    t = table(\\'t\\', column(\\'c1\\'), column(\\'c2\\'))\\n\\n    ins = t.insert().values({\"c1\": \"x\", \"c2\": \"y\"}).cte()\\n\\n    stmt = select(t).add_cte(ins)\\n\\nWould render::\\n\\n    WITH anon_1 AS\\n    (INSERT INTO t (c1, c2) VALUES (:param_1, :param_2))\\n    SELECT t.c1, t.c2\\n    FROM t\\n\\nAbove, the \"anon_1\" CTE is not referenced in the SELECT\\nstatement, however still accomplishes the task of running an INSERT\\nstatement.\\n\\nSimilarly in a DML-related context, using the PostgreSQL\\n:class:`_postgresql.Insert` construct to generate an \"upsert\"::\\n\\n    from sqlalchemy import table, column\\n    from sqlalchemy.dialects.postgresql import insert\\n\\n    t = table(\"t\", column(\"c1\"), column(\"c2\"))\\n\\n    delete_statement_cte = (\\n        t.delete().where(t.c.c1 < 1).cte(\"deletions\")\\n    )\\n\\n    insert_stmt = insert(t).values({\"c1\": 1, \"c2\": 2})\\n    update_statement = insert_stmt.on_conflict_do_update(\\n        index_elements=[t.c.c1],\\n        set_={\\n            \"c1\": insert_stmt.excluded.c1,\\n            \"c2\": insert_stmt.excluded.c2,\\n        },\\n    ).add_cte(delete_statement_cte)\\n\\n    print(update_statement)\\n\\nThe above statement renders as::\\n\\n    WITH deletions AS\\n    (DELETE FROM t WHERE t.c1 < %(c1_1)s)\\n    INSERT INTO t (c1, c2) VALUES (%(c1)s, %(c2)s)\\n    ON CONFLICT (c1) DO UPDATE SET c1 = excluded.c1, c2 = excluded.c2\\n\\n.. versionadded:: 1.4.21\\n\\n:param \\\\*ctes: zero or more :class:`.CTE` constructs.\\n\\n .. versionchanged:: 2.0  Multiple CTE instances are accepted\\n\\n:param nest_here: if True, the given CTE or CTEs will be rendered\\n as though they specified the :paramref:`.HasCTE.cte.nesting` flag\\n to ``True`` when they were added to this :class:`.HasCTE`.\\n Assuming the given CTEs are not referenced in an outer-enclosing\\n statement as well, the CTEs given should render at the level of\\n this statement when this flag is given.\\n\\n .. versionadded:: 2.0\\n\\n .. seealso::\\n\\n    :paramref:`.HasCTE.cte.nesting`',\n",
       " 'test #8759.\\n\\nthis should raise an error.',\n",
       " 'same as precision_numerics_many_significant_digits but within the\\ncontext of a CAST statement (hello MySQL)',\n",
       " 'Handler for the absolute value.\\n\\nExamples\\n========\\n\\n>>> from sympy import Q, Abs\\n>>> from sympy.assumptions.refine import refine_abs\\n>>> from sympy.abc import x\\n>>> refine_abs(Abs(x), Q.real(x))\\n>>> refine_abs(Abs(x), Q.positive(x))\\nx\\n>>> refine_abs(Abs(x), Q.negative(x))\\n-x',\n",
       " 'return the non-strict version of the inequality or self\\n\\nEXAMPLES\\n========\\n\\n>>> from sympy.abc import x\\n>>> (x < 1).weak\\nx <= 1\\n>>> _.weak\\nx <= 1',\n",
       " 'Performs the Lenstraâ€“Lenstraâ€“LovÃ¡sz (LLL) basis reduction algorithm\\nand returns the reduced basis and transformation matrix.\\n\\nExplanation\\n===========\\n\\nParameters, algorithm and basis are the same as for :meth:`lll` except that\\nthe return value is a tuple `(B, T)` with `B` the reduced basis and\\n`T` a transformation matrix. The original basis `A` is transformed to\\n`B` with `T*A == B`. If only `B` is needed then :meth:`lll` should be\\nused as it is a little faster.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.domains import ZZ, QQ\\n>>> from sympy.polys.matrices import DM\\n>>> X = DM([[1, 0, 0, 0, -20160],\\n...         [0, 1, 0, 0, 33768],\\n...         [0, 0, 1, 0, 39578],\\n...         [0, 0, 0, 1, 47757]], ZZ)\\n>>> B, T = X.lll_transform(delta=QQ(5, 6))\\n>>> T * X == B\\nTrue\\n\\nSee also\\n========\\n\\nlll',\n",
       " 'Iterate over a mapping that might have a list of values, yielding\\nall key, value pairs. Almost like iter_multi_items but only allows\\nlists, not tuples, of values so tuples can be used for files.',\n",
       " 'Exhaust the stream by reading until the limit is reached or the client\\ndisconnects, returning the remaining data.\\n\\n.. versionchanged:: 2.3\\n    Return the remaining data.\\n\\n.. versionchanged:: 2.2.3\\n    Handle case where wrapped stream returns fewer bytes than requested.',\n",
       " \"An HTTPS request to an HTTP server doesn't show a traceback.\\nhttps://github.com/pallets/werkzeug/pull/838\",\n",
       " 'Include the given datetime instance in the recurrence set\\nexclusion list. Dates included that way will not be generated,\\neven if some inclusive rrule or rdate matches them. ',\n",
       " 'Send an item immediately if it can be done without waiting.\\n\\n:param item: the item to send\\n:raises ~anyio.ClosedResourceError: if this send stream has been closed\\n:raises ~anyio.BrokenResourceError: if the stream has been closed from the\\n    receiving end\\n:raises ~anyio.WouldBlock: if the buffer is full and there are no tasks waiting\\n    to receive',\n",
       " 'Returns whether node is a statement node.',\n",
       " 'Handle wr.s3.read_parquet_metadata internally.',\n",
       " 'Pre-rpc interceptor for get_aws_open_id_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AwsClusters server.',\n",
       " 'Yields:\\n    All ranges of @string which, if @string were to be split there,\\n    would result in the splitting of an f-expression (which is NOT\\n    allowed).',\n",
       " 'Store one or more lines of input.\\n\\nIf input lines are not newline-terminated, a newline is automatically\\nappended.',\n",
       " \"Windows: if 'enabled_flag' is True, enable the UNICODE and\\n_UNICODE defines in C, and declare the types like TCHAR and LPTCSTR\\nto be (pointers to) wchar_t.  If 'enabled_flag' is False,\\ndeclare these types to be (pointers to) plain 8-bit characters.\\nThis is mostly for backward compatibility; you usually want True.\",\n",
       " 'Decide whether to trace execution in `filename`.\\n\\nCalls `_should_trace_internal`, and returns the FileDisposition.',\n",
       " \"Assert that the hrefs in htmlcov/*.html are valid.\\n\\nDoesn't check external links (those with a protocol).\",\n",
       " \"Return the base directory containing Cython's caches.\\n\\nPriority:\\n\\n1. CYTHON_CACHE_DIR\\n2. (OS X): ~/Library/Caches/Cython\\n   (posix not OS X): XDG_CACHE_HOME/cython if XDG_CACHE_HOME defined\\n3. ~/.cython\",\n",
       " 'Post a metric.',\n",
       " ':param string type:\\n:param string event:\\n:param StoppedEventBody body:\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.',\n",
       " ':param array breakpoints: The instruction references of the breakpoints',\n",
       " 'Iterate over the entries in this pack index.\\n\\nReturns: iterator over tuples with object name, offset in packfile and\\n    crc32 checksum.',\n",
       " 'Iterate over cached submodules.\\n\\nArgs:\\n  store: Object store to iterate\\n  root_tree_id: SHA of root tree\\n\\nReturns:\\n  Iterator over over (path, sha) tuples',\n",
       " 'Releases the file lock. Please note, that the lock is only completely released, if the lock counter is 0.\\nAlso note, that the lock file itself is not automatically deleted.\\n\\n:param force: If true, the lock counter is ignored and the lock is released in every case/',\n",
       " 'Parses a custom_metric path into its component segments.',\n",
       " 'Pre-rpc interceptor for update_enhanced_measurement_settings\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'Post-rpc interceptor for update_repository\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ArtifactRegistry server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the list catalogs method over gRPC.\\n\\nList all catalogs in a specified project.\\n\\nReturns:\\n    Callable[[~.ListCatalogsRequest],\\n            ~.ListCatalogsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Post-rpc interceptor for get_system_policy\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SystemPolicyV1 server but before\\nit is returned to user code.',\n",
       " 'Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'compute.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'Deletes the specified target pool.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.TargetPoolsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteTargetPoolRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n            target_pool=\"target_pool_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteTargetPoolRequest, dict]):\\n        The request object. A request message for\\n        TargetPools.Delete. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    target_pool (str):\\n        Name of the TargetPool resource to\\n        delete.\\n\\n        This corresponds to the ``target_pool`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Call the delete method over HTTP.\\n\\nArgs:\\n    request (~.compute.DeleteVpnTunnelRequest):\\n        The request object. A request message for\\n    VpnTunnels.Delete. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Return a callable for the set logging service method over gRPC.\\n\\nSets the logging service for a specific cluster.\\n\\nReturns:\\n    Callable[[~.SetLoggingServiceRequest],\\n            Awaitable[~.Operation]]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Return a callable for the process open lineage run event method over gRPC.\\n\\nCreates new lineage events together with their\\nparents: process and run. Updates the process and run if\\nthey already exist. Mapped from Open Lineage\\nspecification:\\n\\nhttps://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json.\\n\\nReturns:\\n    Callable[[~.ProcessOpenLineageRunEventRequest],\\n            ~.ProcessOpenLineageRunEventResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataplex_v1.types.ListDataTaxonomiesRequest):\\n        The initial request object.\\n    response (google.cloud.dataplex_v1.types.ListDataTaxonomiesResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Return a callable for the list session entity types method over gRPC.\\n\\nReturns the list of all session entity types in the\\nspecified session.\\n\\nReturns:\\n    Callable[[~.ListSessionEntityTypesRequest],\\n            ~.ListSessionEntityTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Bulk import of multiple\\n[Document][google.cloud.discoveryengine.v1.Document]s. Request\\nprocessing may be synchronous. Non-existing items are created.\\n\\nNote: It is possible for a subset of the\\n[Document][google.cloud.discoveryengine.v1.Document]s to be\\nsuccessfully updated.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1\\n\\n    def sample_import_documents():\\n        # Create a client\\n        client = discoveryengine_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = discoveryengine_v1.ImportDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.import_documents(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1.types.ImportDocumentsRequest, dict]):\\n        The request object. Request message for Import methods.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.discoveryengine_v1.types.ImportDocumentsResponse` Response of the\\n           [ImportDocumentsRequest][google.cloud.discoveryengine.v1.ImportDocumentsRequest].\\n           If the long running operation is done, then this\\n           message is returned by the\\n           google.longrunning.Operations.response field if the\\n           operation was successful.',\n",
       " 'Call the update participant method over HTTP.\\n\\nArgs:\\n    request (~.gcd_participant.UpdateParticipantRequest):\\n        The request object. The request message for\\n    [Participants.UpdateParticipant][google.cloud.dialogflow.v2.Participants.UpdateParticipant].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.gcd_participant.Participant:\\n        Represents a conversation participant\\n    (human agent, virtual agent, end-user).',\n",
       " 'Return a callable for the batch create target sites method over gRPC.\\n\\nCreates [TargetSite][google.cloud.discoveryengine.v1.TargetSite]\\nin a batch.\\n\\nReturns:\\n    Callable[[~.BatchCreateTargetSitesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a file_store_data_profile path into its component segments.',\n",
       " 'Return a callable for the list volume backups method over gRPC.\\n\\nLists the VolumeBackups for a given Backup.\\n\\nReturns:\\n    Callable[[~.ListVolumeBackupsRequest],\\n            ~.ListVolumeBackupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AttachedClustersAsyncClient: The constructed client.',\n",
       " 'Resets an Identity Aware Proxy (IAP) OAuth client\\nsecret. Useful if the secret was compromised. Requires\\nthat the client is owned by IAP.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iap_v1\\n\\n    def sample_reset_identity_aware_proxy_client_secret():\\n        # Create a client\\n        client = iap_v1.IdentityAwareProxyOAuthServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = iap_v1.ResetIdentityAwareProxyClientSecretRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.reset_identity_aware_proxy_client_secret(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iap_v1.types.ResetIdentityAwareProxyClientSecretRequest, dict]):\\n        The request object. The request sent to\\n        ResetIdentityAwareProxyClientSecret.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iap_v1.types.IdentityAwareProxyClient:\\n        Contains the data that describes an\\n        Identity Aware Proxy owned client.',\n",
       " 'Call the list assets method over HTTP.\\n\\nArgs:\\n    request (~.migrationcenter.ListAssetsRequest):\\n        The request object. Message for requesting a list of\\n    assets.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.migrationcenter.ListAssetsResponse:\\n        Response message for listing assets.',\n",
       " 'Post-rpc interceptor for update_http_route\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the NetworkServices server but before\\nit is returned to user code.',\n",
       " 'Deletes a single Exadata Infrastructure.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import oracledatabase_v1\\n\\n    def sample_delete_cloud_exadata_infrastructure():\\n        # Create a client\\n        client = oracledatabase_v1.OracleDatabaseClient()\\n\\n        # Initialize request argument(s)\\n        request = oracledatabase_v1.DeleteCloudExadataInfrastructureRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_cloud_exadata_infrastructure(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.oracledatabase_v1.types.DeleteCloudExadataInfrastructureRequest, dict]):\\n        The request object. The request for ``CloudExadataInfrastructure.Delete``.\\n    name (str):\\n        Required. The name of the Cloud Exadata Infrastructure\\n        in the following format:\\n        projects/{project}/locations/{location}/cloudExadataInfrastructures/{cloud_exadata_infrastructure}.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Parses a ssh_public_key path into its component segments.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsRequest):\\n        The initial request object.\\n    response (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Create the client designed to process long-running operations.\\n\\nThis property caches on the instance; repeated calls return the same\\nclient.',\n",
       " 'Post-rpc interceptor for get_tag_value\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the TagValues server but before\\nit is returned to user code.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " ':param sec_key_ref:\\n    A Security framework SecKeyRef value from loading/importing the\\n    key\\n\\n:param asn1:\\n    An asn1crypto.keys.PrivateKeyInfo object',\n",
       " 'Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'run.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " 'Return a callable for the test iam permissions method over gRPC.\\n\\nReturns the permissions that a caller has on the\\nspecified source.\\n\\nReturns:\\n    Callable[[~.TestIamPermissionsRequest],\\n            ~.TestIamPermissionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list event threat\\ndetection custom modules method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.ListEventThreatDetectionCustomModulesRequest):\\n            The request object. Request to list Event Threat\\n        Detection custom modules.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.securitycenter_service.ListEventThreatDetectionCustomModulesResponse:\\n            Response for listing Event Threat\\n        Detection custom modules.',\n",
       " 'Post-rpc interceptor for get_company\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CompanyService server but before\\nit is returned to user code.',\n",
       " 'Searches across deployment revisions.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import telcoautomation_v1alpha1\\n\\n    def sample_search_deployment_revisions():\\n        # Create a client\\n        client = telcoautomation_v1alpha1.TelcoAutomationClient()\\n\\n        # Initialize request argument(s)\\n        request = telcoautomation_v1alpha1.SearchDeploymentRevisionsRequest(\\n            parent=\"parent_value\",\\n            query=\"query_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_deployment_revisions(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.telcoautomation_v1alpha1.types.SearchDeploymentRevisionsRequest, dict]):\\n        The request object. Request object for ``SearchDeploymentRevisions``.\\n    parent (str):\\n        Required. The name of parent orchestration cluster\\n        resource. Format should be -\\n        \"projects/{project_id}/locations/{location_name}/orchestrationClusters/{orchestration_cluster}\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    query (str):\\n        Required. Supported queries:\\n\\n        1. \"\"                       : Lists all\\n            revisions across all deployments.\\n        2. \"latest=true\"            : Lists\\n            latest revisions across all\\n            deployments.\\n        3. \"name={name}\"            : Lists all\\n            revisions of deployment with name\\n            {name}.\\n        4. \"name={name} latest=true\": Lists\\n            latest revision of deployment with\\n            name {name}\\n\\n        This corresponds to the ``query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.telcoautomation_v1alpha1.services.telco_automation.pagers.SearchDeploymentRevisionsPager:\\n        Response object for SearchDeploymentRevisions.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the list slates method over gRPC.\\n\\nLists all slates in the specified project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListSlatesRequest],\\n            ~.ListSlatesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Cell magics declared via a class, #2',\n",
       " 'A list of the captured rich display outputs, if any.\\n\\nIf you have a CapturedIO object ``c``, these can be displayed in IPython\\nusing::\\n\\n    from IPython.display import display\\n    for o in c.outputs:\\n        display(o)',\n",
       " 'Returns an instance of a compressor file object.',\n",
       " 'preprocess old style comments.\\n\\nexample:\\n\\nfrom mako.ext.preprocessors import convert_comments\\nt = Template(..., preprocessor=convert_comments)',\n",
       " 'Return text with backslash escapes undone (backslashes are restored). ',\n",
       " 'Demonstrates displaying different variables through shade and color.',\n",
       " 'Copy properties from *other* to *self*.',\n",
       " 'Return the depth of the axis used by the picker.',\n",
       " 'Read one page from the file. Return True if successful,\\nFalse if there were no more pages.',\n",
       " \"Return this line's `~matplotlib.transforms.TransformedPath`.\",\n",
       " 'Convert the given BSON value into our own type.',\n",
       " 'Raise error on missing token.',\n",
       " '`MeterProvider.get_meter` arguments are used to create an\\n`InstrumentationScope` object on the created `Meter`.',\n",
       " 'Register a function to convert a core foundation data type into its\\nequivalent in python\\n\\n:param type_id:\\n    The CFTypeId for the type\\n\\n:param callback:\\n    A callback to pass the CFType object to',\n",
       " 'Simple helper method to create data for to ``to_dict(orient=\"split\")``\\nto create the main output data',\n",
       " 'Return a string representation for a particular Series.',\n",
       " 'Appends lines to a buffer.\\n\\nParameters\\n----------\\nbuf\\n    The buffer to write to\\nlines\\n    The lines to append.',\n",
       " 'Check that two objects are not approximately equal.\\n\\nParameters\\n----------\\na : object\\n    The first object to compare.\\nb : object\\n    The second object to compare.\\n**kwargs\\n    The arguments passed to `tm.assert_almost_equal`.',\n",
       " 'Iterate through file entries declared in this distribution.\\n\\nFor modern .dist-info distributions, this is the files listed in the\\n``RECORD`` metadata file. For legacy setuptools distributions, this\\ncomes from ``installed-files.txt``, with entries normalized to be\\ncompatible with the format used by ``RECORD``.\\n\\n:return: An iterator for listed entries, or None if the distribution\\n    contains neither ``RECORD`` nor ``installed-files.txt``.',\n",
       " 'Dispatches a hook dictionary on a given piece of data.',\n",
       " 'Verify that installing works from a link with an extra if there is an indirect\\ndependency on that same package with the same extra (#12372).',\n",
       " 'Returns a dictionary of token types, matched to their action in the parser.\\n\\nOnly returns token types that are accepted by the current state.\\n\\nUpdated by ``feed_token()``.',\n",
       " 'Alternate implementation using /proc/cpuinfo.\\nmin and max frequencies are not available and are set to None.',\n",
       " 'Re-run tests which failed on last run.',\n",
       " 'The context argument for `PydanticKnownError` requires a number or str type, so we do a simple repr() coercion for types like timedelta.\\n\\nSee tests/test_types.py::test_annotated_metadata_any_order for some context.',\n",
       " 'Grant owner access to the current entity.',\n",
       " 'Return the more user-friendly information about the location of a warning, or None.',\n",
       " \"Regression test for importing a submodule 'foo.bar' while there is a 'bar' directory\\nreachable from sys.path -- ensuring the top-level module does not end up imported as a namespace\\npackage.\\n\\n#12194\\nhttps://github.com/pytest-dev/pytest/pull/12208#issuecomment-2056458432\",\n",
       " 'Get the next page in the iterator.\\n\\nReturns:\\n    Page: The next page in the iterator or :data:`None` if\\n        there are no pages left.',\n",
       " \"Patch a table's metadata.\",\n",
       " 'Return query result and display a progress bar while the query running, if tqdm is installed.\\n\\nArgs:\\n    query_job:\\n        The job representing the execution of the query on the server.\\n    progress_bar_type:\\n        The type of progress bar to use to show query progress.\\n    max_results:\\n        The maximum number of rows the row iterator should return.\\n\\nReturns:\\n    A row iterator over the query results.',\n",
       " 'The configuration for this copy job.',\n",
       " 'Optional[str]: Description of the destination table.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.description',\n",
       " \"Scan the input for current state's tokens starting at ``current_offset``.\\n\\nArgs:\\n    state (LexerState): The current lexer state.\\n    current_offset (int): The offset in the input text, i.e. the number\\n        of characters already scanned so far.\\n\\nYields:\\n    The next ``Token`` or ``StateTransition`` instance.\",\n",
       " 'Sets the terminating of this V1EndpointConditions.\\n\\nterminating indicates that this endpoint is terminating. A nil value indicates an unknown state. Consumers should interpret this unknown state to mean that the endpoint is not terminating.  # noqa: E501\\n\\n:param terminating: The terminating of this V1EndpointConditions.  # noqa: E501\\n:type: bool',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Returns the hyperparameters as a dictionary to use for training.\\n\\nThe :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which\\ntrains the model, calls this method to find the hyperparameters.\\n\\nReturns:\\n    dict[str, str]: The hyperparameters.',\n",
       " 'Instantiate a new SparkOutputManager for PySpark functions.\\n\\nArgs:\\n    feature_store_manager_factory (FeatureStoreManagerFactory): A factory to provide\\n        that provides a FeatureStoreManager that handles data ingestion to a Feature Group.\\n        The factory lazily loads the FeatureStoreManager.\\n\\nReturns:\\n    SparkOutputReceiver: An instance that handles outputs of the wrapped function.',\n",
       " 'Returns schema version.',\n",
       " 'Initialize the class.\\n\\nArgs:\\n    force (bool): If True, render colorizes output no matter where the\\n        output is (default: False).',\n",
       " 'Describe the latest baselining job kicked off by the suggest workflow.',\n",
       " 'Deletes the Amazon SageMaker models backing this predictor.',\n",
       " 'Validate mutually exclusive property file / s3uri',\n",
       " 'Compute gradient and hessian of loss w.r.t raw_prediction.\\n\\nParameters\\n----------\\ny_true : C-contiguous array of shape (n_samples,)\\n    Observed, true target values.\\nraw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\\n    Raw prediction values (in link space).\\nsample_weight : None or C-contiguous array of shape (n_samples,)\\n    Sample weights.\\ngradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the gradient is stored. If None, a new array\\n    might be created.\\nhessian_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the hessian is stored. If None, a new array\\n    might be created.\\nn_threads : int, default=1\\n    Might use openmp thread parallelism.\\n\\nReturns\\n-------\\ngradient : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise gradients.\\n\\nhessian : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise hessians.',\n",
       " 'Check that expected error are raised during fit.',\n",
       " 'Time conventional Voronoi diagram calculation.',\n",
       " 'Construct the sigma matrix in SVD from singular values and size M, N.\\n\\nParameters\\n----------\\ns : (M,) or (N,) array_like\\n    Singular values\\nM : int\\n    Size of the matrix whose singular values are `s`.\\nN : int\\n    Size of the matrix whose singular values are `s`.\\n\\nReturns\\n-------\\nS : (M, N) ndarray\\n    The S-matrix in the singular value decomposition\\n\\nSee Also\\n--------\\nsvd : Singular value decomposition of a matrix\\nsvdvals : Compute singular values of a matrix.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy.linalg import diagsvd\\n>>> vals = np.array([1, 2, 3])  # The array representing the computed svd\\n>>> diagsvd(vals, 3, 4)\\narray([[1, 0, 0, 0],\\n       [0, 2, 0, 0],\\n       [0, 0, 3, 0]])\\n>>> diagsvd(vals, 4, 3)\\narray([[1, 0, 0],\\n       [0, 2, 0],\\n       [0, 0, 3],\\n       [0, 0, 0]])',\n",
       " 'Minimize a function using a nonlinear conjugate gradient algorithm.\\n\\nParameters\\n----------\\nf : callable, ``f(x, *args)``\\n    Objective function to be minimized. Here `x` must be a 1-D array of\\n    the variables that are to be changed in the search for a minimum, and\\n    `args` are the other (fixed) parameters of `f`.\\nx0 : ndarray\\n    A user-supplied initial estimate of `xopt`, the optimal value of `x`.\\n    It must be a 1-D array of values.\\nfprime : callable, ``fprime(x, *args)``, optional\\n    A function that returns the gradient of `f` at `x`. Here `x` and `args`\\n    are as described above for `f`. The returned value must be a 1-D array.\\n    Defaults to None, in which case the gradient is approximated\\n    numerically (see `epsilon`, below).\\nargs : tuple, optional\\n    Parameter values passed to `f` and `fprime`. Must be supplied whenever\\n    additional fixed parameters are needed to completely specify the\\n    functions `f` and `fprime`.\\ngtol : float, optional\\n    Stop when the norm of the gradient is less than `gtol`.\\nnorm : float, optional\\n    Order to use for the norm of the gradient\\n    (``-np.inf`` is min, ``np.inf`` is max).\\nepsilon : float or ndarray, optional\\n    Step size(s) to use when `fprime` is approximated numerically. Can be a\\n    scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\\n    floating point machine precision.  Usually ``sqrt(eps)`` is about\\n    1.5e-8.\\nmaxiter : int, optional\\n    Maximum number of iterations to perform. Default is ``200 * len(x0)``.\\nfull_output : bool, optional\\n    If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\\n    addition to `xopt`.  See the Returns section below for additional\\n    information on optional return values.\\ndisp : bool, optional\\n    If True, return a convergence message, followed by `xopt`.\\nretall : bool, optional\\n    If True, add to the returned values the results of each iteration.\\ncallback : callable, optional\\n    An optional user-supplied function, called after each iteration.\\n    Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\\nc1 : float, default: 1e-4\\n    Parameter for Armijo condition rule.\\nc2 : float, default: 0.4\\n    Parameter for curvature condition rule.\\n\\nReturns\\n-------\\nxopt : ndarray\\n    Parameters which minimize f, i.e., ``f(xopt) == fopt``.\\nfopt : float, optional\\n    Minimum value found, f(xopt). Only returned if `full_output` is True.\\nfunc_calls : int, optional\\n    The number of function_calls made. Only returned if `full_output`\\n    is True.\\ngrad_calls : int, optional\\n    The number of gradient calls made. Only returned if `full_output` is\\n    True.\\nwarnflag : int, optional\\n    Integer value with warning status, only returned if `full_output` is\\n    True.\\n\\n    0 : Success.\\n\\n    1 : The maximum number of iterations was exceeded.\\n\\n    2 : Gradient and/or function calls were not changing. May indicate\\n        that precision was lost, i.e., the routine did not converge.\\n\\n    3 : NaN result encountered.\\n\\nallvecs : list of ndarray, optional\\n    List of arrays, containing the results at each iteration.\\n    Only returned if `retall` is True.\\n\\nSee Also\\n--------\\nminimize : common interface to all `scipy.optimize` algorithms for\\n           unconstrained and constrained minimization of multivariate\\n           functions. It provides an alternative way to call\\n           ``fmin_cg``, by specifying ``method=\\'CG\\'``.\\n\\nNotes\\n-----\\nThis conjugate gradient algorithm is based on that of Polak and Ribiere\\n[1]_.\\n\\nConjugate gradient methods tend to work better when:\\n\\n1. `f` has a unique global minimizing point, and no local minima or\\n   other stationary points,\\n2. `f` is, at least locally, reasonably well approximated by a\\n   quadratic function of the variables,\\n3. `f` is continuous and has a continuous gradient,\\n4. `fprime` is not too large, e.g., has a norm less than 1000,\\n5. The initial guess, `x0`, is reasonably close to `f` \\'s global\\n   minimizing point, `xopt`.\\n\\nParameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\\n\\nReferences\\n----------\\n.. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\\n\\nExamples\\n--------\\nExample 1: seek the minimum value of the expression\\n``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\\nof the parameters and an initial guess ``(u, v) = (0, 0)``.\\n\\n>>> import numpy as np\\n>>> args = (2, 3, 7, 8, 9, 10)  # parameter values\\n>>> def f(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\\n>>> def gradf(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     gu = 2*a*u + b*v + d     # u-component of the gradient\\n...     gv = b*u + 2*c*v + e     # v-component of the gradient\\n...     return np.asarray((gu, gv))\\n>>> x0 = np.asarray((0, 0))  # Initial guess.\\n>>> from scipy import optimize\\n>>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\\nOptimization terminated successfully.\\n         Current function value: 1.617021\\n         Iterations: 4\\n         Function evaluations: 8\\n         Gradient evaluations: 8\\n>>> res1\\narray([-1.80851064, -0.25531915])\\n\\nExample 2: solve the same problem using the `minimize` function.\\n(This `myopts` dictionary shows all of the available options,\\nalthough in practice only non-default values would be needed.\\nThe returned value will be a dictionary.)\\n\\n>>> opts = {\\'maxiter\\' : None,    # default value.\\n...         \\'disp\\' : True,    # non-default value.\\n...         \\'gtol\\' : 1e-5,    # default value.\\n...         \\'norm\\' : np.inf,  # default value.\\n...         \\'eps\\' : 1.4901161193847656e-08}  # default value.\\n>>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\\n...                          method=\\'CG\\', options=opts)\\nOptimization terminated successfully.\\n        Current function value: 1.617021\\n        Iterations: 4\\n        Function evaluations: 8\\n        Gradient evaluations: 8\\n>>> res2.x  # minimum found\\narray([-1.80851064, -0.25531915])',\n",
       " 'This test is for backwards compatibility post scipy 1.13.\\nThe behavior observed here is what is to be expected\\nwith the older matrix classes. This test comes with the\\nexception of dok_matrix, which was not working pre scipy1.12\\n(unlike the rest of these).',\n",
       " 'Call this method once you are done using the instance. It is automatically\\ncalled on destruction, and should be called just in time to allow system\\nresources to be freed.\\n\\nOnce you called end_access, you must call begin access before reusing this instance!',\n",
       " \"When join conditions don't express the left side explicitly,\\ndetermine if an existing FROM or entity in this query\\ncan serve as the left hand side.\",\n",
       " 'Test that any link will not match in XHTML (all links are unvisited).',\n",
       " 'Test input direction when input is the root.',\n",
       " 'Runs distributed training.\\n\\nParameters\\n----------\\ntrain_object : callable object or str\\n    Either a PyTorch function, PyTorch Lightning function, or the path to a python file\\n    that launches distributed training.\\nargs :\\n    If train_object is a python function and not a path to a python file, args need\\n    to be the input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, 1e-3, 64)\\n\\n    where train is a function and 1e-3 and 64 are regular numeric inputs to the function.\\n\\n    If train_object is a python file, then args would be the command-line arguments for\\n    that python file which are all in the form of strings. An example would be\\n\\n    >>> distributor.run(\"/path/to/train.py\", \"--learning-rate=1e-3\", \"--batch-size=64\")\\n\\n    where since the input is a path, all of the parameters are strings that can be\\n    handled by argparse in that python file.\\nkwargs :\\n    If train_object is a python function and not a path to a python file, kwargs need\\n    to be the key-word input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, tol=1e-3, max_iter=64)\\n\\n    where train is a function of 2 arguments `tol` and `max_iter`.\\n\\n    If train_object is a python file, then you should not set kwargs arguments.\\n\\nReturns\\n-------\\n    Returns the output of train_object called with args inside spark rank 0 task if the\\n    train_object is a Callable with an expected output. Returns None if train_object is\\n    a file.',\n",
       " 'Rows of the RowMatrix stored as an RDD of vectors.\\n\\nExamples\\n--------\\n>>> mat = RowMatrix(sc.parallelize([[1, 2, 3], [4, 5, 6]]))\\n>>> rows = mat.rows\\n>>> rows.first()\\nDenseVector([1.0, 2.0, 3.0])',\n",
       " \"test additional use case that wasn't considered for #8372\",\n",
       " 'Filter ``only`` nodes which do not match *tags*.',\n",
       " 'Fetch the first column of the first row, and close the result set.\\n\\nReturns ``None`` if there are no rows to fetch.\\n\\nNo validation is performed to test if additional rows remain.\\n\\nAfter calling this method, the object is fully closed,\\ne.g. the :meth:`_engine.CursorResult.close`\\nmethod will have been called.\\n\\n:return: a Python scalar value, or ``None`` if no rows remain.',\n",
       " 'Return the attribute name that should be used to refer from one\\nclass to another, for a collection reference.\\n\\nThe default implementation is::\\n\\n    return referred_cls.__name__.lower() + \"_collection\"\\n\\nAlternate implementations\\ncan be specified using the\\n:paramref:`.AutomapBase.prepare.name_for_collection_relationship`\\nparameter.\\n\\n:param base: the :class:`.AutomapBase` class doing the prepare.\\n\\n:param local_cls: the class to be mapped on the local side.\\n\\n:param referred_cls: the class to be mapped on the referring side.\\n\\n:param constraint: the :class:`_schema.ForeignKeyConstraint` that is being\\n inspected to produce this relationship.',\n",
       " 'Tries to find more initial conditions by substituting the initial\\nvalue point in the differential equation.',\n",
       " 'Converts a term in the expansion of a function from binary to its\\nvariable form (for POS).',\n",
       " \"Young's Modulus of the Beam. \",\n",
       " 'Expands the transfer function matrix',\n",
       " 'The length of the spring at which it produces no force.',\n",
       " 'Tests max degrees function.',\n",
       " 'Returns the first `n` terms of the composed formal power series.\\nTerm by term logic is implemented here.\\n\\nExplanation\\n===========\\n\\nThe coefficient sequence of the :obj:`FormalPowerSeriesCompose` object is the generic sequence.\\nIt is multiplied by ``bell_seq`` to get a sequence, whose terms are added up to get\\nthe final terms for the polynomial.\\n\\nExamples\\n========\\n\\n>>> from sympy import fps, sin, exp\\n>>> from sympy.abc import x\\n>>> f1 = fps(exp(x))\\n>>> f2 = fps(sin(x))\\n>>> fcomp = f1.compose(f2, x)\\n\\n>>> fcomp._eval_terms(6)\\n-x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\n>>> fcomp._eval_terms(8)\\nx**7/90 - x**6/240 - x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\nSee Also\\n========\\n\\nsympy.series.formal.FormalPowerSeries.compose\\nsympy.series.formal.FormalPowerSeries.coeff_bell',\n",
       " 'Apply ``_hasattrs`` and ``_hastypes`` to ``expr``. ',\n",
       " 'Show that options are not passed for older versions of requests.',\n",
       " 'Test CLI --manpath',\n",
       " 'Completion hints for argcomplete',\n",
       " 'validate object to defined invariants.',\n",
       " 'Encodes the value using DER\\n\\n:param force:\\n    If the encoded contents already exist, clear them and regenerate\\n    to ensure they are in DER format instead of BER format\\n\\n:return:\\n    A byte string of the DER-encoded value',\n",
       " 'The native Python datatype representation of this value\\n\\n:return:\\n    A byte string or None',\n",
       " 'Writes the loaded data back out to the JSON index.',\n",
       " 'Should determine names and address with a hostname override.',\n",
       " 'Property suppress_warnings.',\n",
       " 'Filter only running futures.',\n",
       " 'StringTransformer instances have a call signature that mirrors that of\\nthe Transformer type.\\n\\nRaises:\\n    CannotTransform(...) if the concrete StringTransformer class is unable\\n    to transform @line.',\n",
       " \"Acquire the GIL. The thread's thread state must have been initialized\\nby a previous `put_release_gil`\",\n",
       " 'Delete a specific SLO.\\n\\n:param id: SLO id to delete\\n:type id: str\\n\\n:returns: SLO ids removed',\n",
       " \"Sets the tracing function with the pydev debug function and initializes needed facilities.\\n\\n:param host: the user may specify another host, if the debug server is not in the same machine (default is the local\\n    host)\\n\\n:param stdout_to_server: when this is true, the stdout is passed to the debug server\\n\\n:param stderr_to_server: when this is true, the stderr is passed to the debug server\\n    so that they are printed in its console and not in this process console.\\n\\n:param port: specifies which port to use for communicating with the server (note that the server must be started\\n    in the same port). @note: currently it's hard-coded at 5678 in the client\\n\\n:param suspend: whether a breakpoint should be emulated as soon as this function is called.\\n\\n:param trace_only_current_thread: determines if only the current thread will be traced or all current and future\\n    threads will also have the tracing enabled.\\n\\n:param overwrite_prev_trace: deprecated\\n\\n:param patch_multiprocessing: if True we'll patch the functions which create new processes so that launched\\n    processes are debugged.\\n\\n:param stop_at_frame: if passed it'll stop at the given frame, otherwise it'll stop in the function which\\n    called this method.\\n\\n:param wait_for_ready_to_run: if True settrace will block until the ready_to_run flag is set to True,\\n    otherwise, it'll set ready_to_run to True and this function won't block.\\n\\n    Note that if wait_for_ready_to_run == False, there are no guarantees that the debugger is synchronized\\n    with what's configured in the client (IDE), the only guarantee is that when leaving this function\\n    the debugger will be already connected.\\n\\n:param dont_trace_start_patterns: if set, then any path that starts with one fo the patterns in the collection\\n    will not be traced\\n\\n:param dont_trace_end_patterns: if set, then any path that ends with one fo the patterns in the collection\\n    will not be traced\\n\\n:param access_token: token to be sent from the client (i.e.: IDE) to the debugger when a connection\\n    is established (verified by the debugger).\\n\\n:param client_access_token: token to be sent from the debugger to the client (i.e.: IDE) when\\n    a connection is established (verified by the client).\\n\\n:param notify_stdin:\\n    If True sys.stdin will be patched to notify the client when a message is requested\\n    from the IDE. This is done so that when reading the stdin the client is notified.\\n    Clients may need this to know when something that is being written should be interpreted\\n    as an input to the process or as a command to be evaluated.\\n    Note that parallel-python has issues with this (because it tries to assert that sys.stdin\\n    is of a given type instead of just checking that it has what it needs).\\n\\n:param protocol:\\n    When using in Eclipse the protocol should not be passed, but when used in VSCode\\n    or some other IDE/editor that accepts the Debug Adapter Protocol then 'dap' should\\n    be passed.\",\n",
       " 'check version string found_version >= expected_min_or_eq_to_version\\n\\nIf dev/prerelease tags result in TypeError for string-number comparison,\\nit is assumed that the dependency is satisfied.\\nUsers on dev branches are responsible for keeping their own packages up to date.',\n",
       " '@type  crash: L{Crash}\\n@param crash: Crash object.\\n\\n@rtype:  bool\\n@return:\\n    C{True} if a Crash object with the same key is in the container.',\n",
       " '@rtype:  int\\n@return: Exit code of the thread.',\n",
       " '@type  dwThreadId: int\\n@param dwThreadId: Global thread ID.\\n\\n@type  hThread: L{ThreadHandle}\\n@param hThread: (Optional) Handle to the thread.\\n\\n@type  process: L{Process}\\n@param process: (Optional) Parent Process object.',\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['docstring'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# good model for code summarization\n",
    "MODEL = 'google/pegasus-cnn_dailymail'\n",
    "\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try summarization of docstrings\n",
    "# from transformers import pipeline\n",
    "\n",
    "# def generate_summarization_input(row):\n",
    "#     docstring = row['docstring']\n",
    "#     body = row['body']\n",
    "#     function_name = row['name']\n",
    "#     args = row['args']\n",
    "#     args = args.replace('{', '').replace('}', '')\n",
    "\n",
    "#     if len(body) > 50:\n",
    "#         body = body[:50] + '...'\n",
    "\n",
    "#     return f\"def {function_name}({args}): \\n {body} \\n \\n docstring: {docstring} \\n short docstring: \\n\"\n",
    "# values['summarization_input'] = values.apply(generate_summarization_input, axis=1)\n",
    "\n",
    "# print(values['summarization_input'])\n",
    "\n",
    "# input_ids = [tokenizer.encode(x, return_tensors='pt', max_length=4096) for x in values['summarization_input']]\n",
    "\n",
    "# print(values['summarization_input'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summarize to docstring: This sets the terminal window size of the child tty. This will cause\\na SIGWINCH signal to be sent to the child. This does not change the\\nphysical window size. It changes the size reported to TTY-aware\\napplications like vi or curses -- applications that respond to the\\nSIGWINCH signal. ',\n",
       " 'summarize to docstring: PEP 566 compliant JSON-serializable representation of METADATA or PKG-INFO.\\n\\nThis should return an empty dict if the metadata file is unavailable.\\n\\n:raises NoneMetadataError: If the metadata file is available, but does\\n    not contain valid metadata.',\n",
       " 'summarize to docstring: Return the information exported by this distribution.\\n:return: A dictionary of exports, mapping an export category to a dict\\n         of :class:`ExportEntry` instances describing the individual\\n         export entries, and keyed by name.',\n",
       " 'summarize to docstring: Log rich content to the terminal.\\n\\nArgs:\\n    objects (positional args): Objects to log to the terminal.\\n    sep (str, optional): String to write between print data. Defaults to \" \".\\n    end (str, optional): String to write at end of print data. Defaults to \"\\\\\\\\n\".\\n    style (Union[str, Style], optional): A style to apply to output. Defaults to None.\\n    justify (str, optional): One of \"left\", \"right\", \"center\", or \"full\". Defaults to ``None``.\\n    emoji (Optional[bool], optional): Enable emoji code, or ``None`` to use console default. Defaults to None.\\n    markup (Optional[bool], optional): Enable markup, or ``None`` to use console default. Defaults to None.\\n    highlight (Optional[bool], optional): Enable automatic highlighting, or ``None`` to use console default. Defaults to None.\\n    log_locals (bool, optional): Boolean to enable logging of locals where ``log()``\\n        was called. Defaults to False.\\n    _stack_offset (int, optional): Offset of caller from end of call stack. Defaults to 1.',\n",
       " 'summarize to docstring: test that pip.__init__.py does not shadow\\nthe command submodule with a dictionary',\n",
       " 'summarize to docstring: Parses input, which is a list of tokens.',\n",
       " 'summarize to docstring: Test ordering of checkers based on their __gt__ method.',\n",
       " 'summarize to docstring: Test that a toml file has a pylint config.',\n",
       " 'summarize to docstring: If an extension requires an issuer, the `issuer` parameter to\\n`X509Extension` provides its value.',\n",
       " 'summarize to docstring: Tests whether requests can be used importing standard_library modules\\npreviously with the hooks context manager',\n",
       " 'summarize to docstring: Produce coverage reports.',\n",
       " 'summarize to docstring: Return a string or list of strings to be displayed after collection\\nhas finished successfully.\\n\\nThese strings will be displayed after the standard \"collected X items\" message.\\n\\n.. versionadded:: 3.2\\n\\n:param config: The pytest config object.\\n:param start_path: The starting dir.\\n:type start_path: pathlib.Path\\n:param startdir: The starting dir (deprecated).\\n:param items: List of pytest items that are going to be executed; this list should not be modified.\\n\\n.. note::\\n\\n    Lines returned by a plugin are displayed before those of plugins which\\n    ran before it.\\n    If you want to have your line(s) displayed first, use\\n    :ref:`trylast=True <plugin-hookorder>`.\\n\\n.. versionchanged:: 7.0.0\\n    The ``start_path`` parameter was added as a :class:`pathlib.Path`\\n    equivalent of the ``startdir`` parameter. The ``startdir`` parameter\\n    has been deprecated.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest plugin can implement this hook.',\n",
       " 'summarize to docstring: Map errors for Unary-Unary and Stream-Unary gRPC callables.',\n",
       " \"summarize to docstring: Wait for a job to complete and return the results.\\n\\nIf we can't return the results within the ``wait_timeout``, try to cancel\\nthe job.\",\n",
       " 'summarize to docstring: Return a string indicating the HTTP request method.',\n",
       " \"summarize to docstring: Fail unless a warning of class warnClass is triggered\\nby callable_obj when invoked with arguments args and keyword\\narguments kwargs.  If a different type of warning is\\ntriggered, it will not be handled: depending on the other\\nwarning filtering rules in effect, it might be silenced, printed\\nout, or raised as an exception.\\n\\nIf called with callable_obj omitted or None, will return a\\ncontext object used like this::\\n\\n     with self.assertWarns(SomeWarning):\\n         do_something()\\n\\nAn optional keyword argument 'msg' can be provided when assertWarns\\nis used as a context object.\\n\\nThe context manager keeps a reference to the first matching\\nwarning as the 'warning' attribute; similarly, the 'filename'\\nand 'lineno' attributes give you information about the line\\nof Python code from which the warning was triggered.\\nThis allows you to inspect the warning after the assertion::\\n\\n    with self.assertWarns(SomeWarning) as cm:\\n        do_something()\\n    the_warning = cm.warning\\n    self.assertEqual(the_warning.some_attribute, 147)\",\n",
       " 'summarize to docstring: __init__(self, \\\\*, inputCols=None, outputCol=None):',\n",
       " 'summarize to docstring: Test checking a single file that is excluded.',\n",
       " \"summarize to docstring: replace_namespaced_persistent_volume_claim_status  # noqa: E501\\n\\nreplace status of the specified PersistentVolumeClaim  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.replace_namespaced_persistent_volume_claim_status(name, namespace, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the PersistentVolumeClaim (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param V1PersistentVolumeClaim body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1PersistentVolumeClaim\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'summarize to docstring: Sets the collision_count of this V1DaemonSetStatus.\\n\\nCount of hash collisions for the DaemonSet. The DaemonSet controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ControllerRevision.  # noqa: E501\\n\\n:param collision_count: The collision_count of this V1DaemonSetStatus.  # noqa: E501\\n:type: int',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " 'summarize to docstring: Sets the additional_properties of this V1JSONSchemaProps.\\n\\nJSONSchemaPropsOrBool represents JSONSchemaProps or a boolean value. Defaults to true for the boolean property.  # noqa: E501\\n\\n:param additional_properties: The additional_properties of this V1JSONSchemaProps.  # noqa: E501\\n:type: object',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Returns micro-averaged label-based precision.\\n(equals to micro-averaged document-based precision)',\n",
       " 'summarize to docstring: Deserialize with Futures',\n",
       " 'summarize to docstring: Format training args to pass in sagemaker_session.train.\\n\\nArgs:\\n    desc (dict): the response from DescribeTrainingJob API.\\n    inputs (list): a list of input data channels.\\n    name (str): the name of the step job.\\n    volume_kms_key (str): The KMS key id to encrypt data on the storage volume attached to\\n        the ML compute instance(s).\\n    encrypt_inter_container_traffic (bool): To encrypt all communications between ML compute\\n        instances in distributed training.\\n    vpc_config (dict): Specifies a VPC that jobs and hosted models have access to.\\n        Control access to and from training and model containers by configuring the VPC\\n\\nReturns (dcit): a dictionary that can be used as args of\\n    sagemaker_session.train method.',\n",
       " 'summarize to docstring: Retrieves the supported instance types for the model.\\n\\nArgs:\\n    model_id (str): JumpStart model ID of the JumpStart model for which to\\n        retrieve the supported instance types.\\n    model_version (str): Version of the JumpStart model for which to retrieve the\\n        supported instance types.\\n    scope (str): The script type, i.e. what it is used for.\\n        Valid values: \"training\" and \"inference\".\\n    hub_arn (str): The arn of the SageMaker Hub for which to retrieve\\n        model details from. (Default: None).\\n    region (Optional[str]): Region for which to retrieve supported instance types.\\n        (Default: None).\\n    tolerate_vulnerable_model (bool): True if vulnerable versions of model\\n        specifications should be tolerated (exception not raised). If False, raises an\\n        exception if the script used by this version of the model has dependencies with known\\n        security vulnerabilities. (Default: False).\\n    tolerate_deprecated_model (bool): True if deprecated versions of model\\n        specifications should be tolerated (exception not raised). If False, raises\\n        an exception if the version of the model is deprecated. (Default: False).\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions. If not\\n        specified, one is created using the default AWS configuration\\n        chain. (Default: sagemaker.jumpstart.constants.DEFAULT_JUMPSTART_SAGEMAKER_SESSION).\\n    training_instance_type (str): In the case of a model fine-tuned on SageMaker, the training\\n        instance type used for the training job that produced the fine-tuned weights.\\n        Optionally supply this to get a inference instance type conditioned\\n        on the training instance, to ensure compatability of training artifact to inference\\n        instance. (Default: None).\\n    config_name (Optional[str]): Name of the JumpStart Model config to apply. (Default: None).\\nReturns:\\n    list: the supported instance types to use for the model or None.\\n\\nRaises:\\n    ValueError: If the model is not available in the\\n        specified region due to lack of supported computing instances.',\n",
       " 'summarize to docstring: Returns json representation of S3DataSource object.',\n",
       " 'summarize to docstring: Use the lineage query to retrieve transform jobs that use this endpoint.\\n\\nArgs:\\n    direction (LineageQueryDirectionEnum, optional): The query direction.\\n\\nReturns:\\n    list of LineageTrialComponent: Lineage trial component that represent Transform jobs.',\n",
       " 'summarize to docstring: Creates a monitoring schedule to monitor an Amazon SageMaker Endpoint.\\n\\nIf constraints and statistics are provided, or if they are able to be retrieved from a\\nprevious baselining job associated with this monitor, those will be used.\\nIf constraints and statistics cannot be automatically retrieved, baseline_inputs will be\\nrequired in order to kick off a baselining job.\\n\\nArgs:\\n    endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.\\n        This can either be the endpoint name or an EndpointInput. (default: None)\\n    record_preprocessor_script (str): The path to the record preprocessor script. This can\\n        be a local path or an S3 uri.\\n    post_analytics_processor_script (str): The path to the record post-analytics processor\\n        script. This can be a local path or an S3 uri.\\n    output_s3_uri (str): Desired S3 destination of the constraint_violations and\\n        statistics json files.\\n        Default: \"s3://<default_session_bucket>/<job_name>/output\"\\n    constraints (sagemaker.model_monitor.Constraints or str): If provided alongside\\n        statistics, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Constraints object or an s3_uri pointing to a constraints\\n        JSON file.\\n    statistics (sagemaker.model_monitor.Statistic or str): If provided alongside\\n        constraints, these will be used for monitoring the endpoint. This can be a\\n        sagemaker.model_monitor.Statistics object or an s3_uri pointing to a statistics\\n        JSON file.\\n    monitor_schedule_name (str): Schedule name. If not specified, the processor generates\\n        a default job name, based on the image name and current timestamp.\\n    schedule_cron_expression (str): The cron expression that dictates the frequency that\\n        this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid\\n        expressions. Default: Daily.\\n    enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of\\n        the baselining or monitoring jobs.\\n    batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to\\n        run the monitoring schedule on the batch transform (default: None)\\n    data_analysis_start_time (str): Start time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)\\n    data_analysis_end_time (str): End time for the data analysis window\\n        for the one time monitoring schedule (NOW), e.g. \"-PT1H\" (default: None)',\n",
       " 'summarize to docstring: Calculate a multidimensional median filter.\\n\\nParameters\\n----------\\n%(input)s\\n%(size_foot)s\\n%(output)s\\n%(mode_reflect)s\\n%(cval)s\\n%(origin_multiple)s\\naxes : tuple of int or None, optional\\n    If None, `input` is filtered along all axes. Otherwise,\\n    `input` is filtered along the specified axes. When `axes` is\\n    specified, any tuples used for `size`, `origin`, and/or `mode`\\n    must match the length of `axes`. The ith entry in any of these tuples\\n    corresponds to the ith entry in `axes`.\\n\\nReturns\\n-------\\nmedian_filter : ndarray\\n    Filtered array. Has the same shape as `input`.\\n\\nSee Also\\n--------\\nscipy.signal.medfilt2d\\n\\nNotes\\n-----\\nFor 2-dimensional images with ``uint8``, ``float32`` or ``float64`` dtypes\\nthe specialised function `scipy.signal.medfilt2d` may be faster. It is\\nhowever limited to constant mode with ``cval=0``.\\n\\nExamples\\n--------\\n>>> from scipy import ndimage, datasets\\n>>> import matplotlib.pyplot as plt\\n>>> fig = plt.figure()\\n>>> plt.gray()  # show the filtered result in grayscale\\n>>> ax1 = fig.add_subplot(121)  # left side\\n>>> ax2 = fig.add_subplot(122)  # right side\\n>>> ascent = datasets.ascent()\\n>>> result = ndimage.median_filter(ascent, size=20)\\n>>> ax1.imshow(ascent)\\n>>> ax2.imshow(result)\\n>>> plt.show()',\n",
       " 'summarize to docstring: Get information about memory available, not counting swap.',\n",
       " 'summarize to docstring: Compute the inverse FFT of a signal that has Hermitian symmetry.\\n\\nParameters\\n----------\\nx : array_like\\n    Input array.\\nn : int, optional\\n    Length of the inverse FFT, the number of points along\\n    transformation axis in the input to use.  If `n` is smaller than\\n    the length of the input, the input is cropped. If it is larger,\\n    the input is padded with zeros. If `n` is not given, the length of\\n    the input along the axis specified by `axis` is used.\\naxis : int, optional\\n    Axis over which to compute the inverse FFT. If not given, the last\\n    axis is used.\\nnorm : {\"backward\", \"ortho\", \"forward\"}, optional\\n    Normalization mode (see `fft`). Default is \"backward\".\\noverwrite_x : bool, optional\\n    If True, the contents of `x` can be destroyed; the default is False.\\n    See `fft` for more details.\\nworkers : int, optional\\n    Maximum number of workers to use for parallel computation. If negative,\\n    the value wraps around from ``os.cpu_count()``.\\n    See :func:`~scipy.fft.fft` for more details.\\nplan : object, optional\\n    This argument is reserved for passing in a precomputed plan provided\\n    by downstream FFT vendors. It is currently not used in SciPy.\\n\\n    .. versionadded:: 1.5.0\\n\\nReturns\\n-------\\nout : complex ndarray\\n    The truncated or zero-padded input, transformed along the axis\\n    indicated by `axis`, or the last one if `axis` is not specified.\\n    The length of the transformed axis is ``n//2 + 1``.\\n\\nSee Also\\n--------\\nhfft, irfft\\n\\nNotes\\n-----\\n`hfft`/`ihfft` are a pair analogous to `rfft`/`irfft`, but for the\\nopposite case: here, the signal has Hermitian symmetry in the time\\ndomain and is real in the frequency domain. So, here, it\\'s `hfft`, for\\nwhich you must supply the length of the result if it is to be odd:\\n* even: ``ihfft(hfft(a, 2*len(a) - 2) == a``, within roundoff error,\\n* odd: ``ihfft(hfft(a, 2*len(a) - 1) == a``, within roundoff error.\\n\\nExamples\\n--------\\n>>> from scipy.fft import ifft, ihfft\\n>>> import numpy as np\\n>>> spectrum = np.array([ 15, -4, 0, -1, 0, -4])\\n>>> ifft(spectrum)\\narray([1.+0.j,  2.+0.j,  3.+0.j,  4.+0.j,  3.+0.j,  2.+0.j]) # may vary\\n>>> ihfft(spectrum)\\narray([ 1.-0.j,  2.-0.j,  3.-0.j,  4.-0.j]) # may vary',\n",
       " 'summarize to docstring: Match the namespace of the element.',\n",
       " 'summarize to docstring: Remove this state.',\n",
       " 'summarize to docstring: Create a new Spark configuration.',\n",
       " 'summarize to docstring: Convert a locale string to a language tag (ex. en_US -> en-US).\\n\\nrefs: BCP 47 (:rfc:`5646`)',\n",
       " 'summarize to docstring: Get qualified name for given object as a list of string(s).',\n",
       " 'summarize to docstring: helper context manager that will apply appropriate DDL events\\nto a CREATE or DROP operation.',\n",
       " 'summarize to docstring: target database must support retrieval of the columns in a view,\\nsimilarly to how a table is inspected.\\n\\nThis does not include the full CREATE VIEW definition.',\n",
       " 'summarize to docstring: Execute a statement and assert that rows returned equal expected.',\n",
       " 'summarize to docstring: Generates all URLs and templates',\n",
       " 'summarize to docstring: Remove None, convert values to bool, check commutativity *in place*.\\n        ',\n",
       " 'summarize to docstring: This rule covers trigonometric factors by splitting everything into a\\nsum of exponential functions and collecting complex conjugate poles and\\nreal symmetric poles.',\n",
       " 'summarize to docstring: Print a LaTeX representation of the function defining the curve.\\n\\nParameters\\n==========\\n\\nprinter : Printer\\n    The printer to be used to print the LaTeX string representation.',\n",
       " 'summarize to docstring: Return a list of all units formed by unit and the given prefixes.\\n\\nYou can use the predefined PREFIXES or BIN_PREFIXES, but you can also\\npass as argument a subdict of them if you do not want all prefixed units.\\n\\n    >>> from sympy.physics.units.prefixes import (PREFIXES,\\n    ...                                                 prefix_unit)\\n    >>> from sympy.physics.units import m\\n    >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\\n    >>> prefix_unit(m, pref)  # doctest: +SKIP\\n    [millimeter, centimeter, decimeter]',\n",
       " 'summarize to docstring: Tests the coordinate variables functionality',\n",
       " 'summarize to docstring: Scale the function by a term independent of x.\\n\\nExplanation\\n===========\\n\\nf(x) -> s * f(x)\\n\\nThis is fast, if Fourier series of f(x) is already\\ncomputed.\\n\\nExamples\\n========\\n\\n>>> from sympy import fourier_series, pi\\n>>> from sympy.abc import x\\n>>> s = fourier_series(x**2, (x, -pi, pi))\\n>>> s.scale(2).truncate()\\n-8*cos(x) + 2*cos(2*x) + 2*pi**2/3',\n",
       " 'summarize to docstring: Access a group of rows and columns by label(s) or a boolean array.\\n\\n``.loc[]`` is primarily label based, but may also be used with a\\nboolean array.\\n\\nAllowed inputs are:\\n\\n- A single label, e.g. ``5`` or ``\\'a\\'``, (note that ``5`` is\\n  interpreted as a *label* of the index, and **never** as an\\n  integer position along the index).\\n- A list or array of labels, e.g. ``[\\'a\\', \\'b\\', \\'c\\']``.\\n- A slice object with labels, e.g. ``\\'a\\':\\'f\\'``.\\n\\n  .. warning:: Note that contrary to usual python slices, **both** the\\n      start and the stop are included\\n\\n- A boolean array of the same length as the axis being sliced,\\n  e.g. ``[True, False, True]``.\\n- An alignable boolean Series. The index of the key will be aligned before\\n  masking.\\n- An alignable Index. The Index of the returned selection will be the input.\\n- A ``callable`` function with one argument (the calling Series or\\n  DataFrame) and that returns valid output for indexing (one of the above)\\n\\nSee more at :ref:`Selection by Label <indexing.label>`.\\n\\nRaises\\n------\\nKeyError\\n    If any items are not found.\\nIndexingError\\n    If an indexed key is passed and its index is unalignable to the frame index.\\n\\nSee Also\\n--------\\nDataFrame.at : Access a single value for a row/column label pair.\\nDataFrame.iloc : Access group of rows and columns by integer position(s).\\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\\n               Series/DataFrame.\\nSeries.loc : Access group of values using labels.\\n\\nExamples\\n--------\\n**Getting values**\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[\"cobra\", \"viper\", \"sidewinder\"],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label. Note this returns the row as a Series.\\n\\n>>> df.loc[\"viper\"]\\nmax_speed    4\\nshield       5\\nName: viper, dtype: int64\\n\\nList of labels. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"]]\\n            max_speed  shield\\nviper               4       5\\nsidewinder          7       8\\n\\nSingle label for row and column\\n\\n>>> df.loc[\"cobra\", \"shield\"]\\n2\\n\\nSlice with labels for row and single label for column. As mentioned\\nabove, note that both the start and stop of the slice are included.\\n\\n>>> df.loc[\"cobra\":\"viper\", \"max_speed\"]\\ncobra    1\\nviper    4\\nName: max_speed, dtype: int64\\n\\nBoolean list with the same length as the row axis\\n\\n>>> df.loc[[False, False, True]]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nAlignable boolean Series:\\n\\n>>> df.loc[\\n...     pd.Series([False, True, False], index=[\"viper\", \"sidewinder\", \"cobra\"])\\n... ]\\n                     max_speed  shield\\nsidewinder          7       8\\n\\nIndex (same behavior as ``df.reindex``)\\n\\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\\n       max_speed  shield\\nfoo\\ncobra          1       2\\nviper          4       5\\n\\nConditional that returns a boolean Series\\n\\n>>> df.loc[df[\"shield\"] > 6]\\n            max_speed  shield\\nsidewinder          7       8\\n\\nConditional that returns a boolean Series with column labels specified\\n\\n>>> df.loc[df[\"shield\"] > 6, [\"max_speed\"]]\\n            max_speed\\nsidewinder          7\\n\\nMultiple conditional using ``&`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 1) & (df[\"shield\"] < 8)]\\n            max_speed  shield\\nviper          4       5\\n\\nMultiple conditional using ``|`` that returns a boolean Series\\n\\n>>> df.loc[(df[\"max_speed\"] > 4) | (df[\"shield\"] < 5)]\\n            max_speed  shield\\ncobra               1       2\\nsidewinder          7       8\\n\\nPlease ensure that each condition is wrapped in parentheses ``()``.\\nSee the :ref:`user guide<indexing.boolean>`\\nfor more details and explanations of Boolean indexing.\\n\\n.. note::\\n    If you find yourself using 3 or more conditionals in ``.loc[]``,\\n    consider using :ref:`advanced indexing<advanced.advanced_hierarchical>`.\\n\\n    See below for using ``.loc[]`` on MultiIndex DataFrames.\\n\\nCallable that returns a boolean Series\\n\\n>>> df.loc[lambda df: df[\"shield\"] == 8]\\n            max_speed  shield\\nsidewinder          7       8\\n\\n**Setting values**\\n\\nSet value for all items matching the list of labels\\n\\n>>> df.loc[[\"viper\", \"sidewinder\"], [\"shield\"]] = 50\\n>>> df\\n            max_speed  shield\\ncobra               1       2\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire row\\n\\n>>> df.loc[\"cobra\"] = 10\\n>>> df\\n            max_speed  shield\\ncobra              10      10\\nviper               4      50\\nsidewinder          7      50\\n\\nSet value for an entire column\\n\\n>>> df.loc[:, \"max_speed\"] = 30\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper              30      50\\nsidewinder         30      50\\n\\nSet value for rows matching callable condition\\n\\n>>> df.loc[df[\"shield\"] > 35] = 0\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       0\\nsidewinder          0       0\\n\\nAdd value matching location\\n\\n>>> df.loc[\"viper\", \"shield\"] += 5\\n>>> df\\n            max_speed  shield\\ncobra              30      10\\nviper               0       5\\nsidewinder          0       0\\n\\nSetting using a ``Series`` or a ``DataFrame`` sets the values matching the\\nindex labels, not the index positions.\\n\\n>>> shuffled_df = df.loc[[\"viper\", \"cobra\", \"sidewinder\"]]\\n>>> df.loc[:] += shuffled_df\\n>>> df\\n            max_speed  shield\\ncobra              60      20\\nviper               0      10\\nsidewinder          0       0\\n\\n**Getting values on a DataFrame with an index that has integer labels**\\n\\nAnother example using integers for the index\\n\\n>>> df = pd.DataFrame(\\n...     [[1, 2], [4, 5], [7, 8]],\\n...     index=[7, 8, 9],\\n...     columns=[\"max_speed\", \"shield\"],\\n... )\\n>>> df\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\nSlice with integer labels for rows. As mentioned above, note that both\\nthe start and stop of the slice are included.\\n\\n>>> df.loc[7:9]\\n   max_speed  shield\\n7          1       2\\n8          4       5\\n9          7       8\\n\\n**Getting values with a MultiIndex**\\n\\nA number of examples using a DataFrame with a MultiIndex\\n\\n>>> tuples = [\\n...     (\"cobra\", \"mark i\"),\\n...     (\"cobra\", \"mark ii\"),\\n...     (\"sidewinder\", \"mark i\"),\\n...     (\"sidewinder\", \"mark ii\"),\\n...     (\"viper\", \"mark ii\"),\\n...     (\"viper\", \"mark iii\"),\\n... ]\\n>>> index = pd.MultiIndex.from_tuples(tuples)\\n>>> values = [[12, 2], [0, 4], [10, 20], [1, 4], [7, 1], [16, 36]]\\n>>> df = pd.DataFrame(values, columns=[\"max_speed\", \"shield\"], index=index)\\n>>> df\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSingle label. Note this returns a DataFrame with a single index.\\n\\n>>> df.loc[\"cobra\"]\\n         max_speed  shield\\nmark i          12       2\\nmark ii          0       4\\n\\nSingle index tuple. Note this returns a Series.\\n\\n>>> df.loc[(\"cobra\", \"mark ii\")]\\nmax_speed    0\\nshield       4\\nName: (cobra, mark ii), dtype: int64\\n\\nSingle label for row and column. Similar to passing in a tuple, this\\nreturns a Series.\\n\\n>>> df.loc[\"cobra\", \"mark i\"]\\nmax_speed    12\\nshield        2\\nName: (cobra, mark i), dtype: int64\\n\\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\\n\\n>>> df.loc[[(\"cobra\", \"mark ii\")]]\\n               max_speed  shield\\ncobra mark ii          0       4\\n\\nSingle tuple for the index with a single label for the column\\n\\n>>> df.loc[(\"cobra\", \"mark i\"), \"shield\"]\\n2\\n\\nSlice from index tuple to single label\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : \"viper\"]\\n                     max_speed  shield\\ncobra      mark i           12       2\\n           mark ii           0       4\\nsidewinder mark i           10      20\\n           mark ii           1       4\\nviper      mark ii           7       1\\n           mark iii         16      36\\n\\nSlice from index tuple to index tuple\\n\\n>>> df.loc[(\"cobra\", \"mark i\") : (\"viper\", \"mark ii\")]\\n                    max_speed  shield\\ncobra      mark i          12       2\\n           mark ii          0       4\\nsidewinder mark i          10      20\\n           mark ii          1       4\\nviper      mark ii          7       1\\n\\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\\nfor more details and explanations of advanced indexing.',\n",
       " 'summarize to docstring: Test for stalled tqdm instance and monitor deletion',\n",
       " 'summarize to docstring: extract my config from a global Config object\\n\\nwill construct a Config object of only the config values that apply to me\\nbased on my mro(), as well as those of my parent(s) if they exist.\\n\\nIf I am Bar and my parent is Foo, and their parent is Tim,\\nthis will return merge following config sections, in this order::\\n\\n    [Bar, Foo.Bar, Tim.Foo.Bar]\\n\\nWith the last item being the highest priority.',\n",
       " 'summarize to docstring: Optional method. If not provided, the interpreter will use\\n``__iter__`` or the old ``__getitem__`` protocol\\nto implement ``in``.',\n",
       " 'summarize to docstring: Constuct an object holding a date value.\\n\\nThis function is part of the `DBAPI 2.0 specification\\n<http://www.python.org/dev/peps/pep-0249/>`_.\\n\\n:rtype: :class:`datetime.date`',\n",
       " 'summarize to docstring: Returns the normalized identifier of any currency code.\\n\\nAccepts a ``locale`` parameter for fined-grained validation, working as\\nthe one defined above in ``list_currencies()`` method.\\n\\nReturns None if the currency is unknown to Babel.',\n",
       " 'summarize to docstring: Append `leaf` to current line or to new line if appending impossible.',\n",
       " \"summarize to docstring: Parse Prometheus text format from a file descriptor.\\n\\nThis is a laxer parser than the main Go parser,\\nso successful parsing does not imply that the parsed\\ntext meets the specification.\\n\\nYields Metric's.\",\n",
       " 'summarize to docstring: :param string type:\\n:param integer request_seq: Sequence number of the corresponding request.\\n:param boolean success: Outcome of the request.\\nIf true, the request was successful and the `body` attribute may contain the result of the request.\\nIf the value is false, the attribute `message` contains the error in short form and the `body` may contain additional information (see `ErrorResponse.body.error`).\\n:param string command: The command requested.\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.\\n:param string message: Contains the raw error in short form if `success` is false.\\nThis raw error might be interpreted by the client and is not shown in the UI.\\nSome predefined values exist.\\n:param Capabilities body: The capabilities of this debug adapter.',\n",
       " \"summarize to docstring: :param array areas: Set of logical areas that got invalidated. This property has a hint characteristic: a client can only be expected to make a 'best effort' in honoring the areas but there are no guarantees. If this property is missing, empty, or if values are not understood, the client should assume a single value `all`.\\n:param integer threadId: If specified, the client only needs to refetch data related to this thread.\\n:param integer stackFrameId: If specified, the client only needs to refetch data related to this stack frame (and the `threadId` is ignored).\",\n",
       " 'summarize to docstring: Query nameservers to find the answer to the question.\\n\\nThis is a convenience function that uses the default resolver\\nobject to make the query.\\n\\nSee ``dns.resolver.Resolver.resolve`` for more information on the\\nparameters.',\n",
       " \"summarize to docstring: Get log stream for a service.\\nNote: This endpoint works only for services with the ``json-file``\\nor ``journald`` logging drivers.\\n\\nArgs:\\n    service (str): ID or name of the service\\n    details (bool): Show extra details provided to logs.\\n        Default: ``False``\\n    follow (bool): Keep connection open to read logs as they are\\n        sent by the Engine. Default: ``False``\\n    stdout (bool): Return logs from ``stdout``. Default: ``False``\\n    stderr (bool): Return logs from ``stderr``. Default: ``False``\\n    since (int): UNIX timestamp for the logs staring point.\\n        Default: 0\\n    timestamps (bool): Add timestamps to every log line.\\n    tail (string or int): Number of log lines to be returned,\\n        counting from the current end of the logs. Specify an\\n        integer or ``'all'`` to output all log lines.\\n        Default: ``all``\\n    is_tty (bool): Whether the service's :py:class:`ContainerSpec`\\n        enables the TTY option. If omitted, the method will query\\n        the Engine for the information, causing an additional\\n        roundtrip.\\n\\nReturns (generator): Logs for the service.\",\n",
       " \"summarize to docstring: Apply the chain's changes and write the final result using the passed\\nwrite function.\\n:param bbuf: base buffer containing the base of all deltas contained in this\\n    list. It will only be used if the chunk in question does not have a base\\n    chain.\\n:param write: function taking a string of bytes to write to the output\",\n",
       " 'summarize to docstring: Create a JSON representation of an instance of MediaUpload.\\n\\nReturns:\\n   string, a JSON representation of this instance, suitable to pass to\\n   from_json().',\n",
       " 'summarize to docstring: Call the create ad sense link method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.CreateAdSenseLinkRequest):\\n        The request object. Request message to be passed to\\n    CreateAdSenseLink method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.resources.AdSenseLink:\\n        A link between a GA4 Property and an\\n    AdSense for Content ad client.',\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'summarize to docstring: Return a callable for the get project settings method over gRPC.\\n\\nRetrieves the Settings for the Project.\\n\\nReturns:\\n    Callable[[~.GetProjectSettingsRequest],\\n            ~.ProjectSettings]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Sleep *ds* seconds.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    MetastoreServiceAsyncClient: The constructed client.',\n",
       " \"summarize to docstring: Merges capacity commitments of the same plan into a single\\ncommitment.\\n\\nThe resulting capacity commitment has the greater\\ncommitment_end_time out of the to-be-merged capacity\\ncommitments.\\n\\nAttempting to merge capacity commitments of different plan will\\nfail with the error code\\n``google.rpc.Code.FAILED_PRECONDITION``.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_reservation_v1\\n\\n    def sample_merge_capacity_commitments():\\n        # Create a client\\n        client = bigquery_reservation_v1.ReservationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_reservation_v1.MergeCapacityCommitmentsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.merge_capacity_commitments(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_reservation_v1.types.MergeCapacityCommitmentsRequest, dict]):\\n        The request object. The request for\\n        [ReservationService.MergeCapacityCommitments][google.cloud.bigquery.reservation.v1.ReservationService.MergeCapacityCommitments].\\n    parent (str):\\n        Parent resource that identifies admin project and\\n        location e.g., ``projects/myproject/locations/us``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    capacity_commitment_ids (MutableSequence[str]):\\n        Ids of capacity commitments to merge.\\n        These capacity commitments must exist\\n        under admin project and location\\n        specified in the parent.\\n        ID is the last portion of capacity\\n        commitment name e.g., 'abc' for\\n        projects/myproject/locations/US/capacityCommitments/abc\\n\\n        This corresponds to the ``capacity_commitment_ids`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_reservation_v1.types.CapacityCommitment:\\n        Capacity commitment is a way to\\n        purchase compute capacity for BigQuery\\n        jobs (in the form of slots) with some\\n        committed period of usage. Annual\\n        commitments renew by default.\\n        Commitments can be removed after their\\n        commitment end time passes.\\n\\n        In order to remove annual commitment,\\n        its plan needs to be changed to monthly\\n        or flex first.\\n\\n        A capacity commitment resource exists as\\n        a child resource of the admin project.\",\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.billing_v1.types.ListProjectBillingInfoRequest):\\n        The initial request object.\\n    response (google.cloud.billing_v1.types.ListProjectBillingInfoResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Returns the specified network firewall policy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.NetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetNetworkFirewallPolicyRequest(\\n            firewall_policy=\"firewall_policy_value\",\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        NetworkFirewallPolicies.Get. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy (str):\\n        Name of the firewall policy to get.\\n        This corresponds to the ``firewall_policy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.FirewallPolicy:\\n        Represents a Firewall Policy\\n        resource.',\n",
       " 'summarize to docstring: Sets the port of this CoreV1EndpointPort.\\n\\nThe port number of the endpoint.  # noqa: E501\\n\\n:param port: The port of this CoreV1EndpointPort.  # noqa: E501\\n:type: int',\n",
       " 'summarize to docstring: Post-rpc interceptor for export_deployment_statefile\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Config server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Return a callable for the read repository file method over gRPC.\\n\\nReturns the contents of a file (inside a Repository). The\\nRepository must not have a value for\\n``git_remote_settings.url``.\\n\\nReturns:\\n    Callable[[~.ReadRepositoryFileRequest],\\n            ~.ReadRepositoryFileResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Gets the resource representation for an interactive\\nsession.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataproc_v1\\n\\n    def sample_get_session():\\n        # Create a client\\n        client = dataproc_v1.SessionControllerClient()\\n\\n        # Initialize request argument(s)\\n        request = dataproc_v1.GetSessionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_session(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataproc_v1.types.GetSessionRequest, dict]):\\n        The request object. A request to get the resource\\n        representation for a session.\\n    name (str):\\n        Required. The name of the session to\\n        retrieve.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataproc_v1.types.Session:\\n        A representation of a session.',\n",
       " 'summarize to docstring: Call the create connection method over HTTP.\\n\\nArgs:\\n    request (~.developer_connect.CreateConnectionRequest):\\n        The request object. Message for creating a Connection\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Post-rpc interceptor for list_participants\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Return a callable for the check grounding method over gRPC.\\n\\nPerforms a grounding check.\\n\\nReturns:\\n    Callable[[~.CheckGroundingRequest],\\n            ~.CheckGroundingResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the purge suggestion deny list\\nentries method over gRPC.\\n\\nPermanently deletes all\\n[SuggestionDenyListEntry][google.cloud.discoveryengine.v1beta.SuggestionDenyListEntry]\\nfor a DataStore.\\n\\nReturns:\\n    Callable[[~.PurgeSuggestionDenyListEntriesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Return a callable for the list sites method over gRPC.\\n\\nLists sites in a given project and location.\\n\\nReturns:\\n    Callable[[~.ListSitesRequest],\\n            ~.ListSitesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AwsClustersAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Call the create kms config method over HTTP.\\n\\nArgs:\\n    request (~.kms.CreateKmsConfigRequest):\\n        The request object. CreateKmsConfigRequest creates a KMS\\n    Config.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Return if pages/frames are currently being cached.',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Return a callable for the list related account groups method over gRPC.\\n\\nList groups of related accounts.\\n\\nReturns:\\n    Callable[[~.ListRelatedAccountGroupsRequest],\\n            ~.ListRelatedAccountGroupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"summarize to docstring: Return a callable for the add control method over gRPC.\\n\\nEnables a Control on the specified ServingConfig. The control is\\nadded in the last position of the list of controls it belongs to\\n(e.g. if it's a facet spec control it will be applied in the\\nlast position of servingConfig.facetSpecIds) Returns a\\nALREADY_EXISTS error if the control has already been applied.\\nReturns a FAILED_PRECONDITION error if the addition could exceed\\nmaximum number of control allowed for that type of control.\\n\\nReturns:\\n    Callable[[~.AddControlRequest],\\n            ~.ServingConfig]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'summarize to docstring: Return a callable for the list accelerator types method over gRPC.\\n\\nLists accelerator types supported by this API.\\n\\nReturns:\\n    Callable[[~.ListAcceleratorTypesRequest],\\n            ~.ListAcceleratorTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"summarize to docstring: Call the update organization\\nsettings method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.UpdateOrganizationSettingsRequest):\\n            The request object. Request message for updating an\\n        organization's settings.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcs_organization_settings.OrganizationSettings:\\n            User specified settings that are\\n        attached to the Security Command Center\\n        organization.\",\n",
       " 'summarize to docstring: Begins executing a batch create jobs operation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import talent_v4\\n\\n    def sample_batch_create_jobs():\\n        # Create a client\\n        client = talent_v4.JobServiceClient()\\n\\n        # Initialize request argument(s)\\n        jobs = talent_v4.Job()\\n        jobs.company = \"company_value\"\\n        jobs.requisition_id = \"requisition_id_value\"\\n        jobs.title = \"title_value\"\\n        jobs.description = \"description_value\"\\n\\n        request = talent_v4.BatchCreateJobsRequest(\\n            parent=\"parent_value\",\\n            jobs=jobs,\\n        )\\n\\n        # Make the request\\n        operation = client.batch_create_jobs(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.talent_v4.types.BatchCreateJobsRequest, dict]):\\n        The request object. Request to create a batch of jobs.\\n    parent (str):\\n        Required. The resource name of the tenant under which\\n        the job is created.\\n\\n        The format is\\n        \"projects/{project_id}/tenants/{tenant_id}\". For\\n        example, \"projects/foo/tenants/bar\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    jobs (MutableSequence[google.cloud.talent_v4.types.Job]):\\n        Required. The jobs to be created.\\n        A maximum of 200 jobs can be created in\\n        a batch.\\n\\n        This corresponds to the ``jobs`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.talent_v4.types.BatchCreateJobsResponse` The result of\\n           [JobService.BatchCreateJobs][google.cloud.talent.v4.JobService.BatchCreateJobs].\\n           It\\'s used to replace\\n           [google.longrunning.Operation.response][google.longrunning.Operation.response]\\n           in case of success.',\n",
       " 'summarize to docstring: Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for get_management_dns_zone_binding\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VmwareEngine server.',\n",
       " 'summarize to docstring: Call the start scan run method over HTTP.\\n\\nArgs:\\n    request (~.web_security_scanner.StartScanRunRequest):\\n        The request object. Request for the ``StartScanRun`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.scan_run.ScanRun:\\n        A ScanRun is a output-only resource\\n    representing an actual run of the scan.\\n    Next id: 12',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: forward latex should really return nothing in either field if nothing is found.',\n",
       " 'summarize to docstring: Private function that doesn\\'t support extended axis or keepdims.\\nThese methods are extended to this function using _ureduce\\nSee nanpercentile for parameter usage\\nIt computes the quantiles of the array for the given axis.\\nA linear interpolation is performed based on the `interpolation`.\\n\\nBy default, the method is \"linear\" where alpha == beta == 1 which\\nperforms the 7th method of Hyndman&Fan.\\nWith \"median_unbiased\" we get alpha == beta == 1/3\\nthus the 8th method of Hyndman&Fan.',\n",
       " 'summarize to docstring: Custom memmap constructor compatible with numpy.memmap.\\n\\nThis function:\\n- is a backport the numpy memmap offset fix (See\\n  https://github.com/numpy/numpy/pull/8443 for more details.\\n  The numpy fix is available starting numpy 1.13)\\n- adds ``unlink_on_gc_collect``, which specifies  explicitly whether\\n  the process re-constructing the memmap owns a reference to the\\n  underlying file. If set to True, it adds a finalizer to the\\n  newly-created memmap that sends a maybe_unlink request for the\\n  memmaped file to resource_tracker.',\n",
       " 'summarize to docstring: If the most relevant error is an anyOf, then we traverse its context\\nand select the otherwise *least* relevant error, since in this case\\nthat means the most specific, deep, error inside the instance.\\n\\nI.e. since only one of the schemas must match, we look for the most\\nrelevant one.',\n",
       " 'summarize to docstring: Test that a nested `AtomicString` is not parsed. ',\n",
       " 'summarize to docstring: Recall the first view and position from the stack.',\n",
       " 'summarize to docstring: Remove this colorbar from the figure.\\n\\nIf the colorbar was created with ``use_gridspec=True`` the previous\\ngridspec is restored.',\n",
       " 'summarize to docstring: Tests that for loops at deeper levels are picked up',\n",
       " 'summarize to docstring: Add a colorbar to a plot.\\n\\nParameters\\n----------\\nmappable\\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\\n    mandatory for the `.Figure.colorbar` method but optional for the\\n    `.pyplot.colorbar` function, which sets the default to the current\\n    image.\\n\\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\\n    generate colorbars not attached to a previously drawn artist, e.g.\\n    ::\\n\\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\\n\\ncax : `~matplotlib.axes.Axes`, optional\\n    Axes into which the colorbar will be drawn.  If `None`, then a new\\n    Axes is created and the space for it will be stolen from the Axes(s)\\n    specified in *ax*.\\n\\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\\n    The one or more parent Axes from which space for a new colorbar Axes\\n    will be stolen. This parameter is only used if *cax* is not set.\\n\\n    Defaults to the Axes that contains the mappable used to create the\\n    colorbar.\\n\\nuse_gridspec : bool, optional\\n    If *cax* is ``None``, a new *cax* is created as an instance of\\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\\n    is ``True``, then *cax* is also positioned with a subplotspec.\\n\\nReturns\\n-------\\ncolorbar : `~matplotlib.colorbar.Colorbar`\\n\\nOther Parameters\\n----------------\\n%(_make_axes_kw_doc)s\\n%(_colormap_kw_doc)s\\n\\nNotes\\n-----\\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\\nincluded automatically.\\n\\nThe *shrink* kwarg provides a simple way to scale the colorbar with\\nrespect to the Axes. Note that if *cax* is specified, it determines the\\nsize of the colorbar, and *shrink* and *aspect* are ignored.\\n\\nFor more precise control, you can manually specify the positions of the\\naxes objects in which the mappable and the colorbar are drawn.  In this\\ncase, do not use any of the Axes properties kwargs.\\n\\nIt is known that some vector graphics viewers (svg and pdf) render\\nwhite gaps between segments of the colorbar.  This is due to bugs in\\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\\nrendered with overlapping segments::\\n\\n    cbar = colorbar()\\n    cbar.solids.set_edgecolor(\"face\")\\n    draw()\\n\\nHowever, this has negative consequences in other circumstances, e.g.\\nwith semi-transparent images (alpha < 1) and colorbar extensions;\\ntherefore, this workaround is not used by default (see issue #1188).',\n",
       " 'summarize to docstring: Set slider value to *val*.\\n\\nParameters\\n----------\\nval : float',\n",
       " 'summarize to docstring: Return the ``ZAxis`` (`~.axis3d.Axis`) instance.',\n",
       " 'summarize to docstring: Whether the returned results should be verbose.',\n",
       " 'summarize to docstring: Test scale function with non-array',\n",
       " \"summarize to docstring: Verify MAC challenge\\n\\nIf our message did not include a digest_name prefix, the client is allowed\\nto select a stronger digest_name from _ALLOWED_DIGESTS.\\n\\nIn case our message is prefixed, a client cannot downgrade to a weaker\\nalgorithm, because the MAC is calculated over the entire message\\nincluding the '{digest_name}' prefix.\",\n",
       " 'summarize to docstring: Given a non-complete graph G, returns a missing edge.',\n",
       " 'summarize to docstring: Make X orthogonal to the nullspace of L.',\n",
       " 'summarize to docstring: Determine if a provided dtype is of a specified data type ``kind``.\\n\\nThis function only supports built-in NumPy\\'s data types.\\nThird-party dtypes are not yet supported.\\n\\nParameters\\n----------\\ndtype : dtype\\n    The input dtype.\\nkind : dtype or str or tuple of dtypes/strs.\\n    dtype or dtype kind. Allowed dtype kinds are:\\n    * ``\\'bool\\'`` : boolean kind\\n    * ``\\'signed integer\\'`` : signed integer data types\\n    * ``\\'unsigned integer\\'`` : unsigned integer data types\\n    * ``\\'integral\\'`` : integer data types\\n    * ``\\'real floating\\'`` : real-valued floating-point data types\\n    * ``\\'complex floating\\'`` : complex floating-point data types\\n    * ``\\'numeric\\'`` : numeric data types\\n\\nReturns\\n-------\\nout : bool\\n\\nSee Also\\n--------\\nissubdtype\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> np.isdtype(np.float32, np.float64)\\nFalse\\n>>> np.isdtype(np.float32, \"real floating\")\\nTrue\\n>>> np.isdtype(np.complex128, (\"real floating\", \"complex floating\"))\\nTrue',\n",
       " 'summarize to docstring: Sets the pod_selector of this V1NetworkPolicySpec.\\n\\n\\n:param pod_selector: The pod_selector of this V1NetworkPolicySpec.  # noqa: E501\\n:type: V1LabelSelector',\n",
       " 'summarize to docstring: Ensure the logged in user has authorized silent OpenID authorization.\\n\\nSilent OpenID authorization allows access tokens and id tokens to be\\ngranted to clients without any user prompt or interaction.\\n\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n:rtype: True or False\\n\\nMethod is used by:\\n    - OpenIDConnectAuthCode\\n    - OpenIDConnectImplicit\\n    - OpenIDConnectHybrid',\n",
       " 'summarize to docstring: This property can be used as a prefix for any HTTP method call to return the\\nthe raw response object instead of the parsed content.\\n\\nFor more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers',\n",
       " 'summarize to docstring: Test that the meter provides a function to create a new ObservableCounter',\n",
       " 'summarize to docstring: Loads an ASN.1 object of an x509 certificate into a Certificate object\\n\\n:param certificate:\\n    An asn1crypto.x509.Certificate object\\n\\n:return:\\n    A Certificate object',\n",
       " 'summarize to docstring: Convert a native series structure to a Series object.',\n",
       " 'summarize to docstring: Return the list of thead row elements from the parsed table element.\\n\\nParameters\\n----------\\ntable : a table element that contains zero or more thead elements.\\n\\nReturns\\n-------\\nlist of node-like\\n    These are the <tr> row elements of a table.',\n",
       " 'summarize to docstring: Try to format axes if they are datelike.',\n",
       " 'summarize to docstring: return the root node',\n",
       " 'summarize to docstring: Convert the data from this selection to the appropriate pandas type.\\n\\nParameters\\n----------\\nvalues : np.ndarray\\nnan_rep : str\\nencoding : str\\nerrors : str',\n",
       " 'summarize to docstring: Return different versions of data for count times',\n",
       " 'summarize to docstring: Check if we have a not-outdated version loaded already.',\n",
       " 'summarize to docstring: Remove a number of characters from the end of the text.',\n",
       " 'summarize to docstring: This method should only be called once, before the connection is used.',\n",
       " 'summarize to docstring: Negate a polynomial in ``K[x]``.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x = ring(\"x\", ZZ)\\n\\n>>> R.dup_neg(x**2 - 1)\\n-x**2 + 1',\n",
       " 'summarize to docstring: Test Subversion.get_vcs_version() with previously cached result.',\n",
       " 'summarize to docstring: Execute a statement using :sql:`VALUES` with a sequence of parameters.\\n\\n:param cur: the cursor to use to execute the query.\\n\\n:param sql: the query to execute. It must contain a single ``%s``\\n    placeholder, which will be replaced by a `VALUES list`__.\\n    Example: ``\"INSERT INTO mytable (id, f1, f2) VALUES %s\"``.\\n\\n:param argslist: sequence of sequences or dictionaries with the arguments\\n    to send to the query. The type and content must be consistent with\\n    *template*.\\n\\n:param template: the snippet to merge to every item in *argslist* to\\n    compose the query.\\n\\n    - If the *argslist* items are sequences it should contain positional\\n      placeholders (e.g. ``\"(%s, %s, %s)\"``, or ``\"(%s, %s, 42)``\" if there\\n      are constants value...).\\n\\n    - If the *argslist* items are mappings it should contain named\\n      placeholders (e.g. ``\"(%(id)s, %(f1)s, 42)\"``).\\n\\n    If not specified, assume the arguments are sequence and use a simple\\n    positional template (i.e.  ``(%s, %s, ...)``), with the number of\\n    placeholders sniffed by the first element in *argslist*.\\n\\n:param page_size: maximum number of *argslist* items to include in every\\n    statement. If there are more items the function will execute more than\\n    one statement.\\n\\n:param fetch: if `!True` return the query results into a list (like in a\\n    `~cursor.fetchall()`).  Useful for queries with :sql:`RETURNING`\\n    clause.\\n\\n.. __: https://www.postgresql.org/docs/current/static/queries-values.html\\n\\nAfter the execution of the function the `cursor.rowcount` property will\\n**not** contain a total result.\\n\\nWhile :sql:`INSERT` is an obvious candidate for this function it is\\npossible to use it with other statements, for example::\\n\\n    >>> cur.execute(\\n    ... \"create table test (id int primary key, v1 int, v2 int)\")\\n\\n    >>> execute_values(cur,\\n    ... \"INSERT INTO test (id, v1, v2) VALUES %s\",\\n    ... [(1, 2, 3), (4, 5, 6), (7, 8, 9)])\\n\\n    >>> execute_values(cur,\\n    ... \"\"\"UPDATE test SET v1 = data.v1 FROM (VALUES %s) AS data (id, v1)\\n    ... WHERE test.id = data.id\"\"\",\\n    ... [(1, 20), (4, 50)])\\n\\n    >>> cur.execute(\"select * from test order by id\")\\n    >>> cur.fetchall()\\n    [(1, 20, 3), (4, 50, 6), (7, 8, 9)])',\n",
       " 'summarize to docstring: Finalize build system configuration on win32 platform.',\n",
       " 'summarize to docstring: Instantiate a cipher object that performs ECB encryption/decryption.\\n\\n:Parameters:\\n  factory : module\\n    The underlying block cipher, a module from ``Crypto.Cipher``.\\n\\nAll keywords are passed to the underlying block cipher.\\nSee the relevant documentation for details (at least ``key`` will need\\nto be present',\n",
       " 'summarize to docstring: `version` is the mypy version string.\\n\\nWe might want to use this to print a warning if the mypy version being used is\\nnewer, or especially older, than we expect (or need).\\n\\nArgs:\\n    version: The mypy version string.\\n\\nReturn:\\n    The Pydantic mypy plugin type.',\n",
       " 'summarize to docstring: Returns a dict of config keys to values.\\n\\nIt reads configs from toml file and returns `None` if the file is not a toml file.',\n",
       " 'summarize to docstring: :calls: `POST /repos/{owner}/{repo}/forks <https://docs.github.com/en/rest/reference/repos#forks>`_\\n:param organization: :class:`github.Organization.Organization` or string\\n:param name: string\\n:param default_branch_only: bool\\n:rtype: :class:`github.Repository.Repository`',\n",
       " 'summarize to docstring: Issue the warning :param:`message` for the definition of the given :param:`method`\\n\\nthis helps to log warnings for functions defined prior to finding an issue with them\\n(like hook wrappers being marked in a legacy mechanism)',\n",
       " \"summarize to docstring: Check whether 'value' should be coerced to 'field' type.\",\n",
       " 'summarize to docstring: Error 301 -- also relocated (permanently).',\n",
       " 'summarize to docstring: Issue #96',\n",
       " 'summarize to docstring: Polls a Cloud Pub/Sub subscription for new GCS events for display.',\n",
       " 'summarize to docstring: Sets the api_server_id of this V1alpha1ServerStorageVersion.\\n\\nThe ID of the reporting API server.  # noqa: E501\\n\\n:param api_server_id: The api_server_id of this V1alpha1ServerStorageVersion.  # noqa: E501\\n:type: str',\n",
       " \"summarize to docstring: Differentiate polynomials represented with coefficients.\\n\\np must be a 1-D or 2-D array.  In the 2-D case, each column gives\\nthe coefficients of a polynomial; the first row holds the coefficients\\nassociated with the highest power. m must be a nonnegative integer.\\n(numpy.polyder doesn't handle the 2-D case.)\",\n",
       " 'summarize to docstring: Return a :class:`.MapperOption` that will indicate to the\\n:class:`_query.Query`\\nthat the main table has been aliased.',\n",
       " \"summarize to docstring: Initialize an ``AlgorithmEstimator`` instance.\\n\\nArgs:\\n    algorithm_arn (str): algorithm arn used for training. Can be just the name if your\\n        account owns the algorithm.\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker\\n        training jobs and APIsthat create Amazon SageMaker endpoints use this role to\\n        access training data and model artifacts. After the endpoint\\n        is created, the inference code might use the IAM role, if it\\n        needs to access an AWS resource.\\n    instance_count (int or PipelineVariable): Number of Amazon EC2 instances to use\\n        for training.\\n    instance_type (str or PipelineVariable): Type of EC2 instance to use for training,\\n        for example, 'ml.c4.xlarge'.\\n    volume_size (int or PipelineVariable): Size in GB of the EBS volume to use for\\n        storing input data during training (default: 30). Must be large enough to store\\n        training data if File Mode is used (which is the default).\\n    volume_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting\\n        EBS volume attached to the training instance (default: None).\\n    max_run (int or PipelineVariable): Timeout in seconds for training\\n        (default: 24 * 60 * 60).\\n        After this amount of time Amazon SageMaker terminates the\\n        job regardless of its current status.\\n    input_mode (str or PipelineVariable): The input mode that the algorithm supports\\n        (default: 'File'). Valid modes:\\n\\n        * 'File' - Amazon SageMaker copies the training dataset from\\n          the S3 location to a local directory.\\n        * 'Pipe' - Amazon SageMaker streams data directly from S3 to\\n          the container via a Unix-named pipe.\\n\\n        This argument can be overriden on a per-channel basis using\\n        ``sagemaker.inputs.TrainingInput.input_mode``.\\n\\n    output_path (str or PipelineVariable): S3 location for saving the training result\\n        (model artifacts and output files). If not specified,\\n        results are stored to a default bucket. If\\n        the bucket with the specific name does not exist, the\\n        estimator creates the bucket during the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit` method\\n        execution.\\n    output_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting the\\n        training output (default: None). base_job_name (str): Prefix for\\n        training job name when the\\n        :meth:`~sagemaker.estimator.EstimatorBase.fit`\\n        method launches. If not specified, the estimator generates a\\n        default job name, based on the training image name and\\n        current timestamp.\\n    sagemaker_session (sagemaker.session.Session): Session object which manages\\n        interactions with Amazon SageMaker APIs and any other AWS services needed. If\\n        not specified, the estimator creates one using the default\\n        AWS configuration chain.\\n    tags (Union[Tags]): Tags for\\n        labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    subnets (list[str] or list[PipelineVariable]): List of subnet ids. If not specified\\n        training job will be created without VPC config.\\n        security_group_ids (list[str]): List of security group ids. If\\n        not specified training job will be created without VPC config.\\n    model_uri (str): URI where a pre-trained model is stored, either locally or in S3\\n        (default: None). If specified, the estimator will create a channel pointing to\\n        the model so the training job can download it. This model\\n        can be a 'model.tar.gz' from a previous training job, or\\n        other artifacts coming from a different source.\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization\\n    model_channel_name (str or PipelineVariable): Name of the channel where 'model_uri'\\n        will be downloaded (default: 'model'). metric_definitions\\n        (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for\\n        the name of the metric, and 'Regex' for the regular\\n        expression used to extract the metric from the logs.\\n    encrypt_inter_container_traffic (bool or PipelineVariable): Specifies whether traffic\\n        between training containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool or PipelineVariable): Specifies whether to use SageMaker\\n        Managed Spot instances for training. If enabled then the\\n        `max_wait` arg should also be set.\\n\\n        More information:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\\n        (default: ``False``).\\n    max_wait (int or PipelineVariable): Timeout in seconds waiting for spot training\\n        instances (default: None). After this amount of time Amazon\\n        SageMaker will stop waiting for Spot instances to become\\n        available (default: ``None``).\\n    **kwargs: Additional kwargs. This is unused. It's only added for AlgorithmEstimator\\n        to ignore the irrelevant arguments.\\n\\nRaises:\\n    ValueError:\\n    - If an AWS IAM Role is not provided.\\n    - Bad value for instance type.\\n    RuntimeError:\\n    - When setting up custom VPC, both subnets and security_group_ids are not provided\\n    - If instance_count > 1 (distributed training) with instance type local or local gpu\\n    - If LocalSession is not used with instance type local or local gpu\\n    - file:// output path used outside of local mode\\n    botocore.exceptions.ClientError:\\n    - algorithm arn is incorrect\\n    - insufficient permission to access/ describe algorithm\\n    - algorithm is in a different region\",\n",
       " 'summarize to docstring: Sets fields in object based on json of header.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of spec.',\n",
       " 'summarize to docstring: The full SELECT statement represented by this Query.\\n\\nThe statement by default will not have disambiguating labels\\napplied to the construct unless with_labels(True) is called\\nfirst.',\n",
       " 'summarize to docstring: Set Additional Model Source to ``this`` model.\\n\\nArgs:\\n    speculative_decoding_config (Optional[Dict[str, Any]]): Speculative decoding config.\\n    accept_eula (Optional[bool]): For models that require a Model Access Config.',\n",
       " 'summarize to docstring: Decorator to emit telemetry logs for SageMaker Python SDK functions',\n",
       " 'summarize to docstring: Memoized fast path for _get_funcs instances',\n",
       " 'summarize to docstring: Access nonzero values, possibly after summing duplicates.\\n\\nParameters\\n----------\\ns : sparse array\\n    Input sparse array.\\n\\nReturns\\n-------\\ndata: ndarray\\n  Nonzero values of the array, with shape (s.nnz,)',\n",
       " 'summarize to docstring: Compute matrices to transform rot. vector derivatives to angular rates.\\n\\nThe matrices depend on the current attitude represented as a rotation\\nvector.\\n\\nParameters\\n----------\\nrotvecs : ndarray, shape (n, 3)\\n    Set of rotation vectors.\\n\\nReturns\\n-------\\nndarray, shape (n, 3, 3)',\n",
       " 'summarize to docstring: Return a GeoJSON-like mapping of the LineString geometry.',\n",
       " 'summarize to docstring: Unsupported.',\n",
       " 'summarize to docstring: Does s3 multipart chunking work correctly?',\n",
       " 'summarize to docstring: test #6696',\n",
       " 'summarize to docstring: Gets the value of withCentering or its default value.',\n",
       " 'summarize to docstring: Read HTML tables into a ``list`` of ``DataFrame`` objects.\\n\\nParameters\\n----------\\nio : str or file-like\\n    A URL, a file-like object, or a raw string containing HTML. Note that\\n    lxml only accepts the http, FTP and file URL protocols. If you have a\\n    URL that starts with ``\\'https\\'`` you might try removing the ``\\'s\\'``.\\n\\n    .. deprecated:: 4.0.0\\n        Passing html literal strings is deprecated.\\n        Wrap literal string/bytes input in io.StringIO/io.BytesIO instead.\\n\\nmatch : str or compiled regular expression, optional\\n    The set of tables containing text matching this regex or string will be\\n    returned. Unless the HTML is extremely simple you will probably need to\\n    pass a non-empty string here. Defaults to \\'.+\\' (match any non-empty\\n    string). The default value will return all tables contained on a page.\\n    This value is converted to a regular expression so that there is\\n    consistent behavior between Beautiful Soup and lxml.\\n\\nflavor : str or None, container of strings\\n    The parsing engine to use. \\'bs4\\' and \\'html5lib\\' are synonymous with\\n    each other, they are both there for backwards compatibility. The\\n    default of ``None`` tries to use ``lxml`` to parse and if that fails it\\n    falls back on ``bs4`` + ``html5lib``.\\n\\nheader : int or list-like or None, optional\\n    The row (or list of rows for a :class:`~ps.MultiIndex`) to use to\\n    make the columns headers.\\n\\nindex_col : int or list-like or None, optional\\n    The column (or list of columns) to use to create the index.\\n\\nskiprows : int or list-like or slice or None, optional\\n    0-based. Number of rows to skip after parsing the column integer. If a\\n    sequence of integers or a slice is given, will skip the rows indexed by\\n    that sequence.  Note that a single element sequence means \\'skip the nth\\n    row\\' whereas an integer means \\'skip n rows\\'.\\n\\nattrs : dict or None, optional\\n    This is a dictionary of attributes that you can pass to use to identify\\n    the table in the HTML. These are not checked for validity before being\\n    passed to lxml or Beautiful Soup. However, these attributes must be\\n    valid HTML table attributes to work correctly. For example, ::\\n\\n        attrs = {\\'id\\': \\'table\\'}\\n\\n    is a valid attribute dictionary because the \\'id\\' HTML tag attribute is\\n    a valid HTML attribute for *any* HTML tag as per `this document\\n    <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\\n\\n        attrs = {\\'asdf\\': \\'table\\'}\\n\\n    is *not* a valid attribute dictionary because \\'asdf\\' is not a valid\\n    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\\n    table attributes can be found `here\\n    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\\n    working draft of the HTML 5 spec can be found `here\\n    <http://www.w3.org/TR/html-markup/table.html>`__. It contains the\\n    latest information on table attributes for the modern web.\\n\\nparse_dates : bool, optional\\n    See :func:`~ps.read_csv` for more details.\\n\\nthousands : str, optional\\n    Separator to use to parse thousands. Defaults to ``\\',\\'``.\\n\\nencoding : str or None, optional\\n    The encoding used to decode the web page. Defaults to ``None``.``None``\\n    preserves the previous encoding behavior, which depends on the\\n    underlying parser library (e.g., the parser library will try to use\\n    the encoding provided by the document).\\n\\ndecimal : str, default \\'.\\'\\n    Character to recognize as decimal point (example: use \\',\\' for European\\n    data).\\n\\nconverters : dict, default None\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\n\\nna_values : iterable, default None\\n    Custom NA values\\n\\nkeep_default_na : bool, default True\\n    If na_values are specified and keep_default_na is False the default NaN\\n    values are overridden, otherwise they\\'re appended to\\n\\ndisplayed_only : bool, default True\\n    Whether elements with \"display: none\" should be parsed\\n\\nReturns\\n-------\\ndfs : list of DataFrames\\n\\nSee Also\\n--------\\nread_csv\\nDataFrame.to_html',\n",
       " 'summarize to docstring: Truncate the timestamps for nanoseconds.',\n",
       " 'summarize to docstring: Test the different timestamp, date, and timedelta types.',\n",
       " 'summarize to docstring: Return the \"comment\" for the table identified by ``table_name``.\\n\\nGiven a string ``table_name`` and an optional string ``schema``, return\\ntable comment information as a dictionary corresponding to the\\n:class:`.ReflectedTableComment` dictionary.\\n\\nThis is an internal dialect method. Applications should use\\n:meth:`.Inspector.get_table_comment`.\\n\\n:raise: ``NotImplementedError`` for dialects that don\\'t support\\n comments.\\n\\n.. versionadded:: 1.2',\n",
       " 'summarize to docstring: Driver quirk where the cursor.fetchall() will work even if\\nthe connection has been rolled back.\\n\\nThis generally refers to buffered cursors but also seems to work\\nwith cx_oracle, for example.',\n",
       " 'summarize to docstring: Target must support simultaneous, independent database connections\\nthat will be used in a readonly fashion.',\n",
       " 'summarize to docstring: Start date column, if not present already.',\n",
       " 'summarize to docstring: Returns the typecast or ``None`` of this object as a string.',\n",
       " 'summarize to docstring: Target must support simultaneous, independent database connections.',\n",
       " 'summarize to docstring: If ``key`` is in ``dictionary``, set the new value of ``key``\\nto be the union between the old value and ``value``.\\nOtherwise, set the value of ``key`` to ``value.\\n\\nReturns ``True`` if the key already was in the dictionary and\\n``False`` otherwise.',\n",
       " 'summarize to docstring: Return True if expr1 and expr2 are numerically close.\\n\\nThe expressions must have the same structure, but any Rational, Integer, or\\nFloat numbers they contain are compared approximately using rtol and atol.\\nAny other parts of expressions are compared exactly. However, allowance is\\nmade to allow for the additive and multiplicative identities.\\n\\nRelative tolerance is measured with respect to expr2 so when used in\\ntesting expr2 should be the expected correct answer.\\n\\nExamples\\n========\\n\\n>>> from sympy import exp\\n>>> from sympy.abc import x, y\\n>>> from sympy.core.numbers import all_close\\n>>> expr1 = 0.1*exp(x - y)\\n>>> expr2 = exp(x - y)/10\\n>>> expr1\\n0.1*exp(x - y)\\n>>> expr2\\nexp(x - y)/10\\n>>> expr1 == expr2\\nFalse\\n>>> all_close(expr1, expr2)\\nTrue\\n\\nIdentities are automatically supplied:\\n\\n>>> all_close(x, x + 1e-10)\\nTrue\\n>>> all_close(x, 1.0*x)\\nTrue\\n>>> all_close(x, 1.0*x + 1e-10)\\nTrue',\n",
       " 'summarize to docstring: Turn all numbers in eq into their polar equivalents (under the standard\\nchoice of argument).\\n\\nNote that no attempt is made to guess a formal convention of adding\\npolar numbers, expressions like $1 + x$ will generally not be altered.\\n\\nNote also that this function does not promote ``exp(x)`` to ``exp_polar(x)``.\\n\\nIf ``subs`` is ``True``, all symbols which are not already polar will be\\nsubstituted for polar dummies; in this case the function behaves much\\nlike :func:`~.posify`.\\n\\nIf ``lift`` is ``True``, both addition statements and non-polar symbols are\\nchanged to their ``polar_lift()``ed versions.\\nNote that ``lift=True`` implies ``subs=False``.\\n\\nExamples\\n========\\n\\n>>> from sympy import polarify, sin, I\\n>>> from sympy.abc import x, y\\n>>> expr = (-x)**y\\n>>> expr.expand()\\n(-x)**y\\n>>> polarify(expr)\\n((_x*exp_polar(I*pi))**_y, {_x: x, _y: y})\\n>>> polarify(expr)[0].expand()\\n_x**_y*exp_polar(_y*I*pi)\\n>>> polarify(x, lift=True)\\npolar_lift(x)\\n>>> polarify(x*(1+y), lift=True)\\npolar_lift(x)*polar_lift(y + 1)\\n\\nAdds are treated carefully:\\n\\n>>> polarify(1 + sin((1 + I)*x))\\n(sin(_x*polar_lift(1 + I)) + 1, {_x: x})',\n",
       " 'summarize to docstring: Front-end function of the inverse Laplace transform. It tries to apply all\\nknown rules recursively.  If everything else fails, it tries to integrate.',\n",
       " 'summarize to docstring: Returns denominator of ``a``. ',\n",
       " 'summarize to docstring: Returns a field associated with ``self``. ',\n",
       " 'summarize to docstring: Return the Smith Normal Form of a matrix `m` over the ring `domain`.\\nThis will only work if the ring is a principal ideal domain.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_form\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> print(smith_normal_form(m).to_Matrix())\\nMatrix([[1, 0, 0], [0, 10, 0], [0, 0, 30]])',\n",
       " 'summarize to docstring: Returns ``True`` if ``f`` is a cyclotomic polynomial. ',\n",
       " 'summarize to docstring: Calls the given callback on the next IOLoop iteration.\\n\\nAs of Tornado 6.0, this method is equivalent to `add_callback`.\\n\\n.. versionadded:: 4.0',\n",
       " 'summarize to docstring: Put a connection back into the pool.\\n\\n:param conn:\\n    Connection object for the current host and port as returned by\\n    :meth:`._new_conn` or :meth:`._get_conn`.\\n\\nIf the pool is already full, the connection is closed and discarded\\nbecause we exceeded maxsize. If connections are discarded frequently,\\nthen maxsize should be increased.\\n\\nIf the pool is closed, then the connection will be closed and discarded.',\n",
       " \"summarize to docstring: Create a WebSocket server listening on a Unix socket.\\n\\nThis function is identical to :func:`serve`, except the ``host`` and\\n``port`` arguments are replaced by ``path``. It's only available on Unix.\\n\\nIt's useful for deploying a server behind a reverse proxy such as nginx.\\n\\nArgs:\\n    handler: Connection handler. It receives the WebSocket connection,\\n        which is a :class:`ServerConnection`, in argument.\\n    path: File system path to the Unix socket.\",\n",
       " 'summarize to docstring: Handshake succeeds without subprotocols.',\n",
       " 'summarize to docstring: Creates datatype test schema, tables, and inserts test data.',\n",
       " 'summarize to docstring: Write File.',\n",
       " \"summarize to docstring: Locale display names for months.\\n\\n>>> Locale('de', 'DE').months['format']['wide'][10]\\nu'Oktober'\",\n",
       " 'summarize to docstring: Return the era names used by the locale for the specified format.\\n\\n>>> get_era_names(\\'wide\\', locale=\\'en_US\\')[1]\\nu\\'Anno Domini\\'\\n>>> get_era_names(\\'abbreviated\\', locale=\\'de_DE\\')[1]\\nu\\'n. Chr.\\'\\n\\n:param width: the width to use, either \"wide\", \"abbreviated\", or \"narrow\"\\n:param locale: the `Locale` object, or a locale string',\n",
       " 'summarize to docstring: Return a node that represents the (type) result of an indexing operation,\\ne.g. for tuple unpacking or iteration.',\n",
       " 'summarize to docstring: Compacts the frames to deduplicate recursive calls.',\n",
       " 'summarize to docstring: Many of these arguments duplicate and override values that can be\\nprovided in a configuration file.  Parameters that are missing here\\nwill use values from the config file.\\n\\n`data_file` is the base name of the data file to use. The config value\\ndefaults to \".coverage\".  None can be provided to prevent writing a data\\nfile.  `data_suffix` is appended (with a dot) to `data_file` to create\\nthe final file name.  If `data_suffix` is simply True, then a suffix is\\ncreated with the machine and process identity included.\\n\\n`cover_pylib` is a boolean determining whether Python code installed\\nwith the Python interpreter is measured.  This includes the Python\\nstandard library and any packages installed with the interpreter.\\n\\nIf `auto_data` is true, then any existing data file will be read when\\ncoverage measurement starts, and data will be saved automatically when\\nmeasurement stops.\\n\\nIf `timid` is true, then a slower and simpler trace function will be\\nused.  This is important for some environments where manipulation of\\ntracing functions breaks the faster trace function.\\n\\nIf `branch` is true, then branch coverage will be measured in addition\\nto the usual statement coverage.\\n\\n`config_file` determines what configuration file to read:\\n\\n    * If it is \".coveragerc\", it is interpreted as if it were True,\\n      for backward compatibility.\\n\\n    * If it is a string, it is the name of the file to read.  If the\\n      file can\\'t be read, it is an error.\\n\\n    * If it is True, then a few standard files names are tried\\n      (\".coveragerc\", \"setup.cfg\", \"tox.ini\").  It is not an error for\\n      these files to not be found.\\n\\n    * If it is False, then no configuration file is read.\\n\\n`source` is a list of file paths or package names.  Only code located\\nin the trees indicated by the file paths or package names will be\\nmeasured.\\n\\n`source_pkgs` is a list of package names. It works the same as\\n`source`, but can be used to name packages where the name can also be\\ninterpreted as a file path.\\n\\n`include` and `omit` are lists of file name patterns. Files that match\\n`include` will be measured, files that match `omit` will not.  Each\\nwill also accept a single string argument.\\n\\n`debug` is a list of strings indicating what debugging information is\\ndesired.\\n\\n`concurrency` is a string indicating the concurrency library being used\\nin the measured code.  Without this, coverage.py will get incorrect\\nresults if these libraries are in use.  Valid strings are \"greenlet\",\\n\"eventlet\", \"gevent\", \"multiprocessing\", or \"thread\" (the default).\\nThis can also be a list of these strings.\\n\\nIf `check_preimported` is true, then when coverage is started, the\\nalready-imported files will be checked to see if they should be\\nmeasured by coverage.  Importing measured files before coverage is\\nstarted can mean that code is missed.\\n\\n`context` is a string to use as the :ref:`static context\\n<static_contexts>` label for collected data.\\n\\nIf `messages` is true, some messages will be printed to stdout\\nindicating what is happening.\\n\\n.. versionadded:: 4.0\\n    The `concurrency` parameter.\\n\\n.. versionadded:: 4.2\\n    The `concurrency` parameter can now be a list of strings.\\n\\n.. versionadded:: 5.0\\n    The `check_preimported` and `context` parameters.\\n\\n.. versionadded:: 5.3\\n    The `source_pkgs` parameter.\\n\\n.. versionadded:: 6.0\\n    The `messages` parameter.',\n",
       " 'summarize to docstring: Make sure our htmlcov directory exists.',\n",
       " 'summarize to docstring: Assert that numbits is good.',\n",
       " 'summarize to docstring: Make a tree of packages.\\n\\nMakes `width` directories, named d0 .. d{width-1}. Each directory has\\n__init__.py, and `width` files, named f0.py .. f{width-1}.py.  Each\\ndirectory also has `width` sub-directories, in the same fashion, until\\na depth of `depth` is reached.',\n",
       " 'summarize to docstring: Changes the value of a variable',\n",
       " 'summarize to docstring: Align the given address to the end of the page it occupies.\\nThat is, to point to the start of the next page.\\n\\n@type  address: int\\n@param address: Memory address.\\n\\n@rtype:  int\\n@return: Aligned memory address.',\n",
       " 'summarize to docstring: @rtype:  bool\\n@return: C{True} if the memory in this region belongs to a mapped file.',\n",
       " 'summarize to docstring: Set a the value for ``key`` to ``value`` inside the ``config``\\ndict.',\n",
       " 'summarize to docstring: Unpack the crc32 checksum for the ith object from the index file.',\n",
       " 'summarize to docstring: All refs present in this container.',\n",
       " \"summarize to docstring: Return a named tuple containing ISO year, week number, and weekday.\\n\\nThe first ISO week of the year is the (Mon-Sun) week\\ncontaining the year's first Thursday; everything else derives\\nfrom that.\\n\\nThe first week is 1; Monday is 1 ... Sunday is 7.\\n\\nISO calendar algorithm taken from\\nhttp://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\\n(used with permission)\",\n",
       " 'summarize to docstring: Mirror attributes and methods from the given\\norigin_name attribute of the instance to the\\ndecorated class',\n",
       " 'summarize to docstring: Post-rpc interceptor for create_analytics_account_link\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the MarketingplatformAdminService server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Call the list google ads links method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.ListGoogleAdsLinksRequest):\\n        The request object. Request message for\\n    ListGoogleAdsLinks RPC.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.analytics_admin.ListGoogleAdsLinksResponse:\\n        Response message for\\n    ListGoogleAdsLinks RPC.',\n",
       " 'summarize to docstring: Returns matching deployments.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import apigee_registry_v1\\n\\n    def sample_list_api_deployments():\\n        # Create a client\\n        client = apigee_registry_v1.RegistryClient()\\n\\n        # Initialize request argument(s)\\n        request = apigee_registry_v1.ListApiDeploymentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_api_deployments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.apigee_registry_v1.types.ListApiDeploymentsRequest, dict]):\\n        The request object. Request message for\\n        ListApiDeployments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        deployments. Format: ``projects/*/locations/*/apis/*``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.apigee_registry_v1.services.registry.pagers.ListApiDeploymentsPager:\\n        Response message for\\n        ListApiDeployments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ApiHubAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Call the get api spec method over HTTP.\\n\\nArgs:\\n    request (~.registry_service.GetApiSpecRequest):\\n        The request object. Request message for GetApiSpec.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.registry_models.ApiSpec:\\n        Describes a version of an API in a\\n    structured way. ApiSpecs provide formal\\n    descriptions that consumers can use to\\n    use a version. ApiSpec resources are\\n    intended to be fully-resolved\\n    descriptions of an ApiVersion. When\\n    specs consist of multiple files, these\\n    should be bundled together (e.g., in a\\n    zip archive) and stored as a unit.\\n    Multiple specs can exist to provide\\n    representations in different API\\n    description formats. Synchronization of\\n    these representations would be provided\\n    by tooling and background services.',\n",
       " 'summarize to docstring: return a boolean if I am possibly a view',\n",
       " 'summarize to docstring: Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'summarize to docstring: Returns the specified disk type.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.DiskTypesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetDiskTypeRequest(\\n            disk_type=\"disk_type_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetDiskTypeRequest, dict]):\\n        The request object. A request message for DiskTypes.Get.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone for this\\n        request.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk_type (str):\\n        Name of the disk type to return.\\n        This corresponds to the ``disk_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.DiskType:\\n        Represents a Disk Type resource. Google Compute Engine\\n        has two Disk Type resources: \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDiskTypes)\\n        \\\\* [Zonal](/compute/docs/reference/rest/v1/diskTypes)\\n        You can choose from a variety of disk types based on\\n        your needs. For more information, read Storage options.\\n        The diskTypes resource represents disk types for a zonal\\n        persistent disk. For more information, read Zonal\\n        persistent disks. The regionDiskTypes resource\\n        represents disk types for a regional persistent disk.\\n        For more information, read Regional persistent disks.',\n",
       " 'summarize to docstring: Retrieves a list of resize requests that are\\ncontained in the managed instance group.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InstanceGroupManagerResizeRequestsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInstanceGroupManagerResizeRequestsRequest(\\n            instance_group_manager=\"instance_group_manager_value\",\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInstanceGroupManagerResizeRequestsRequest, dict]):\\n        The request object. A request message for\\n        InstanceGroupManagerResizeRequests.List.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone where the\\n        managed instance group is located. The\\n        name should conform to RFC1035.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    instance_group_manager (str):\\n        The name of the managed instance\\n        group. The name should conform to\\n        RFC1035.\\n\\n        This corresponds to the ``instance_group_manager`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.instance_group_manager_resize_requests.pagers.ListPager:\\n        [Output Only] A list of resize requests.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'summarize to docstring: Snapshot the state of a streaming job.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataflow_v1beta3\\n\\n    def sample_snapshot_job():\\n        # Create a client\\n        client = dataflow_v1beta3.JobsV1Beta3Client()\\n\\n        # Initialize request argument(s)\\n        request = dataflow_v1beta3.SnapshotJobRequest(\\n        )\\n\\n        # Make the request\\n        response = client.snapshot_job(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataflow_v1beta3.types.SnapshotJobRequest, dict]):\\n        The request object. Request to create a snapshot of a\\n        job.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataflow_v1beta3.types.Snapshot:\\n        Represents a snapshot of a job.',\n",
       " 'summarize to docstring: Use this method to delete a private connectivity\\nconfiguration.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datastream_v1alpha1\\n\\n    def sample_delete_private_connection():\\n        # Create a client\\n        client = datastream_v1alpha1.DatastreamClient()\\n\\n        # Initialize request argument(s)\\n        request = datastream_v1alpha1.DeletePrivateConnectionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_private_connection(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datastream_v1alpha1.types.DeletePrivateConnectionRequest, dict]):\\n        The request object.\\n    name (str):\\n        Required. The name of the private\\n        connectivity configuration to delete.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'summarize to docstring: Call the fetch git hub\\ninstallations method over HTTP.\\n\\n    Args:\\n        request (~.developer_connect.FetchGitHubInstallationsRequest):\\n            The request object. Request for fetching github\\n        installations.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.developer_connect.FetchGitHubInstallationsResponse:\\n            Response of fetching github\\n        installations.',\n",
       " 'summarize to docstring: Call the update conversation\\nprofile method over HTTP.\\n\\n    Args:\\n        request (~.gcd_conversation_profile.UpdateConversationProfileRequest):\\n            The request object. The request message for\\n        [ConversationProfiles.UpdateConversationProfile][google.cloud.dialogflow.v2.ConversationProfiles.UpdateConversationProfile].\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcd_conversation_profile.ConversationProfile:\\n            Defines the services to connect to\\n        incoming Dialogflow conversations.',\n",
       " 'summarize to docstring: Updates the specified entity type.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflow_v2\\n\\n    def sample_update_entity_type():\\n        # Create a client\\n        client = dialogflow_v2.EntityTypesClient()\\n\\n        # Initialize request argument(s)\\n        entity_type = dialogflow_v2.EntityType()\\n        entity_type.display_name = \"display_name_value\"\\n        entity_type.kind = \"KIND_REGEXP\"\\n\\n        request = dialogflow_v2.UpdateEntityTypeRequest(\\n            entity_type=entity_type,\\n        )\\n\\n        # Make the request\\n        response = client.update_entity_type(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflow_v2.types.UpdateEntityTypeRequest, dict]):\\n        The request object. The request message for\\n        [EntityTypes.UpdateEntityType][google.cloud.dialogflow.v2.EntityTypes.UpdateEntityType].\\n    entity_type (google.cloud.dialogflow_v2.types.EntityType):\\n        Required. The entity type to update.\\n        This corresponds to the ``entity_type`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    language_code (str):\\n        Optional. The language used to access language-specific\\n        data. If not specified, the agent\\'s default language is\\n        used. For more information, see `Multilingual intent and\\n        entity\\n        data <https://cloud.google.com/dialogflow/docs/agents-multilingual#intent-entity>`__.\\n\\n        This corresponds to the ``language_code`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflow_v2.types.EntityType:\\n        Each intent parameter has a type, called the entity type, which dictates\\n           exactly how data from an end-user expression is\\n           extracted.\\n\\n           Dialogflow provides predefined system entities that\\n           can match many common types of data. For example,\\n           there are system entities for matching dates, times,\\n           colors, email addresses, and so on. You can also\\n           create your own custom entities for matching custom\\n           data. For example, you could define a vegetable\\n           entity that can match the types of vegetables\\n           available for purchase with a grocery store agent.\\n\\n           For more information, see the [Entity\\n           guide](\\\\ https://cloud.google.com/dialogflow/docs/entities-overview).',\n",
       " 'summarize to docstring: Call the complete query method over HTTP.\\n\\nArgs:\\n    request (~.completion_service.CompleteQueryRequest):\\n        The request object. Request message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.completion_service.CompleteQueryResponse:\\n        Response message for\\n    [CompletionService.CompleteQuery][google.cloud.discoveryengine.v1.CompletionService.CompleteQuery]\\n    method.',\n",
       " 'summarize to docstring: Deletes a single Subnet.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import edgenetwork_v1\\n\\n    def sample_delete_subnet():\\n        # Create a client\\n        client = edgenetwork_v1.EdgeNetworkClient()\\n\\n        # Initialize request argument(s)\\n        request = edgenetwork_v1.DeleteSubnetRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_subnet(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.edgenetwork_v1.types.DeleteSubnetRequest, dict]):\\n        The request object. Message for deleting a Subnet\\n    name (str):\\n        Required. Name of the resource\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'summarize to docstring: List OS policies compliance data for all Compute\\nEngine VM instances in the specified zone.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import osconfig_v1alpha\\n\\n    def sample_list_instance_os_policies_compliances():\\n        # Create a client\\n        client = osconfig_v1alpha.OsConfigZonalServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = osconfig_v1alpha.ListInstanceOSPoliciesCompliancesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_instance_os_policies_compliances(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.osconfig_v1alpha.types.ListInstanceOSPoliciesCompliancesRequest, dict]):\\n        The request object. A request message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n    parent (str):\\n        Required. The parent resource name.\\n\\n        Format: ``projects/{project}/locations/{location}``\\n\\n        For ``{project}``, either Compute Engine project-number\\n        or project-id can be provided.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.osconfig_v1alpha.services.os_config_zonal_service.pagers.ListInstanceOSPoliciesCompliancesPager:\\n        A response message for listing OS\\n        policies compliance data for all Compute\\n        Engine VMs in the given location.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'summarize to docstring: Return GeoTIFF metadata from first page as dict.',\n",
       " 'summarize to docstring: Call the list crypto keys method over HTTP.\\n\\nArgs:\\n    request (~.service.ListCryptoKeysRequest):\\n        The request object. Request message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.service.ListCryptoKeysResponse:\\n        Response message for\\n    [KeyManagementService.ListCryptoKeys][google.cloud.kms.v1.KeyManagementService.ListCryptoKeys].',\n",
       " 'summarize to docstring: Lists\\n[CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.security import privateca_v1\\n\\n    def sample_list_certificate_templates():\\n        # Create a client\\n        client = privateca_v1.CertificateAuthorityServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = privateca_v1.ListCertificateTemplatesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_certificate_templates(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.security.privateca_v1.types.ListCertificateTemplatesRequest, dict]):\\n        The request object. Request message for\\n        [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n    parent (str):\\n        Required. The resource name of the location associated\\n        with the\\n        [CertificateTemplates][google.cloud.security.privateca.v1.CertificateTemplate],\\n        in the format ``projects/*/locations/*``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.security.privateca_v1.services.certificate_authority_service.pagers.ListCertificateTemplatesPager:\\n        Response message for\\n           [CertificateAuthorityService.ListCertificateTemplates][google.cloud.security.privateca.v1.CertificateAuthorityService.ListCertificateTemplates].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'summarize to docstring: Post-rpc interceptor for update_certificate_template\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Deletes permanently all user events specified by the\\nfilter provided. Depending on the number of events\\nspecified by the filter, this operation could take hours\\nor days to complete. To test a filter, use the list\\ncommand first.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import retail_v2alpha\\n\\n    def sample_purge_user_events():\\n        # Create a client\\n        client = retail_v2alpha.UserEventServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = retail_v2alpha.PurgeUserEventsRequest(\\n            parent=\"parent_value\",\\n            filter=\"filter_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.purge_user_events(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.retail_v2alpha.types.PurgeUserEventsRequest, dict]):\\n        The request object. Request message for PurgeUserEvents\\n        method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.retail_v2alpha.types.PurgeUserEventsResponse` Response of the PurgeUserEventsRequest. If the long running operation is\\n           successfully done, then this message is returned by\\n           the google.longrunning.Operations.response field.',\n",
       " 'summarize to docstring: Return a callable for the pause job method over gRPC.\\n\\nPauses a job.\\n\\nIf a job is paused then the system will stop executing the job\\nuntil it is re-enabled via\\n[ResumeJob][google.cloud.scheduler.v1.CloudScheduler.ResumeJob].\\nThe state of the job is stored in\\n[state][google.cloud.scheduler.v1.Job.state]; if paused it will\\nbe set to\\n[Job.State.PAUSED][google.cloud.scheduler.v1.Job.State.PAUSED].\\nA job must be in\\n[Job.State.ENABLED][google.cloud.scheduler.v1.Job.State.ENABLED]\\nto be paused.\\n\\nReturns:\\n    Callable[[~.PauseJobRequest],\\n            ~.Job]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the delete series method over HTTP.\\n\\nArgs:\\n    request (~.streams_service.DeleteSeriesRequest):\\n        The request object. Message for deleting a Series.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Return whether the supplied file name fn matches pattern filename.',\n",
       " 'summarize to docstring: Start the main event loop.\\n\\nThis method is called by `.FigureManagerBase.pyplot_show`, which is the\\nimplementation of `.pyplot.show`.  To customize the behavior of\\n`.pyplot.show`, interactive backends should usually override\\n`~.FigureManagerBase.start_main_loop`; if more customized logic is\\nnecessary, `~.FigureManagerBase.pyplot_show` can also be overridden.',\n",
       " 'summarize to docstring: Process every newly added tool.',\n",
       " \"summarize to docstring: Enable or disable IPython GUI event loop integration.\\n\\n%gui [GUINAME]\\n\\nThis magic replaces IPython's threaded shells that were activated\\nusing the (pylab/wthread/etc.) command line flags.  GUI toolkits\\ncan now be enabled at runtime and keyboard\\ninterrupts should work without any problems.  The following toolkits\\nare supported:  wxPython, PyQt4, PyGTK, Tk and Cocoa (OSX)::\\n\\n    %gui wx      # enable wxPython event loop integration\\n    %gui qt      # enable PyQt/PySide event loop integration\\n                 # with the latest version available.\\n    %gui qt6     # enable PyQt6/PySide6 event loop integration\\n    %gui qt5     # enable PyQt5/PySide2 event loop integration\\n    %gui gtk     # enable PyGTK event loop integration\\n    %gui gtk3    # enable Gtk3 event loop integration\\n    %gui gtk4    # enable Gtk4 event loop integration\\n    %gui tk      # enable Tk event loop integration\\n    %gui osx     # enable Cocoa event loop integration\\n                 # (requires %matplotlib 1.1)\\n    %gui         # disable all event loop integration\\n\\nWARNING:  after any of these has been called you can simply create\\nan application object, but DO NOT start the event loop yourself, as\\nwe have already handled that.\",\n",
       " 'summarize to docstring: Run the conda package manager within the current kernel.\\n\\nUsage:\\n  %micromamba install [pkgs]',\n",
       " 'summarize to docstring: Sagemath use custom prompt and we broke them in 8.19.',\n",
       " 'summarize to docstring: Create a table of color schemes.\\n\\nThe table can be created empty and manually filled or it can be\\ncreated with a list of valid color schemes AND the specification for\\nthe default active scheme.',\n",
       " 'summarize to docstring: Some functions use type vars that are not defined by the class, but rather\\nonly defined in the function. See for example `iter`. In those cases we\\nwant to:\\n\\n1. Search for undefined type vars.\\n2. Infer type vars with the execution state we have.\\n3. Return the union of all type vars that have been found.',\n",
       " 'summarize to docstring: Provided only as a backward-compatible wrapper around `.SSHConfig`.\\n\\n.. deprecated:: 2.7\\n    Use `SSHConfig.from_file` instead.',\n",
       " 'summarize to docstring: Binds the app context to the current context.',\n",
       " 'summarize to docstring: Return the corners of the rectangle, moving anti-clockwise from\\n(x0, y0).',\n",
       " 'summarize to docstring: Check that the animated artists changed in callbacks are updated.',\n",
       " \"summarize to docstring: Parameters\\n----------\\nnbins : int or 'auto', optional\\n    Number of ticks. Only used if minor is False.\\nminor : bool, default: False\\n    Indicate if this locator is for minor ticks or not.\",\n",
       " 'summarize to docstring: Set list of module names to try to load in forkserver process.',\n",
       " 'summarize to docstring: Convert the given BSON value into our own type.',\n",
       " 'summarize to docstring: Raise a BulkWriteError from the full bulk api result.',\n",
       " 'summarize to docstring: Discover python modules and packages in sub-directory.\\n\\nReturns iterator of paths to discovered modules and packages.',\n",
       " 'summarize to docstring: Node betweenness_centrality helper:\\n\\nSee betweenness_centrality for what you probably want.\\nThis actually computes \"load\" and not betweenness.\\nSee https://networkx.lanl.gov/ticket/103\\n\\nThis calculates the load of each node for paths from a single source.\\n(The fraction of number of shortests paths from source that go\\nthrough each node.)\\n\\nTo get the load for a node you need to do all-pairs shortest paths.\\n\\nIf weight is not None then use Dijkstra for finding shortest paths.',\n",
       " 'summarize to docstring: Tests for providing an alternate distance metric to the generator.',\n",
       " 'summarize to docstring: Write a graph `G` in GML format to the file or file handle `path`.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n    The graph to be converted to GML.\\n\\npath : filename or filehandle\\n    The filename or filehandle to write. Files whose names end with .gz or\\n    .bz2 will be compressed.\\n\\nstringizer : callable, optional\\n    A `stringizer` which converts non-int/non-float/non-dict values into\\n    strings. If it cannot convert a value into a string, it should raise a\\n    `ValueError` to indicate that. Default value: None.\\n\\nRaises\\n------\\nNetworkXError\\n    If `stringizer` cannot convert a value into a string, or the value to\\n    convert is not a string while `stringizer` is None.\\n\\nSee Also\\n--------\\nread_gml, generate_gml\\nliteral_stringizer\\n\\nNotes\\n-----\\nGraph attributes named \\'directed\\', \\'multigraph\\', \\'node\\' or\\n\\'edge\\', node attributes named \\'id\\' or \\'label\\', edge attributes\\nnamed \\'source\\' or \\'target\\' (or \\'key\\' if `G` is a multigraph)\\nare ignored because these attribute names are used to encode the graph\\nstructure.\\n\\nGML files are stored using a 7-bit ASCII encoding with any extended\\nASCII characters (iso8859-1) appearing as HTML character entities.\\nWithout specifying a `stringizer`/`destringizer`, the code is capable of\\nwriting `int`/`float`/`str`/`dict`/`list` data as required by the GML\\nspecification.  For writing other data types, and for reading data other\\nthan `str` you need to explicitly supply a `stringizer`/`destringizer`.\\n\\nNote that while we allow non-standard GML to be read from a file, we make\\nsure to write GML format. In particular, underscores are not allowed in\\nattribute names.\\nFor additional documentation on the GML file format, please see the\\n`GML url <https://web.archive.org/web/20190207140002/http://www.fim.uni-passau.de/index.php?id=17297&L=1>`_.\\n\\nSee the module docstring :mod:`networkx.readwrite.gml` for more details.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(4)\\n>>> nx.write_gml(G, \"test.gml\")\\n\\nFilenames ending in .gz or .bz2 will be compressed.\\n\\n>>> nx.write_gml(G, \"test.gml.gz\")',\n",
       " \"summarize to docstring: Get information about the arguments accepted by a code object.\\n\\nThree things are returned: (args, varargs, varkw), where 'args' is\\na list of argument names (possibly containing nested lists), and\\n'varargs' and 'varkw' are the names of the * and ** arguments or None.\",\n",
       " 'summarize to docstring: validate multi targets that defined between parentheses()',\n",
       " 'summarize to docstring: Ensures that filename is opened with correct encoding parameter.\\n\\nThis function uses charset_normalizer package, when available, for\\ndetermining the encoding of the file to be opened. When charset_normalizer\\nis not available, the function detects only UTF encodings, otherwise, ASCII\\nencoding is used as fallback.',\n",
       " 'summarize to docstring: Coefficients should be modifiable ',\n",
       " \"summarize to docstring: Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.\\n\\nThis function returns the values:\\n\\n.. math:: p(a,b,c) = \\\\sum_{i,j,k} c_{i,j,k} * T_i(a) * T_j(b) * T_k(c)\\n\\nwhere the points ``(a, b, c)`` consist of all triples formed by taking\\n`a` from `x`, `b` from `y`, and `c` from `z`. The resulting points form\\na grid with `x` in the first dimension, `y` in the second, and `z` in\\nthe third.\\n\\nThe parameters `x`, `y`, and `z` are converted to arrays only if they\\nare tuples or a lists, otherwise they are treated as a scalars. In\\neither case, either `x`, `y`, and `z` or their elements must support\\nmultiplication and addition both with themselves and with the elements\\nof `c`.\\n\\nIf `c` has fewer than three dimensions, ones are implicitly appended to\\nits shape to make it 3-D. The shape of the result will be c.shape[3:] +\\nx.shape + y.shape + z.shape.\\n\\nParameters\\n----------\\nx, y, z : array_like, compatible objects\\n    The three dimensional series is evaluated at the points in the\\n    Cartesian product of `x`, `y`, and `z`.  If `x`, `y`, or `z` is a\\n    list or tuple, it is first converted to an ndarray, otherwise it is\\n    left unchanged and, if it isn't an ndarray, it is treated as a\\n    scalar.\\nc : array_like\\n    Array of coefficients ordered so that the coefficients for terms of\\n    degree i,j are contained in ``c[i,j]``. If `c` has dimension\\n    greater than two the remaining indices enumerate multiple sets of\\n    coefficients.\\n\\nReturns\\n-------\\nvalues : ndarray, compatible object\\n    The values of the two dimensional polynomial at points in the Cartesian\\n    product of `x` and `y`.\\n\\nSee Also\\n--------\\nchebval, chebval2d, chebgrid2d, chebval3d\\n\\nNotes\\n-----\",\n",
       " 'summarize to docstring: Forces the mask to soft',\n",
       " 'summarize to docstring: Initialize the RuntimeContext\\n\\nReturns:\\n    An instance of RuntimeContext.',\n",
       " 'summarize to docstring: Returns a `Tracer` for use by the given instrumentation library.\\n\\nThis function is a convenience wrapper for\\nopentelemetry.trace.TracerProvider.get_tracer.\\n\\nIf tracer_provider is omitted the current configured one is used.',\n",
       " 'summarize to docstring: Test `inject()` method for Format.BINARY.',\n",
       " 'summarize to docstring: Preloads asn1crypto and optionally oscrypto from a local source checkout,\\nor from a normal install\\n\\n:param require_oscrypto:\\n    A bool if oscrypto needs to be preloaded\\n\\n:param print_info:\\n    A bool if info about asn1crypto and oscrypto should be printed',\n",
       " 'summarize to docstring: Pairwise frames test_pairwise',\n",
       " 'summarize to docstring: Sends a Unix signal to the subprocess.\\n\\nUse constants from the :mod:`signal` module to specify which signal.',\n",
       " 'summarize to docstring: Return a comparison of actual and expected hash values.\\n\\nExample::\\n\\n       Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde\\n                    or 123451234512345123451234512345123451234512345\\n            Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef',\n",
       " 'summarize to docstring: Return True if `name` is a considered as an archive file.',\n",
       " 'summarize to docstring: Replace the password in a given url with ****.',\n",
       " 'summarize to docstring: Return all the distribution names known to this locator.',\n",
       " 'summarize to docstring: Recursive helper for :func:`_rec_strip`.',\n",
       " 'summarize to docstring: Test that Link.from_json() produces Links with consistent cache\\nlocations',\n",
       " 'summarize to docstring: Test deprecated default_value=None behavior for Container subclass traits',\n",
       " 'summarize to docstring: See 2.3.3 in RFC6979',\n",
       " \"summarize to docstring: Parse the input into a new type, deferring resolution of the type until the current class\\nis fully defined.\\n\\nThis is useful when you need to reference the class in it's own type annotations.\",\n",
       " \"summarize to docstring: Don't die on unary +.\",\n",
       " \"summarize to docstring: Called to perform the setup phase for a test item.\\n\\nThe default implementation runs ``setup()`` on ``item`` and all of its\\nparents (which haven't been setup yet). This includes obtaining the\\nvalues of fixtures required by the item (which haven't been obtained\\nyet).\\n\\n:param item:\\n    The item.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest file can implement this hook. For a given item, only conftest\\nfiles in the item's directory and its parent directories are consulted.\",\n",
       " 'summarize to docstring: Custom .cfg files with [tool:pytest] section are read correctly',\n",
       " 'summarize to docstring: `pythonpath` kicks early enough to load plugins via -p (#11118).',\n",
       " 'summarize to docstring: Additional properties to set if ``sourceFormat`` is set to AVRO.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/tables#ExternalDataConfiguration.FIELDS.avro_options',\n",
       " 'summarize to docstring: Test whether a resource is enabled.  Known resources are set by\\nregrtest.py.',\n",
       " 'summarize to docstring: Saves the private key in PKCS#1 DER format.\\n\\n:returns: the DER-encoded private key.\\n:rtype: bytes',\n",
       " 'summarize to docstring: Compute minimum distances between one point and a set of points.\\n\\nThis function computes for each row in X, the index of the row of Y which\\nis closest (according to the specified distance).\\n\\nThis is mostly equivalent to calling::\\n\\n    pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\\n\\nbut uses much less memory, and is faster for large arrays.\\n\\nThis function works with dense 2D arrays only.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples_X, n_features)\\n    Array containing points.\\n\\nY : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\\n    Arrays containing points.\\n\\naxis : int, default=1\\n    Axis along which the argmin and distances are to be computed.\\n\\nmetric : str or callable, default=\"euclidean\"\\n    Metric to use for distance computation. Any metric from scikit-learn\\n    or scipy.spatial.distance can be used.\\n\\n    If metric is a callable function, it is called on each\\n    pair of instances (rows) and the resulting value recorded. The callable\\n    should take two arrays as input and return one value indicating the\\n    distance between them. This works for Scipy\\'s metrics, but is less\\n    efficient than passing the metric name as a string.\\n\\n    Distance matrices are not supported.\\n\\n    Valid values for metric are:\\n\\n    - from scikit-learn: [\\'cityblock\\', \\'cosine\\', \\'euclidean\\', \\'l1\\', \\'l2\\',\\n      \\'manhattan\\']\\n\\n    - from scipy.spatial.distance: [\\'braycurtis\\', \\'canberra\\', \\'chebyshev\\',\\n      \\'correlation\\', \\'dice\\', \\'hamming\\', \\'jaccard\\', \\'kulsinski\\',\\n      \\'mahalanobis\\', \\'minkowski\\', \\'rogerstanimoto\\', \\'russellrao\\',\\n      \\'seuclidean\\', \\'sokalmichener\\', \\'sokalsneath\\', \\'sqeuclidean\\',\\n      \\'yule\\']\\n\\n    See the documentation for scipy.spatial.distance for details on these\\n    metrics.\\n\\n    .. note::\\n       `\\'kulsinski\\'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\\n\\n    .. note::\\n       `\\'matching\\'` has been removed in SciPy 1.9 (use `\\'hamming\\'` instead).\\n\\nmetric_kwargs : dict, default=None\\n    Keyword arguments to pass to specified metric function.\\n\\nReturns\\n-------\\nargmin : numpy.ndarray\\n    Y[argmin[i], :] is the row in Y that is closest to X[i, :].\\n\\nSee Also\\n--------\\npairwise_distances : Distances between every pair of samples of X and Y.\\npairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\\n    returns the distances.\\n\\nExamples\\n--------\\n>>> from sklearn.metrics.pairwise import pairwise_distances_argmin\\n>>> X = [[0, 0, 0], [1, 1, 1]]\\n>>> Y = [[1, 0, 0], [1, 1, 0]]\\n>>> pairwise_distances_argmin(X, Y)\\narray([0, 1])',\n",
       " 'summarize to docstring: Internal: Align multiline celltext text to bottom',\n",
       " 'summarize to docstring: delete_collection_namespaced_deployment  # noqa: E501\\n\\ndelete collection of Deployment  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_deployment_with_http_info(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'summarize to docstring: list_storage_version_migration  # noqa: E501\\n\\nlist or watch objects of kind StorageVersionMigration  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.list_storage_version_migration(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param bool allow_watch_bookmarks: allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server\\'s discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1alpha1StorageVersionMigrationList\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: override if setattr should do something other than call self.set',\n",
       " 'summarize to docstring: Sets the _continue of this V1ListMeta.\\n\\ncontinue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available. The value is opaque and may be used to issue another request to the endpoint that served this list to retrieve the next set of available objects. Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed. The resourceVersion field returned when using this continue value will be identical to the value in the first response, unless you have received this token from an error message.  # noqa: E501\\n\\n:param _continue: The _continue of this V1ListMeta.  # noqa: E501\\n:type: str',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " \"summarize to docstring: Gets the max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n\\nMaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | |  P P  |  P P  |   P   | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed.  # noqa: E501\\n\\n:return: The max_skew of this V1TopologySpreadConstraint.  # noqa: E501\\n:rtype: int\",\n",
       " 'summarize to docstring: Append SCORE_FIELD and SCORE.',\n",
       " \"summarize to docstring: Return a :class:`~sagemaker.amazon.LinearLearnerModel`.\\n\\nIt references the latest s3 model data produced by this Estimator.\\n\\nArgs:\\n    vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on\\n        the model. Default: use subnets and security groups from this Estimator.\\n        * 'Subnets' (list[str]): List of subnet ids.\\n        * 'SecurityGroupIds' (list[str]): List of security group ids.\\n    **kwargs: Additional kwargs passed to the LinearLearnerModel constructor.\",\n",
       " 'summarize to docstring: Determines if the ``ast.Call`` node points to an ``ast.Name`` node with a matching name.\\n\\nArgs:\\n    node (ast.Call): a node that represents a function call. For more,\\n        see https://docs.python.org/3/library/ast.html#abstract-grammar.\\n    name (str): the function name.\\n\\nReturns:\\n    bool: if ``node.func`` is an ``ast.Name`` node with a matching name.',\n",
       " 'summarize to docstring: Construct a dictionary based on the attributes.\\n\\nReturns:\\n    dict represents the attributes.',\n",
       " 'summarize to docstring: Initialize source algorithm object\\n\\nArgs:\\n    algorithm_name (str): The ARN of an algorithm resource that was used to create the model package.\\n    model_data_url (str, optional): The Amazon S3 path where the model artifacts, which result from model training, are stored (default: None).',\n",
       " 'summarize to docstring: Initialize a TrainingDetails object.\\n\\nArgs:\\n    objective_function (ObjectiveFunction, optional): The objective function that is optimized during training (default: None).\\n    training_observations (str, optional): Any observations about training (default: None).\\n    training_job_details (TrainingJobDetails, optional): Details about any associated training jobs (default: None).',\n",
       " 'summarize to docstring: Generates indices to split data into training and test set.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    Training data, where `n_samples` is the number of samples\\n    and `n_features` is the number of features.\\n\\ny : array-like of shape (n_samples,)\\n    The target variable for supervised learning problems.\\n\\ngroups : array-like of shape (n_samples,), default=None\\n    Group labels for the samples used while splitting the dataset into\\n    train/test set.\\n\\nYields\\n------\\ntrain : ndarray\\n    The training set indices for that split.\\n\\ntest : ndarray\\n    The testing set indices for that split.',\n",
       " 'summarize to docstring: Get feature names from X.\\n\\nSupport for other array containers should place its implementation here.\\n\\nParameters\\n----------\\nX : {ndarray, dataframe} of shape (n_samples, n_features)\\n    Array container to extract feature names.\\n\\n    - pandas dataframe : The columns will be considered to be feature\\n      names. If the dataframe contains non-string feature names, `None` is\\n      returned.\\n    - All other array containers will return `None`.\\n\\nReturns\\n-------\\nnames: ndarray or None\\n    Feature names of `X`. Unrecognized array containers will return `None`.',\n",
       " \"summarize to docstring: Add a new knot.\\n\\n(Approximately) replicate FITPACK's logic:\\n  1. split the `x` array into knot intervals, ``t(j+k) <= x(i) <= t(j+k+1)``\\n  2. find the interval with the maximum sum of residuals\\n  3. insert a new knot into the middle of that interval.\\n\\nNB: a new knot is in fact an `x` value at the middle of the interval.\\nSo *the knots are a subset of `x`*.\\n\\nThis routine is an analog of\\nhttps://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpcurf.f#L190-L215\\n(cf _split function)\\n\\nand https://github.com/scipy/scipy/blob/v1.11.4/scipy/interpolate/fitpack/fpknot.f\",\n",
       " 'summarize to docstring: Encode bson.objectid.ObjectId.',\n",
       " 'summarize to docstring: Sets the value of :py:attr:`labelType`.',\n",
       " 'summarize to docstring: Returns R^2^, the coefficient of determination.',\n",
       " 'summarize to docstring: Returns a rotation matrix for a rotation of theta (in radians)\\nabout the 3-axis.\\n\\nExplanation\\n===========\\n\\nFor a right-handed coordinate system, this corresponds to a\\nclockwise rotation around the `z`-axis, given by:\\n\\n.. math::\\n\\n    R  = \\\\begin{bmatrix}\\n             \\\\cos(\\\\theta) & \\\\sin(\\\\theta) & 0 \\\\\\\\\\n            -\\\\sin(\\\\theta) & \\\\cos(\\\\theta) & 0 \\\\\\\\\\n                        0 &            0 & 1\\n        \\\\end{bmatrix}\\n\\nExamples\\n========\\n\\n>>> from sympy import pi, rot_axis3\\n\\nA rotation of pi/3 (60 degrees):\\n\\n>>> theta = pi/3\\n>>> rot_axis3(theta)\\nMatrix([\\n[       1/2, sqrt(3)/2, 0],\\n[-sqrt(3)/2,       1/2, 0],\\n[         0,         0, 1]])\\n\\nIf we rotate by pi/2 (90 degrees):\\n\\n>>> rot_axis3(pi/2)\\nMatrix([\\n[ 0, 1, 0],\\n[-1, 0, 0],\\n[ 0, 0, 1]])\\n\\nSee Also\\n========\\n\\nrot_givens: Returns a Givens rotation matrix (generalized rotation for\\n    any number of dimensions)\\nrot_ccw_axis3: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 3-axis (counterclockwise around the z axis)\\nrot_axis1: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 1-axis (clockwise around the x axis)\\nrot_axis2: Returns a rotation matrix for a rotation of theta (in radians)\\n    about the 2-axis (clockwise around the y axis)',\n",
       " 'summarize to docstring: Apply \"render derived\" to this :class:`_sql.TableValuedAlias`.\\n\\nThis has the effect of the individual column names listed out\\nafter the alias name in the \"AS\" sequence, e.g.:\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.unnest(array([\"one\", \"two\", \"three\"])).\\n                table_valued(\"x\", with_ordinality=\"o\").render_derived()\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.x, anon_1.o\\n    FROM unnest(ARRAY[%(param_1)s, %(param_2)s, %(param_3)s]) WITH ORDINALITY AS anon_1(x, o)\\n\\nThe ``with_types`` keyword will render column types inline within\\nthe alias expression (this syntax currently applies to the\\nPostgreSQL database):\\n\\n.. sourcecode:: pycon+sql\\n\\n    >>> print(\\n    ...     select(\\n    ...         func.json_to_recordset(\\n    ...             \\'[{\"a\":1,\"b\":\"foo\"},{\"a\":\"2\",\"c\":\"bar\"}]\\'\\n    ...         )\\n    ...         .table_valued(column(\"a\", Integer), column(\"b\", String))\\n    ...         .render_derived(with_types=True)\\n    ...     )\\n    ... )\\n    {printsql}SELECT anon_1.a, anon_1.b FROM json_to_recordset(:json_to_recordset_1)\\n    AS anon_1(a INTEGER, b VARCHAR)\\n\\n:param name: optional string name that will be applied to the alias\\n generated.  If left as None, a unique anonymizing name will be used.\\n\\n:param with_types: if True, the derived columns will include the\\n datatype specification with each column. This is a special syntax\\n currently known to be required by PostgreSQL for some SQL functions.',\n",
       " 'summarize to docstring: Matches any differential equation that nth_algebraic can solve. Uses\\n`sympy.solve` but teaches it how to integrate derivatives.\\n\\nThis involves calling `sympy.solve` and does most of the work of finding a\\nsolution (apart from evaluating the integrals).',\n",
       " \"summarize to docstring: Create a string item.\\n\\nBy default, this function will create *single line basic* strings, but\\nboolean flags (e.g. ``literal=True`` and/or ``multiline=True``)\\ncan be used for personalization.\\n\\nFor more information, please check the spec: `<https://toml.io/en/v1.0.0#string>`__.\\n\\nCommon escaping rules will be applied for basic strings.\\nThis can be controlled by explicitly setting ``escape=False``.\\nPlease note that, if you disable escaping, you will have to make sure that\\nthe given strings don't contain any forbidden character or sequence.\",\n",
       " 'summarize to docstring: Return a decorated class with a constructor signature that contain Trait names as kwargs.',\n",
       " 'summarize to docstring: Figures out the full host name for the given domain part.  The\\ndomain part is a subdomain in case host matching is disabled or\\na full host name.',\n",
       " 'summarize to docstring: Check if the mimetype indicates JSON data, either\\n:mimetype:`application/json` or :mimetype:`application/*+json`.',\n",
       " 'summarize to docstring: Sets ``Content-Disposition`` header.',\n",
       " 'summarize to docstring: Determines if the labels in a domain are a match for labels from a\\nwildcard valid domain name\\n\\n:param domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in the domain name to check\\n\\n:param valid_domain_labels:\\n    A list of unicode strings, with A-label form for IDNs, of the labels\\n    in a wildcard domain pattern\\n\\n:return:\\n    A boolean - if the domain matches the valid domain',\n",
       " 'summarize to docstring: Indicates whether the :class:`Arrow <arrow.arrow.Arrow>` object is a repeated wall time in the current\\ntimezone.',\n",
       " 'summarize to docstring: :return:\\n    Integer',\n",
       " 'summarize to docstring: Upload Part.',\n",
       " 'summarize to docstring: Parse the .c file written by pgen.  (Internal)\\n\\nThe file looks as follows.  The first two lines are always this:\\n\\n#include \"pgenheaders.h\"\\n#include \"grammar.h\"\\n\\nAfter that come four blocks:\\n\\n1) one or more state definitions\\n2) a table defining dfas\\n3) a table defining labels\\n4) a struct defining the grammar\\n\\nA state definition has the following form:\\n- one or more arc arrays, each of the form:\\n  static arc arcs_<n>_<m>[<k>] = {\\n          {<i>, <j>},\\n          ...\\n  };\\n- followed by a state array, of the form:\\n  static state states_<s>[<t>] = {\\n          {<k>, arcs_<n>_<m>},\\n          ...\\n  };',\n",
       " \"summarize to docstring: Check if a stream's encoding and errors attributes are\\ncompatible with the desired values.\",\n",
       " 'summarize to docstring: Alias for :meth:`main`.',\n",
       " 'summarize to docstring: Returns the sample value, or None if not found.\\n\\nThis is inefficient, and intended only for use in unittests.',\n",
       " 'summarize to docstring: Run a benchmarking experiment and print a table of results.\\n\\nArguments:\\n\\n    py_versions: The Python versions to test.\\n    cov_versions: The coverage versions to test.\\n    projects: The projects to run.\\n    rows: A list of strings chosen from `\"pyver\"`, `\"cov\"`, and `\"proj\"`.\\n    column: The remaining dimension not used in `rows`.\\n    ratios: A list of triples: (title, slug1, slug2).\\n    num_runs: The number of times to run each matrix element.',\n",
       " 'summarize to docstring: Import modules randomly to stress coverage.',\n",
       " 'summarize to docstring: Yields a sequence of (PyObjectPtr key, PyObjectPtr value) pairs,\\nanalogous to dict.iteritems()',\n",
       " 'summarize to docstring: Main entry point to process a list of .py files and inject type inferred declarations.',\n",
       " 'summarize to docstring: >>> in_lambda_in_list_comprehension1()\\n[[0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6], [0, 2, 4, 6]]',\n",
       " 'summarize to docstring: Build an EDNS option object from wire format.\\n\\n*otype*, an ``int``, is the option type.\\n\\n*parser*, a ``dns.wire.Parser``, the parser, which should be\\nrestricted to the option length.\\n\\nReturns an instance of a subclass of ``dns.edns.Option``.',\n",
       " 'summarize to docstring: Add the specified rdata to the rdataset.\\n\\nIf the optional *ttl* parameter is supplied, then\\n``self.update_ttl(ttl)`` will be called prior to adding the rdata.\\n\\n*rd*, a ``dns.rdata.Rdata``, the rdata\\n\\n*ttl*, an ``int``, the TTL.\\n\\nRaises ``dns.rdataset.IncompatibleTypes`` if the type and class\\ndo not match the type and class of the rdataset.\\n\\nRaises ``dns.rdataset.DifferingCovers`` if the type is a signature\\ntype and the covered type does not match that of the rdataset.',\n",
       " 'summarize to docstring: Remove a container. Similar to the ``docker rm`` command.\\n\\nArgs:\\n    container (str): The container to remove\\n    v (bool): Remove the volumes associated with the container\\n    link (bool): Remove the specified link and not the underlying\\n        container\\n    force (bool): Force the removal of a running container (uses\\n        ``SIGKILL``)\\n\\nRaises:\\n    :py:class:`docker.errors.APIError`\\n        If the server returns an error.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " \"summarize to docstring: Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'summarize to docstring: Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsFirewallPolicyRequest):\\n        The request object. A request message for\\n    FirewallPolicies.TestIamPermissions. See\\n    the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'summarize to docstring: Pre-rpc interceptor for list_terraform_versions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Config server.',\n",
       " 'summarize to docstring: Gets a previously created question.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataqna_v1alpha\\n\\n    def sample_get_question():\\n        # Create a client\\n        client = dataqna_v1alpha.QuestionServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = dataqna_v1alpha.GetQuestionRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_question(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataqna_v1alpha.types.GetQuestionRequest, dict]):\\n        The request object. A request to get a previously created\\n        question.\\n    name (str):\\n        Required. The unique identifier for the question.\\n        Example: ``projects/foo/locations/bar/questions/1234``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataqna_v1alpha.types.Question:\\n        The question resource represents a\\n        natural language query, its settings,\\n        understanding generated by the system,\\n        and answer retrieval status. A question\\n        cannot be modified.',\n",
       " 'summarize to docstring: Call the get job metrics method over HTTP.\\n\\nArgs:\\n    request (~.metrics.GetJobMetricsRequest):\\n        The request object. Request to get job metrics.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.metrics.JobMetrics:\\n        JobMetrics contains a collection of\\n    metrics describing the detailed progress\\n    of a Dataflow job. Metrics correspond to\\n    user-defined and system-defined metrics\\n    in the job.\\n\\n    This resource captures only the most\\n    recent values of each metric;\\n    time-series data can be queried for them\\n    (under the same metric names) from Cloud\\n    Monitoring.',\n",
       " 'summarize to docstring: Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.metastore.CreateServiceRequest):\\n        The request object. Request message for\\n    [DataprocMetastore.CreateService][google.cloud.metastore.v1.DataprocMetastore.CreateService].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CloudDeploy server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for update_target\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CloudDeploy server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Return a callable for the get validation result method over gRPC.\\n\\nGets agent validation result. Agent validation is\\nperformed during training time and is updated\\nautomatically when training is completed.\\n\\nReturns:\\n    Callable[[~.GetValidationResultRequest],\\n            ~.ValidationResult]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Creates a\\n[SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1alpha\\n\\n    def sample_create_sample_query():\\n        # Create a client\\n        client = discoveryengine_v1alpha.SampleQueryServiceClient()\\n\\n        # Initialize request argument(s)\\n        sample_query = discoveryengine_v1alpha.SampleQuery()\\n        sample_query.query_entry.query = \"query_value\"\\n\\n        request = discoveryengine_v1alpha.CreateSampleQueryRequest(\\n            parent=\"parent_value\",\\n            sample_query=sample_query,\\n            sample_query_id=\"sample_query_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_sample_query(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1alpha.types.CreateSampleQueryRequest, dict]):\\n        The request object. Request message for\\n        [SampleQueryService.CreateSampleQuery][google.cloud.discoveryengine.v1alpha.SampleQueryService.CreateSampleQuery]\\n        method.\\n    parent (str):\\n        Required. The parent resource name, such as\\n        ``projects/{project}/locations/{location}/sampleQuerySets/{sampleQuerySet}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query (google.cloud.discoveryengine_v1alpha.types.SampleQuery):\\n        Required. The\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]\\n        to create.\\n\\n        This corresponds to the ``sample_query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    sample_query_id (str):\\n        Required. The ID to use for the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        which will become the final component of the\\n        [SampleQuery.name][google.cloud.discoveryengine.v1alpha.SampleQuery.name].\\n\\n        If the caller does not have permission to create the\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery],\\n        regardless of whether or not it exists, a\\n        ``PERMISSION_DENIED`` error is returned.\\n\\n        This field must be unique among all\\n        [SampleQuery][google.cloud.discoveryengine.v1alpha.SampleQuery]s\\n        with the same\\n        [parent][google.cloud.discoveryengine.v1alpha.CreateSampleQueryRequest.parent].\\n        Otherwise, an ``ALREADY_EXISTS`` error is returned.\\n\\n        This field must conform to\\n        `RFC-1034 <https://tools.ietf.org/html/rfc1034>`__\\n        standard with a length limit of 63 characters.\\n        Otherwise, an ``INVALID_ARGUMENT`` error is returned.\\n\\n        This corresponds to the ``sample_query_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.discoveryengine_v1alpha.types.SampleQuery:\\n        Sample Query captures metadata to be\\n        used for evaluation.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for create_discovery_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DlpService server.',\n",
       " 'summarize to docstring: Updates metadata associated with a dataset. Note that this\\nmethod requires the\\n``documentai.googleapis.com/datasets.update`` permission on the\\nproject, which is highly privileged. A user or service account\\nwith this permission can create new processors that can interact\\nwith any gcs bucket in your project.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_update_dataset():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        dataset = documentai_v1beta3.Dataset()\\n        dataset.state = \"INITIALIZED\"\\n\\n        request = documentai_v1beta3.UpdateDatasetRequest(\\n            dataset=dataset,\\n        )\\n\\n        # Make the request\\n        operation = client.update_dataset(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.UpdateDatasetRequest, dict]):\\n        The request object.\\n    dataset (google.cloud.documentai_v1beta3.types.Dataset):\\n        Required. The ``name`` field of the ``Dataset`` is used\\n        to identify the resource to be updated.\\n\\n        This corresponds to the ``dataset`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        The update mask applies to the\\n        resource.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.documentai_v1beta3.types.Dataset` A singleton resource under a\\n           [Processor][google.cloud.documentai.v1beta3.Processor]\\n           which configures a collection of documents.',\n",
       " 'summarize to docstring: Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    ManagedIdentitiesServiceAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Lists the clusters in a given project and location.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import managedkafka_v1\\n\\n    def sample_list_clusters():\\n        # Create a client\\n        client = managedkafka_v1.ManagedKafkaClient()\\n\\n        # Initialize request argument(s)\\n        request = managedkafka_v1.ListClustersRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_clusters(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.managedkafka_v1.types.ListClustersRequest, dict]):\\n        The request object. Request for ListClusters.\\n    parent (str):\\n        Required. The parent location whose clusters are to be\\n        listed. Structured like\\n        ``projects/{project}/locations/{location}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.managedkafka_v1.services.managed_kafka.pagers.ListClustersPager:\\n        Response for ListClusters.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'summarize to docstring: Creates a new GrpcRoute in a given project and\\nlocation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import network_services_v1\\n\\n    def sample_create_grpc_route():\\n        # Create a client\\n        client = network_services_v1.NetworkServicesClient()\\n\\n        # Initialize request argument(s)\\n        grpc_route = network_services_v1.GrpcRoute()\\n        grpc_route.name = \"name_value\"\\n        grpc_route.hostnames = [\\'hostnames_value1\\', \\'hostnames_value2\\']\\n\\n        request = network_services_v1.CreateGrpcRouteRequest(\\n            parent=\"parent_value\",\\n            grpc_route_id=\"grpc_route_id_value\",\\n            grpc_route=grpc_route,\\n        )\\n\\n        # Make the request\\n        operation = client.create_grpc_route(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.network_services_v1.types.CreateGrpcRouteRequest, dict]):\\n        The request object. Request used by the CreateGrpcRoute\\n        method.\\n    parent (str):\\n        Required. The parent resource of the GrpcRoute. Must be\\n        in the format ``projects/*/locations/global``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route (google.cloud.network_services_v1.types.GrpcRoute):\\n        Required. GrpcRoute resource to be\\n        created.\\n\\n        This corresponds to the ``grpc_route`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    grpc_route_id (str):\\n        Required. Short name of the GrpcRoute\\n        resource to be created.\\n\\n        This corresponds to the ``grpc_route_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.network_services_v1.types.GrpcRoute` GrpcRoute is the resource defining how gRPC traffic routed by a Mesh\\n           or Gateway resource is routed.',\n",
       " 'summarize to docstring: Quantize the bitmap to make it 8-bit (paletted). Returns a new\\nFIBitmap object.\\nOnly for 24 bit images.',\n",
       " 'summarize to docstring: Call the list models method over HTTP.\\n\\nArgs:\\n    request (~.model_service.ListModelsRequest):\\n        The request object. Request for listing models associated\\n    with a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.model_service.ListModelsResponse:\\n        Response to a ListModelRequest.',\n",
       " 'summarize to docstring: Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'summarize to docstring: Performs bidirectional streaming speech synthesis:\\nreceive audio while sending text.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import texttospeech_v1\\n\\n    async def sample_streaming_synthesize():\\n        # Create a client\\n        client = texttospeech_v1.TextToSpeechAsyncClient()\\n\\n        # Initialize request argument(s)\\n        streaming_config = texttospeech_v1.StreamingSynthesizeConfig()\\n        streaming_config.voice.language_code = \"language_code_value\"\\n\\n        request = texttospeech_v1.StreamingSynthesizeRequest(\\n            streaming_config=streaming_config,\\n        )\\n\\n        # This method expects an iterator which contains\\n        # \\'texttospeech_v1.StreamingSynthesizeRequest\\' objects\\n        # Here we create a generator that yields a single `request` for\\n        # demonstrative purposes.\\n        requests = [request]\\n\\n        def request_generator():\\n            for request in requests:\\n                yield request\\n\\n        # Make the request\\n        stream = await client.streaming_synthesize(requests=request_generator())\\n\\n        # Handle the response\\n        async for response in stream:\\n            print(response)\\n\\nArgs:\\n    requests (AsyncIterator[`google.cloud.texttospeech_v1.types.StreamingSynthesizeRequest`]):\\n        The request object AsyncIterator. Request message for the ``StreamingSynthesize`` method.\\n        Multiple ``StreamingSynthesizeRequest`` messages are\\n        sent in one call. The first message must contain a\\n        ``streaming_config`` that fully specifies the request\\n        configuration and must not contain ``input``. All\\n        subsequent messages must only have ``input`` set.\\n    retry (google.api_core.retry_async.AsyncRetry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    AsyncIterable[google.cloud.texttospeech_v1.types.StreamingSynthesizeResponse]:\\n        StreamingSynthesizeResponse is the only message returned to the\\n           client by StreamingSynthesize method. A series of\\n           zero or more StreamingSynthesizeResponse messages are\\n           streamed back to the client.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for get_connector\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VpcAccessService server.',\n",
       " 'summarize to docstring: Returns `True` for 4xx status codes, `False` otherwise.',\n",
       " 'summarize to docstring: Print the docstring for an object.\\n\\nIf the given object is a class, it will print both the class and the\\nconstructor docstrings.',\n",
       " 'summarize to docstring: Idle, the editor bundled with python\\n\\nParameters\\n----------\\nexe : str, None\\n    If none, should be pretty smart about finding the executable.',\n",
       " 'summarize to docstring: Tests for definition_start_position and definition_end_position',\n",
       " \"summarize to docstring: Configure the null keyring as the default.\\n\\n>>> fs = getfixture('fs')\\n>>> disable()\\n>>> disable()\\nTraceback (most recent call last):\\n...\\nRuntimeError: Refusing to overwrite...\",\n",
       " 'summarize to docstring: Parameters\\n----------\\nxy : (float, float)\\n  The lower left corner of the box.\\n\\nwidth : float\\n    The width of the box.\\n\\nheight : float\\n    The height of the box.\\n\\nboxstyle : str or `~matplotlib.patches.BoxStyle`\\n    The style of the fancy box. This can either be a `.BoxStyle`\\n    instance or a string of the style name and optionally comma\\n    separated attributes (e.g. \"Round, pad=0.2\"). This string is\\n    passed to `.BoxStyle` to construct a `.BoxStyle` object. See\\n    there for a full documentation.\\n\\n    The following box styles are available:\\n\\n    %(BoxStyle:table)s\\n\\nmutation_scale : float, default: 1\\n    Scaling factor applied to the attributes of the box style\\n    (e.g. pad or rounding_size).\\n\\nmutation_aspect : float, default: 1\\n    The height of the rectangle will be squeezed by this value before\\n    the mutation and the mutated box will be stretched by the inverse\\n    of it. For example, this allows different horizontal and vertical\\n    padding.\\n\\nOther Parameters\\n----------------\\n**kwargs : `~matplotlib.patches.Patch` properties\\n\\n%(Patch:kwdoc)s',\n",
       " 'summarize to docstring: Test constrained_layout for nested gridspecs',\n",
       " 'summarize to docstring: Parameters\\n----------\\nh : list of :mod:`~mpl_toolkits.axes_grid1.axes_size`\\n    sizes for horizontal division',\n",
       " 'summarize to docstring: Parameters\\n----------\\nresult : np.ndarray\\nfill_value : object, default iNaT\\nconvert : str, dtype or None\\n\\nReturns\\n-------\\nresult : ndarray with values replace by the fill_value\\n\\nmask the result if needed, convert to the provided dtype if its not\\nNone\\n\\nThis is an internal routine.',\n",
       " 'summarize to docstring: Reset the option store to its initial state\\n\\nReturns\\n-------\\nNone',\n",
       " 'summarize to docstring: Create a new, parsed `SSHConfig` from the file found at ``path``.\\n\\n.. versionadded:: 2.7',\n",
       " 'summarize to docstring: Blocking expect',\n",
       " \"summarize to docstring: Recompute this distribution's dependencies.\",\n",
       " 'summarize to docstring: Test using global distutils options.\\n(In particular those that disable the actual install action)',\n",
       " 'summarize to docstring: Create a new hash object.\\n\\n:parameter data:\\n    Optional. The very first chunk of the message to hash.\\n    It is equivalent to an early call to :meth:`SHA224Hash.update`.\\n:type data: byte string/byte array/memoryview\\n\\n:Return: A :class:`SHA224Hash` hash object',\n",
       " 'summarize to docstring: :calls: `GET /users/{user}/events/orgs/{org} <http://docs.github.com/en/rest/reference/activity#events>`_',\n",
       " 'summarize to docstring: (Experimental) Rate limit for GraphQL API, use with caution.',\n",
       " 'summarize to docstring: Registers a new Algorithm for use when creating and verifying tokens.',\n",
       " 'summarize to docstring: Transforms a yes/no or stringified bool into a bool.',\n",
       " 'summarize to docstring: Set an attribute.',\n",
       " 'summarize to docstring: This is a Sphinx docstring.\\n\\n:raises NameError: Never',\n",
       " 'summarize to docstring: Depends on `undefined1` in function return annotation. ',\n",
       " 'summarize to docstring: The value of `OpenSSL.SSL.OP_NO_QUERY_MTU` is 0x1000, the value\\nof `SSL_OP_NO_QUERY_MTU` defined by `openssl/ssl.h`.',\n",
       " \"summarize to docstring: The 'uses no fixture' error tells the user at collection time\\nthat the parametrize data they've set up doesn't correspond to the\\nfixtures in their test function, rather than silently ignoring this\\nand letting the test potentially pass.\\n\\n#714\",\n",
       " 'summarize to docstring: #3498',\n",
       " 'summarize to docstring: Call the update topic method over HTTP.\\n\\nArgs:\\n    request (~.pubsub.UpdateTopicRequest):\\n        The request object. Request for the UpdateTopic method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.pubsub.Topic:\\n        A topic resource.',\n",
       " 'summarize to docstring: Mode of the Wishart distribution\\n\\nOnly valid if the degrees of freedom are greater than the dimension of\\nthe scale matrix.\\n\\nParameters\\n----------\\n%(_doc_default_callparams)s\\n\\nReturns\\n-------\\nmode : float or None\\n    The Mode of the distribution',\n",
       " 'summarize to docstring: Check for non-CSR input to private method `_silhouette_reduce`.',\n",
       " \"summarize to docstring: create_ip_address  # noqa: E501\\n\\ncreate an IPAddress  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.create_ip_address_with_http_info(body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param V1beta1IPAddress body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1beta1IPAddress, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'summarize to docstring: Return new instance of configuration.\\n\\nThis method returns newly created, based on default constructor,\\nobject of Configuration class or returns a copy of default\\nconfiguration passed by the set_default method.\\n\\n:return: The configuration object.',\n",
       " 'summarize to docstring: Gets the rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n\\n\\n:return: The rolling_update of this V1DaemonSetUpdateStrategy.  # noqa: E501\\n:rtype: V1RollingUpdateDaemonSet',\n",
       " \"summarize to docstring: Sets the propagation_policy of this V1DeleteOptions.\\n\\nWhether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.  # noqa: E501\\n\\n:param propagation_policy: The propagation_policy of this V1DeleteOptions.  # noqa: E501\\n:type: str\",\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " \"summarize to docstring: Returns the raw data of the ``event``'s latency spikes time series.\\n\\nFor more information see https://redis.io/commands/latency-history\",\n",
       " \"summarize to docstring: AWS4Auth instances can be created by supplying key scope parameters\\ndirectly or by using an AWS4SigningKey instance:\\n\\n>>> auth = AWS4Auth(access_id, secret_key, region, service\\n...                 [, date][, raise_invalid_date=False][, session_token=None])\\n\\n  or\\n\\n>>> auth = AWS4Auth(access_id, signing_key[, raise_invalid_date=False])\\n\\n  or using auto-refreshed STS temporary creds via botocore RefreshableCredentials\\n  (useful for long-running processes):\\n\\n>>> auth = AWS4Auth(refreshable_credentials=botocore.session.Session().get_credentials(),\\n...                 region='eu-west-1', service='es')\\n\\naccess_id   -- This is your AWS access ID\\nsecret_key  -- This is your AWS secret access key\\nregion      -- The region you're connecting to, as per the list at\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\\n               e.g. us-east-1. For services which don't require a region\\n               (e.g. IAM), use us-east-1.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\nservice     -- The name of the service you're connecting to, as per\\n               endpoints at:\\n               http://docs.aws.amazon.com/general/latest/gr/rande.html\\n               e.g. elasticbeanstalk.\\n               Must be supplied as a keyword argument iff refreshable_credentials\\n               is set.\\ndate        -- Date this instance is valid for. 8-digit date as str of the\\n               form YYYYMMDD. Key is only valid for requests with a\\n               Date or X-Amz-Date header matching this date. If date is\\n               not supplied the current date is used.\\nsigning_key -- An AWS4SigningKey instance.\\nraise_invalid_date\\n            -- Must be supplied as keyword argument. AWS4Auth tries to\\n               parse a date from the X-Amz-Date and Date headers of the\\n               request, first trying X-Amz-Date, and then Date if\\n               X-Amz-Date is not present or is in an unrecognised\\n               format. If one or both of the two headers are present\\n               yet neither are in a format which AWS4Auth recognises\\n               then it will remove both headers and replace with a new\\n               X-Amz-Date header using the current date.\\n\\n               If this behaviour is not wanted, set the\\n               raise_invalid_date keyword argument to True, and\\n               instead an InvalidDateError will be raised when neither\\n               date is recognised. If neither header is present at all\\n               then an X-Amz-Date header will still be added containing\\n               the current date.\\n\\n               See the AWS4Auth class docstring for supported date\\n               formats.\\nsession_token\\n            -- Must be supplied as keyword argument. If session_token\\n               is set, then it is used for the x-amz-security-token\\n               header, for use with STS temporary credentials.\\nrefreshable_credentials\\n            -- A botocore.credentials.RefreshableCredentials instance.\\n               Must be supplied as keyword argument. This instance is\\n               used to generate valid per-request static credentials,\\n               without needing to re-generate the AWS4Auth instance.                       \\n               If refreshable_credentials is set, the following arguments\\n               are ignored: access_id, secret_key, signing_key,\\n               session_token.\",\n",
       " 'summarize to docstring: Rename namespace ``session`` to ``inputs``.',\n",
       " 'summarize to docstring: Instantiates HubNotebookDocument object.\\n\\nArgs:\\n    json_obj (Dict[str, Any]): Dictionary representation of hub content document.',\n",
       " 'summarize to docstring: Return a scalar result corresponding to the given\\ncolumn expression.',\n",
       " 'summarize to docstring: Initializes a ConstraintViolations object from a file path.\\n\\nArgs:\\n    constraint_violations_file_path (str): The path to the constraint violations file.\\n    kms_key (str): The kms_key to use when encrypting the file in S3.\\n    sagemaker_session (sagemaker.session.Session): A SageMaker Session\\n        object, used for SageMaker interactions (default: None). If not\\n        specified, one is created using the default AWS configuration\\n        chain.\\n\\nReturns:\\n    sagemaker.model_monitor.ConstraintViolations: The instance of ConstraintViolations\\n        generated from the local file path.',\n",
       " \"summarize to docstring: Save this hyperparameter to be applied to the graph_manager object when\\nit's ready.\",\n",
       " 'summarize to docstring: Iterate over the points in the grid.\\n\\nReturns\\n-------\\nparams : iterator over dict of str to any\\n    Yields dictionaries mapping each estimator parameter to one of its\\n    allowed values.',\n",
       " \"summarize to docstring: Find root of a function within an interval using bisection.\\n\\nBasic bisection routine to find a root of the function `f` between the\\narguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\\nSlow but sure.\\n\\nParameters\\n----------\\nf : function\\n    Python function returning a number.  `f` must be continuous, and\\n    f(a) and f(b) must have opposite signs.\\na : scalar\\n    One end of the bracketing interval [a,b].\\nb : scalar\\n    The other end of the bracketing interval [a,b].\\nxtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter must be positive.\\nrtol : number, optional\\n    The computed root ``x0`` will satisfy ``np.allclose(x, x0,\\n    atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\\n    parameter cannot be smaller than its default value of\\n    ``4*np.finfo(float).eps``.\\nmaxiter : int, optional\\n    If convergence is not achieved in `maxiter` iterations, an error is\\n    raised. Must be >= 0.\\nargs : tuple, optional\\n    Containing extra arguments for the function `f`.\\n    `f` is called by ``apply(f, (x)+args)``.\\nfull_output : bool, optional\\n    If `full_output` is False, the root is returned. If `full_output` is\\n    True, the return value is ``(x, r)``, where x is the root, and r is\\n    a `RootResults` object.\\ndisp : bool, optional\\n    If True, raise RuntimeError if the algorithm didn't converge.\\n    Otherwise, the convergence status is recorded in a `RootResults`\\n    return object.\\n\\nReturns\\n-------\\nroot : float\\n    Root of `f` between `a` and `b`.\\nr : `RootResults` (present if ``full_output = True``)\\n    Object containing information about the convergence. In particular,\\n    ``r.converged`` is True if the routine converged.\\n\\nExamples\\n--------\\n\\n>>> def f(x):\\n...     return (x**2 - 1)\\n\\n>>> from scipy import optimize\\n\\n>>> root = optimize.bisect(f, 0, 2)\\n>>> root\\n1.0\\n\\n>>> root = optimize.bisect(f, -2, 0)\\n>>> root\\n-1.0\\n\\nSee Also\\n--------\\nbrentq, brenth, bisect, newton\\nfixed_point : scalar fixed-point finder\\nfsolve : n-dimensional root-finding\",\n",
       " 'summarize to docstring: Checks whether a matrix contains only independent rows of another',\n",
       " \"summarize to docstring: Compare to output from 'qvoronoi o Fv < data' to Voronoi()\",\n",
       " 'summarize to docstring: Return True if a Geometry is prepared.\\n\\nNote that it is not necessary to check if a geometry is already prepared\\nbefore preparing it. It is more efficient to call ``prepare`` directly\\nbecause it will skip geometries that are already prepared.\\n\\nThis function will return False for missing geometries (None).\\n\\nParameters\\n----------\\ngeometry : Geometry or array_like\\n    Geometry or geometries to check.\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\nis_valid_input : check if an object is a geometry or None\\nprepare : prepare a geometry\\n\\nExamples\\n--------\\n>>> from shapely import Point, prepare\\n>>> geometry = Point(0, 0)\\n>>> is_prepared(Point(0, 0))\\nFalse\\n>>> prepare(geometry)\\n>>> is_prepared(geometry)\\nTrue\\n>>> is_prepared(None)\\nFalse',\n",
       " 'summarize to docstring: Is prerelease.',\n",
       " 'summarize to docstring: Trim values at input threshold(s).\\n\\nAssigns values outside boundary-to-boundary values.\\n\\nParameters\\n----------\\nlower : float or int, default None\\n    Minimum threshold value. All values below this threshold will be set to it.\\nupper : float or int, default None\\n    Maximum threshold value. All values above this threshold will be set to it.\\n\\nReturns\\n-------\\nDataFrame\\n    DataFrame with the values outside the clip boundaries replaced.\\n\\nExamples\\n--------\\n>>> ps.DataFrame({\\'A\\': [0, 2, 4]}).clip(1, 3)\\n   A\\n0  1\\n1  2\\n2  3\\n\\nNotes\\n-----\\nOne difference between this implementation and pandas is that running\\npd.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1) will crash with \"TypeError: \\'<=\\' not supported\\nbetween instances of \\'str\\' and \\'int\\'\" while ps.DataFrame({\\'A\\': [\\'a\\', \\'b\\']}).clip(0, 1)\\nwill output the original DataFrame, simply ignoring the incompatible types.',\n",
       " 'summarize to docstring: Generates an RDD comprised of vectors containing i.i.d. samples drawn\\nfrom the uniform distribution U(0.0, 1.0).\\n\\n.. versionadded:: 1.1.0\\n\\nParameters\\n----------\\nsc : :py:class:`pyspark.SparkContext`\\n    SparkContext used to create the RDD.\\nnumRows : int\\n    Number of Vectors in the RDD.\\nnumCols : int\\n    Number of elements in each Vector.\\nnumPartitions : int, optional\\n    Number of partitions in the RDD.\\nseed : int, optional\\n    Seed for the RNG that generates the seed for the generator in each partition.\\n\\nReturns\\n-------\\n:py:class:`pyspark.RDD`\\n    RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n>>> mat.shape\\n(10, 10)\\n>>> bool(mat.max() <= 1.0 and mat.min() >= 0.0)\\nTrue\\n>>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n4',\n",
       " \"summarize to docstring: Return ``eq`` with non-commutative objects replaced with Dummy\\nsymbols. A dictionary that can be used to restore the original\\nvalues is returned: if it is None, the expression is noncommutative\\nand cannot be made commutative. The third value returned is a list\\nof any non-commutative symbols that appear in the returned equation.\\n\\nExplanation\\n===========\\n\\nAll non-commutative objects other than Symbols are replaced with\\na non-commutative Symbol. Identical objects will be identified\\nby identical symbols.\\n\\nIf there is only 1 non-commutative object in an expression it will\\nbe replaced with a commutative symbol. Otherwise, the non-commutative\\nentities are retained and the calling routine should handle\\nreplacements in this case since some care must be taken to keep\\ntrack of the ordering of symbols when they occur within Muls.\\n\\nParameters\\n==========\\n\\nname : str\\n    ``name``, if given, is the name that will be used with numbered Dummy\\n    variables that will replace the non-commutative objects and is mainly\\n    used for doctesting purposes.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.secondquant import Commutator, NO, F, Fd\\n>>> from sympy import symbols\\n>>> from sympy.core.exprtools import _mask_nc\\n>>> from sympy.abc import x, y\\n>>> A, B, C = symbols('A,B,C', commutative=False)\\n\\nOne nc-symbol:\\n\\n>>> _mask_nc(A**2 - x**2, 'd')\\n(_d0**2 - x**2, {_d0: A}, [])\\n\\nMultiple nc-symbols:\\n\\n>>> _mask_nc(A**2 - B**2, 'd')\\n(A**2 - B**2, {}, [A, B])\\n\\nAn nc-object with nc-symbols but no others outside of it:\\n\\n>>> _mask_nc(1 + x*Commutator(A, B), 'd')\\n(_d0*x + 1, {_d0: Commutator(A, B)}, [])\\n>>> _mask_nc(NO(Fd(x)*F(y)), 'd')\\n(_d0, {_d0: NO(CreateFermion(x)*AnnihilateFermion(y))}, [])\\n\\nMultiple nc-objects:\\n\\n>>> eq = x*Commutator(A, B) + x*Commutator(A, C)*Commutator(A, B)\\n>>> _mask_nc(eq, 'd')\\n(x*_d0 + x*_d1*_d0, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1])\\n\\nMultiple nc-objects and nc-symbols:\\n\\n>>> eq = A*Commutator(A, B) + B*Commutator(A, C)\\n>>> _mask_nc(eq, 'd')\\n(A*_d0 + B*_d1, {_d0: Commutator(A, B), _d1: Commutator(A, C)}, [_d0, _d1, A, B])\",\n",
       " 'summarize to docstring: Returns the wavefunction psi_{n} for the One-dimensional harmonic oscillator.\\n\\nParameters\\n==========\\n\\nn :\\n    the \"nodal\" quantum number.  Corresponds to the number of nodes in the\\n    wavefunction.  ``n >= 0``\\nx :\\n    x coordinate.\\nm :\\n    Mass of the particle.\\nomega :\\n    Angular frequency of the oscillator.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.qho_1d import psi_n\\n>>> from sympy.abc import m, x, omega\\n>>> psi_n(0, x, m, omega)\\n(m*omega)**(1/4)*exp(-m*omega*x**2/(2*hbar))/(hbar**(1/4)*pi**(1/4))',\n",
       " 'summarize to docstring: Factor out gcd of the elements of a matrix.\\n\\nRequires ``gcd`` in the ground domain.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DM\\n>>> from sympy import ZZ\\n>>> M = DM([[2, 4], [4, 12]], ZZ)\\n>>> content, M_primitive = M.primitive()\\n>>> content\\n2\\n>>> M_primitive\\nDomainMatrix([[1, 2], [2, 6]], (2, 2), ZZ)\\n>>> content * M_primitive == M\\nTrue\\n>>> M_primitive.content() == ZZ(1)\\nTrue\\n\\nSee Also\\n========\\n\\ncontent\\ncancel_denom',\n",
       " 'summarize to docstring: Returns\\n=======\\n\\nsize: int\\n    The size of set T. Set T is the set of all possible\\n    monomials of the n variables for degree equal to the\\n    degree_m',\n",
       " 'summarize to docstring: Each entry fundamental matrix can be interpreted as\\nthe expected number of times the chains is in state j\\nif it started in state i.\\n\\nReferences\\n==========\\n\\n.. [1] https://lips.cs.princeton.edu/the-fundamental-matrix-of-a-finite-markov-chain/',\n",
       " 'summarize to docstring: Build up the response data into a bytearray.',\n",
       " 'summarize to docstring: This injects a type check into an assignment expression (a := foo()).',\n",
       " 'summarize to docstring: Test that bare HTTPSConnection can connect, make requests',\n",
       " 'summarize to docstring: Create a handshake response to reject the connection.\\n\\nA short plain text response is the best fallback when failing to\\nestablish a WebSocket connection.\\n\\nYou must send the handshake response with :meth:`send_response`.\\n\\nYou may modify the response before sending it, for example by changing\\nHTTP headers.\\n\\nArgs:\\n    status: HTTP status code.\\n    text: HTTP response body; it will be encoded to UTF-8.\\n\\nReturns:\\n    HTTP response to send to the client.',\n",
       " 'summarize to docstring: Parse upgrade command syntax :target to retrieve the target revision\\nand given the :current_revisions stamp of the database.\\n\\nReturns a tuple of Revision objects which should be iterated/upgraded\\nto. The target may be specified in absolute form, or relative to\\n:current_revisions.',\n",
       " 'summarize to docstring: :return:\\n    A unicode string',\n",
       " \"summarize to docstring: Upload a file to an S3 object.\\n\\nVariants have also been injected into S3 client, Bucket and Object.\\nYou don't have to use S3Transfer.upload_file() directly.\\n\\n.. seealso::\\n    :py:meth:`S3.Client.upload_file`\\n    :py:meth:`S3.Client.upload_fileobj`\",\n",
       " 'summarize to docstring: Get the value of a command option.',\n",
       " 'summarize to docstring: Get a stringified version of the param for use in error messages to\\nindicate which param caused the error.',\n",
       " 'summarize to docstring: The bulk of the command line interface to coverage.py.\\n\\n`argv` is the argument list to process.\\n\\nReturns 0 if all is well, 1 if something went wrong.',\n",
       " 'summarize to docstring: Construct and initialize a new FileDisposition object.',\n",
       " 'summarize to docstring: Find strings in `v`, and replace backslashes with slashes throughout.',\n",
       " 'summarize to docstring: Update a class object.',\n",
       " 'summarize to docstring: Notify breakpoints of a single step exception event.\\n\\n@type  event: L{ExceptionEvent}\\n@param event: Single step exception event.\\n\\n@rtype:  bool\\n@return: C{True} to call the user-defined handle, C{False} otherwise.',\n",
       " 'summarize to docstring: Dump the x86/x64 processor register values.\\nThe output mimics that of the WinDBG debugger.\\n\\n@type  registers: dict( str S{->} int )\\n@param registers: Dictionary mapping register names to their values.\\n\\n@type  arch: str\\n@param arch: Architecture of the machine whose registers were dumped.\\n    Defaults to the current architecture.\\n    Currently only the following architectures are supported:\\n     - L{win32.ARCH_I386}\\n     - L{win32.ARCH_AMD64}\\n\\n@rtype:  str\\n@return: Text suitable for logging.',\n",
       " 'summarize to docstring: Convert a space-separated list of EDNS flag text values into a EDNS\\nflags value.\\n\\nReturns an ``int``',\n",
       " 'summarize to docstring: see ``_offset_v2``',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for create_rollup_property\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'summarize to docstring: Return a callable for the list playbooks method over gRPC.\\n\\nReturns a list of playbooks in the specified agent.\\n\\nReturns:\\n    Callable[[~.ListPlaybooksRequest],\\n            ~.ListPlaybooksResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.TestIamPermissionsRequest):\\n        The request object for TestIamPermissions method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    iam_policy_pb2.TestIamPermissionsResponse: Response from TestIamPermissions method.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Deletes an existing catalog specified by the catalog\\nID.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_biglake_v1\\n\\n    def sample_delete_catalog():\\n        # Create a client\\n        client = bigquery_biglake_v1.MetastoreServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_biglake_v1.DeleteCatalogRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete_catalog(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_biglake_v1.types.DeleteCatalogRequest, dict]):\\n        The request object. Request message for the DeleteCatalog\\n        method.\\n    name (str):\\n        Required. The name of the catalog to delete. Format:\\n        projects/{project_id_or_number}/locations/{location_id}/catalogs/{catalog_id}\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_biglake_v1.types.Catalog:\\n        Catalog is the container of\\n        databases.',\n",
       " 'summarize to docstring: Return a callable for the get attestor method over gRPC.\\n\\nGets an\\n[attestor][google.cloud.binaryauthorization.v1.Attestor].\\nReturns NOT_FOUND if the\\n[attestor][google.cloud.binaryauthorization.v1.Attestor] does\\nnot exist.\\n\\nReturns:\\n    Callable[[~.GetAttestorRequest],\\n            ~.Attestor]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Unassigns a license from a user.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import commerce_consumer_procurement_v1\\n\\n    def sample_unassign():\\n        # Create a client\\n        client = commerce_consumer_procurement_v1.LicenseManagementServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = commerce_consumer_procurement_v1.UnassignRequest(\\n            parent=\"parent_value\",\\n            usernames=[\\'usernames_value1\\', \\'usernames_value2\\'],\\n        )\\n\\n        # Make the request\\n        response = client.unassign(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.commerce_consumer_procurement_v1.types.UnassignRequest, dict]):\\n        The request object. Request message for\\n        [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].\\n    parent (str):\\n        Required. License pool name.\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    usernames (MutableSequence[str]):\\n        Required. Username. Format: ``name@domain.com``.\\n        This corresponds to the ``usernames`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.commerce_consumer_procurement_v1.types.UnassignResponse:\\n        Response message for\\n           [LicenseManagementService.Unassign][google.cloud.commerce.consumer.procurement.v1.LicenseManagementService.Unassign].',\n",
       " 'summarize to docstring: Call the list network endpoints method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListNetworkEndpointsGlobalNetworkEndpointGroupsRequest):\\n        The request object. A request message for\\n    GlobalNetworkEndpointGroups.ListNetworkEndpoints.\\n    See the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.NetworkEndpointGroupsListNetworkEndpoints:',\n",
       " 'summarize to docstring: Retrieves the list of interconnect attachments\\ncontained within the specified region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.InterconnectAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListInterconnectAttachmentsRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListInterconnectAttachmentsRequest, dict]):\\n        The request object. A request message for\\n        InterconnectAttachments.List. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.interconnect_attachments.pagers.ListPager:\\n        Response to the list request, and\\n        contains a list of interconnect\\n        attachments.  Iterating over this object\\n        will yield results and resolve\\n        additional pages automatically.',\n",
       " 'summarize to docstring: Deletes the specified NetworkAttachment in the given\\nscope\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.NetworkAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteNetworkAttachmentRequest(\\n            network_attachment=\"network_attachment_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteNetworkAttachmentRequest, dict]):\\n        The request object. A request message for\\n        NetworkAttachments.Delete. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region of this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    network_attachment (str):\\n        Name of the NetworkAttachment\\n        resource to delete.\\n\\n        This corresponds to the ``network_attachment`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.compute_v1.types.ListRegionUrlMapsRequest):\\n        The initial request object.\\n    response (google.cloud.compute_v1.types.UrlMapList):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Creates an analysis. The long running operation is\\ndone when the analysis has completed.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contact_center_insights_v1\\n\\n    def sample_create_analysis():\\n        # Create a client\\n        client = contact_center_insights_v1.ContactCenterInsightsClient()\\n\\n        # Initialize request argument(s)\\n        request = contact_center_insights_v1.CreateAnalysisRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.create_analysis(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contact_center_insights_v1.types.CreateAnalysisRequest, dict]):\\n        The request object. The request to create an analysis.\\n    parent (str):\\n        Required. The parent resource of the\\n        analysis.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    analysis (google.cloud.contact_center_insights_v1.types.Analysis):\\n        Required. The analysis to create.\\n        This corresponds to the ``analysis`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.contact_center_insights_v1.types.Analysis`\\n        The analysis resource.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for get_phrase_matcher\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ContactCenterInsights server.',\n",
       " 'summarize to docstring: Call the execute question method over HTTP.\\n\\nArgs:\\n    request (~.question_service.ExecuteQuestionRequest):\\n        The request object. Request to execute an interpretation.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.question.Question:\\n        The question resource represents a\\n    natural language query, its settings,\\n    understanding generated by the system,\\n    and answer retrieval status. A question\\n    cannot be modified.',\n",
       " 'summarize to docstring: Instantiates the job controller client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,JobControllerTransport,Callable[..., JobControllerTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the JobControllerTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'summarize to docstring: Returns the list of all experiments in the specified\\n[Environment][google.cloud.dialogflow.cx.v3.Environment].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflowcx_v3\\n\\n    def sample_list_experiments():\\n        # Create a client\\n        client = dialogflowcx_v3.ExperimentsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflowcx_v3.ListExperimentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_experiments(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflowcx_v3.types.ListExperimentsRequest, dict]):\\n        The request object. The request message for\\n        [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n    parent (str):\\n        Required. The\\n        [Environment][google.cloud.dialogflow.cx.v3.Environment]\\n        to list all environments for. Format:\\n        ``projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/environments/<Environment ID>``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflowcx_v3.services.experiments.pagers.ListExperimentsPager:\\n        The response message for\\n           [Experiments.ListExperiments][google.cloud.dialogflow.cx.v3.Experiments.ListExperiments].\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'summarize to docstring: Return a callable for the delete agent method over gRPC.\\n\\nDeletes the specified agent.\\n\\nReturns:\\n    Callable[[~.DeleteAgentRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the start experiment method over gRPC.\\n\\nStarts the specified\\n[Experiment][google.cloud.dialogflow.cx.v3beta1.Experiment].\\nThis rpc only changes the state of experiment from PENDING to\\nRUNNING.\\n\\nReturns:\\n    Callable[[~.StartExperimentRequest],\\n            ~.Experiment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Returns True if the graph G is a triad, else False.\\n\\nParameters\\n----------\\nG : graph\\n   A NetworkX Graph\\n\\nReturns\\n-------\\nistriad : boolean\\n   Whether G is a valid triad\\n\\nExamples\\n--------\\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\\n>>> nx.is_triad(G)\\nTrue\\n>>> G.add_edge(0, 1)\\n>>> nx.is_triad(G)\\nFalse',\n",
       " 'summarize to docstring: Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'summarize to docstring: Deletes the processor, unloads all deployed model\\nartifacts if it was enabled and then deletes all\\nartifacts associated with this processor.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import documentai_v1beta3\\n\\n    def sample_delete_processor():\\n        # Create a client\\n        client = documentai_v1beta3.DocumentProcessorServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = documentai_v1beta3.DeleteProcessorRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_processor(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.documentai_v1beta3.types.DeleteProcessorRequest, dict]):\\n        The request object. Request message for the\\n        [DeleteProcessor][google.cloud.documentai.v1beta3.DocumentProcessorService.DeleteProcessor]\\n        method.\\n    name (str):\\n        Required. The processor resource name\\n        to be deleted.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " \"summarize to docstring: Publish events to a subscriber's channel.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import eventarc_publishing_v1\\n\\n    def sample_publish_events():\\n        # Create a client\\n        client = eventarc_publishing_v1.PublisherClient()\\n\\n        # Initialize request argument(s)\\n        request = eventarc_publishing_v1.PublishEventsRequest(\\n        )\\n\\n        # Make the request\\n        response = client.publish_events(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.eventarc_publishing_v1.types.PublishEventsRequest, dict]):\\n        The request object. The request message for the\\n        PublishEvents method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.eventarc_publishing_v1.types.PublishEventsResponse:\\n        The response message for the\\n        PublishEvents method.\",\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'summarize to docstring: Return a callable for the create comment method over gRPC.\\n\\nCreates a new comment on an order.\\n\\nReturns:\\n    Callable[[~.CreateCommentRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the delete hardware group method over HTTP.\\n\\nArgs:\\n    request (~.service.DeleteHardwareGroupRequest):\\n        The request object. A request to delete a hardware group.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.network_management_v1.types.ListConnectivityTestsRequest):\\n        The initial request object.\\n    response (google.cloud.network_management_v1.types.ListConnectivityTestsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Return a callable for the get inventory method over gRPC.\\n\\nGet inventory data for the specified VM instance. If the VM has\\nno associated inventory, the message ``NOT_FOUND`` is returned.\\n\\nReturns:\\n    Callable[[~.GetInventoryRequest],\\n            ~.Inventory]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"summarize to docstring: Read image data from file and return as numpy array.\\n\\nRaise ValueError if format is unsupported.\\n\\nParameters\\n----------\\nout : numpy.ndarray, str, or file-like object; optional\\n    Buffer where image data will be saved.\\n    If None (default), a new array will be created.\\n    If numpy.ndarray, a writable array of compatible dtype and shape.\\n    If 'memmap', directly memory-map the image data in the TIFF file\\n    if possible; else create a memory-mapped array in a temporary file.\\n    If str or open file, the file name or file object used to\\n    create a memory-map to an array stored in a binary file on disk.\\nsqueeze : bool\\n    If True, all length-1 dimensions (except X and Y) are\\n    squeezed out from the array.\\n    If False, the shape of the returned array might be different from\\n    the page.shape.\\nlock : {RLock, NullContext}\\n    A reentrant lock used to synchronize reads from file.\\n    If None (default), the lock of the parent's filehandle is used.\\nreopen : bool\\n    If True (default) and the parent file handle is closed, the file\\n    is temporarily re-opened and closed if no exception occurs.\\nmaxsize: int or None\\n    Maximum size of data before a ValueError is raised.\\n    Can be used to catch DOS. Default: 16 TB.\\nvalidate : bool\\n    If True (default), validate various parameters.\\n    If None, only validate parameters and return None.\",\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    RecaptchaEnterpriseServiceAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Return a callable for the mark recommendation failed method over gRPC.\\n\\nMarks the Recommendation State as Failed. Users can use this\\nmethod to indicate to the Recommender API that they have applied\\nthe recommendation themselves, and the operation failed. This\\nstops the recommendation content from being updated. Associated\\ninsights are frozen and placed in the ACCEPTED state.\\n\\nMarkRecommendationFailed can be applied to recommendations in\\nACTIVE, CLAIMED, SUCCEEDED, or FAILED state.\\n\\nRequires the recommender.*.update IAM permission for the\\nspecified recommender.\\n\\nReturns:\\n    Callable[[~.MarkRecommendationFailedRequest],\\n            ~.Recommendation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the remove fulfillment places method over gRPC.\\n\\nWe recommend that you use the\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nmethod instead of the\\n[ProductService.RemoveFulfillmentPlaces][google.cloud.retail.v2alpha.ProductService.RemoveFulfillmentPlaces]\\nmethod.\\n[ProductService.RemoveLocalInventories][google.cloud.retail.v2alpha.ProductService.RemoveLocalInventories]\\nachieves the same results but provides more fine-grained control\\nover ingesting local inventory data.\\n\\nIncrementally removes place IDs from a\\n[Product.fulfillment_info.place_ids][google.cloud.retail.v2alpha.FulfillmentInfo.place_ids].\\n\\nThis process is asynchronous and does not require the\\n[Product][google.cloud.retail.v2alpha.Product] to exist before\\nupdating fulfillment information. If the request is valid, the\\nupdate will be enqueued and processed downstream. As a\\nconsequence, when a response is returned, the removed place IDs\\nare not immediately manifested in the\\n[Product][google.cloud.retail.v2alpha.Product] queried by\\n[ProductService.GetProduct][google.cloud.retail.v2alpha.ProductService.GetProduct]\\nor\\n[ProductService.ListProducts][google.cloud.retail.v2alpha.ProductService.ListProducts].\\n\\nThe returned [Operation][google.longrunning.Operation]s will be\\nobsolete after 1 day, and\\n[GetOperation][google.longrunning.Operations.GetOperation] API\\nwill return NOT_FOUND afterwards.\\n\\nIf conflicting updates are issued, the\\n[Operation][google.longrunning.Operation]s associated with the\\nstale updates will not be marked as\\n[done][google.longrunning.Operation.done] until being obsolete.\\n\\nReturns:\\n    Callable[[~.RemoveFulfillmentPlacesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the get big query export method over gRPC.\\n\\nGets a BigQuery export.\\n\\nReturns:\\n    Callable[[~.GetBigQueryExportRequest],\\n            ~.BigQueryExport]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the list operations method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.ListOperationsRequest):\\n        The request object for ListOperations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    operations_pb2.ListOperationsResponse: Response from ListOperations method.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for analyze_asset\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Warehouse server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for create_clone_job\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the VmMigration server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Call the list locations method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.ListLocationsRequest):\\n        The request object for ListLocations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.ListLocationsResponse: Response from ListLocations method.',\n",
       " \"summarize to docstring: Process the response from an HTTP request that canceled the upload.\\n\\nThis is everything that must be done after a request that doesn't\\nrequire network I/O (or other I/O). This is based on the `sans-I/O`_\\nphilosophy.\\n\\nArgs:\\n    response (object): The HTTP response object.\\n\\nRaises:\\n    ~google.resumable_media.common.InvalidResponse: If the status\\n        code is not 204.\\n\\n.. _sans-I/O: https://sans-io.readthedocs.io/\",\n",
       " 'summarize to docstring: maybe_int : NUMBER\\n| empty',\n",
       " 'summarize to docstring: Like os.path.walk, but follows symlinks on POSIX systems.\\n\\nIf the symlinks create a loop, this function will never finish.',\n",
       " 'summarize to docstring: Run a test only if the client is not connected to a mongos.',\n",
       " 'summarize to docstring: create a unix-style long description of the file (like ls -l)',\n",
       " \"summarize to docstring: Set how the Axes adjusts to achieve the required aspect ratio.\\n\\nParameters\\n----------\\nadjustable : {'box', 'datalim'}\\n    If 'box', change the physical dimensions of the Axes.\\n    If 'datalim', change the ``x`` or ``y`` data limits. This\\n    may ignore explicitly defined axis limits.\\n\\nshare : bool, default: False\\n    If ``True``, apply the settings to all shared Axes.\\n\\nSee Also\\n--------\\nmatplotlib.axes.Axes.set_aspect\\n    For a description of aspect handling.\\n\\nNotes\\n-----\\nShared Axes (of which twinned Axes are a special case)\\nimpose restrictions on how aspect ratios can be imposed.\\nFor twinned Axes, use 'datalim'.  For Axes that share both\\nx and y, use 'box'.  Otherwise, either 'datalim' or 'box'\\nmay be used.  These limitations are partly a requirement\\nto avoid over-specification, and partly a result of the\\nparticular implementation we are currently using, in\\nwhich the adjustments for aspect ratios are done sequentially\\nand independently on each Axes as it is drawn.\",\n",
       " 'summarize to docstring: Return the size of the image as tuple (numrows, numcols).',\n",
       " 'summarize to docstring: Function used for unpickling proxy objects.',\n",
       " 'summarize to docstring: Helper function to implement map, starmap and their async counterparts.',\n",
       " 'summarize to docstring: Create a Timestamp from posix timestamp in nanoseconds.\\n\\n:param int unix_ns: Posix timestamp in nanoseconds.\\n:rtype: Timestamp',\n",
       " 'summarize to docstring: Compute the Katz centrality for the graph G.\\n\\nKatz centrality computes the centrality for a node based on the centrality\\nof its neighbors. It is a generalization of the eigenvector centrality. The\\nKatz centrality for node $i$ is\\n\\n.. math::\\n\\n    x_i = \\\\alpha \\\\sum_{j} A_{ij} x_j + \\\\beta,\\n\\nwhere $A$ is the adjacency matrix of graph G with eigenvalues $\\\\lambda$.\\n\\nThe parameter $\\\\beta$ controls the initial centrality and\\n\\n.. math::\\n\\n    \\\\alpha < \\\\frac{1}{\\\\lambda_{\\\\max}}.\\n\\nKatz centrality computes the relative influence of a node within a\\nnetwork by measuring the number of the immediate neighbors (first\\ndegree nodes) and also all other nodes in the network that connect\\nto the node under consideration through these immediate neighbors.\\n\\nExtra weight can be provided to immediate neighbors through the\\nparameter $\\\\beta$.  Connections made with distant neighbors\\nare, however, penalized by an attenuation factor $\\\\alpha$ which\\nshould be strictly less than the inverse largest eigenvalue of the\\nadjacency matrix in order for the Katz centrality to be computed\\ncorrectly. More information is provided in [1]_.\\n\\nParameters\\n----------\\nG : graph\\n  A NetworkX graph\\n\\nalpha : float\\n  Attenuation factor\\n\\nbeta : scalar or dictionary, optional (default=1.0)\\n  Weight attributed to the immediate neighborhood. If not a scalar the\\n  dictionary must have an value for every node.\\n\\nnormalized : bool\\n  If True normalize the resulting values.\\n\\nweight : None or string, optional\\n  If None, all edge weights are considered equal.\\n  Otherwise holds the name of the edge attribute used as weight.\\n  In this measure the weight is interpreted as the connection strength.\\n\\nReturns\\n-------\\nnodes : dictionary\\n   Dictionary of nodes with Katz centrality as the value.\\n\\nRaises\\n------\\nNetworkXError\\n   If the parameter `beta` is not a scalar but lacks a value for at least\\n   one node\\n\\nExamples\\n--------\\n>>> import math\\n>>> G = nx.path_graph(4)\\n>>> phi = (1 + math.sqrt(5)) / 2.0  # largest eigenvalue of adj matrix\\n>>> centrality = nx.katz_centrality_numpy(G, 1 / phi)\\n>>> for n, c in sorted(centrality.items()):\\n...     print(f\"{n} {c:.2f}\")\\n0 0.37\\n1 0.60\\n2 0.60\\n3 0.37\\n\\nSee Also\\n--------\\nkatz_centrality\\neigenvector_centrality_numpy\\neigenvector_centrality\\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\\n\\nNotes\\n-----\\nKatz centrality was introduced by [2]_.\\n\\nThis algorithm uses a direct linear solver to solve the above equation.\\nThe parameter ``alpha`` should be strictly less than the inverse of largest\\neigenvalue of the adjacency matrix for there to be a solution.\\nYou can use ``max(nx.adjacency_spectrum(G))`` to get $\\\\lambda_{\\\\max}$ the largest\\neigenvalue of the adjacency matrix.\\n\\nFor strongly connected graphs, as $\\\\alpha \\\\to 1/\\\\lambda_{\\\\max}$, and $\\\\beta > 0$,\\nKatz centrality approaches the results for eigenvector centrality.\\n\\nFor directed graphs this finds \"left\" eigenvectors which corresponds\\nto the in-edges in the graph. For out-edges Katz centrality,\\nfirst reverse the graph with ``G.reverse()``.\\n\\nReferences\\n----------\\n.. [1] Mark E. J. Newman:\\n   Networks: An Introduction.\\n   Oxford University Press, USA, 2010, p. 173.\\n.. [2] Leo Katz:\\n   A New Status Index Derived from Sociometric Index.\\n   Psychometrika 18(1):39â€“43, 1953\\n   https://link.springer.com/content/pdf/10.1007/BF02289026.pdf',\n",
       " 'summarize to docstring: Remove node attributes from all nodes in the graph.\\n\\nParameters\\n----------\\nG : NetworkX Graph\\n\\n*attr_names : List of Strings\\n    The attribute names to remove from the graph.\\n\\nnbunch : List of Nodes\\n    Remove the node attributes only from the nodes in this list.\\n\\nExamples\\n--------\\n>>> G = nx.Graph()\\n>>> G.add_nodes_from([1, 2, 3], color=\"blue\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{1: \\'blue\\', 2: \\'blue\\', 3: \\'blue\\'}\\n>>> nx.remove_node_attributes(G, \"color\")\\n>>> nx.get_node_attributes(G, \"color\")\\n{}',\n",
       " 'summarize to docstring: Check the message is formatted correctly for the decimal value.\\nAlso check the message when input includes inf or nan (gh12200)',\n",
       " 'summarize to docstring: Check that ._metadata attributes are equivalent.',\n",
       " 'summarize to docstring: Check for the `min_count` keyword. Returns True if below `min_count` (when\\nmissing value should be returned from the reduction).\\n\\nParameters\\n----------\\nshape : tuple\\n    The shape of the values (`values.shape`).\\nmask : ndarray[bool] or None\\n    Boolean numpy array (typically of same shape as `shape`) or None.\\nmin_count : int\\n    Keyword passed through from sum/prod call.\\n\\nReturns\\n-------\\nbool',\n",
       " 'summarize to docstring: Issue #96 (for newbytes instead of newobject)',\n",
       " \"summarize to docstring: Convert ``color_spec`` to an openpyxl v2 Color object.\\n\\nParameters\\n----------\\ncolor_spec : str, dict\\n    A 32-bit ARGB hex string, or a dict with zero or more of the\\n    following keys.\\n        'rgb'\\n        'indexed'\\n        'auto'\\n        'theme'\\n        'tint'\\n        'index'\\n        'type'\\n\\nReturns\\n-------\\ncolor : openpyxl.styles.Color\",\n",
       " 'summarize to docstring: A simplified json_normalize\\n\\nConverts a nested dict into a flat dict (\"record\"), unlike json_normalize,\\nit does not attempt to extract a subset of the data.\\n\\nParameters\\n----------\\nds : dict or list of dicts\\nprefix: the prefix, optional, default: \"\"\\nsep : str, default \\'.\\'\\n    Nested records will generate names separated by sep,\\n    e.g., for sep=\\'.\\', { \\'foo\\' : { \\'bar\\' : 0 } } -> foo.bar\\nlevel: int, optional, default: 0\\n    The number of levels in the json string.\\n\\nmax_level: int, optional, default: None\\n    The max depth to normalize.\\n\\nReturns\\n-------\\nd - dict or list of dicts, matching `ds`\\n\\nExamples\\n--------\\n>>> nested_to_record(\\n...     dict(flat1=1, dict1=dict(c=1, d=2), nested=dict(e=dict(c=1, d=2), d=2))\\n... )\\n{\\'flat1\\': 1, \\'dict1.c\\': 1, \\'dict1.d\\': 2, \\'nested.e.c\\': 1, \\'nested.e.d\\': 2, \\'nested.d\\': 2}',\n",
       " \"summarize to docstring: Converts lists of lists/tuples into DataFrames with proper type inference\\nand optional (e.g. string to datetime) conversion. Also enables iterating\\nlazily over chunks of large files\\n\\nParameters\\n----------\\ndata : file-like object or list\\ndelimiter : separator character to use\\ndialect : str or csv.Dialect instance, optional\\n    Ignored if delimiter is longer than 1 character\\nnames : sequence, default\\nheader : int, default 0\\n    Row to use to parse column labels. Defaults to the first row. Prior\\n    rows will be discarded\\nindex_col : int or list, optional\\n    Column or columns to use as the (possibly hierarchical) index\\nhas_index_names: bool, default False\\n    True if the cols defined in index_col have an index name and are\\n    not in the header.\\nna_values : scalar, str, list-like, or dict, optional\\n    Additional strings to recognize as NA/NaN.\\nkeep_default_na : bool, default True\\nthousands : str, optional\\n    Thousands separator\\ncomment : str, optional\\n    Comment out remainder of line\\nparse_dates : bool, default False\\ndate_format : str or dict of column -> format, default ``None``\\n\\n    .. versionadded:: 2.0.0\\nskiprows : list of integers\\n    Row numbers to skip\\nskipfooter : int\\n    Number of line at bottom of file to skip\\nconverters : dict, optional\\n    Dict of functions for converting values in certain columns. Keys can\\n    either be integers or column labels, values are functions that take one\\n    input argument, the cell (not column) content, and return the\\n    transformed content.\\nencoding : str, optional\\n    Encoding to use for UTF when reading/writing (ex. 'utf-8')\\nfloat_precision : str, optional\\n    Specifies which converter the C engine should use for floating-point\\n    values. The options are `None` or `high` for the ordinary converter,\\n    `legacy` for the original lower precision pandas converter, and\\n    `round_trip` for the round-trip converter.\",\n",
       " \"summarize to docstring: This reads until EOF using readline() and returns a list containing\\nthe lines thus read. The optional 'sizehint' argument is ignored.\\nRemember, because this reads until EOF that means the child\\nprocess should have closed its stdout. If you run this method on\\na child that is still running with its stdout open then this\\nmethod will block until it timesout.\",\n",
       " \"summarize to docstring: Return the address of the remote side of this Channel, if possible.\\n\\nThis simply wraps `.Transport.getpeername`, used to provide enough of a\\nsocket-like interface to allow asyncore to work. (asyncore likes to\\ncall ``'getpeername'``.)\",\n",
       " 'summarize to docstring: Returns a generator of `funcdef` nodes.',\n",
       " \"summarize to docstring: Fixture for 'na_action' argument in sort_values/sort_index/rank.\",\n",
       " 'summarize to docstring: Test the wheel cache filters on wheel name when several wheels\\nfor different package are stored under the same cache directory.',\n",
       " 'summarize to docstring: Get the sha256 digest of a string\\n\\nSupports the `usedforsecurity` argument for Python 3.9+ to allow running on\\na FIPS-enabled system.',\n",
       " \"summarize to docstring: create a modified version of this path. A 'rev' argument\\nindicates a new revision.\\nthe following keyword arguments modify various path parts::\\n\\n  http://host.com/repo/path/file.ext\\n  |-----------------------|          dirname\\n                            |------| basename\\n                            |--|     purebasename\\n                                |--| ext\",\n",
       " 'summarize to docstring: Assign |ASN.1| type component by position.\\n\\nEquivalent to Python sequence item assignment operation (e.g. `[]`).\\n\\nParameters\\n----------\\nidx : :class:`int`\\n    Component index (zero-based). Must either refer to existing\\n    component (if *componentType* is set) or to N+1 component\\n    otherwise. In the latter case a new component of given ASN.1\\n    type gets instantiated and appended to |ASN.1| sequence.\\n\\nKeyword Args\\n------------\\nvalue: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\\n    A Python value to initialize |ASN.1| component with (if *componentType* is set)\\n    or ASN.1 value object to assign to |ASN.1| component.\\n    If `value` is not given, schema object will be set as a component.\\n\\nverifyConstraints : :class:`bool`\\n     If :obj:`False`, skip constraints validation\\n\\nmatchTags: :class:`bool`\\n     If :obj:`False`, skip component tags matching\\n\\nmatchConstraints: :class:`bool`\\n     If :obj:`False`, skip component constraints matching\\n\\nReturns\\n-------\\nself',\n",
       " 'summarize to docstring: Avoid extraneous whitespace around an operator.\\n\\nOkay: a = 12 + 3\\nE221: a = 4  + 5\\nE222: a = 4 +  5\\nE223: a = 4\\\\t+ 5\\nE224: a = 4 +\\\\t5',\n",
       " 'summarize to docstring: The error reported for source files which end prematurely causing a\\nsyntax error reflects the cause for the syntax error.',\n",
       " 'summarize to docstring: Test line referrals.',\n",
       " 'summarize to docstring: The check',\n",
       " 'summarize to docstring: Similar, but with a subscript in a key-value pair rather than the test\\nSee https://github.com/pylint-dev/pylint/issues/6069',\n",
       " 'summarize to docstring: The type name of the exception.',\n",
       " 'summarize to docstring: Raise pytest.skip() if all examples in the given DocTest have the SKIP\\noption set.',\n",
       " 'summarize to docstring: See :meth:`Pytester.parseconfigure`.',\n",
       " 'summarize to docstring: Regarding issue pytest-xdist#241.\\n\\nThis test came originally from test_remote.py in xdist (ca03269).',\n",
       " 'summarize to docstring: From the backport of the email package',\n",
       " 'summarize to docstring: patch_service_cidr_status  # noqa: E501\\n\\npartially update status of the specified ServiceCIDR  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.patch_service_cidr_status(name, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ServiceCIDR (required)\\n:param object body: (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param bool force: Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1beta1ServiceCIDR\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'summarize to docstring: test router-router MQ devices',\n",
       " 'summarize to docstring: Tests for the regular expressions used in ISO8601Highlighter.',\n",
       " 'summarize to docstring: Placeholder docstring',\n",
       " 'summarize to docstring: Update to enable or disable remote debug for a training job.\\n\\nThis method updates the ``_enable_remote_debug`` parameter\\nand enables or disables remote debug for a training job',\n",
       " 'summarize to docstring: Create an Amazon SageMaker training job.\\n\\nArgs:\\n    input_mode (str): The input mode that the algorithm supports. Valid modes:\\n        * \\'File\\' - Amazon SageMaker copies the training dataset from the S3 location to\\n        a directory in the Docker container.\\n        * \\'Pipe\\' - Amazon SageMaker streams data directly from S3 to the container via a\\n        Unix-named pipe.\\n        * \\'FastFile\\' - Amazon SageMaker streams data from S3 on demand instead of\\n        downloading the entire dataset before training begins.\\n    input_config (list): A list of Channel objects. Each channel is a named input source.\\n        Please refer to the format details described:\\n        https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\\n    role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\\n        jobs and APIs that create Amazon SageMaker endpoints use this role to access\\n        training data and model artifacts. You must grant sufficient permissions to this\\n        role.\\n    job_name (str): Name of the training job being created.\\n    output_config (dict): The S3 URI where you want to store the training results and\\n        optional KMS key ID.\\n    resource_config (dict): Contains values for ResourceConfig:\\n        * instance_count (int): Number of EC2 instances to use for training.\\n        The key in resource_config is \\'InstanceCount\\'.\\n        * instance_type (str): Type of EC2 instance to use for training, for example,\\n        \\'ml.c4.xlarge\\'. The key in resource_config is \\'InstanceType\\'.\\n    vpc_config (dict): Contains values for VpcConfig:\\n        * subnets (list[str]): List of subnet ids.\\n        The key in vpc_config is \\'Subnets\\'.\\n        * security_group_ids (list[str]): List of security group ids.\\n        The key in vpc_config is \\'SecurityGroupIds\\'.\\n    hyperparameters (dict): Hyperparameters for model training. The hyperparameters are\\n        made accessible as a dict[str, str] to the training code on SageMaker. For\\n        convenience, this accepts other types for keys and values, but ``str()`` will be\\n        called to convert them before training.\\n    stop_condition (dict): Defines when training shall finish. Contains entries that can\\n        be understood by the service like ``MaxRuntimeInSeconds``.\\n    tags (Optional[Tags]): Tags for labeling a training job. For more, see\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\\n    metric_definitions (list[dict]): A list of dictionaries that defines the metric(s)\\n        used to evaluate the training jobs. Each dictionary contains two keys: \\'Name\\' for\\n        the name of the metric, and \\'Regex\\' for the regular expression used to extract the\\n        metric from the logs.\\n    enable_network_isolation (bool): Whether to request for the training job to run with\\n        network isolation or not.\\n    image_uri (str): Docker image containing training code.\\n    training_image_config(dict): Training image configuration.\\n        Optionally, the dict can contain \\'TrainingRepositoryAccessMode\\' and\\n        \\'TrainingRepositoryCredentialsProviderArn\\' (under \\'TrainingRepositoryAuthConfig\\').\\n        For example,\\n\\n        .. code:: python\\n\\n            training_image_config = {\\n                \"TrainingRepositoryAccessMode\": \"Vpc\",\\n                \"TrainingRepositoryAuthConfig\": {\\n                    \"TrainingRepositoryCredentialsProviderArn\":\\n                      \"arn:aws:lambda:us-west-2:1234567890:function:test\"\\n                },\\n            }\\n\\n        If TrainingRepositoryAccessMode is set to Vpc, the training image is accessed\\n        through a private Docker registry in customer Vpc. If it\\'s set to Platform or None,\\n        the training image is accessed through ECR.\\n        If TrainingRepositoryCredentialsProviderArn is provided, the credentials to\\n        authenticate to the private Docker registry will be retrieved from this AWS Lambda\\n        function. (default: ``None``). When it\\'s set to None, SageMaker will not do\\n        authentication before pulling the image in the private Docker registry.\\n    container_entry_point (List[str]): Optional. The entrypoint script for a Docker\\n        container used to run a training job. This script takes precedence over\\n        the default train processing instructions.\\n    container_arguments (List[str]): Optional. The arguments for a container used to run\\n        a training job.\\n    algorithm_arn (str): Algorithm Arn from Marketplace.\\n    encrypt_inter_container_traffic (bool): Specifies whether traffic between training\\n        containers is encrypted for the training job (default: ``False``).\\n    use_spot_instances (bool): whether to use spot instances for training.\\n    checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\\n        that the algorithm persists (if any) during training. (default:\\n        ``None``).\\n    checkpoint_local_path (str): The local path that the algorithm\\n        writes its checkpoints to. SageMaker will persist all files\\n        under this path to `checkpoint_s3_uri` continually during\\n        training. On job startup the reverse happens - data from the\\n        s3 location is downloaded to this path before the algorithm is\\n        started. If the path is unset then SageMaker assumes the\\n        checkpoints will be provided under `/opt/ml/checkpoints/`.\\n        (default: ``None``).\\n    experiment_config (dict[str, str]): Experiment management configuration.\\n        Optionally, the dict can contain four keys:\\n        \\'ExperimentName\\', \\'TrialName\\',  \\'TrialComponentDisplayName\\' and \\'RunName\\'.\\n        The behavior of setting these keys is as follows:\\n        * If `ExperimentName` is supplied but `TrialName` is not a Trial will be\\n        automatically created and the job\\'s Trial Component associated with the Trial.\\n        * If `TrialName` is supplied and the Trial already exists the job\\'s Trial Component\\n        will be associated with the Trial.\\n        * If both `ExperimentName` and `TrialName` are not supplied the trial component\\n        will be unassociated.\\n        * `TrialComponentDisplayName` is used for display in Studio.\\n        * `RunName` is used to record an experiment run.\\n    enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\\n        Series. For more information see:\\n        https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html\\n        #SageMaker-Type\\n        -AlgorithmSpecification-EnableSageMakerMetricsTimeSeries\\n        (default: ``None``).\\n    profiler_rule_configs (list[dict]): A list of profiler rule\\n        configurations.src/sagemaker/lineage/artifact.py:285\\n    profiler_config (dict): Configuration for how profiling information is emitted\\n        with SageMaker Profiler. (default: ``None``).\\n    remote_debug_config(dict): Configuration for RemoteDebug. (default: ``None``)\\n        The dict can contain \\'EnableRemoteDebug\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            remote_debug_config = {\\n                \"EnableRemoteDebug\": True,\\n            }\\n    session_chaining_config(dict): Configuration for SessionChaining. (default: ``None``)\\n        The dict can contain \\'EnableSessionTagChaining\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            session_chaining_config = {\\n                \"EnableSessionTagChaining\": True,\\n            }\\n    environment (dict[str, str]) : Environment variables to be set for\\n        use during training job (default: ``None``)\\n    retry_strategy(dict): Defines RetryStrategy for InternalServerFailures.\\n        * max_retry_attsmpts (int): Number of times a job should be retried.\\n        The key in RetryStrategy is \\'MaxRetryAttempts\\'.\\n    infra_check_config(dict): Infra check configuration.\\n        Optionally, the dict can contain \\'EnableInfraCheck\\'(bool).\\n        For example,\\n\\n        .. code:: python\\n\\n            infra_check_config = {\\n                \"EnableInfraCheck\": True,\\n            }\\nReturns:\\n    str: ARN of the training job, if it is created.\\n\\nRaises:\\n    - botocore.exceptions.ClientError: If Sagemaker throws an exception while creating\\n    training job.\\n    - ValueError: If both image_uri and algorithm are provided, or if neither is provided.',\n",
       " 'summarize to docstring: Convenience method for accessing the SageMaker session.\\n\\nIt access :class:`~sagemaker.session.Session` object associated with the estimator\\nfor the ``HyperparameterTuner``.',\n",
       " 'summarize to docstring: Check for correspondence between linkage and condensed distance matrices.\\n\\nThey must have the same number of original observations for\\nthe check to succeed.\\n\\nThis function is useful as a sanity check in algorithms that make\\nextensive use of linkage and distance matrices that must\\ncorrespond to the same set of original observations.\\n\\nParameters\\n----------\\nZ : array_like\\n    The linkage matrix to check for correspondence.\\nY : array_like\\n    The condensed distance matrix to check for correspondence.\\n\\nReturns\\n-------\\nb : bool\\n    A boolean indicating whether the linkage matrix and distance\\n    matrix could possibly correspond to one another.\\n\\nSee Also\\n--------\\nlinkage : for a description of what a linkage matrix is.\\n\\nExamples\\n--------\\n>>> from scipy.cluster.hierarchy import ward, correspond\\n>>> from scipy.spatial.distance import pdist\\n\\nThis method can be used to check if a given linkage matrix ``Z`` has been\\nobtained from the application of a cluster method over a dataset ``X``:\\n\\n>>> X = [[0, 0], [0, 1], [1, 0],\\n...      [0, 4], [0, 3], [1, 4],\\n...      [4, 0], [3, 0], [4, 1],\\n...      [4, 4], [3, 4], [4, 3]]\\n>>> X_condensed = pdist(X)\\n>>> Z = ward(X_condensed)\\n\\nHere, we can compare ``Z`` and ``X`` (in condensed form):\\n\\n>>> correspond(Z, X_condensed)\\nTrue',\n",
       " 'summarize to docstring: Fit the gradient boosting model.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The input samples.\\n\\ny : array-like of shape (n_samples,)\\n    Target values.\\n\\nsample_weight : array-like of shape (n_samples,) default=None\\n    Weights of training data.\\n\\n    .. versionadded:: 0.23\\n\\nReturns\\n-------\\nself : object\\n    Fitted estimator.',\n",
       " 'summarize to docstring: Informative warnings should be raised when mixing sklearn and joblib API',\n",
       " \"summarize to docstring: Options\\n-------\\nnit : int, optional\\n    Number of iterations to make. If omitted (default), make as many\\n    as required to meet tolerances.\\ndisp : bool, optional\\n    Print status to stdout on every iteration.\\nmaxiter : int, optional\\n    Maximum number of iterations to make.\\nftol : float, optional\\n    Relative tolerance for the residual. If omitted, not used.\\nfatol : float, optional\\n    Absolute tolerance (in max-norm) for the residual.\\n    If omitted, default is 6e-6.\\nxtol : float, optional\\n    Relative minimum step size. If omitted, not used.\\nxatol : float, optional\\n    Absolute minimum step size, as determined from the Jacobian\\n    approximation. If the step size is smaller than this, optimization\\n    is terminated as successful. If omitted, not used.\\ntol_norm : function(vector) -> scalar, optional\\n    Norm to use in convergence check. Default is the maximum norm.\\nline_search : {None, 'armijo' (default), 'wolfe'}, optional\\n    Which type of a line search to use to determine the step size in\\n    the direction given by the Jacobian approximation. Defaults to\\n    'armijo'.\\njac_options : dict, optional\\n    Options for the respective Jacobian approximation.\\n\\n    alpha : float, optional\\n        Initial guess for the Jacobian is (-1/alpha).\\n    reduction_method : str or tuple, optional\\n        Method used in ensuring that the rank of the Broyden\\n        matrix stays low. Can either be a string giving the\\n        name of the method, or a tuple of the form ``(method,\\n        param1, param2, ...)`` that gives the name of the\\n        method and values for additional parameters.\\n\\n        Methods available:\\n\\n        - ``restart``: drop all matrix columns. Has no extra parameters.\\n        - ``simple``: drop oldest matrix column. Has no extra parameters.\\n        - ``svd``: keep only the most significant SVD components.\\n          Takes an extra parameter, ``to_retain``, which determines the\\n          number of SVD components to retain when rank reduction is done.\\n          Default is ``max_rank - 2``.\\n\\n    max_rank : int, optional\\n        Maximum rank for the Broyden matrix.\\n        Default is infinity (i.e., no rank reduction).\\n\\nExamples\\n--------\\n>>> def func(x):\\n...     return np.cos(x) + x[::-1] - [1, 2, 3, 4]\\n...\\n>>> from scipy import optimize\\n>>> res = optimize.root(func, [1, 1, 1, 1], method='broyden1', tol=1e-14)\\n>>> x = res.x\\n>>> x\\narray([4.04674914, 3.91158389, 2.71791677, 1.61756251])\\n>>> np.cos(x) + x[::-1]\\narray([1., 2., 3., 4.])\",\n",
       " 'summarize to docstring: Check stats.rankdata with an array of length 1.',\n",
       " 'summarize to docstring: List commits in reverse chronological order.\\n\\nOnly the first `num_commits` are shown.',\n",
       " 'summarize to docstring: Returns UNIX timestamp (integer) representing the time\\nwhen the item was created.\\n\\n.. versionadded:: 1.1',\n",
       " 'summarize to docstring: Return a buffer object which allows access to our memory region from our offset\\nto the window size. Please note that it might be smaller than you requested when calling use_region()\\n\\n**Note:** You can only obtain a buffer if this instance is_valid() !\\n\\n**Note:** buffers should not be cached passed the duration of your access as it will\\nprevent resources from being freed even though they might not be accounted for anymore !',\n",
       " 'summarize to docstring: Test focus within.',\n",
       " \"summarize to docstring: Round each value in a Series to the given number of decimals.\\n\\nParameters\\n----------\\ndecimals : int\\n    Number of decimal places to round to (default: 0).\\n    If decimals are negative, it specifies the number of\\n    positions to the left of the decimal point.\\n\\nReturns\\n-------\\nSeries object\\n\\nSee Also\\n--------\\nDataFrame.round\\n\\nExamples\\n--------\\n>>> df = ps.Series([0.028208, 0.038683, 0.877076], name='x')\\n>>> df\\n0    0.028208\\n1    0.038683\\n2    0.877076\\nName: x, dtype: float64\\n\\n>>> df.round(2)\\n0    0.03\\n1    0.04\\n2    0.88\\nName: x, dtype: float64\",\n",
       " 'summarize to docstring: For python2 compatibility.',\n",
       " \"summarize to docstring: Generates data for a given partition and returns an iterator of tuples or rows.\\n\\nThis method is invoked once per partition to read the data. Implementing\\nthis method is required for stream reader. You can initialize any\\nnon-serializable resources required for reading data from the data source\\nwithin this method.\\n\\nNotes\\n-----\\nThis method is static and stateless. You shouldn't access mutable class member\\nor keep in memory state between different invocations of read().\\n\\nParameters\\n----------\\npartition : :class:`InputPartition`\\n    The partition to read. It must be one of the partition values returned by\\n    :meth:`DataSourceStreamReader.partitions`.\\n\\nReturns\\n-------\\niterator of tuples or PyArrow's `RecordBatch`\\n    An iterator of tuples or rows. Each tuple or row will be converted to a row\\n    in the final DataFrame.\\n    It can also return an iterator of PyArrow's `RecordBatch` if the data source\\n    supports it.\",\n",
       " 'summarize to docstring: test #9635',\n",
       " 'summarize to docstring: Just parse the simple ones.',\n",
       " \"summarize to docstring: test that 'literal binds' mode works - no bound params.\",\n",
       " 'summarize to docstring: Clear the current scope, if any.',\n",
       " 'summarize to docstring: Target must support simultaneous, independent database cursors\\non a single connection.',\n",
       " \"summarize to docstring: Encodes a plaintext into popular Morse Code with letters\\nseparated by ``sep`` and words by a double ``sep``.\\n\\nExamples\\n========\\n\\n>>> from sympy.crypto.crypto import encode_morse\\n>>> msg = 'ATTACK RIGHT FLANK'\\n>>> encode_morse(msg)\\n'.-|-|-|.-|-.-.|-.-||.-.|..|--.|....|-||..-.|.-..|.-|-.|-.-'\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Morse_code\",\n",
       " 'summarize to docstring: Returns True if the axis of the pure quaternions seen as 3D vectors\\n``q1``, ``q2``, and ``q3`` are coplanar.\\n\\nExplanation\\n===========\\n\\nThree pure quaternions are vector coplanar if the quaternions seen as 3D vectors are coplanar.\\n\\nParameters\\n==========\\n\\nq1\\n    A pure Quaternion.\\nq2\\n    A pure Quaternion.\\nq3\\n    A pure Quaternion.\\n\\nReturns\\n=======\\n\\nTrue : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar.\\nFalse : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are not coplanar.\\nNone : if the axis of the pure quaternions seen as 3D vectors\\nq1, q2, and q3 are coplanar is unknown.\\n\\nExamples\\n========\\n\\n>>> from sympy.algebras.quaternion import Quaternion\\n>>> q1 = Quaternion(0, 4, 4, 4)\\n>>> q2 = Quaternion(0, 8, 8, 8)\\n>>> q3 = Quaternion(0, 24, 24, 24)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nTrue\\n\\n>>> q1 = Quaternion(0, 8, 16, 8)\\n>>> q2 = Quaternion(0, 8, 3, 12)\\n>>> Quaternion.vector_coplanar(q1, q2, q3)\\nFalse\\n\\nSee Also\\n========\\n\\naxis\\nis_pure',\n",
       " \"summarize to docstring: Convert SymPy's expression to ``dtype``. \",\n",
       " 'summarize to docstring: Construct new raw ``RootSum`` instance. ',\n",
       " 'summarize to docstring: Prepend a minus sign to a pretty form. ',\n",
       " 'summarize to docstring: Equality as defined by https://tools.ietf.org/html/rfc5280#section-7.1\\n\\n:param other:\\n    Another RDNSequence object\\n\\n:return:\\n    A boolean',\n",
       " 'summarize to docstring: This extension is used to prevent mapping of the any policy to\\nspecific requirements\\n\\n:return:\\n    None or a Integer object',\n",
       " 'summarize to docstring: Builds a scope and request body into a WSGI environ object.',\n",
       " 'summarize to docstring: Dispatch on engine function decorator.',\n",
       " \"summarize to docstring: Get Pandas DataFrame of tables filtered by a search string.\\n\\nParameters\\n----------\\ntext\\n    Select only tables with the given string in table's properties.\\ncatalog_id\\n    The ID of the Data Catalog from which to retrieve Databases.\\n    If ``None`` is provided, the AWS account ID is used by default.\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** receive ``None``.\\n\\nReturns\\n-------\\n    Iterator of tables.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> df_tables = wr.catalog.search_tables(text='my_property')\",\n",
       " 'summarize to docstring: Write records stored in a DataFrame into Microsoft SQL Server.\\n\\nParameters\\n----------\\ndf\\n    Pandas DataFrame https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\\ncon\\n    Use pyodbc.connect() to use credentials directly or wr.sqlserver.connect() to fetch it from the Glue Catalog.\\ntable\\n    Table name\\nschema\\n    Schema name\\nmode\\n    Append, overwrite or upsert.\\n\\n    - append: Inserts new records into table.\\n    - overwrite: Drops table and recreates.\\n    - upsert: Perform an upsert which checks for conflicts on columns given by ``upsert_conflict_columns`` and sets the new values on conflicts. Note that column names of the Dataframe will be used for this operation, as if ``use_column_names`` was set to True.\\n\\nindex\\n    True to store the DataFrame index as a column in the table,\\n    otherwise False to ignore it.\\ndtype\\n    Dictionary of columns names and Microsoft SQL Server types to be casted.\\n    Useful when you have columns with undetermined or mixed data types.\\n    (e.g. {\\'col name\\': \\'TEXT\\', \\'col2 name\\': \\'FLOAT\\'})\\nvarchar_lengths\\n    Dict of VARCHAR length by columns. (e.g. {\"col1\": 10, \"col5\": 200}).\\nuse_column_names\\n    If set to True, will use the column names of the DataFrame for generating the INSERT SQL Query.\\n    E.g. If the DataFrame has two columns `col1` and `col3` and `use_column_names` is True, data will only be\\n    inserted into the database columns `col1` and `col3`.\\nuspert_conflict_columns\\n    List of columns to be used as conflict columns in the upsert operation.\\nchunksize\\n    Number of rows which are inserted with each SQL query. Defaults to inserting 200 rows per query.\\nfast_executemany\\n    Mode of execution which greatly reduces round trips for a DBAPI executemany() call when using\\n    Microsoft ODBC drivers, for limited size batches that fit in memory. `False` by default.\\n\\n    https://github.com/mkleehammer/pyodbc/wiki/Cursor#executemanysql-params-with-fast_executemanytrue\\n\\n    Note: when using this mode, pyodbc converts the Python parameter values to their ODBC \"C\" equivalents,\\n    based on the target column types in the database which may lead to subtle data type conversion\\n    differences depending on whether fast_executemany is True or False.\\n\\nExamples\\n--------\\nWriting to Microsoft SQL Server using a Glue Catalog Connections\\n\\n>>> import awswrangler as wr\\n>>> with wr.sqlserver.connect(connection=\"MY_GLUE_CONNECTION\", odbc_driver_version=17) as con:\\n...     wr.sqlserver.to_sql(\\n...         df=df,\\n...         table=\"table\",\\n...         schema=\"dbo\",\\n...         con=con\\n...     )',\n",
       " 'summarize to docstring: Get a list of available services that can be loaded as resource\\nclients via :py:meth:`Session.resource`.\\n\\n:rtype: list\\n:return: List of service names',\n",
       " \"summarize to docstring: Convert and validate a value against the option's\\n:attr:`type`, :attr:`multiple`, and :attr:`nargs`.\",\n",
       " 'summarize to docstring: Reset all resolver configuration to the defaults.',\n",
       " 'summarize to docstring: The coefficient of kinetic friction.',\n",
       " 'summarize to docstring: Call the optimize tours method over HTTP.\\n\\nArgs:\\n    request (~.route_optimization_service.OptimizeToursRequest):\\n        The request object. Request to be given to a tour\\n    optimization solver which defines the\\n    shipment model to solve as well as\\n    optimization parameters.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.route_optimization_service.OptimizeToursResponse:\\n        Response after solving a tour\\n    optimization problem containing the\\n    routes followed by each vehicle, the\\n    shipments which have been skipped and\\n    the overall cost of the solution.',\n",
       " 'summarize to docstring: Tests against python glob to check if our posix tests are accurate.',\n",
       " 'summarize to docstring: Make a blank PluginOptions, mostly used for tests.',\n",
       " 'summarize to docstring: Handles an HTTP exception.  By default this will invoke the\\nregistered error handlers and fall back to returning the\\nexception as response.\\n\\n.. versionchanged:: 1.0.3\\n    ``RoutingException``, used internally for actions such as\\n     slash redirects during routing, is not passed to error\\n     handlers.\\n\\n.. versionchanged:: 1.0\\n    Exceptions are looked up by code *and* by MRO, so\\n    ``HTTPException`` subclasses can be handled with a catch-all\\n    handler for the base ``HTTPException``.\\n\\n.. versionadded:: 0.3',\n",
       " 'summarize to docstring: Check a ref name for validity.\\n\\nThis is based on the rules described in :manpage:`git-check-ref-format(1)`.',\n",
       " 'summarize to docstring: API to retrieve a list of ``TaxonomyCategory`` objects.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.ads import admanager_v1\\n\\n    def sample_list_taxonomy_categories():\\n        # Create a client\\n        client = admanager_v1.TaxonomyCategoryServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = admanager_v1.ListTaxonomyCategoriesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_taxonomy_categories(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.ads.admanager_v1.types.ListTaxonomyCategoriesRequest, dict]):\\n        The request object. Request object for ``ListTaxonomyCategories`` method.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        TaxonomyCategories. Format: ``networks/{network_code}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.ads.admanager_v1.services.taxonomy_category_service.pagers.ListTaxonomyCategoriesPager:\\n        Response object for ListTaxonomyCategoriesRequest containing matching\\n           TaxonomyCategory objects.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'summarize to docstring: Return a callable for the update google ads link method over gRPC.\\n\\nUpdates a GoogleAdsLink on a property\\n\\nReturns:\\n    Callable[[~.UpdateGoogleAdsLinkRequest],\\n            ~.GoogleAdsLink]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the update channel group method over gRPC.\\n\\nUpdates a ChannelGroup.\\n\\nReturns:\\n    Callable[[~.UpdateChannelGroupRequest],\\n            ~.ChannelGroup]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the list display video360\\nadvertiser links method over HTTP.\\n\\n    Args:\\n        request (~.analytics_admin.ListDisplayVideo360AdvertiserLinksRequest):\\n            The request object. Request message for\\n        ListDisplayVideo360AdvertiserLinks RPC.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.analytics_admin.ListDisplayVideo360AdvertiserLinksResponse:\\n            Response message for\\n        ListDisplayVideo360AdvertiserLinks RPC.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for list_participant_sessions\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ConferenceRecordsService server.',\n",
       " 'summarize to docstring: Return a callable for the list versions method over gRPC.\\n\\nList API versions of an API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.ListVersionsRequest],\\n            ~.ListVersionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the create external api method over gRPC.\\n\\nCreate an External API resource in the API hub.\\n\\nReturns:\\n    Callable[[~.CreateExternalApiRequest],\\n            ~.ExternalApi]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the create service method over HTTP.\\n\\nArgs:\\n    request (~.apphub_service.CreateServiceRequest):\\n        The request object. Request for CreateService.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Call the get iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.GetIamPolicyRequest):\\n        The request object for GetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from GetIamPolicy method.',\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " \"summarize to docstring: Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'summarize to docstring: Call the test iam permissions method over HTTP.\\n\\nArgs:\\n    request (~.compute.TestIamPermissionsStoragePoolRequest):\\n        The request object. A request message for\\n    StoragePools.TestIamPermissions. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.TestPermissionsResponse:',\n",
       " 'summarize to docstring: Searches for documents using provided\\n[SearchDocumentsRequest][google.cloud.contentwarehouse.v1.SearchDocumentsRequest].\\nThis call only returns documents that the caller has permission\\nto search against.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contentwarehouse_v1\\n\\n    def sample_search_documents():\\n        # Create a client\\n        client = contentwarehouse_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = contentwarehouse_v1.SearchDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_documents(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contentwarehouse_v1.types.SearchDocumentsRequest, dict]):\\n        The request object. Request message for\\n        DocumentService.SearchDocuments.\\n    parent (str):\\n        Required. The parent, which owns this collection of\\n        documents. Format:\\n        projects/{project_number}/locations/{location}.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.contentwarehouse_v1.services.document_service.pagers.SearchDocumentsPager:\\n        Response message for\\n        DocumentService.SearchDocuments.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for batch_update_entities\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the EntityTypes server.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Parses a session path into its component segments.',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'summarize to docstring: Return a callable for the initialize encryption spec method over gRPC.\\n\\nInitializes a location-level encryption key\\nspecification.  An error will be thrown if the location\\nhas resources already created before the initialization.\\nOnce the encryption specification is initialized at a\\nlocation, it is immutable and all newly created\\nresources under the location will be encrypted with the\\nexisting specification.\\n\\nReturns:\\n    Callable[[~.InitializeEncryptionSpecRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the update intent method over gRPC.\\n\\nUpdates the specified intent.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\nReturns:\\n    Callable[[~.UpdateIntentRequest],\\n            ~.Intent]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the streaming analyze content method over gRPC.\\n\\nAdds a text (e.g., chat) or audio (e.g., phone recording)\\nmessage from a participant into the conversation. Note: This\\nmethod is only available through the gRPC API (not REST).\\n\\nThe top-level message sent to the client by the server is\\n``StreamingAnalyzeContentResponse``. Multiple response messages\\ncan be returned in order. The first one or more messages contain\\nthe ``recognition_result`` field. Each result represents a more\\ncomplete transcript of what the user said. The next message\\ncontains the ``reply_text`` field, and potentially the\\n``reply_audio`` and/or the ``automated_agent_reply`` fields.\\n\\nNote: Always use agent versions for production traffic sent to\\nvirtual agents. See `Versions and\\nenvironments <https://cloud.google.com/dialogflow/es/docs/agents-versions>`__.\\n\\nReturns:\\n    Callable[[~.StreamingAnalyzeContentRequest],\\n            ~.StreamingAnalyzeContentResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    EventarcAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: jsonpath : fields_or_any',\n",
       " 'summarize to docstring: Returns a list of services that allow you to opt into audit logs\\nthat are not generated by default.\\n\\nTo learn more about audit logs, see the `Logging\\ndocumentation <https://cloud.google.com/logging/docs/audit>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iam_admin_v1\\n\\n    def sample_query_auditable_services():\\n        # Create a client\\n        client = iam_admin_v1.IAMClient()\\n\\n        # Initialize request argument(s)\\n        request = iam_admin_v1.QueryAuditableServicesRequest(\\n        )\\n\\n        # Make the request\\n        response = client.query_auditable_services(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iam_admin_v1.types.QueryAuditableServicesRequest, dict]):\\n        The request object. A request to get the list of\\n        auditable services for a resource.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iam_admin_v1.types.QueryAuditableServicesResponse:\\n        A response containing a list of\\n        auditable services for a resource.',\n",
       " 'summarize to docstring: Post-rpc interceptor for delete_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the IDS server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Call the create ekm connection method over HTTP.\\n\\nArgs:\\n    request (~.ekm_service.CreateEkmConnectionRequest):\\n        The request object. Request message for\\n    [EkmService.CreateEkmConnection][google.cloud.kms.v1.EkmService.CreateEkmConnection].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.ekm_service.EkmConnection:\\n        An [EkmConnection][google.cloud.kms.v1.EkmConnection]\\n    represents an individual EKM connection. It can be used\\n    for creating [CryptoKeys][google.cloud.kms.v1.CryptoKey]\\n    and\\n    [CryptoKeyVersions][google.cloud.kms.v1.CryptoKeyVersion]\\n    with a\\n    [ProtectionLevel][google.cloud.kms.v1.ProtectionLevel]\\n    of\\n    [EXTERNAL_VPC][CryptoKeyVersion.ProtectionLevel.EXTERNAL_VPC],\\n    as well as performing cryptographic operations using\\n    keys created within the\\n    [EkmConnection][google.cloud.kms.v1.EkmConnection].',\n",
       " 'summarize to docstring: Pre-rpc interceptor for get_crypto_key\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the KeyManagementService server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for revoke_certificate\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CertificateAuthorityService server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Post-rpc interceptor for delete_collector\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the RapidMigrationAssessment server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Initiates a failover of the primary node to current\\nreplica node for a specific STANDARD tier Cloud\\nMemorystore for Redis instance.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import redis_v1beta1\\n\\n    def sample_failover_instance():\\n        # Create a client\\n        client = redis_v1beta1.CloudRedisClient()\\n\\n        # Initialize request argument(s)\\n        request = redis_v1beta1.FailoverInstanceRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.failover_instance(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.redis_v1beta1.types.FailoverInstanceRequest, dict]):\\n        The request object. Request for\\n        [Failover][google.cloud.redis.v1beta1.CloudRedis.FailoverInstance].\\n    name (str):\\n        Required. Redis instance resource name using the form:\\n        ``projects/{project_id}/locations/{location_id}/instances/{instance_id}``\\n        where ``location_id`` refers to a GCP region.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    data_protection_mode (google.cloud.redis_v1beta1.types.FailoverInstanceRequest.DataProtectionMode):\\n        Optional. Available data protection modes that the user\\n        can choose. If it\\'s unspecified, data protection mode\\n        will be LIMITED_DATA_LOSS by default.\\n\\n        This corresponds to the ``data_protection_mode`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.redis_v1beta1.types.Instance` A\\n        Memorystore for Redis instance.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for set_default_branch\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CatalogService server.',\n",
       " 'summarize to docstring: Gets metadata of a repository.\\n\\n**Host: Data Plane**\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securesourcemanager_v1\\n\\n    def sample_get_repository():\\n        # Create a client\\n        client = securesourcemanager_v1.SecureSourceManagerClient()\\n\\n        # Initialize request argument(s)\\n        request = securesourcemanager_v1.GetRepositoryRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_repository(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securesourcemanager_v1.types.GetRepositoryRequest, dict]):\\n        The request object. GetRepositoryRequest is the request\\n        for getting a repository.\\n    name (str):\\n        Required. Name of the repository to retrieve. The format\\n        is\\n        ``projects/{project_number}/locations/{location_id}/repositories/{repository_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securesourcemanager_v1.types.Repository:\\n        Metadata of a Secure Source Manager\\n        repository.',\n",
       " 'summarize to docstring: Return a callable for the delete service method over gRPC.\\n\\nDeletes a service. This also deletes all endpoints\\nassociated with the service.\\n\\nReturns:\\n    Callable[[~.DeleteServiceRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Returns a fully-qualified folder string.',\n",
       " 'summarize to docstring: Return a callable for the list queues method over gRPC.\\n\\nLists queues.\\n\\nQueues are returned in lexicographical order.\\n\\nReturns:\\n    Callable[[~.ListQueuesRequest],\\n            ~.ListQueuesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " \"summarize to docstring: Call the update migrating vm method over HTTP.\\n\\nArgs:\\n    request (~.vmmigration.UpdateMigratingVmRequest):\\n        The request object. Request message for\\n    'UpdateMigratingVm' request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.\",\n",
       " 'summarize to docstring: Return a callable for the update vmware engine network method over gRPC.\\n\\nModifies a VMware Engine network resource. Only the following\\nfields can be updated: ``description``. Only fields specified in\\n``updateMask`` are applied.\\n\\nReturns:\\n    Callable[[~.UpdateVmwareEngineNetworkRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Sends a single request, without handling any redirections.',\n",
       " 'summarize to docstring: Yield Traversable objects in self',\n",
       " 'summarize to docstring: The __init__ file can be searched in a directory. If found return it, else\\nNone.',\n",
       " 'summarize to docstring: Returns a checksum for the source.',\n",
       " 'summarize to docstring: Parse multiple statements into a list until one of the end tokens\\nis reached.  This is used to parse the body of statements as it also\\nparses template data if appropriate.  The parser checks first if the\\ncurrent token is a colon and skips it if there is one.  Then it checks\\nfor the block end and parses until if one of the `end_tokens` is\\nreached.  Per default the active token in the stream at the end of\\nthe call is the matched end token.  If this is not wanted `drop_needle`\\ncan be set to `True` and the end token is removed.',\n",
       " 'summarize to docstring: Get items to delete to keep the store under size, file, & age limits.',\n",
       " 'summarize to docstring: Schedule a func to be run',\n",
       " 'summarize to docstring: Specifically, `const` validation applies for Draft 7.',\n",
       " 'summarize to docstring: Executes core chain rules.',\n",
       " 'summarize to docstring: Create an instant of an simple tag processor.\\n\\nArguments:\\n    pattern: A regular expression that matches a pattern.\\n    tag: Tag of element.',\n",
       " 'summarize to docstring: Return the axis label as a Text instance.\\n\\n.. admonition:: Discouraged\\n\\n   This overrides `.Artist.get_label`, which is for legend labels, with a new\\n   semantic. It is recommended to use the attribute ``Axis.label`` instead.',\n",
       " \"summarize to docstring: Add *tool* to `ToolManager`.\\n\\nIf successful, adds a new event ``tool_trigger_{name}`` where\\n``{name}`` is the *name* of the tool; the event is fired every time the\\ntool is triggered.\\n\\nParameters\\n----------\\nname : str\\n    Name of the tool, treated as the ID, has to be unique.\\ntool : type\\n    Class of the tool to be added.  A subclass will be used\\n    instead if one was registered for the current canvas class.\\n*args, **kwargs\\n    Passed to the *tool*'s constructor.\\n\\nSee Also\\n--------\\nmatplotlib.backend_tools.ToolBase : The base class for tools.\",\n",
       " 'summarize to docstring: Convert a premultiplied ARGB32 buffer to an unmultiplied RGBA8888 buffer.',\n",
       " \"summarize to docstring: Set the rotation of the text.\\n\\nParameters\\n----------\\ns : float or {'vertical', 'horizontal'}\\n    The rotation angle in degrees in mathematically positive direction\\n    (counterclockwise). 'horizontal' equals 0, 'vertical' equals 90.\",\n",
       " 'summarize to docstring: Button release event handler.',\n",
       " 'summarize to docstring: Encode bson.decimal128.Decimal128.',\n",
       " 'summarize to docstring: Return internal buffer contents as bytes object',\n",
       " 'summarize to docstring: Tests for conditions specific to\\nparse_edge_list method',\n",
       " 'summarize to docstring: Find shortest weighted paths and lengths between all nodes.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\ncutoff : integer or float, optional\\n    Length (sum of edge weights) at which the search is stopped.\\n    If cutoff is provided, only return paths with summed weight <= cutoff.\\n\\nweight : string or function\\n    If this is a string, then edge weights will be accessed via the\\n    edge attribute with this key (that is, the weight of the edge\\n    joining `u` to `v` will be ``G.edge[u][v][weight]``). If no\\n    such edge attribute exists, the weight of the edge is assumed to\\n    be one.\\n\\n    If this is a function, the weight of an edge is the value\\n    returned by the function. The function must accept exactly three\\n    positional arguments: the two endpoints of an edge and the\\n    dictionary of edge attributes for that edge. The function must\\n    return a number or None to indicate a hidden edge.\\n\\nYields\\n------\\n(node, (distance, path)) : (node obj, (dict, dict))\\n    Each source node has two associated dicts. The first holds distance\\n    keyed by target and the second holds paths keyed by target.\\n    (See single_source_dijkstra for the source/target node terminology.)\\n    If desired you can apply `dict()` to this function to create a dict\\n    keyed by source node to the two dicts.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(5)\\n>>> len_path = dict(nx.all_pairs_dijkstra(G))\\n>>> len_path[3][0][1]\\n2\\n>>> for node in [0, 1, 2, 3, 4]:\\n...     print(f\"3 - {node}: {len_path[3][0][node]}\")\\n3 - 0: 3\\n3 - 1: 2\\n3 - 2: 1\\n3 - 3: 0\\n3 - 4: 1\\n>>> len_path[3][1][1]\\n[3, 2, 1]\\n>>> for n, (dist, path) in nx.all_pairs_dijkstra(G):\\n...     print(path[1])\\n[0, 1]\\n[1]\\n[2, 1]\\n[3, 2, 1]\\n[4, 3, 2, 1]\\n\\nNotes\\n-----\\nEdge weight attributes must be numerical.\\nDistances are calculated as sums of weighted edges traversed.\\n\\nThe yielded dicts only have keys for reachable nodes.',\n",
       " 'summarize to docstring: Converts a SciPy sparse array in **Coordinate** format to an iterable\\nof weighted edge triples.',\n",
       " 'summarize to docstring: get_dirs_from_args() skips over non-existing directories and files',\n",
       " 'summarize to docstring: Check if file is in free format Fortran.',\n",
       " 'summarize to docstring: Returns the dtype unchanged if it contained no metadata or a copy of the\\ndtype if it (or any of its structure dtypes) contained metadata.\\n\\nThis utility is used by `np.save` and `np.savez` to drop metadata before\\nsaving.\\n\\n.. note::\\n\\n    Due to its limitation this function may move to a more appropriate\\n    home or change in the future and is considered semi-public API only.\\n\\n.. warning::\\n\\n    This function does not preserve more strange things like record dtypes\\n    and user dtypes may simply return the wrong thing.  If you need to be\\n    sure about the latter, check the result with:\\n    ``np.can_cast(new_dtype, dtype, casting=\"no\")``.',\n",
       " 'summarize to docstring: Decrypts AES/RC4/RC2/3DES/DES ciphertext via CryptoAPI\\n\\n:param cipher:\\n    A unicode string of \"aes\", \"des\", \"tripledes_2key\", \"tripledes_3key\",\\n    \"rc2\", \"rc4\"\\n\\n:param key:\\n    The encryption key - a byte string 5-16 bytes long\\n\\n:param data:\\n    The ciphertext - a byte string\\n\\n:param iv:\\n    The initialization vector - a byte string - unused for RC4\\n\\n:param padding:\\n    Boolean, if padding should be used - unused for RC4\\n\\n:raises:\\n    ValueError - when any of the parameters contain an invalid value\\n    TypeError - when any of the parameters are of the wrong type\\n    OSError - when an error is returned by the OS crypto library\\n\\n:return:\\n    A byte string of the plaintext',\n",
       " 'summarize to docstring: Numpy version of itertools.product.\\nSometimes faster (for large inputs)...\\n\\nParameters\\n----------\\nX : list-like of list-likes\\n\\nReturns\\n-------\\nproduct : list of ndarrays\\n\\nExamples\\n--------\\n>>> cartesian_product([list(\"ABC\"), [1, 2]])\\n[array([\\'A\\', \\'A\\', \\'B\\', \\'B\\', \\'C\\', \\'C\\'], dtype=\\'<U1\\'), array([1, 2, 1, 2, 1, 2])]\\n\\nSee Also\\n--------\\nitertools.product : Cartesian product of input iterables.  Equivalent to\\n    nested for-loops.',\n",
       " 'summarize to docstring: compute and set our version',\n",
       " 'summarize to docstring: Return a Timezone instance given its name.',\n",
       " 'summarize to docstring: specifier_qualifier_list    : specifier_qualifier_list type_specifier_no_typeid\\n        ',\n",
       " 'summarize to docstring: struct_declaration : pppragma_directive\\n        ',\n",
       " 'summarize to docstring: :param e: key or index of element on value\\n:return: raw values for element if self._items is dict and contain needed element',\n",
       " 'summarize to docstring: :calls: `GET /projects/columns/{column_id} <https://docs.github.com/en/rest/reference/projects#get-a-project-column>`_',\n",
       " 'summarize to docstring: :type: string',\n",
       " 'summarize to docstring: Parse and handle errors of a toml configuration file.\\n\\nRaises ``tomllib.TOMLDecodeError``.',\n",
       " 'summarize to docstring: This is a numpy docstring.\\n\\nRaises\\n------\\n~re.error\\n    Sometimes',\n",
       " 'summarize to docstring: Name used safely inside the loop.',\n",
       " 'summarize to docstring: Build an API representation of this object.\\n\\nReturns:\\n    Dict[str, Any]:\\n        A dictionary in the format used by the BigQuery API.',\n",
       " 'summarize to docstring: Read everything up to one of the chars in endchars.\\n\\nThis is outside the formal grammar.  The InvalidMailbox TokenList that is\\nreturned acts like a Mailbox, but the data attributes are None.',\n",
       " 'summarize to docstring: Reads the robots.txt URL and feeds it to the parser.',\n",
       " 'summarize to docstring: Tests whether the fixer_util.is_encoding_comment() function is working.',\n",
       " 'summarize to docstring: Issue #43: Is shebang line preserved as the first\\nline by futurize when followed by a docstring?',\n",
       " 'summarize to docstring: Tests whether itertools.zip_longest is available.',\n",
       " \"summarize to docstring: Retrieve timestamp at which the object's retention period expires.\\n\\nSee https://cloud.google.com/storage/docs/json_api/v1/objects\\n\\n:rtype: :class:`datetime.datetime` or ``NoneType``\\n:returns: Datetime object parsed from RFC3339 valid timestamp, or\\n          ``None`` if the property is not set locally.\",\n",
       " \"summarize to docstring: read_namespaced_network_policy  # noqa: E501\\n\\nread the specified NetworkPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.read_namespaced_network_policy_with_http_info(name, namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the NetworkPolicy (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1NetworkPolicy, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'summarize to docstring: Sets the resource_rules of this V1MatchResources.\\n\\nResourceRules describes what operations on what resources/subresources the ValidatingAdmissionPolicy matches. The policy cares about an operation if it matches _any_ Rule.  # noqa: E501\\n\\n:param resource_rules: The resource_rules of this V1MatchResources.  # noqa: E501\\n:type: list[V1NamedRuleWithOperations]',\n",
       " 'summarize to docstring: Sets the config_source of this V1NodeSpec.\\n\\n\\n:param config_source: The config_source of this V1NodeSpec.  # noqa: E501\\n:type: V1NodeConfigSource',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Sets the max_surge of this V1RollingUpdateDaemonSet.\\n\\nThe maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up to a minimum of 1. Default value is 0. Example: when this is set to 30%, at most 30% of the total number of nodes that should be running the daemon pod (i.e. status.desiredNumberScheduled) can have their a new pod created before the old pod is marked as deleted. The update starts by launching new pods on 30% of nodes. Once an updated pod is available (Ready for at least minReadySeconds) the old DaemonSet pod on that node is marked deleted. If the old pod becomes unavailable for any reason (Ready transitions to false, is evicted, or is drained) an updated pod is immediatedly created on that node without considering surge limits. Allowing surge implies the possibility that the resources consumed by the daemonset on any given node can double if the readiness check fails, and so resource intensive daemonsets should take into account that they may cause evictions during disruption.  # noqa: E501\\n\\n:param max_surge: The max_surge of this V1RollingUpdateDaemonSet.  # noqa: E501\\n:type: object',\n",
       " 'summarize to docstring: Return a dictionary of memory stats\\n\\nFor more information see https://redis.io/commands/memory-stats',\n",
       " \"summarize to docstring: Increment a bitfield by a given amount.\\n:param fmt: format-string for the bitfield being updated, e.g. 'u8'\\n    for an unsigned 8-bit integer.\\n:param offset: offset (in number of bits). If prefixed with a\\n    '#', this is an offset multiplier, e.g. given the arguments\\n    fmt='u8', offset='#2', the offset will be 16.\\n:param int increment: value to increment the bitfield by.\\n:param str overflow: overflow algorithm. Defaults to WRAP, but other\\n    acceptable values are SAT and FAIL. See the Redis docs for\\n    descriptions of these algorithms.\\n:returns: a :py:class:`BitFieldOperation` instance.\",\n",
       " 'summarize to docstring: Placeholder docstring',\n",
       " 'summarize to docstring: Ingest a single Dataframe row into FeatureStore.\\n\\nArgs:\\n    data_frame (DataFrame): source DataFrame to be ingested.\\n    row (Iterable[tuple[Any, ...]]): current row that is being ingested\\n    feature_group_name (str): name of the Feature Group.\\n    feature_definitions (Dict[str, Dict[Any, Any]]):  dictionary of feature definitions.\\n        where the key is the feature name and the value is the FeatureDefinition.\\n        The FeatureDefinition contains the data type of the feature.\\n    sagemaker_fs_runtime_client (Session): session instance to perform boto calls.\\n    failed_rows (List[int]): list of indices from the data frame for which ingestion failed.\\n    target_stores (Sequence[TargetStoreEnum]): stores to be used for ingestion.\\n\\n\\nReturns:\\n    int of row indices that failed to be ingested.',\n",
       " 'summarize to docstring: Retrieves instance types GPU info of the given region.\\n\\nArgs:\\n    region (str): The AWS region.\\n\\nReturns:\\n    dict[str, dict[str, int]]: A dictionary that contains instance types as keys\\n                               and GPU info as values or empty dictionary if the\\n                               config for the given region is not found.\\n\\nRaises:\\n    ValueError: If no config found.',\n",
       " 'summarize to docstring: Return ``True`` if the given instance has locally\\nmodified attributes.\\n\\n.. container:: class_bases\\n\\n    Proxied for the :class:`_orm.Session` class on\\n    behalf of the :class:`_orm.scoping.scoped_session` class.\\n\\nThis method retrieves the history for each instrumented\\nattribute on the instance and performs a comparison of the current\\nvalue to its previously flushed or committed value, if any.\\n\\nIt is in effect a more expensive and accurate\\nversion of checking for the given instance in the\\n:attr:`.Session.dirty` collection; a full test for\\neach attribute\\'s net \"dirty\" status is performed.\\n\\nE.g.::\\n\\n    return session.is_modified(someobject)\\n\\nA few caveats to this method apply:\\n\\n* Instances present in the :attr:`.Session.dirty` collection may\\n  report ``False`` when tested with this method.  This is because\\n  the object may have received change events via attribute mutation,\\n  thus placing it in :attr:`.Session.dirty`, but ultimately the state\\n  is the same as that loaded from the database, resulting in no net\\n  change here.\\n* Scalar attributes may not have recorded the previously set\\n  value when a new value was applied, if the attribute was not loaded,\\n  or was expired, at the time the new value was received - in these\\n  cases, the attribute is assumed to have a change, even if there is\\n  ultimately no net change against its database value. SQLAlchemy in\\n  most cases does not need the \"old\" value when a set event occurs, so\\n  it skips the expense of a SQL call if the old value isn\\'t present,\\n  based on the assumption that an UPDATE of the scalar value is\\n  usually needed, and in those few cases where it isn\\'t, is less\\n  expensive on average than issuing a defensive SELECT.\\n\\n  The \"old\" value is fetched unconditionally upon set only if the\\n  attribute container has the ``active_history`` flag set to ``True``.\\n  This flag is set typically for primary key attributes and scalar\\n  object references that are not a simple many-to-one.  To set this\\n  flag for any arbitrary mapped column, use the ``active_history``\\n  argument with :func:`.column_property`.\\n\\n:param instance: mapped instance to be tested for pending changes.\\n:param include_collections: Indicates if multivalued collections\\n should be included in the operation.  Setting this to ``False`` is a\\n way to detect only local-column based properties (i.e. scalar columns\\n or many-to-one foreign keys) that would result in an UPDATE for this\\n instance upon flush.',\n",
       " 'summarize to docstring: Initializes a `DelayedReturn` object.\\n\\nArgs:\\n    function_step: A `sagemaker.workflow.step._FunctionStep` instance.\\n    reference_path: A tuple that represents the path to the child member.',\n",
       " 'summarize to docstring: For one-to-many collections, produce a :class:`_dml.Insert` which\\nwill insert new rows in terms of this this instance-local\\n:class:`_orm.WriteOnlyCollection`.\\n\\nThis construct is only supported for a :class:`_orm.Relationship`\\nthat does **not** include the :paramref:`_orm.relationship.secondary`\\nparameter.  For relationships that refer to a many-to-many table,\\nuse ordinary bulk insert techniques to produce new objects, then\\nuse :meth:`_orm.AbstractCollectionWriter.add_all` to associate them\\nwith the collection.',\n",
       " 'summarize to docstring: Check scores attribute shape',\n",
       " 'summarize to docstring: Generate (train, test) indices',\n",
       " 'summarize to docstring: Perform spherical Voronoi calculation, but not the sorting of\\nvertices in the Voronoi polygons.',\n",
       " \"summarize to docstring: Compute the determinant of a matrix\\n\\nThe determinant is a scalar that is a function of the associated square\\nmatrix coefficients. The determinant value is zero for singular matrices.\\n\\nParameters\\n----------\\na : (..., M, M) array_like\\n    Input array to compute determinants for.\\noverwrite_a : bool, optional\\n    Allow overwriting data in a (may enhance performance).\\ncheck_finite : bool, optional\\n    Whether to check that the input matrix contains only finite numbers.\\n    Disabling may give a performance gain, but may result in problems\\n    (crashes, non-termination) if the inputs do contain infinities or NaNs.\\n\\nReturns\\n-------\\ndet : (...) float or complex\\n    Determinant of `a`. For stacked arrays, a scalar is returned for each\\n    (m, m) slice in the last two dimensions of the input. For example, an\\n    input of shape (p, q, m, m) will produce a result of shape (p, q). If\\n    all dimensions are 1 a scalar is returned regardless of ndim.\\n\\nNotes\\n-----\\nThe determinant is computed by performing an LU factorization of the\\ninput with LAPACK routine 'getrf', and then calculating the product of\\ndiagonal entries of the U factor.\\n\\nEven if the input array is single precision (float32 or complex64), the\\nresult will be returned in double precision (float64 or complex128) to\\nprevent overflows.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy import linalg\\n>>> a = np.array([[1,2,3], [4,5,6], [7,8,9]])  # A singular matrix\\n>>> linalg.det(a)\\n0.0\\n>>> b = np.array([[0,2,3], [4,5,6], [7,8,9]])\\n>>> linalg.det(b)\\n3.0\\n>>> # An array with the shape (3, 2, 2, 2)\\n>>> c = np.array([[[[1., 2.], [3., 4.]],\\n...                [[5., 6.], [7., 8.]]],\\n...               [[[9., 10.], [11., 12.]],\\n...                [[13., 14.], [15., 16.]]],\\n...               [[[17., 18.], [19., 20.]],\\n...                [[21., 22.], [23., 24.]]]])\\n>>> linalg.det(c)  # The resulting shape is (3, 2)\\narray([[-2., -2.],\\n       [-2., -2.],\\n       [-2., -2.]])\\n>>> linalg.det(c[0, 0])  # Confirm the (0, 0) slice, [[1, 2], [3, 4]]\\n-2.0\",\n",
       " 'summarize to docstring: Butterworth filter order selection.\\n\\nReturn the order of the lowest order digital or analog Butterworth filter\\nthat loses no more than `gpass` dB in the passband and has at least\\n`gstop` dB attenuation in the stopband.\\n\\nParameters\\n----------\\nwp, ws : float\\n    Passband and stopband edge frequencies.\\n\\n    For digital filters, these are in the same units as `fs`. By default,\\n    `fs` is 2 half-cycles/sample, so these are normalized from 0 to 1,\\n    where 1 is the Nyquist frequency. (`wp` and `ws` are thus in\\n    half-cycles / sample.) For example:\\n\\n        - Lowpass:   wp = 0.2,          ws = 0.3\\n        - Highpass:  wp = 0.3,          ws = 0.2\\n        - Bandpass:  wp = [0.2, 0.5],   ws = [0.1, 0.6]\\n        - Bandstop:  wp = [0.1, 0.6],   ws = [0.2, 0.5]\\n\\n    For analog filters, `wp` and `ws` are angular frequencies (e.g., rad/s).\\ngpass : float\\n    The maximum loss in the passband (dB).\\ngstop : float\\n    The minimum attenuation in the stopband (dB).\\nanalog : bool, optional\\n    When True, return an analog filter, otherwise a digital filter is\\n    returned.\\nfs : float, optional\\n    The sampling frequency of the digital system.\\n\\n    .. versionadded:: 1.2.0\\n\\nReturns\\n-------\\nord : int\\n    The lowest order for a Butterworth filter which meets specs.\\nwn : ndarray or float\\n    The Butterworth natural frequency (i.e. the \"3dB frequency\"). Should\\n    be used with `butter` to give filter results. If `fs` is specified,\\n    this is in the same units, and `fs` must also be passed to `butter`.\\n\\nSee Also\\n--------\\nbutter : Filter design using order and critical points\\ncheb1ord : Find order and critical points from passband and stopband spec\\ncheb2ord, ellipord\\niirfilter : General filter design using order and critical frequencies\\niirdesign : General filter design using passband and stopband spec\\n\\nExamples\\n--------\\nDesign an analog bandpass filter with passband within 3 dB from 20 to\\n50 rad/s, while rejecting at least -40 dB below 14 and above 60 rad/s.\\nPlot its frequency response, showing the passband and stopband\\nconstraints in gray.\\n\\n>>> from scipy import signal\\n>>> import matplotlib.pyplot as plt\\n>>> import numpy as np\\n\\n>>> N, Wn = signal.buttord([20, 50], [14, 60], 3, 40, True)\\n>>> b, a = signal.butter(N, Wn, \\'band\\', True)\\n>>> w, h = signal.freqs(b, a, np.logspace(1, 2, 500))\\n>>> plt.semilogx(w, 20 * np.log10(abs(h)))\\n>>> plt.title(\\'Butterworth bandpass filter fit to constraints\\')\\n>>> plt.xlabel(\\'Frequency [rad/s]\\')\\n>>> plt.ylabel(\\'Amplitude [dB]\\')\\n>>> plt.grid(which=\\'both\\', axis=\\'both\\')\\n>>> plt.fill([1,  14,  14,   1], [-40, -40, 99, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.fill([20, 20,  50,  50], [-99, -3, -3, -99], \\'0.9\\', lw=0) # pass\\n>>> plt.fill([60, 60, 1e9, 1e9], [99, -40, -40, 99], \\'0.9\\', lw=0) # stop\\n>>> plt.axis([10, 100, -60, 3])\\n>>> plt.show()',\n",
       " 'summarize to docstring: Identity matrix in sparse format\\n\\nReturns an identity matrix with shape (n,n) using a given\\nsparse format and dtype. This differs from `eye_array` in\\nthat it has a square shape with ones only on the main diagonal.\\nIt is thus the multiplicative identity. `eye_array` allows\\nrectangular shapes and the diagonal can be offset from the main one.\\n\\n.. warning::\\n\\n    This function returns a sparse matrix -- not a sparse array.\\n    You are encouraged to use ``eye_array`` to take advantage\\n    of the sparse array functionality.\\n\\nParameters\\n----------\\nn : int\\n    Shape of the identity matrix.\\ndtype : dtype, optional\\n    Data type of the matrix\\nformat : str, optional\\n    Sparse format of the result, e.g., format=\"csr\", etc.\\n\\nExamples\\n--------\\n>>> import scipy as sp\\n>>> sp.sparse.identity(3).toarray()\\narray([[ 1.,  0.,  0.],\\n       [ 0.,  1.,  0.],\\n       [ 0.,  0.,  1.]])\\n>>> sp.sparse.identity(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse matrix of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>\\n>>> sp.sparse.eye_array(3, dtype=\\'int8\\', format=\\'dia\\')\\n<DIAgonal sparse array of dtype \\'int8\\'\\n    with 3 stored elements (1 diagonals) and shape (3, 3)>',\n",
       " 'summarize to docstring: Partial singular value decomposition of a sparse matrix using ARPACK.\\n\\nCompute the largest or smallest `k` singular values and corresponding\\nsingular vectors of a sparse matrix `A`. The order in which the singular\\nvalues are returned is not guaranteed.\\n\\nIn the descriptions below, let ``M, N = A.shape``.\\n\\nParameters\\n----------\\nA : sparse matrix or LinearOperator\\n    Matrix to decompose.\\nk : int, optional\\n    Number of singular values and singular vectors to compute.\\n    Must satisfy ``1 <= k <= min(M, N) - 1``.\\n    Default is 6.\\nncv : int, optional\\n    The number of Lanczos vectors generated.\\n    The default is ``min(n, max(2*k + 1, 20))``.\\n    If specified, must satisfy ``k + 1 < ncv < min(M, N)``; ``ncv > 2*k``\\n    is recommended.\\ntol : float, optional\\n    Tolerance for singular values. Zero (default) means machine precision.\\nwhich : {\\'LM\\', \\'SM\\'}\\n    Which `k` singular values to find: either the largest magnitude (\\'LM\\')\\n    or smallest magnitude (\\'SM\\') singular values.\\nv0 : ndarray, optional\\n    The starting vector for iteration:\\n    an (approximate) left singular vector if ``N > M`` and a right singular\\n    vector otherwise. Must be of length ``min(M, N)``.\\n    Default: random\\nmaxiter : int, optional\\n    Maximum number of Arnoldi update iterations allowed;\\n    default is ``min(M, N) * 10``.\\nreturn_singular_vectors : {True, False, \"u\", \"vh\"}\\n    Singular values are always computed and returned; this parameter\\n    controls the computation and return of singular vectors.\\n\\n    - ``True``: return singular vectors.\\n    - ``False``: do not return singular vectors.\\n    - ``\"u\"``: if ``M <= N``, compute only the left singular vectors and\\n      return ``None`` for the right singular vectors. Otherwise, compute\\n      all singular vectors.\\n    - ``\"vh\"``: if ``M > N``, compute only the right singular vectors and\\n      return ``None`` for the left singular vectors. Otherwise, compute\\n      all singular vectors.\\n\\nsolver :  {\\'arpack\\', \\'propack\\', \\'lobpcg\\'}, optional\\n        This is the solver-specific documentation for ``solver=\\'arpack\\'``.\\n        :ref:`\\'lobpcg\\' <sparse.linalg.svds-lobpcg>` and\\n        :ref:`\\'propack\\' <sparse.linalg.svds-propack>`\\n        are also supported.\\nrandom_state : {None, int, `numpy.random.Generator`,\\n                `numpy.random.RandomState`}, optional\\n\\n    Pseudorandom number generator state used to generate resamples.\\n\\n    If `random_state` is ``None`` (or `np.random`), the\\n    `numpy.random.RandomState` singleton is used.\\n    If `random_state` is an int, a new ``RandomState`` instance is used,\\n    seeded with `random_state`.\\n    If `random_state` is already a ``Generator`` or ``RandomState``\\n    instance then that instance is used.\\noptions : dict, optional\\n    A dictionary of solver-specific options. No solver-specific options\\n    are currently supported; this parameter is reserved for future use.\\n\\nReturns\\n-------\\nu : ndarray, shape=(M, k)\\n    Unitary matrix having left singular vectors as columns.\\ns : ndarray, shape=(k,)\\n    The singular values.\\nvh : ndarray, shape=(k, N)\\n    Unitary matrix having right singular vectors as rows.\\n\\nNotes\\n-----\\nThis is a naive implementation using ARPACK as an eigensolver\\non ``A.conj().T @ A`` or ``A @ A.conj().T``, depending on which one is more\\nefficient.\\n\\nExamples\\n--------\\nConstruct a matrix ``A`` from singular values and vectors.\\n\\n>>> import numpy as np\\n>>> from scipy.stats import ortho_group\\n>>> from scipy.sparse import csc_matrix, diags\\n>>> from scipy.sparse.linalg import svds\\n>>> rng = np.random.default_rng()\\n>>> orthogonal = csc_matrix(ortho_group.rvs(10, random_state=rng))\\n>>> s = [0.0001, 0.001, 3, 4, 5]  # singular values\\n>>> u = orthogonal[:, :5]         # left singular vectors\\n>>> vT = orthogonal[:, 5:].T      # right singular vectors\\n>>> A = u @ diags(s) @ vT\\n\\nWith only three singular values/vectors, the SVD approximates the original\\nmatrix.\\n\\n>>> u2, s2, vT2 = svds(A, k=3, solver=\\'arpack\\')\\n>>> A2 = u2 @ np.diag(s2) @ vT2\\n>>> np.allclose(A2, A.toarray(), atol=1e-3)\\nTrue\\n\\nWith all five singular values/vectors, we can reproduce the original\\nmatrix.\\n\\n>>> u3, s3, vT3 = svds(A, k=5, solver=\\'arpack\\')\\n>>> A3 = u3 @ np.diag(s3) @ vT3\\n>>> np.allclose(A3, A.toarray())\\nTrue\\n\\nThe singular values match the expected singular values, and the singular\\nvectors are as expected up to a difference in sign.\\n\\n>>> (np.allclose(s3, s) and\\n...  np.allclose(np.abs(u3), np.abs(u.toarray())) and\\n...  np.allclose(np.abs(vT3), np.abs(vT.toarray())))\\nTrue\\n\\nThe singular vectors are also orthogonal.\\n\\n>>> (np.allclose(u3.T @ u3, np.eye(5)) and\\n...  np.allclose(vT3 @ vT3.T, np.eye(5)))\\nTrue',\n",
       " 'summarize to docstring: Gauss-Jacobi (shifted) quadrature.\\n\\nCompute the sample points and weights for Gauss-Jacobi (shifted)\\nquadrature. The sample points are the roots of the nth degree\\nshifted Jacobi polynomial, :math:`G^{p,q}_n(x)`. These sample\\npoints and weights correctly integrate polynomials of degree\\n:math:`2n - 1` or less over the interval :math:`[0, 1]` with\\nweight function :math:`w(x) = (1 - x)^{p-q} x^{q-1}`. See 22.2.2\\nin [AS]_ for details.\\n\\nParameters\\n----------\\nn : int\\n    quadrature order\\np1 : float\\n    (p1 - q1) must be > -1\\nq1 : float\\n    q1 must be > 0\\nmu : bool, optional\\n    If True, return the sum of the weights, optional.\\n\\nReturns\\n-------\\nx : ndarray\\n    Sample points\\nw : ndarray\\n    Weights\\nmu : float\\n    Sum of the weights\\n\\nSee Also\\n--------\\nscipy.integrate.fixed_quad\\n\\nReferences\\n----------\\n.. [AS] Milton Abramowitz and Irene A. Stegun, eds.\\n    Handbook of Mathematical Functions with Formulas,\\n    Graphs, and Mathematical Tables. New York: Dover, 1972.',\n",
       " 'summarize to docstring: Type is treated as case insensitive in HTML.',\n",
       " 'summarize to docstring: Test direction in `iframe`.',\n",
       " 'summarize to docstring: Model intercept.',\n",
       " 'summarize to docstring: Standard deviation of the StandardScalerModel.',\n",
       " 'summarize to docstring: Gets the value of fitLinear or its default value.',\n",
       " 'summarize to docstring: Returns registered Documenter classes',\n",
       " 'summarize to docstring: Invalidate mocked modules on sys.modules.',\n",
       " 'summarize to docstring: Target database must support EXCEPT or equivalent (i.e. MINUS).',\n",
       " \"summarize to docstring: FastIntFlag still causes elements to be global symbols.\\n\\nWhile we do this and haven't yet changed it, make sure conflicting\\nint values for the same name don't come in.\",\n",
       " 'summarize to docstring: Inserts *token* before *where*.',\n",
       " 'summarize to docstring: Returns the class of objects of this category.\\n\\nExamples\\n========\\n\\n>>> from sympy.categories import Object, Category\\n>>> from sympy import FiniteSet\\n>>> A = Object(\"A\")\\n>>> B = Object(\"B\")\\n>>> K = Category(\"K\", FiniteSet(A, B))\\n>>> K.objects\\nClass({Object(\"A\"), Object(\"B\")})',\n",
       " 'summarize to docstring: Check if ``a`` belongs to this domain. ',\n",
       " \"summarize to docstring: Convert GMPY's ``mpz`` to ``dtype``. \",\n",
       " \"summarize to docstring: Convert ``ModularInteger(int)`` to GMPY's ``mpz``. \",\n",
       " 'summarize to docstring: Return a sparse DomainMatrix representation of *self*.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy import QQ\\n>>> A = DomainMatrix([[1, 0],[0, 2]], (2, 2), QQ)\\n>>> A.rep\\n[[1, 0], [0, 2]]\\n>>> B = A.to_sparse()\\n>>> B.rep\\n{0: {0: 1}, 1: {1: 2}}',\n",
       " 'summarize to docstring: Return the Smith-Normal form decomposition of matrix `m`.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> from sympy.polys.matrices.normalforms import smith_normal_decomp\\n>>> m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\\n...                   [ZZ(3), ZZ(9), ZZ(6)],\\n...                   [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\\n>>> a, s, t = smith_normal_decomp(m)\\n>>> assert a == s * m * t',\n",
       " 'summarize to docstring: Computes the ``m``-th order indefinite integral of ``f`` in ``x_j``. ',\n",
       " 'summarize to docstring: Return height of the complex isolating interval. ',\n",
       " \"summarize to docstring: Fateman's GCD benchmark: sparse inputs (deg f ~ vars f) \",\n",
       " 'summarize to docstring: Returns the roots of characteristic equation of constant coefficient\\nlinear ODE and list of collectterms which is later on used by simplification\\nto use collect on solution.\\n\\nThe parameter `r` is a dict of order:coeff terms, where order is the order of the\\nderivative on each term, and coeff is the coefficient of that derivative.',\n",
       " 'summarize to docstring: Returns a fully-qualified backup string.',\n",
       " \"summarize to docstring: Return maximum delimiter priority inside `node`.\\n\\nThis is specific to atoms with contents contained in a pair of parentheses.\\nIf `node` isn't an atom or there are no enclosing parentheses, returns 0.\",\n",
       " 'summarize to docstring: a => b',\n",
       " 'summarize to docstring: Returns the default stream encoding if not found.',\n",
       " 'summarize to docstring: Add a new state to the machine and return it.',\n",
       " 'summarize to docstring: Classify a node.\\n\\nA node which contains a CNAME or RRSIG(CNAME) is a\\n``NodeKind.CNAME`` node.\\n\\nA node which contains only \"neutral\" types, i.e. types allowed to\\nco-exist with a CNAME, is a ``NodeKind.NEUTRAL`` node.  The neutral\\ntypes are NSEC, NSEC3, KEY, and their associated RRSIGS.  An empty node\\nis also considered neutral.\\n\\nA node which contains some rdataset which is not a CNAME, RRSIG(CNAME),\\nor a neutral type is a a ``NodeKind.REGULAR`` node.  Regular nodes are\\nalso commonly referred to as \"other data\".',\n",
       " 'summarize to docstring: Register a URL value preprocessor function for all view\\nfunctions in the application. These functions will be called before the\\n:meth:`before_request` functions.\\n\\nThe function can modify the values captured from the matched url before\\nthey are passed to the view. For example, this can be used to pop a\\ncommon language code value and place it in ``g`` rather than pass it to\\nevery view.\\n\\nThe function is passed the endpoint name and values dict. The return\\nvalue is ignored.\\n\\nThis is available on both app and blueprint objects. When used on an app, this\\nis called for every request. When used on a blueprint, this is called for\\nrequests that the blueprint handles. To register with a blueprint and affect\\nevery request, use :meth:`.Blueprint.app_url_value_preprocessor`.',\n",
       " 'summarize to docstring: Gets configuration metadata about a specific audience list. This\\nmethod can be used to understand an audience list after it has\\nbeen created.\\n\\nSee `Creating an Audience\\nList <https://developers.google.com/analytics/devguides/reporting/data/v1/audience-list-basics>`__\\nfor an introduction to Audience Lists with examples.\\n\\nThis method is available at beta stability at\\n`audienceExports.get <https://developers.google.com/analytics/devguides/reporting/data/v1/rest/v1beta/properties.audienceExports/get>`__.\\nTo give your feedback on this API, complete the `Google\\nAnalytics Audience Export API\\nFeedback <https://forms.gle/EeA5u5LW6PEggtCEA>`__ form.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.analytics import data_v1alpha\\n\\n    def sample_get_audience_list():\\n        # Create a client\\n        client = data_v1alpha.AlphaAnalyticsDataClient()\\n\\n        # Initialize request argument(s)\\n        request = data_v1alpha.GetAudienceListRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_audience_list(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.analytics.data_v1alpha.types.GetAudienceListRequest, dict]):\\n        The request object. A request to retrieve configuration\\n        metadata about a specific audience list.\\n    name (str):\\n        Required. The audience list resource name. Format:\\n        ``properties/{property}/audienceLists/{audience_list}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.analytics.data_v1alpha.types.AudienceList:\\n        An audience list is a list of users\\n        in an audience at the time of the list\\'s\\n        creation. One audience may have multiple\\n        audience lists created for different\\n        days.',\n",
       " 'summarize to docstring: @rtype:  int\\n@return: Process global ID.',\n",
       " 'summarize to docstring: Return a callable for the create build trigger method over gRPC.\\n\\nCreates a new ``BuildTrigger``.\\n\\nThis API is experimental.\\n\\nReturns:\\n    Callable[[~.CreateBuildTriggerRequest],\\n            ~.BuildTrigger]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Transfers customer entitlements from their current reseller to\\nGoogle.\\n\\nPossible error codes:\\n\\n-  PERMISSION_DENIED: The customer doesn\\'t belong to the\\n   reseller.\\n-  INVALID_ARGUMENT: Required request parameters are missing or\\n   invalid.\\n-  NOT_FOUND: The customer or offer resource was not found.\\n-  ALREADY_EXISTS: The SKU was already transferred for the\\n   customer.\\n-  CONDITION_NOT_MET or FAILED_PRECONDITION:\\n\\n   -  The SKU requires domain verification to transfer, but the\\n      domain is not verified.\\n   -  An Add-On SKU (example, Vault or Drive) is missing the\\n      pre-requisite SKU (example, G Suite Basic).\\n   -  (Developer accounts only) Reseller and resold domain must\\n      meet the following naming requirements:\\n\\n      -  Domain names must start with goog-test.\\n      -  Domain names must include the reseller domain.\\n\\n-  INTERNAL: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n-  UNKNOWN: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n\\nReturn value: The ID of a long-running operation.\\n\\nTo get the results of the operation, call the GetOperation\\nmethod of CloudChannelOperationsService. The response will\\ncontain google.protobuf.Empty on success. The Operation metadata\\nwill contain an instance of\\n[OperationMetadata][google.cloud.channel.v1.OperationMetadata].\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import channel_v1\\n\\n    def sample_transfer_entitlements_to_google():\\n        # Create a client\\n        client = channel_v1.CloudChannelServiceClient()\\n\\n        # Initialize request argument(s)\\n        entitlements = channel_v1.Entitlement()\\n        entitlements.offer = \"offer_value\"\\n\\n        request = channel_v1.TransferEntitlementsToGoogleRequest(\\n            parent=\"parent_value\",\\n            entitlements=entitlements,\\n        )\\n\\n        # Make the request\\n        operation = client.transfer_entitlements_to_google(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.channel_v1.types.TransferEntitlementsToGoogleRequest, dict]):\\n        The request object. Request message for\\n        [CloudChannelService.TransferEntitlementsToGoogle][google.cloud.channel.v1.CloudChannelService.TransferEntitlementsToGoogle].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: Returns a specified regional persistent disk.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get():\\n        # Create a client\\n        client = compute_v1.RegionDisksClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetRegionDiskRequest(\\n            disk=\"disk_value\",\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetRegionDiskRequest, dict]):\\n        The request object. A request message for\\n        RegionDisks.Get. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk (str):\\n        Name of the regional persistent disk\\n        to return.\\n\\n        This corresponds to the ``disk`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.Disk:\\n        Represents a Persistent Disk resource. Google Compute\\n        Engine has two Disk resources: \\\\*\\n        [Zonal](/compute/docs/reference/rest/v1/disks) \\\\*\\n        [Regional](/compute/docs/reference/rest/v1/regionDisks)\\n        Persistent disks are required for running your VM\\n        instances. Create both boot and non-boot (data)\\n        persistent disks. For more information, read Persistent\\n        Disks. For more storage options, read Storage options.\\n        The disks resource represents a zonal persistent disk.\\n        For more information, read Zonal persistent disks. The\\n        regionDisks resource represents a regional persistent\\n        disk. For more information, read Regional resources.',\n",
       " 'summarize to docstring: Creates a new network firewall policy in the\\nspecified project and region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_insert():\\n        # Create a client\\n        client = compute_v1.RegionNetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.InsertRegionNetworkFirewallPolicyRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.insert(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.InsertRegionNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        RegionNetworkFirewallPolicies.Insert.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy_resource (google.cloud.compute_v1.types.FirewallPolicy):\\n        The body resource for this request\\n        This corresponds to the ``firewall_policy_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'summarize to docstring: Call the list document schemas method over HTTP.\\n\\nArgs:\\n    request (~.document_schema_service.ListDocumentSchemasRequest):\\n        The request object. Request message for\\n    DocumentSchemaService.ListDocumentSchemas.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_schema_service.ListDocumentSchemasResponse:\\n        Response message for\\n    DocumentSchemaService.ListDocumentSchemas.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TemplatesServiceAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for batch_delete_test_cases\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the TestCases server.',\n",
       " 'summarize to docstring: Return a callable for the update participant method over gRPC.\\n\\nUpdates the specified participant.\\n\\nReturns:\\n    Callable[[~.UpdateParticipantRequest],\\n            ~.Participant]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Parses a asset path into its component segments.',\n",
       " 'summarize to docstring: Return a callable for the list preference sets method over gRPC.\\n\\nLists all the preference sets in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListPreferenceSetsRequest],\\n            ~.ListPreferenceSetsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return a callable for the create grpc route method over gRPC.\\n\\nCreates a new GrpcRoute in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateGrpcRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return string with information about image sequence.',\n",
       " 'summarize to docstring: Post-rpc interceptor for get_replay\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Simulator server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    info.\\n\\nArgs:\\n    info (dict): The service account private key info.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    TagKeysAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Updates the mute state of a finding.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securitycenter_v1\\n\\n    def sample_set_mute():\\n        # Create a client\\n        client = securitycenter_v1.SecurityCenterClient()\\n\\n        # Initialize request argument(s)\\n        request = securitycenter_v1.SetMuteRequest(\\n            name=\"name_value\",\\n            mute=\"UNDEFINED\",\\n        )\\n\\n        # Make the request\\n        response = client.set_mute(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securitycenter_v1.types.SetMuteRequest, dict]):\\n        The request object. Request message for updating a\\n        finding\\'s mute status.\\n    name (str):\\n        Required. The `relative resource\\n        name <https://cloud.google.com/apis/design/resource_names#relative_resource_name>`__\\n        of the finding. Example:\\n        ``organizations/{organization_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``folders/{folder_id}/sources/{source_id}/findings/{finding_id}``,\\n        ``projects/{project_id}/sources/{source_id}/findings/{finding_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    mute (google.cloud.securitycenter_v1.types.Finding.Mute):\\n        Required. The desired state of the\\n        Mute.\\n\\n        This corresponds to the ``mute`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securitycenter_v1.types.Finding:\\n        Security Command Center finding.\\n\\n        A finding is a record of assessment data\\n        like security, risk, health, or privacy,\\n        that is ingested into Security Command\\n        Center for presentation, notification,\\n        analysis, policy testing, and\\n        enforcement. For example, a cross-site\\n        scripting (XSS) vulnerability in an App\\n        Engine application is a finding.',\n",
       " 'summarize to docstring: Return a callable for the update deployment method over gRPC.\\n\\nUpdates a deployment.\\n\\nReturns:\\n    Callable[[~.UpdateDeploymentRequest],\\n            ~.Deployment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'texttospeech.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'summarize to docstring: Return a callable for the delete product method over gRPC.\\n\\nPermanently deletes a product and its reference\\nimages.\\nMetadata of the product and all its images will be\\ndeleted right away, but search queries against\\nProductSets containing the product may still work until\\nall related caches are refreshed.\\n\\nReturns:\\n    Callable[[~.DeleteProductRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for search_index_endpoint\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Warehouse server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Returns a fully-qualified account string.',\n",
       " 'summarize to docstring: Call the list data sources method over HTTP.\\n\\nArgs:\\n    request (~.datasources.ListDataSourcesRequest):\\n        The request object. Request message for the\\n    ListDataSources method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.datasources.ListDataSourcesResponse:\\n        Response message for the\\n    ListDataSources method.',\n",
       " 'summarize to docstring: SPE specific metadata.\\n\\nParameters\\n----------\\nindex : int\\n    Ignored as SPE files only store global metadata.\\nexclude_applied : bool\\n    Ignored. Exists for API compatibility.\\nchar_encoding : str\\n    The encoding to use when parsing strings.\\nsdt_control : bool\\n    If `True`, decode special metadata written by the\\n    SDT-control software if present.\\n\\nReturns\\n-------\\nmetadata : dict\\n    Key-value pairs of metadata.\\n\\nNotes\\n-----\\nSPE v3 stores metadata as XML, whereas SPE v2 uses a binary format.\\n\\n.. rubric:: Supported SPE v2 Metadata fields\\n\\nROIs : list of dict\\n    Regions of interest used for recording images. Each dict has the\\n    \"top_left\" key containing x and y coordinates of the top left corner,\\n    the \"bottom_right\" key with x and y coordinates of the bottom right\\n    corner, and the \"bin\" key with number of binned pixels in x and y\\n    directions.\\ncomments : list of str\\n    The SPE format allows for 5 comment strings of 80 characters each.\\ncontroller_version : int\\n    Hardware version\\nlogic_output : int\\n    Definition of output BNC\\namp_hi_cap_low_noise : int\\n    Amp switching mode\\nmode : int\\n    Timing mode\\nexp_sec : float\\n    Alternative exposure in seconds\\ndate : str\\n    Date string\\ndetector_temp : float\\n    Detector temperature\\ndetector_type : int\\n    CCD / diode array type\\nst_diode : int\\n    Trigger diode\\ndelay_time : float\\n    Used with async mode\\nshutter_control : int\\n    Normal, disabled open, or disabled closed\\nabsorb_live : bool\\n    on / off\\nabsorb_mode : int\\n    Reference strip or file\\ncan_do_virtual_chip : bool\\n    True or False whether chip can do virtual chip\\nthreshold_min_live : bool\\n    on / off\\nthreshold_min_val : float\\n    Threshold minimum value\\nthreshold_max_live : bool\\n    on / off\\nthreshold_max_val : float\\n    Threshold maximum value\\ntime_local : str\\n    Experiment local time\\ntime_utc : str\\n    Experiment UTC time\\nadc_offset : int\\n    ADC offset\\nadc_rate : int\\n    ADC rate\\nadc_type : int\\n    ADC type\\nadc_resolution : int\\n    ADC resolution\\nadc_bit_adjust : int\\n    ADC bit adjust\\ngain : int\\n    gain\\nsw_version : str\\n    Version of software which created this file\\nspare_4 : bytes\\n    Reserved space\\nreadout_time : float\\n    Experiment readout time\\ntype : str\\n    Controller type\\nclockspeed_us : float\\n    Vertical clock speed in microseconds\\nreadout_mode : [\"full frame\", \"frame transfer\", \"kinetics\", \"\"]\\n    Readout mode. Empty string means that this was not set by the\\n    Software.\\nwindow_size : int\\n    Window size for Kinetics mode\\nfile_header_ver : float\\n    File header version\\nchip_size : [int, int]\\n    x and y dimensions of the camera chip\\nvirt_chip_size : [int, int]\\n    Virtual chip x and y dimensions\\npre_pixels : [int, int]\\n    Pre pixels in x and y dimensions\\npost_pixels : [int, int],\\n    Post pixels in x and y dimensions\\ngeometric : list of {\"rotate\", \"reverse\", \"flip\"}\\n    Geometric operations\\nsdt_major_version : int\\n    (only for files created by SDT-control)\\n    Major version of SDT-control software\\nsdt_minor_version : int\\n    (only for files created by SDT-control)\\n    Minor version of SDT-control software\\nsdt_controller_name : str\\n    (only for files created by SDT-control)\\n    Controller name\\nexposure_time : float\\n    (only for files created by SDT-control)\\n    Exposure time in seconds\\ncolor_code : str\\n    (only for files created by SDT-control)\\n    Color channels used\\ndetection_channels : int\\n    (only for files created by SDT-control)\\n    Number of channels\\nbackground_subtraction : bool\\n    (only for files created by SDT-control)\\n    Whether background subtraction war turned on\\nem_active : bool\\n    (only for files created by SDT-control)\\n    Whether EM was turned on\\nem_gain : int\\n    (only for files created by SDT-control)\\n    EM gain\\nmodulation_active : bool\\n    (only for files created by SDT-control)\\n    Whether laser modulation (â€œattenuateâ€) was turned on\\npixel_size : float\\n    (only for files created by SDT-control)\\n    Camera pixel size\\nsequence_type : str\\n    (only for files created by SDT-control)\\n    Type of sequnce (standard, TOCCSL, arbitrary, â€¦)\\ngrid : float\\n    (only for files created by SDT-control)\\n    Sequence time unit (â€œgrid sizeâ€) in seconds\\nn_macro : int\\n    (only for files created by SDT-control)\\n    Number of macro loops\\ndelay_macro : float\\n    (only for files created by SDT-control)\\n    Time between macro loops in seconds\\nn_mini : int\\n    (only for files created by SDT-control)\\n    Number of mini loops\\ndelay_mini : float\\n    (only for files created by SDT-control)\\n    Time between mini loops in seconds\\nn_micro : int (only for files created by SDT-control)\\n    Number of micro loops\\ndelay_micro : float (only for files created by SDT-control)\\n    Time between micro loops in seconds\\nn_subpics : int\\n    (only for files created by SDT-control)\\n    Number of sub-pictures\\ndelay_shutter : float\\n    (only for files created by SDT-control)\\n    Camera shutter delay in seconds\\ndelay_prebleach : float\\n    (only for files created by SDT-control)\\n    Pre-bleach delay in seconds\\nbleach_time : float\\n    (only for files created by SDT-control)\\n    Bleaching time in seconds\\nrecovery_time : float\\n    (only for files created by SDT-control)\\n    Recovery time in seconds\\ncomment : str\\n    (only for files created by SDT-control)\\n    User-entered comment. This replaces the \"comments\" field.\\ndatetime : datetime.datetime\\n    (only for files created by SDT-control)\\n    Combines the \"date\" and \"time_local\" keys. The latter two plus\\n    \"time_utc\" are removed.\\nmodulation_script : str\\n    (only for files created by SDT-control)\\n    Laser modulation script. Replaces the \"spare_4\" key.\\nbleach_piezo_active : bool\\n    (only for files created by SDT-control)\\n    Whether piezo for bleaching was enabled',\n",
       " 'summarize to docstring: Get the global InteractiveShell instance.\\n\\nReturns None if no InteractiveShell instance is registered.',\n",
       " 'summarize to docstring: Activate the interactive debugger.\\n\\nThis magic command support two ways of activating debugger.\\nOne is to activate debugger before executing code.  This way, you\\ncan set a break point, to step through the code from the point.\\nYou can use this mode by giving statements to execute and optionally\\na breakpoint.\\n\\nThe other one is to activate debugger in post-mortem mode.  You can\\nactivate this mode simply running %debug without any argument.\\nIf an exception has just occurred, this lets you inspect its stack\\nframes interactively.  Note that this will always work only on the last\\ntraceback that occurred, so you must call this quickly after an\\nexception that you wish to inspect has fired, because if another one\\noccurs, it clobbers the previous one.\\n\\nIf you want IPython to automatically do this on every exception, see\\nthe %pdb magic for more details.\\n\\n.. versionchanged:: 7.3\\n    When running code, user variables are no longer expanded,\\n    the magic line is always left unmodified.',\n",
       " \"summarize to docstring: var_expand on invalid formats shouldn't raise\",\n",
       " 'summarize to docstring: Check whether the instance conforms to the given format.\\n\\nArguments:\\n\\n    instance (*any primitive type*, i.e. str, number, bool):\\n\\n        The instance to check\\n\\n    format:\\n\\n        The format that instance should conform to\\n\\nReturns:\\n\\n    bool: whether it conformed',\n",
       " 'summarize to docstring: Return the x-axis view limits.\\n\\nReturns\\n-------\\nleft, right : (float, float)\\n    The current x-axis limits in data coordinates.\\n\\nSee Also\\n--------\\n.Axes.set_xlim\\n.Axes.set_xbound, .Axes.get_xbound\\n.Axes.invert_xaxis, .Axes.xaxis_inverted\\n\\nNotes\\n-----\\nThe x-axis may be inverted, in which case the *left* value will\\nbe greater than the *right* value.',\n",
       " 'summarize to docstring: Draw a multibyte character from a Type 3 font as an XObject.',\n",
       " 'summarize to docstring: Return a list of (ind0, ind1) such that ``mask[ind0:ind1].all()`` is\\nTrue and we cover all such regions.',\n",
       " 'summarize to docstring: You cannot define methods out of the spec',\n",
       " 'summarize to docstring: Send a batch of write operations to the server.\\n\\nRequests are passed as a list of write operation instances (\\n:class:`~pymongo.operations.InsertOne`,\\n:class:`~pymongo.operations.UpdateOne`,\\n:class:`~pymongo.operations.UpdateMany`,\\n:class:`~pymongo.operations.ReplaceOne`,\\n:class:`~pymongo.operations.DeleteOne`, or\\n:class:`~pymongo.operations.DeleteMany`).\\n\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634ef\\')}\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  >>> # DeleteMany, UpdateOne, and UpdateMany are also available.\\n  ...\\n  >>> from pymongo import InsertOne, DeleteOne, ReplaceOne\\n  >>> requests = [InsertOne({\\'y\\': 1}), DeleteOne({\\'x\\': 1}),\\n  ...             ReplaceOne({\\'w\\': 1}, {\\'z\\': 1}, upsert=True)]\\n  >>> result = db.test.bulk_write(requests)\\n  >>> result.inserted_count\\n  1\\n  >>> result.deleted_count\\n  1\\n  >>> result.modified_count\\n  0\\n  >>> result.upserted_ids\\n  {2: ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n  >>> for doc in db.test.find({}):\\n  ...     print(doc)\\n  ...\\n  {\\'x\\': 1, \\'_id\\': ObjectId(\\'54f62e60fba5226811f634f0\\')}\\n  {\\'y\\': 1, \\'_id\\': ObjectId(\\'54f62ee2fba5226811f634f1\\')}\\n  {\\'z\\': 1, \\'_id\\': ObjectId(\\'54f62ee28891e756a6e1abd5\\')}\\n\\n:param requests: A list of write operations (see examples above).\\n:param ordered: If ``True`` (the default) requests will be\\n    performed on the server serially, in the order provided. If an error\\n    occurs all remaining operations are aborted. If ``False`` requests\\n    will be performed on the server in arbitrary order, possibly in\\n    parallel, and all operations will be attempted.\\n:param bypass_document_validation: (optional) If ``True``, allows the\\n    write to opt-out of document level validation. Default is\\n    ``False``.\\n:param session: a\\n    :class:`~pymongo.client_session.ClientSession`.\\n:param comment: A user-provided comment to attach to this\\n    command.\\n:param let: Map of parameter names and values. Values must be\\n    constant or closed expressions that do not reference document\\n    fields. Parameters can then be accessed as variables in an\\n    aggregate expression context (e.g. \"$$var\").\\n\\n:return: An instance of :class:`~pymongo.results.BulkWriteResult`.\\n\\n.. seealso:: :ref:`writes-and-ids`\\n\\n.. note:: `bypass_document_validation` requires server version\\n  **>= 3.2**\\n\\n.. versionchanged:: 4.1\\n   Added ``comment`` parameter.\\n   Added ``let`` parameter.\\n\\n.. versionchanged:: 3.6\\n   Added ``session`` parameter.\\n\\n.. versionchanged:: 3.2\\n  Added bypass_document_validation support\\n\\n.. versionadded:: 3.0',\n",
       " 'summarize to docstring: Test a complete undirected graph.',\n",
       " 'summarize to docstring: Test that the new (>=v1.15) implementation (see #10073) is equal to the original (<=v1.14) ',\n",
       " 'summarize to docstring: Load data from a text file.\\n\\nParameters\\n----------\\nfname : file, str, pathlib.Path, list of str, generator\\n    File, filename, list, or generator to read.  If the filename\\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\\n    that generators must return bytes or strings. The strings\\n    in a list or produced by a generator are treated as lines.\\ndtype : data-type, optional\\n    Data-type of the resulting array; default: float.  If this is a\\n    structured data-type, the resulting array will be 1-dimensional, and\\n    each row will be interpreted as an element of the array.  In this\\n    case, the number of columns used must match the number of fields in\\n    the data-type.\\ncomments : str or sequence of str or None, optional\\n    The characters or list of characters used to indicate the start of a\\n    comment. None implies no comments. For backwards compatibility, byte\\n    strings will be decoded as \\'latin1\\'. The default is \\'#\\'.\\ndelimiter : str, optional\\n    The character used to separate the values. For backwards compatibility,\\n    byte strings will be decoded as \\'latin1\\'. The default is whitespace.\\n\\n    .. versionchanged:: 1.23.0\\n       Only single character delimiters are supported. Newline characters\\n       cannot be used as the delimiter.\\n\\nconverters : dict or callable, optional\\n    Converter functions to customize value parsing. If `converters` is\\n    callable, the function is applied to all columns, else it must be a\\n    dict that maps column number to a parser function.\\n    See examples for further details.\\n    Default: None.\\n\\n    .. versionchanged:: 1.23.0\\n       The ability to pass a single callable to be applied to all columns\\n       was added.\\n\\nskiprows : int, optional\\n    Skip the first `skiprows` lines, including comments; default: 0.\\nusecols : int or sequence, optional\\n    Which columns to read, with 0 being the first. For example,\\n    ``usecols = (1,4,5)`` will extract the 2nd, 5th and 6th columns.\\n    The default, None, results in all columns being read.\\nunpack : bool, optional\\n    If True, the returned array is transposed, so that arguments may be\\n    unpacked using ``x, y, z = loadtxt(...)``.  When used with a\\n    structured data-type, arrays are returned for each field.\\n    Default is False.\\nndmin : int, optional\\n    The returned array will have at least `ndmin` dimensions.\\n    Otherwise mono-dimensional axes will be squeezed.\\n    Legal values: 0 (default), 1 or 2.\\nencoding : str, optional\\n    Encoding used to decode the inputfile. Does not apply to input streams.\\n    The special value \\'bytes\\' enables backward compatibility workarounds\\n    that ensures you receive byte arrays as results if possible and passes\\n    \\'latin1\\' encoded strings to converters. Override this value to receive\\n    unicode arrays and pass strings as input to converters.  If set to None\\n    the system default is used. The default value is \\'bytes\\'.\\n\\n    .. versionchanged:: 2.0\\n        Before NumPy 2, the default was ``\\'bytes\\'`` for Python 2\\n        compatibility. The default is now ``None``.\\n\\nmax_rows : int, optional\\n    Read `max_rows` rows of content after `skiprows` lines. The default is\\n    to read all the rows. Note that empty rows containing no data such as\\n    empty lines and comment lines are not counted towards `max_rows`,\\n    while such lines are counted in `skiprows`.\\n\\n    .. versionchanged:: 1.23.0\\n        Lines containing no data, including comment lines (e.g., lines\\n        starting with \\'#\\' or as specified via `comments`) are not counted\\n        towards `max_rows`.\\nquotechar : unicode character or None, optional\\n    The character used to denote the start and end of a quoted item.\\n    Occurrences of the delimiter or comment characters are ignored within\\n    a quoted item. The default value is ``quotechar=None``, which means\\n    quoting support is disabled.\\n\\n    If two consecutive instances of `quotechar` are found within a quoted\\n    field, the first is treated as an escape character. See examples.\\n\\n    .. versionadded:: 1.23.0\\n${ARRAY_FUNCTION_LIKE}\\n\\n    .. versionadded:: 1.20.0\\n\\nReturns\\n-------\\nout : ndarray\\n    Data read from the text file.\\n\\nSee Also\\n--------\\nload, fromstring, fromregex\\ngenfromtxt : Load data with missing values handled as specified.\\nscipy.io.loadmat : reads MATLAB data files\\n\\nNotes\\n-----\\nThis function aims to be a fast reader for simply formatted files.  The\\n`genfromtxt` function provides more sophisticated handling of, e.g.,\\nlines with missing values.\\n\\nEach row in the input text file must have the same number of values to be\\nable to read all values. If all rows do not have same number of values, a\\nsubset of up to n columns (where n is the least number of values present\\nin all rows) can be read by specifying the columns via `usecols`.\\n\\nThe strings produced by the Python float.hex method can be used as\\ninput for floats.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from io import StringIO   # StringIO behaves like a file object\\n>>> c = StringIO(\"0 1\\\\n2 3\")\\n>>> np.loadtxt(c)\\narray([[0., 1.],\\n       [2., 3.]])\\n\\n>>> d = StringIO(\"M 21 72\\\\nF 35 58\")\\n>>> np.loadtxt(d, dtype={\\'names\\': (\\'gender\\', \\'age\\', \\'weight\\'),\\n...                      \\'formats\\': (\\'S1\\', \\'i4\\', \\'f4\\')})\\narray([(b\\'M\\', 21, 72.), (b\\'F\\', 35, 58.)],\\n      dtype=[(\\'gender\\', \\'S1\\'), (\\'age\\', \\'<i4\\'), (\\'weight\\', \\'<f4\\')])\\n\\n>>> c = StringIO(\"1,0,2\\\\n3,0,4\")\\n>>> x, y = np.loadtxt(c, delimiter=\\',\\', usecols=(0, 2), unpack=True)\\n>>> x\\narray([1., 3.])\\n>>> y\\narray([2., 4.])\\n\\nThe `converters` argument is used to specify functions to preprocess the\\ntext prior to parsing. `converters` can be a dictionary that maps\\npreprocessing functions to each column:\\n\\n>>> s = StringIO(\"1.618, 2.296\\\\n3.141, 4.669\\\\n\")\\n>>> conv = {\\n...     0: lambda x: np.floor(float(x)),  # conversion fn for column 0\\n...     1: lambda x: np.ceil(float(x)),  # conversion fn for column 1\\n... }\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([[1., 3.],\\n       [3., 5.]])\\n\\n`converters` can be a callable instead of a dictionary, in which case it\\nis applied to all columns:\\n\\n>>> s = StringIO(\"0xDE 0xAD\\\\n0xC0 0xDE\")\\n>>> import functools\\n>>> conv = functools.partial(int, base=16)\\n>>> np.loadtxt(s, converters=conv)\\narray([[222., 173.],\\n       [192., 222.]])\\n\\nThis example shows how `converters` can be used to convert a field\\nwith a trailing minus sign into a negative number.\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> def conv(fld):\\n...     return -float(fld[:-1]) if fld.endswith(\"-\") else float(fld)\\n...\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nUsing a callable as the converter can be particularly useful for handling\\nvalues with different formatting, e.g. floats with underscores:\\n\\n>>> s = StringIO(\"1 2.7 100_000\")\\n>>> np.loadtxt(s, converters=float)\\narray([1.e+00, 2.7e+00, 1.e+05])\\n\\nThis idea can be extended to automatically handle values specified in\\nmany different formats, such as hex values:\\n\\n>>> def conv(val):\\n...     try:\\n...         return float(val)\\n...     except ValueError:\\n...         return float.fromhex(val)\\n>>> s = StringIO(\"1, 2.5, 3_000, 0b4, 0x1.4000000000000p+2\")\\n>>> np.loadtxt(s, delimiter=\",\", converters=conv)\\narray([1.0e+00, 2.5e+00, 3.0e+03, 1.8e+02, 5.0e+00])\\n\\nOr a format where the ``-`` sign comes after the number:\\n\\n>>> s = StringIO(\"10.01 31.25-\\\\n19.22 64.31\\\\n17.57- 63.94\")\\n>>> conv = lambda x: -float(x[:-1]) if x.endswith(\"-\") else float(x)\\n>>> np.loadtxt(s, converters=conv)\\narray([[ 10.01, -31.25],\\n       [ 19.22,  64.31],\\n       [-17.57,  63.94]])\\n\\nSupport for quoted fields is enabled with the `quotechar` parameter.\\nComment and delimiter characters are ignored when they appear within a\\nquoted item delineated by `quotechar`:\\n\\n>>> s = StringIO(\\'\"alpha, #42\", 10.0\\\\n\"beta, #64\", 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=\",\", quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nQuoted fields can be separated by multiple whitespace characters:\\n\\n>>> s = StringIO(\\'\"alpha, #42\"       10.0\\\\n\"beta, #64\" 2.0\\\\n\\')\\n>>> dtype = np.dtype([(\"label\", \"U12\"), (\"value\", float)])\\n>>> np.loadtxt(s, dtype=dtype, delimiter=None, quotechar=\\'\"\\')\\narray([(\\'alpha, #42\\', 10.), (\\'beta, #64\\',  2.)],\\n      dtype=[(\\'label\\', \\'<U12\\'), (\\'value\\', \\'<f8\\')])\\n\\nTwo consecutive quote characters within a quoted field are treated as a\\nsingle escaped character:\\n\\n>>> s = StringIO(\\'\"Hello, my name is \"\"Monty\"\"!\"\\')\\n>>> np.loadtxt(s, dtype=\"U\", delimiter=\",\", quotechar=\\'\"\\')\\narray(\\'Hello, my name is \"Monty\"!\\', dtype=\\'<U26\\')\\n\\nRead subset of columns when all rows do not contain equal number of values:\\n\\n>>> d = StringIO(\"1 2\\\\n2 4\\\\n3 9 12\\\\n4 16 20\")\\n>>> np.loadtxt(d, usecols=(0, 1))\\narray([[ 1.,  2.],\\n       [ 2.,  4.],\\n       [ 3.,  9.],\\n       [ 4., 16.]])',\n",
       " 'summarize to docstring: Re-pack the fields of a structured array or dtype in memory.\\n\\nThe memory layout of structured datatypes allows fields at arbitrary\\nbyte offsets. This means the fields can be separated by padding bytes,\\ntheir offsets can be non-monotonically increasing, and they can overlap.\\n\\nThis method removes any overlaps and reorders the fields in memory so they\\nhave increasing byte offsets, and adds or removes padding bytes depending\\non the `align` option, which behaves like the `align` option to\\n`numpy.dtype`.\\n\\nIf `align=False`, this method produces a \"packed\" memory layout in which\\neach field starts at the byte the previous field ended, and any padding\\nbytes are removed.\\n\\nIf `align=True`, this methods produces an \"aligned\" memory layout in which\\neach field\\'s offset is a multiple of its alignment, and the total itemsize\\nis a multiple of the largest alignment, by adding padding bytes as needed.\\n\\nParameters\\n----------\\na : ndarray or dtype\\n   array or dtype for which to repack the fields.\\nalign : boolean\\n   If true, use an \"aligned\" memory layout, otherwise use a \"packed\" layout.\\nrecurse : boolean\\n   If True, also repack nested structures.\\n\\nReturns\\n-------\\nrepacked : ndarray or dtype\\n   Copy of `a` with fields repacked, or `a` itself if no repacking was\\n   needed.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n\\n>>> from numpy.lib import recfunctions as rfn\\n>>> def print_offsets(d):\\n...     print(\"offsets:\", [d.fields[name][1] for name in d.names])\\n...     print(\"itemsize:\", d.itemsize)\\n...\\n>>> dt = np.dtype(\\'u1, <i8, <f8\\', align=True)\\n>>> dt\\ndtype({\\'names\\': [\\'f0\\', \\'f1\\', \\'f2\\'], \\'formats\\': [\\'u1\\', \\'<i8\\', \\'<f8\\'], \\'offsets\\': [0, 8, 16], \\'itemsize\\': 24}, align=True)\\n>>> print_offsets(dt)\\noffsets: [0, 8, 16]\\nitemsize: 24\\n>>> packed_dt = rfn.repack_fields(dt)\\n>>> packed_dt\\ndtype([(\\'f0\\', \\'u1\\'), (\\'f1\\', \\'<i8\\'), (\\'f2\\', \\'<f8\\')])\\n>>> print_offsets(packed_dt)\\noffsets: [0, 1, 9]\\nitemsize: 17',\n",
       " 'summarize to docstring: Test that bin width for integer data is at least 1.',\n",
       " 'summarize to docstring: Returns the data as a recarray.',\n",
       " 'summarize to docstring: Invalidate an authorization code after use.\\n\\n:param client_id: Unicode client identifier.\\n:param code: The authorization code grant (request.code).\\n:param request: OAuthlib request.\\n:type request: oauthlib.common.Request\\n\\nMethod is used by:\\n    - Authorization Code Grant',\n",
       " 'summarize to docstring: Write an object to file specified by a pathlib.Path and read it back\\n\\nParameters\\n----------\\nwriter : callable bound to pandas object\\n    IO writing function (e.g. DataFrame.to_csv )\\nreader : callable\\n    IO reading function (e.g. pd.read_csv )\\npath : str, default None\\n    The path where the object is written and then read.\\n\\nReturns\\n-------\\npandas object\\n    The original object that was serialized and then re-read.',\n",
       " 'summarize to docstring: For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results\\nin PeriodArray.  We cannot use ._ndarray directly for the affected\\nmethods because the i8 data has different semantics on NaT values.',\n",
       " 'summarize to docstring: :calls: `POST /gists <http://docs.github.com/en/rest/reference/gists>`_',\n",
       " \"summarize to docstring: `argslist_list` is a list that can contain an argslist as a first item, but\\nmost not. It's basically the items between the parameter brackets (which is\\nat most one item).\\nThis function modifies the parser structure. It generates `Param` objects\\nfrom the normal ast. Those param objects do not exist in a normal ast, but\\nmake the evaluation of the ast tree so much easier.\\nYou could also say that this function replaces the argslist node with a\\nlist of Param objects.\",\n",
       " 'summarize to docstring: Erases from the current cursor position to the end of the current\\nline.',\n",
       " 'summarize to docstring: Apply reduction rules to `word` excluding the reduction rule\\nfor the lhs equal to `exclude`',\n",
       " 'summarize to docstring: Initialize a SpecifierSet instance.\\n\\n:param specifiers:\\n    The string representation of a specifier or a comma-separated list of\\n    specifiers which will be parsed and normalized before use.\\n:param prereleases:\\n    This tells the SpecifierSet if it should accept prerelease versions if\\n    applicable or not. The default of ``None`` will autodetect it from the\\n    given specifiers.\\n\\n:raises InvalidSpecifier:\\n    If the given ``specifiers`` are not parseable than this exception will be\\n    raised.',\n",
       " 'summarize to docstring: Register `provider_factory` to make providers for `loader_type`\\n\\n`loader_type` is the type or class of a PEP 302 ``module.__loader__``,\\nand `provider_factory` is a function that, passed a *module* object,\\nreturns an ``IResourceProvider`` for that module.',\n",
       " 'summarize to docstring: :return: runtime directory tied to the user, e.g. ``~/Library/Caches/TemporaryItems/$appname/$version``',\n",
       " \"summarize to docstring: Connect to *address* and return the socket object.\\n\\nConvenience function.  Connect to *address* (a 2-tuple ``(host,\\nport)``) and return the socket object.  Passing the optional\\n*timeout* parameter will set the timeout on the socket instance\\nbefore attempting to connect.  If no *timeout* is supplied, the\\nglobal default timeout setting returned by :func:`socket.getdefaulttimeout`\\nis used.  If *source_address* is set it must be a tuple of (host, port)\\nfor the socket to bind as a source address before making the connection.\\nAn host of '' or port 0 tells the OS to use the default.\",\n",
       " 'summarize to docstring: return a Traceback instance wrapping part of this Traceback\\n\\nby provding any combination of path, lineno and firstlineno, the\\nfirst frame to start the to-be-returned traceback is determined\\n\\nthis allows cutting the first part of a Traceback instance e.g.\\nfor formatting reasons (removing some uninteresting bits that deal\\nwith handling of the exception/traceback)',\n",
       " 'summarize to docstring: A wrapper around typing.List that adds validation.\\n\\nArgs:\\n    item_type: The type of the items in the list.\\n    min_length: The minimum length of the list. Defaults to None.\\n    max_length: The maximum length of the list. Defaults to None.\\n    unique_items: Whether the items in the list must be unique. Defaults to None.\\n        !!! warning Deprecated\\n            The `unique_items` parameter is deprecated, use `Set` instead.\\n            See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.\\n\\nReturns:\\n    The wrapped list type.',\n",
       " 'summarize to docstring: Check if nodes.Name corresponds to first attribute variable name.\\n\\nName is `self` for method, `cls` for classmethod and `mcs` for metaclass.\\nStatic methods return False.',\n",
       " 'summarize to docstring: Format and print messages in the context of the path.',\n",
       " 'summarize to docstring: Return triangle having side of length l1 on the x-axis.',\n",
       " \"summarize to docstring: Checks that base_var is not seen as defined outsite '__init__'\\n        \",\n",
       " 'summarize to docstring: https://github.com/pylint-dev/pylint/issues/7131',\n",
       " 'summarize to docstring: The path from which pytest was invoked.\\n\\n.. versionadded:: 7.0.0',\n",
       " 'summarize to docstring: Ensure we can collect files with weird file extensions as Python\\nmodules (#2369)',\n",
       " 'summarize to docstring: Union[google.cloud.bigquery.ExternalConfig, None]: Configuration for\\nan external data source (defaults to :data:`None`).\\n\\nRaises:\\n    ValueError: For invalid value types.',\n",
       " \"summarize to docstring: Return items as ``(key, value)`` pairs.\\n\\nReturns:\\n    Iterable[Tuple[str, object]]:\\n        The ``(key, value)`` pairs representing this row.\\n\\nExamples:\\n\\n    >>> list(Row(('a', 'b'), {'x': 0, 'y': 1}).items())\\n    [('x', 'a'), ('y', 'b')]\",\n",
       " \"summarize to docstring: Coerce 'value' to a datetime, if set or not nullable.\\n\\nArgs:\\n    value (str): The timestamp.\\n\\n    field (google.cloud.bigquery.schema.SchemaField):\\n        The field corresponding to the value.\\n\\nReturns:\\n    Optional[datetime.datetime]:\\n        The parsed datetime object from\\n        ``value`` if the ``field`` is not null (otherwise it is\\n        :data:`None`).\",\n",
       " 'summarize to docstring: Return the estimated number of bytes processed by the query.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobStatistics2.FIELDS.estimated_bytes_processed\\n\\nReturns:\\n    Optional[int]:\\n        number of DML rows affected by the job, or None if job is not\\n        yet complete.',\n",
       " 'summarize to docstring: Move an existing element to the end (or beginning if last==False).\\n\\nRaises KeyError if the element does not exist.\\nWhen last=True, acts like a fast version of self[key]=self.pop(key).',\n",
       " 'summarize to docstring: Call finish_request.\\n\\nOverridden by ForkingMixIn and ThreadingMixIn.',\n",
       " 'summarize to docstring: Note the case normalization of header names here, to\\n.capitalize()-case.  This should be preserved for\\nbackwards-compatibility.  (In the HTTP case, normalization to\\n.title()-case is done by urllib2 before sending headers to\\nhttp.client).\\n\\nNote that e.g. r.has_header(\"spam-EggS\") is currently False, and\\nr.get_header(\"spam-EggS\") returns None, but that could be changed in\\nfuture.\\n\\nMethod r.remove_header should remove items both from r.headers and\\nr.unredirected_hdrs dictionaries',\n",
       " 'summarize to docstring: Prepending the signature with zeroes should be detected.',\n",
       " 'summarize to docstring: Input: a list of UserDicts.',\n",
       " 'summarize to docstring: Output: Using maxcolwidth in conjunction with disable_parsenum is honored',\n",
       " 'summarize to docstring: Sets the service of this AdmissionregistrationV1WebhookClientConfig.\\n\\n\\n:param service: The service of this AdmissionregistrationV1WebhookClientConfig.  # noqa: E501\\n:type: AdmissionregistrationV1ServiceReference',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Gets the azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n\\n\\n:return: The azure_disk of this V1PersistentVolumeSpec.  # noqa: E501\\n:rtype: V1AzureDiskVolumeSource',\n",
       " 'summarize to docstring: Gets the number of this V1ServiceBackendPort.  # noqa: E501\\n\\nnumber is the numerical port number (e.g. 80) on the Service. This is a mutually exclusive setting with \"Name\".  # noqa: E501\\n\\n:return: The number of this V1ServiceBackendPort.  # noqa: E501\\n:rtype: int',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Compute and set the OOB score and attributes.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The data matrix.\\ny : ndarray of shape (n_samples, n_outputs)\\n    The target matrix.\\nscoring_function : callable, default=None\\n    Scoring function for OOB score. Default depends on whether\\n    this is a regression (R2 score) or classification problem\\n    (accuracy score).',\n",
       " 'summarize to docstring: This provides dict.get method functionality with type checking',\n",
       " \"summarize to docstring: Ellipsoidal harmonic functions F^p_n(l)\\n\\nThese are also known as Lame functions of the second kind, and are\\nsolutions to the Lame equation:\\n\\n.. math:: (s^2 - h^2)(s^2 - k^2)F''(s)\\n          + s(2s^2 - h^2 - k^2)F'(s) + (a - q s^2)F(s) = 0\\n\\nwhere :math:`q = (n+1)n` and :math:`a` is the eigenvalue (not\\nreturned) corresponding to the solutions.\\n\\nParameters\\n----------\\nh2 : float\\n    ``h**2``\\nk2 : float\\n    ``k**2``; should be larger than ``h**2``\\nn : int\\n    Degree.\\np : int\\n    Order, can range between [1,2n+1].\\ns : float\\n    Coordinate\\n\\nReturns\\n-------\\nF : float\\n    The harmonic :math:`F^p_n(s)`\\n\\nSee Also\\n--------\\nellip_harm, ellip_normal\\n\\nNotes\\n-----\\nLame functions of the second kind are related to the functions of the first kind:\\n\\n.. math::\\n\\n   F^p_n(s)=(2n + 1)E^p_n(s)\\\\int_{0}^{1/s}\\n   \\\\frac{du}{(E^p_n(1/u))^2\\\\sqrt{(1-u^2k^2)(1-u^2h^2)}}\\n\\n.. versionadded:: 0.15.0\\n\\nExamples\\n--------\\n>>> from scipy.special import ellip_harm_2\\n>>> w = ellip_harm_2(5,8,2,1,10)\\n>>> w\\n0.00108056853382\",\n",
       " 'summarize to docstring: Results above from SAS PROC NPAR1WAY, e.g.\\n\\nDATA myData;\\nINPUT X Y;\\nCARDS;\\n1 1\\n1 2\\n1 3\\n1 4\\n2 1.5\\n2 2\\n2 2.5\\nods graphics on;\\nproc npar1way AB data=myData;\\n    class X;\\n    EXACT;\\nrun;\\nods graphics off;\\n\\nNote: SAS provides Pr >= |S-Mean|, which is different from our\\ndefinition of a two-sided p-value.',\n",
       " 'summarize to docstring: Match default.',\n",
       " 'summarize to docstring: See the cancel function in sympy.polys',\n",
       " 'summarize to docstring: Format the argument signature of *self.object*.\\n\\nShould return None if the object does not have a signature.',\n",
       " 'summarize to docstring: Outputs the table of ``op``.',\n",
       " 'summarize to docstring: extension version of the same test in test_mapper.\\n\\nfixes #3408',\n",
       " \"summarize to docstring: Return a bool indicating whether the error between z1 and z2\\nis $\\\\le$ ``tol``.\\n\\nExamples\\n========\\n\\nIf ``tol`` is ``None`` then ``True`` will be returned if\\n:math:`|z1 - z2|\\\\times 10^p \\\\le 5` where $p$ is minimum value of the\\ndecimal precision of each value.\\n\\n>>> from sympy import comp, pi\\n>>> pi4 = pi.n(4); pi4\\n3.142\\n>>> comp(_, 3.142)\\nTrue\\n>>> comp(pi4, 3.141)\\nFalse\\n>>> comp(pi4, 3.143)\\nFalse\\n\\nA comparison of strings will be made\\nif ``z1`` is a Number and ``z2`` is a string or ``tol`` is ''.\\n\\n>>> comp(pi4, 3.1415)\\nTrue\\n>>> comp(pi4, 3.1415, '')\\nFalse\\n\\nWhen ``tol`` is provided and $z2$ is non-zero and\\n:math:`|z1| > 1` the error is normalized by :math:`|z1|`:\\n\\n>>> abs(pi4 - 3.14)/pi4\\n0.000509791731426756\\n>>> comp(pi4, 3.14, .001)  # difference less than 0.1%\\nTrue\\n>>> comp(pi4, 3.14, .0005)  # difference less than 0.1%\\nFalse\\n\\nWhen :math:`|z1| \\\\le 1` the absolute error is used:\\n\\n>>> 1/pi4\\n0.3183\\n>>> abs(1/pi4 - 0.3183)/(1/pi4)\\n3.07371499106316e-5\\n>>> abs(1/pi4 - 0.3183)\\n9.78393554684764e-6\\n>>> comp(1/pi4, 0.3183, 1e-5)\\nTrue\\n\\nTo see if the absolute error between ``z1`` and ``z2`` is less\\nthan or equal to ``tol``, call this as ``comp(z1 - z2, 0, tol)``\\nor ``comp(z1 - z2, tol=tol)``:\\n\\n>>> abs(pi4 - 3.14)\\n0.00160156249999988\\n>>> comp(pi4 - 3.14, 0, .002)\\nTrue\\n>>> comp(pi4 - 3.14, 0, .001)\\nFalse\",\n",
       " 'summarize to docstring: Converts a ``Feedback`` object to SymPy Expr.\\n\\nExamples\\n========\\n\\n>>> from sympy.abc import s, a, b\\n>>> from sympy.physics.control.lti import TransferFunction, Feedback\\n>>> from sympy import Expr\\n>>> tf1 = TransferFunction(a+s, 1, s)\\n>>> tf2 = TransferFunction(b+s, 1, s)\\n>>> fd1 = Feedback(tf1, tf2)\\n>>> fd1.to_expr()\\n(a + s)/((a + s)*(b + s) + 1)\\n>>> isinstance(_, Expr)\\nTrue',\n",
       " 'summarize to docstring: Returns the number of inputs of the system.',\n",
       " 'summarize to docstring: Returns state space model where numerical expressions are evaluated into floating point numbers.',\n",
       " 'summarize to docstring: Convert a mpmath ``mpf`` object to ``dtype``. ',\n",
       " 'summarize to docstring: This supports the functions that compute Hermite Normal Form.\\n\\nExplanation\\n===========\\n\\nLet x, y be the coefficients returned by the extended Euclidean\\nAlgorithm, so that x*a + y*b = g. In the algorithms for computing HNF,\\nit is critical that x, y not only satisfy the condition of being small\\nin magnitude -- namely that |x| <= |b|/g, |y| <- |a|/g -- but also that\\ny == 0 when a | b.',\n",
       " \"summarize to docstring: Reduce a system of inequalities with nested absolute values.\\n\\nExamples\\n========\\n\\n>>> from sympy import reduce_abs_inequalities, Abs, Symbol\\n>>> x = Symbol('x', extended_real=True)\\n\\n>>> reduce_abs_inequalities([(Abs(3*x - 5) - 7, '<'),\\n... (Abs(x + 25) - 13, '>')], x)\\n(-2/3 < x) & (x < 4) & (((-oo < x) & (x < -38)) | ((-12 < x) & (x < oo)))\\n\\n>>> reduce_abs_inequalities([(Abs(x - 4) + Abs(3*x - 5) - 7, '<')], x)\\n(1/2 < x) & (x < 4)\\n\\nSee Also\\n========\\n\\nreduce_abs_inequality\",\n",
       " 'summarize to docstring: The usage is simple: you just pass the match method the current\\npath info as well as the method (which defaults to `GET`).  The\\nfollowing things can then happen:\\n\\n- you receive a `NotFound` exception that indicates that no URL is\\n  matching.  A `NotFound` exception is also a WSGI application you\\n  can call to get a default page not found page (happens to be the\\n  same object as `werkzeug.exceptions.NotFound`)\\n\\n- you receive a `MethodNotAllowed` exception that indicates that there\\n  is a match for this URL but not for the current request method.\\n  This is useful for RESTful applications.\\n\\n- you receive a `RequestRedirect` exception with a `new_url`\\n  attribute.  This exception is used to notify you about a request\\n  Werkzeug requests from your WSGI application.  This is for example the\\n  case if you request ``/foo`` although the correct URL is ``/foo/``\\n  You can use the `RequestRedirect` instance as response-like object\\n  similar to all other subclasses of `HTTPException`.\\n\\n- you receive a ``WebsocketMismatch`` exception if the only\\n  match is a WebSocket rule but the bind is an HTTP request, or\\n  if the match is an HTTP rule but the bind is a WebSocket\\n  request.\\n\\n- you get a tuple in the form ``(endpoint, arguments)`` if there is\\n  a match (unless `return_rule` is True, in which case you get a tuple\\n  in the form ``(rule, arguments)``)\\n\\nIf the path info is not passed to the match method the default path\\ninfo of the map is used (defaults to the root URL if not defined\\nexplicitly).\\n\\nAll of the exceptions raised are subclasses of `HTTPException` so they\\ncan be used as WSGI responses. They will all render generic error or\\nredirect pages.\\n\\nHere is a small example for matching:\\n\\n>>> m = Map([\\n...     Rule(\\'/\\', endpoint=\\'index\\'),\\n...     Rule(\\'/downloads/\\', endpoint=\\'downloads/index\\'),\\n...     Rule(\\'/downloads/<int:id>\\', endpoint=\\'downloads/show\\')\\n... ])\\n>>> urls = m.bind(\"example.com\", \"/\")\\n>>> urls.match(\"/\", \"GET\")\\n(\\'index\\', {})\\n>>> urls.match(\"/downloads/42\")\\n(\\'downloads/show\\', {\\'id\\': 42})\\n\\nAnd here is what happens on redirect and missing URLs:\\n\\n>>> urls.match(\"/downloads\")\\nTraceback (most recent call last):\\n  ...\\nRequestRedirect: http://example.com/downloads/\\n>>> urls.match(\"/missing\")\\nTraceback (most recent call last):\\n  ...\\nNotFound: 404 Not Found\\n\\n:param path_info: the path info to use for matching.  Overrides the\\n                  path info specified on binding.\\n:param method: the HTTP method used for matching.  Overrides the\\n               method specified on binding.\\n:param return_rule: return the rule that matched instead of just the\\n                    endpoint (defaults to `False`).\\n:param query_args: optional query arguments that are used for\\n                   automatic redirects as string or dictionary.  It\\'s\\n                   currently not possible to use the query arguments\\n                   for URL matching.\\n:param websocket: Match WebSocket instead of HTTP requests. A\\n    websocket request has a ``ws`` or ``wss``\\n    :attr:`url_scheme`. This overrides that detection.\\n\\n.. versionadded:: 1.0\\n    Added ``websocket``.\\n\\n.. versionchanged:: 0.8\\n    ``query_args`` can be a string.\\n\\n.. versionadded:: 0.7\\n    Added ``query_args``.\\n\\n.. versionadded:: 0.6\\n    Added ``return_rule``.',\n",
       " 'summarize to docstring: Sets vertical border characters.\\n\\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\\nâ•‘ ISBN          â”‚ Title                    â”‚ Author           â•‘\\nâ• â•â•â•â•â•â•â•1â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\\nâ•‘ 99921-58-10-7 â”‚ Divine Comedy            â”‚ Dante Alighieri  â•‘\\nâ•‘ 9971-5-0210-0 â”‚ A Tale of Two Cities     â”‚ Charles Dickens  â•‘\\nâ•Ÿâ”€â”€â”€â”€â”€â”€â”€2â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢\\nâ•‘ 960-425-059-0 â”‚ The Lord of the Rings    â”‚ J. R. R. Tolkien â•‘\\nâ•‘ 80-902734-1-6 â”‚ And Then There Were None â”‚ Agatha Christie  â•‘\\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•',\n",
       " 'summarize to docstring: Initialize the metric object as a child, i.e. when it has labels (if any) set.\\n\\nThis is factored as a separate function to allow for deferred initialization.',\n",
       " 'summarize to docstring: Concatenate a list of points or None into a single point array or None, with NaNs used to\\nseparate each line.',\n",
       " 'summarize to docstring: How much RAM is this process using? (Windows)',\n",
       " \"summarize to docstring: Returns the source and destination addresses of the last taken branch.\\n\\n@rtype: tuple( int, int )\\n@return: Source and destination addresses of the last taken branch.\\n\\n@raise WindowsError:\\n    Raises an exception on error.\\n\\n@raise NotImplementedError:\\n    Current architecture is not C{i386} or C{amd64}.\\n\\n@warning:\\n    This method uses the processor's machine specific registers (MSR).\\n    It could potentially brick your machine.\\n    It works on my machine, but your mileage may vary.\\n\\n@note:\\n    It doesn't seem to work in VMWare or VirtualBox machines.\\n    Maybe it fails in other virtualization/emulation environments,\\n    no extensive testing was made so far.\",\n",
       " 'summarize to docstring: Disassemble instructions from the address space of the process.\\n\\n@type  lpAddress: int\\n@param lpAddress: Memory address where to read the code from.\\n\\n@type  dwSize: int\\n@param dwSize: Size of binary code to disassemble.\\n\\n@rtype:  list of tuple( long, int, str, str )\\n@return: List of tuples. Each tuple represents an assembly instruction\\n    and contains:\\n     - Memory address of instruction.\\n     - Size of instruction in bytes.\\n     - Disassembly line of instruction.\\n     - Hexadecimal dump of instruction.',\n",
       " 'summarize to docstring: Return a tuple of arguments that must be passed to __new__ in order to support pickling this object.',\n",
       " 'summarize to docstring: Extract the URL prefix from a regex match.\\n\\nArgs:\\n  mat: A regex match object.\\nReturns: The URL prefix, defined as the text before the match in the\\n    original string. Normalized to start with one leading slash and end\\n    with zero.',\n",
       " 'summarize to docstring: Decorate a WebSocket function.\\n\\nRead more about it in the\\n[FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\\n\\n**Example**\\n\\n```python\\nfrom fastapi import FastAPI, WebSocket\\n\\napp = FastAPI()\\n\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n    await websocket.accept()\\n    while True:\\n        data = await websocket.receive_text()\\n        await websocket.send_text(f\"Message text was: {data}\")\\n```',\n",
       " 'summarize to docstring: Check that /items/{item_id} returns expected data',\n",
       " 'summarize to docstring: Post-rpc interceptor for list_api_operations\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ApiHub server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: If the given type is `typing.Iterable[T]`',\n",
       " 'summarize to docstring: Call the update backup method over HTTP.\\n\\nArgs:\\n    request (~.backupvault.UpdateBackupRequest):\\n        The request object. Request message for updating a\\n    Backup.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Return a single Task.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import batch_v1\\n\\n    def sample_get_task():\\n        # Create a client\\n        client = batch_v1.BatchServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = batch_v1.GetTaskRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_task(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.batch_v1.types.GetTaskRequest, dict]):\\n        The request object. Request for a single Task by name.\\n    name (str):\\n        Required. Task name.\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.batch_v1.types.Task:\\n        A Cloud Batch task.',\n",
       " 'summarize to docstring: Sets the default network tier of the project. The\\ndefault network tier is used when an\\naddress/forwardingRule/instance is created without\\nspecifying the network tier field.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_default_network_tier():\\n        # Create a client\\n        client = compute_v1.ProjectsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetDefaultNetworkTierProjectRequest(\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_default_network_tier(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetDefaultNetworkTierProjectRequest, dict]):\\n        The request object. A request message for\\n        Projects.SetDefaultNetworkTier. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    projects_set_default_network_tier_request_resource (google.cloud.compute_v1.types.ProjectsSetDefaultNetworkTierRequest):\\n        The body resource for this request\\n        This corresponds to the ``projects_set_default_network_tier_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'summarize to docstring: Post-rpc interceptor for get_participant\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Participants server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Call the get method over HTTP.\\n\\nArgs:\\n    request (~.compute.GetPublicDelegatedPrefixeRequest):\\n        The request object. A request message for\\n    PublicDelegatedPrefixes.Get. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.PublicDelegatedPrefix:\\n        A PublicDelegatedPrefix resource\\n    represents an IP block within a\\n    PublicAdvertisedPrefix that is\\n    configured within a single cloud scope\\n    (global or region). IPs in the block can\\n    be allocated to resources within that\\n    scope. Public delegated prefixes may be\\n    further broken up into smaller IP blocks\\n    in the same scope as the parent block.',\n",
       " 'summarize to docstring: Post-rpc interceptor for compute_repository_access_token_status\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Dataform server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsRequest):\\n        The initial request object.\\n    response (google.cloud.dataform_v1beta1.types.ListWorkflowConfigsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for alter_metadata_resource_location\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the DataprocMetastore server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for submit_answer_feedback\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Sessions server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Call the create discovery config method over HTTP.\\n\\nArgs:\\n    request (~.dlp.CreateDiscoveryConfigRequest):\\n        The request object. Request message for\\n    CreateDiscoveryConfig.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.dlp.DiscoveryConfig:\\n        Configuration for discovery to scan resources for\\n    profile generation. Only one discovery configuration may\\n    exist per organization, folder, or project.\\n\\n    The generated data profiles are retained according to\\n    the [data retention policy]\\n    (https://cloud.google.com/sensitive-data-protection/docs/data-profiles#retention).',\n",
       " 'summarize to docstring: Set the session to be used when the TLS/SSL connection is established.\\n\\n:param session: A Session instance representing the session to use.\\n:returns: None\\n\\n.. versionadded:: 0.14',\n",
       " 'summarize to docstring: Call the list processor types method over HTTP.\\n\\nArgs:\\n    request (~.document_processor_service.ListProcessorTypesRequest):\\n        The request object. Request message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method. Some processor types may require the project be\\n    added to an allowlist.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.document_processor_service.ListProcessorTypesResponse:\\n        Response message for the\\n    [ListProcessorTypes][google.cloud.documentai.v1.DocumentProcessorService.ListProcessorTypes]\\n    method.',\n",
       " 'summarize to docstring: Return a callable for the delete http route method over gRPC.\\n\\nDeletes a single HttpRoute.\\n\\nReturns:\\n    Callable[[~.DeleteHttpRouteRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Parses a collector path into its component segments.',\n",
       " 'summarize to docstring: Returns the specified key.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import recaptchaenterprise_v1\\n\\n    def sample_get_key():\\n        # Create a client\\n        client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = recaptchaenterprise_v1.GetKeyRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_key(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.recaptchaenterprise_v1.types.GetKeyRequest, dict]):\\n        The request object. The get key request message.\\n    name (str):\\n        Required. The name of the requested key, in the format\\n        ``projects/{project}/keys/{key}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.recaptchaenterprise_v1.types.Key:\\n        A key used to identify and configure\\n        applications (web and/or mobile) that\\n        use reCAPTCHA Enterprise.',\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'summarize to docstring: Returns a fully-qualified mute_config string.',\n",
       " 'summarize to docstring: Post-rpc interceptor for list_effective_event_threat_detection_custom_modules\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SecurityCenter server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: `Connection.bio_read` raises `TypeError` if passed a non-integer\\nargument.',\n",
       " 'summarize to docstring: Lists all AdaptiveMtFiles associated to an\\nAdaptiveMtDataset.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import translate_v3\\n\\n    def sample_list_adaptive_mt_files():\\n        # Create a client\\n        client = translate_v3.TranslationServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = translate_v3.ListAdaptiveMtFilesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_adaptive_mt_files(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.translate_v3.types.ListAdaptiveMtFilesRequest, dict]):\\n        The request object. The request to list all AdaptiveMt\\n        files under a given dataset.\\n    parent (str):\\n        Required. The resource name of the project from which to\\n        list the Adaptive MT files.\\n        ``projects/{project}/locations/{location}/adaptiveMtDatasets/{dataset}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.translate_v3.services.translation_service.pagers.ListAdaptiveMtFilesPager:\\n        The response for listing all\\n        AdaptiveMt files under a given dataset.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'summarize to docstring: Call the delete application method over HTTP.\\n\\nArgs:\\n    request (~.platform.DeleteApplicationRequest):\\n        The request object. Message for deleting an Application.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'summarize to docstring: Return a callable for the create private connection method over gRPC.\\n\\nCreates a new private connection that can be used for\\naccessing private Clouds.\\n\\nReturns:\\n    Callable[[~.CreatePrivateConnectionRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"summarize to docstring: Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'summarize to docstring: Convert a av.VideoFrame into a ndarray\\n\\nParameters\\n----------\\nframe : av.VideoFrame\\n    The frame to unpack.\\nformat : str\\n    If not None, convert the frame to the given format before unpacking.',\n",
       " 'summarize to docstring: Loads the encoded object. This function raises\\n:class:`.BadPayload` if the payload is not valid. The\\n``serializer`` parameter can be used to override the serializer\\nstored on the class. The encoded ``payload`` should always be\\nbytes.',\n",
       " 'summarize to docstring: Call the callable with the arguments and keyword arguments\\nprovided but inject the active context or environment as first\\nargument if the callable has :func:`pass_context` or\\n:func:`pass_environment`.',\n",
       " 'summarize to docstring: Format a date the way Atom likes it (RFC3339?)',\n",
       " 'summarize to docstring: Removes PEM-encoding from a public key, private key or certificate. If the\\nprivate key is encrypted, the password will be used to decrypt it.\\n\\n:param data:\\n    A byte string of the PEM-encoded data\\n\\n:param password:\\n    A byte string of the encryption password, or None\\n\\n:return:\\n    A 3-element tuple in the format: (key_type, algorithm, der_bytes). The\\n    key_type will be a unicode string of \"public key\", \"private key\" or\\n    \"certificate\". The algorithm will be a unicode string of \"rsa\", \"dsa\"\\n    or \"ec\".',\n",
       " 'summarize to docstring: Calculate any free parameters based on the current cmap and norm,\\nand do all the drawing.',\n",
       " 'summarize to docstring: Generate a new value for this ObjectId.',\n",
       " 'summarize to docstring: GH 43787\\n\\nTest correct handling of UTF-8 chars when memory_map=True and encoding is UTF-8',\n",
       " \"summarize to docstring: Returns a bipartite graph from two given degree sequences using a\\nHavel-Hakimi style construction.\\n\\nThe graph is composed of two partitions. Set A has nodes 0 to\\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\\nNodes from the set A are connected to nodes in the set B by\\nconnecting the highest degree nodes in set A to the highest degree\\nnodes in set B until all stubs are connected.\\n\\nParameters\\n----------\\naseq : list\\n   Degree sequence for node set A.\\nbseq : list\\n   Degree sequence for node set B.\\ncreate_using : NetworkX graph instance, optional\\n   Return graph of this type.\\n\\nNotes\\n-----\\nThe sum of the two sequences must be equal: sum(aseq)=sum(bseq)\\nIf no graph type is specified use MultiGraph with parallel edges.\\nIf you want a graph with no parallel edges use create_using=Graph()\\nbut then the resulting degree sequences might not be exact.\\n\\nThe nodes are assigned the attribute 'bipartite' with the value 0 or 1\\nto indicate which bipartite set the node belongs to.\\n\\nThis function is not imported in the main namespace.\\nTo use it use nx.bipartite.havel_hakimi_graph\",\n",
       " 'summarize to docstring: Tests for forbidding self-loops.',\n",
       " 'summarize to docstring: Return a list of -arch flags for every supported architecture.',\n",
       " 'summarize to docstring: Tests that it is possible to return to file processing mode\\nCLI :: :\\nBUG: numpy-gh #20520',\n",
       " 'summarize to docstring: Test that the API Counter is an abstract class.',\n",
       " 'summarize to docstring: The time the process has been running',\n",
       " 'summarize to docstring: TAG[.postDISTANCE[.dev0]+gHEX] .\\n\\nThe \".dev0\" means dirty. Note that .dev0 sorts backwards\\n(a dirty tree will appear \"older\" than the corresponding clean one),\\nbut you shouldn\\'t be releasing software with -dirty anyways.\\n\\nExceptions:\\n1: no tags. 0.postDISTANCE[.dev0]',\n",
       " 'summarize to docstring: Create a new DataFrame by selecting a subset of columns by index.',\n",
       " 'summarize to docstring: Highlight the minimum with a style.\\n\\nParameters\\n----------\\n%(subset)s\\n%(color)s\\naxis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n    Apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n    (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n    with ``axis=None``.\\n%(props)s\\n\\n    .. versionadded:: 1.3.0\\n\\nReturns\\n-------\\nStyler\\n    Instance of class where min value is highlighted in given style.\\n\\nSee Also\\n--------\\nStyler.highlight_null: Highlight missing values with a style.\\nStyler.highlight_max: Highlight the maximum with a style.\\nStyler.highlight_between: Highlight a defined range with a style.\\nStyler.highlight_quantile: Highlight values defined by a quantile with a style.\\n\\nExamples\\n--------\\n>>> df = pd.DataFrame({\"A\": [2, 1], \"B\": [3, 4]})\\n>>> df.style.highlight_min(color=\"yellow\")  # doctest: +SKIP\\n\\nPlease see:\\n`Table Visualization <../../user_guide/style.ipynb>`_ for more examples.',\n",
       " \"summarize to docstring: Convert css-string to sequence of tuples format if needed.\\n'color:red; border:1px solid black;' -> [('color', 'red'),\\n                                         ('border','1px solid red')]\",\n",
       " 'summarize to docstring: verify that select works when a channel is already closed.',\n",
       " 'summarize to docstring: Normalize the given rows of a RECORD file.\\n\\nItems in each row are converted into str. Rows are then sorted to make\\nthe value more predictable for tests.\\n\\nEach row is a 3-tuple (path, hash, size) and corresponds to a record of\\na RECORD file (see PEP 376 and PEP 427 for details).  For the rows\\npassed to this function, the size can be an integer as an int or string,\\nor the empty string.',\n",
       " 'summarize to docstring: Return our best guess of encoding for the given *term*.',\n",
       " \"summarize to docstring: Verifies that header parts don't contain leading whitespace\\nreserved characters, or return characters.\\n\\n:param header: tuple, in the format (name, value).\",\n",
       " 'summarize to docstring: Get the current row style.',\n",
       " 'summarize to docstring: Test that wrong hash in direct URL dependency stops installation.',\n",
       " 'summarize to docstring: Test that we fall back to setuptools develop when using a backend that\\ndoes not support build_editable. Since there is a pyproject.toml,\\nthe prepare_metadata_for_build_wheel hook is called.',\n",
       " 'summarize to docstring: Test the log message for an invalid Requires-Python.',\n",
       " 'summarize to docstring: Evaluate a polynomial at ``x_0 = a`` in ``K[X]`` using the Horner scheme.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x,y = ring(\"x,y\", ZZ)\\n\\n>>> R.dmp_eval(2*x*y + 3*x + y + 2, 2)\\n5*y + 8',\n",
       " 'summarize to docstring: The source type of system site packages\\nmust not be falsely identified as \"directory\".',\n",
       " 'summarize to docstring: commit with support for non-recursive commits ',\n",
       " 'summarize to docstring: Builds a function definition.\\n        ',\n",
       " 'summarize to docstring: struct_or_union : STRUCT\\n| UNION',\n",
       " 'summarize to docstring: Check if field name is in validator fields.\\n\\nArgs:\\n    info: The field info.\\n    field: The field name to check.\\n\\nReturns:\\n    `True` if field name is in validator fields, `False` otherwise.',\n",
       " 'summarize to docstring: Tests that a SymilarChecker can return and reduce mapped data.',\n",
       " 'summarize to docstring: list index is a str constant',\n",
       " 'summarize to docstring: Say it load',\n",
       " 'summarize to docstring: An assignment assumed to execute in one Try should continue to be\\nassumed to execute in a consecutive Try.',\n",
       " 'summarize to docstring: https://github.com/pylint-dev/pylint/issues/5965',\n",
       " \"summarize to docstring: Tests that an error in the initializer for the parallel jobs doesn't\\nlead to a deadlock.\",\n",
       " 'summarize to docstring: Import and return a module from the given path, which can be a file (a module) or\\na directory (a package).\\n\\n:param path:\\n    Path to the file to import.\\n\\n:param mode:\\n    Controls the underlying import mechanism that will be used:\\n\\n    * ImportMode.prepend: the directory containing the module (or package, taking\\n      `__init__.py` files into account) will be put at the *start* of `sys.path` before\\n      being imported with `importlib.import_module`.\\n\\n    * ImportMode.append: same as `prepend`, but the directory will be appended\\n      to the end of `sys.path`, if not already in `sys.path`.\\n\\n    * ImportMode.importlib: uses more fine control mechanisms provided by `importlib`\\n      to import the module, which avoids having to muck with `sys.path` at all. It effectively\\n      allows having same-named test modules in different places.\\n\\n:param root:\\n    Used as an anchor when mode == ImportMode.importlib to obtain\\n    a unique name for the module being imported so it can safely be stored\\n    into ``sys.modules``.\\n\\n:param consider_namespace_packages:\\n    If True, consider namespace packages when resolving module names.\\n\\n:raises ImportPathMismatchError:\\n    If after importing the given `path` and the module `__file__`\\n    are different. Only raised in `prepend` and `append` modes.',\n",
       " 'summarize to docstring: Shortcut for .makefile() with a .txt extension.\\n\\nDefaults to the test name with a \\'.txt\\' extension, e.g test_foobar.txt, overwriting\\nexisting files.\\n\\nExamples:\\n\\n.. code-block:: python\\n\\n    def test_something(pytester):\\n        # Initial file is created test_something.txt.\\n        pytester.maketxtfile(\"foobar\")\\n        # To create multiple files, pass kwargs accordingly.\\n        pytester.maketxtfile(custom=\"foobar\")\\n        # At this point, both \\'test_something.txt\\' & \\'custom.txt\\' exist in the test directory.',\n",
       " 'summarize to docstring: Check asynctest support (#7110)',\n",
       " 'summarize to docstring: As above, but explicitly feeding in a long on Py2. Note that\\nchecks like:\\n    isinstance(n, int)\\nare fragile on Py2, because isinstance(10L, int) is False.',\n",
       " \"summarize to docstring: It would nice if the b'' literal syntax could be coaxed into producing\\nbytes objects somehow ... ;)\",\n",
       " 'summarize to docstring: Test spec addition using :data:`+=` operator.',\n",
       " 'summarize to docstring: delete_validating_admission_policy  # noqa: E501\\n\\ndelete a ValidatingAdmissionPolicy  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_validating_admission_policy_with_http_info(name, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ValidatingAdmissionPolicy (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param V1DeleteOptions body:\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Status, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'summarize to docstring: get_api_versions  # noqa: E501\\n\\nget available API versions  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.get_api_versions_with_http_info(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1APIGroupList, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'summarize to docstring: Returns true if both objects are equal',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " 'summarize to docstring: Gets the git_repo of this V1Volume.  # noqa: E501\\n\\n\\n:return: The git_repo of this V1Volume.  # noqa: E501\\n:rtype: V1GitRepoVolumeSource',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " 'summarize to docstring: Re-subscribe to any channels and patterns previously subscribed to',\n",
       " 'summarize to docstring: When out of connections, block until another connection is released\\nto the pool',\n",
       " 'summarize to docstring: Appends an arbitrary number of strings to use as path constants',\n",
       " 'summarize to docstring: Search recommendation based on recommendation id',\n",
       " 'summarize to docstring: Get a metric group.\\n\\nArgs:\\n    group_name (str): The metric group name.',\n",
       " 'summarize to docstring: Check that check_is_fitted passes for stateless estimators.',\n",
       " 'summarize to docstring: Entry point for bootstrap script',\n",
       " 'summarize to docstring: Whitespace sensitive char-n-gram tokenization.\\n\\nTokenize text_document into a sequence of character n-grams\\noperating only inside word boundaries. n-grams at the edges\\nof words are padded with space.',\n",
       " 'summarize to docstring: Returns whether the kernel is stationary.',\n",
       " 'summarize to docstring: Is there a file at the given path',\n",
       " 'summarize to docstring: Objective function',\n",
       " 'summarize to docstring: Generate argument list for calling the C++ function',\n",
       " 'summarize to docstring: Merge multiple polygons into one.\\n\\nThis is an optimized version of union which assumes the polygons to be\\nnon-overlapping.\\n\\nParameters\\n----------\\na, b : Geometry or array_like\\n    Geometry or geometries to merge (union).\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nSee Also\\n--------\\ncoverage_union_all\\n\\nExamples\\n--------\\n>>> from shapely import normalize, Polygon\\n>>> polygon = Polygon([(0, 0), (0, 1), (1, 1), (1, 0), (0, 0)])\\n>>> normalize(coverage_union(polygon, Polygon([(1, 0), (1, 1), (2, 1), (2, 0), (1, 0)])))\\n<POLYGON ((0 0, 0 1, 1 1, 2 1, 2 0, 1 0, 0 0))>\\n\\nUnion with None returns same polygon\\n\\n>>> normalize(coverage_union(polygon, None))\\n<POLYGON ((0 0, 0 1, 1 1, 1 0, 0 0))>',\n",
       " 'summarize to docstring: Test when there is no root due to double root tags.',\n",
       " 'summarize to docstring: Return any actually-specified arguments.',\n",
       " 'summarize to docstring: Check if a file uses PySpark-specific errors correctly.\\n\\nParameters\\n----------\\nfile_path : str\\n    Path to the file to check.\\npyspark_error_list : list of str\\n    List of PySpark-specific error names.\\n\\nReturns\\n-------\\nlist of str\\n    A list of strings describing the errors found in the file, with line numbers.',\n",
       " 'summarize to docstring: Gets the value of numPartitions or its default value.',\n",
       " 'summarize to docstring: Saves metadata + Params to: path + \"/metadata\"\\n\\n- class\\n- timestamp\\n- sparkVersion\\n- uid\\n- paramMap\\n- defaultParamMap (since 2.4.0)\\n- (optionally, extra metadata)\\n\\nParameters\\n----------\\nextraMetadata : dict, optional\\n    Extra metadata to be saved at same level as uid, paramMap, etc.\\nparamMap : dict, optional\\n    If given, this is saved in the \"paramMap\" field.',\n",
       " 'summarize to docstring: Convert string styled html_css_files to tuple styled one.',\n",
       " 'summarize to docstring: this is func1',\n",
       " 'summarize to docstring: Remove the remote with the given name.\\n\\n:return:\\n    The passed remote name to remove',\n",
       " 'summarize to docstring: Add one or more :class:`_sql.CTE` constructs to this statement.\\n\\nThis method will associate the given :class:`_sql.CTE` constructs with\\nthe parent statement such that they will each be unconditionally\\nrendered in the WITH clause of the final statement, even if not\\nreferenced elsewhere within the statement or any sub-selects.\\n\\nThe optional :paramref:`.HasCTE.add_cte.nest_here` parameter when set\\nto True will have the effect that each given :class:`_sql.CTE` will\\nrender in a WITH clause rendered directly along with this statement,\\nrather than being moved to the top of the ultimate rendered statement,\\neven if this statement is rendered as a subquery within a larger\\nstatement.\\n\\nThis method has two general uses. One is to embed CTE statements that\\nserve some purpose without being referenced explicitly, such as the use\\ncase of embedding a DML statement such as an INSERT or UPDATE as a CTE\\ninline with a primary statement that may draw from its results\\nindirectly.  The other is to provide control over the exact placement\\nof a particular series of CTE constructs that should remain rendered\\ndirectly in terms of a particular statement that may be nested in a\\nlarger statement.\\n\\nE.g.::\\n\\n    from sqlalchemy import table, column, select\\n    t = table(\\'t\\', column(\\'c1\\'), column(\\'c2\\'))\\n\\n    ins = t.insert().values({\"c1\": \"x\", \"c2\": \"y\"}).cte()\\n\\n    stmt = select(t).add_cte(ins)\\n\\nWould render::\\n\\n    WITH anon_1 AS\\n    (INSERT INTO t (c1, c2) VALUES (:param_1, :param_2))\\n    SELECT t.c1, t.c2\\n    FROM t\\n\\nAbove, the \"anon_1\" CTE is not referenced in the SELECT\\nstatement, however still accomplishes the task of running an INSERT\\nstatement.\\n\\nSimilarly in a DML-related context, using the PostgreSQL\\n:class:`_postgresql.Insert` construct to generate an \"upsert\"::\\n\\n    from sqlalchemy import table, column\\n    from sqlalchemy.dialects.postgresql import insert\\n\\n    t = table(\"t\", column(\"c1\"), column(\"c2\"))\\n\\n    delete_statement_cte = (\\n        t.delete().where(t.c.c1 < 1).cte(\"deletions\")\\n    )\\n\\n    insert_stmt = insert(t).values({\"c1\": 1, \"c2\": 2})\\n    update_statement = insert_stmt.on_conflict_do_update(\\n        index_elements=[t.c.c1],\\n        set_={\\n            \"c1\": insert_stmt.excluded.c1,\\n            \"c2\": insert_stmt.excluded.c2,\\n        },\\n    ).add_cte(delete_statement_cte)\\n\\n    print(update_statement)\\n\\nThe above statement renders as::\\n\\n    WITH deletions AS\\n    (DELETE FROM t WHERE t.c1 < %(c1_1)s)\\n    INSERT INTO t (c1, c2) VALUES (%(c1)s, %(c2)s)\\n    ON CONFLICT (c1) DO UPDATE SET c1 = excluded.c1, c2 = excluded.c2\\n\\n.. versionadded:: 1.4.21\\n\\n:param \\\\*ctes: zero or more :class:`.CTE` constructs.\\n\\n .. versionchanged:: 2.0  Multiple CTE instances are accepted\\n\\n:param nest_here: if True, the given CTE or CTEs will be rendered\\n as though they specified the :paramref:`.HasCTE.cte.nesting` flag\\n to ``True`` when they were added to this :class:`.HasCTE`.\\n Assuming the given CTEs are not referenced in an outer-enclosing\\n statement as well, the CTEs given should render at the level of\\n this statement when this flag is given.\\n\\n .. versionadded:: 2.0\\n\\n .. seealso::\\n\\n    :paramref:`.HasCTE.cte.nesting`',\n",
       " 'summarize to docstring: test #8759.\\n\\nthis should raise an error.',\n",
       " 'summarize to docstring: same as precision_numerics_many_significant_digits but within the\\ncontext of a CAST statement (hello MySQL)',\n",
       " 'summarize to docstring: Handler for the absolute value.\\n\\nExamples\\n========\\n\\n>>> from sympy import Q, Abs\\n>>> from sympy.assumptions.refine import refine_abs\\n>>> from sympy.abc import x\\n>>> refine_abs(Abs(x), Q.real(x))\\n>>> refine_abs(Abs(x), Q.positive(x))\\nx\\n>>> refine_abs(Abs(x), Q.negative(x))\\n-x',\n",
       " 'summarize to docstring: return the non-strict version of the inequality or self\\n\\nEXAMPLES\\n========\\n\\n>>> from sympy.abc import x\\n>>> (x < 1).weak\\nx <= 1\\n>>> _.weak\\nx <= 1',\n",
       " 'summarize to docstring: Performs the Lenstraâ€“Lenstraâ€“LovÃ¡sz (LLL) basis reduction algorithm\\nand returns the reduced basis and transformation matrix.\\n\\nExplanation\\n===========\\n\\nParameters, algorithm and basis are the same as for :meth:`lll` except that\\nthe return value is a tuple `(B, T)` with `B` the reduced basis and\\n`T` a transformation matrix. The original basis `A` is transformed to\\n`B` with `T*A == B`. If only `B` is needed then :meth:`lll` should be\\nused as it is a little faster.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.domains import ZZ, QQ\\n>>> from sympy.polys.matrices import DM\\n>>> X = DM([[1, 0, 0, 0, -20160],\\n...         [0, 1, 0, 0, 33768],\\n...         [0, 0, 1, 0, 39578],\\n...         [0, 0, 0, 1, 47757]], ZZ)\\n>>> B, T = X.lll_transform(delta=QQ(5, 6))\\n>>> T * X == B\\nTrue\\n\\nSee also\\n========\\n\\nlll',\n",
       " 'summarize to docstring: Iterate over a mapping that might have a list of values, yielding\\nall key, value pairs. Almost like iter_multi_items but only allows\\nlists, not tuples, of values so tuples can be used for files.',\n",
       " 'summarize to docstring: Exhaust the stream by reading until the limit is reached or the client\\ndisconnects, returning the remaining data.\\n\\n.. versionchanged:: 2.3\\n    Return the remaining data.\\n\\n.. versionchanged:: 2.2.3\\n    Handle case where wrapped stream returns fewer bytes than requested.',\n",
       " \"summarize to docstring: An HTTPS request to an HTTP server doesn't show a traceback.\\nhttps://github.com/pallets/werkzeug/pull/838\",\n",
       " 'summarize to docstring: Include the given datetime instance in the recurrence set\\nexclusion list. Dates included that way will not be generated,\\neven if some inclusive rrule or rdate matches them. ',\n",
       " 'summarize to docstring: Send an item immediately if it can be done without waiting.\\n\\n:param item: the item to send\\n:raises ~anyio.ClosedResourceError: if this send stream has been closed\\n:raises ~anyio.BrokenResourceError: if the stream has been closed from the\\n    receiving end\\n:raises ~anyio.WouldBlock: if the buffer is full and there are no tasks waiting\\n    to receive',\n",
       " 'summarize to docstring: Returns whether node is a statement node.',\n",
       " 'summarize to docstring: Handle wr.s3.read_parquet_metadata internally.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for get_aws_open_id_config\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AwsClusters server.',\n",
       " 'summarize to docstring: Yields:\\n    All ranges of @string which, if @string were to be split there,\\n    would result in the splitting of an f-expression (which is NOT\\n    allowed).',\n",
       " 'summarize to docstring: Store one or more lines of input.\\n\\nIf input lines are not newline-terminated, a newline is automatically\\nappended.',\n",
       " \"summarize to docstring: Windows: if 'enabled_flag' is True, enable the UNICODE and\\n_UNICODE defines in C, and declare the types like TCHAR and LPTCSTR\\nto be (pointers to) wchar_t.  If 'enabled_flag' is False,\\ndeclare these types to be (pointers to) plain 8-bit characters.\\nThis is mostly for backward compatibility; you usually want True.\",\n",
       " 'summarize to docstring: Decide whether to trace execution in `filename`.\\n\\nCalls `_should_trace_internal`, and returns the FileDisposition.',\n",
       " \"summarize to docstring: Assert that the hrefs in htmlcov/*.html are valid.\\n\\nDoesn't check external links (those with a protocol).\",\n",
       " \"summarize to docstring: Return the base directory containing Cython's caches.\\n\\nPriority:\\n\\n1. CYTHON_CACHE_DIR\\n2. (OS X): ~/Library/Caches/Cython\\n   (posix not OS X): XDG_CACHE_HOME/cython if XDG_CACHE_HOME defined\\n3. ~/.cython\",\n",
       " 'summarize to docstring: Post a metric.',\n",
       " 'summarize to docstring: :param string type:\\n:param string event:\\n:param StoppedEventBody body:\\n:param integer seq: Sequence number of the message (also known as message ID). The `seq` for the first message sent by a client or debug adapter is 1, and for each subsequent message is 1 greater than the previous message sent by that actor. `seq` can be used to order requests, responses, and events, and to associate requests with their corresponding responses. For protocol messages of type `request` the sequence number can be used to cancel the request.',\n",
       " 'summarize to docstring: :param array breakpoints: The instruction references of the breakpoints',\n",
       " 'summarize to docstring: Iterate over the entries in this pack index.\\n\\nReturns: iterator over tuples with object name, offset in packfile and\\n    crc32 checksum.',\n",
       " 'summarize to docstring: Iterate over cached submodules.\\n\\nArgs:\\n  store: Object store to iterate\\n  root_tree_id: SHA of root tree\\n\\nReturns:\\n  Iterator over over (path, sha) tuples',\n",
       " 'summarize to docstring: Releases the file lock. Please note, that the lock is only completely released, if the lock counter is 0.\\nAlso note, that the lock file itself is not automatically deleted.\\n\\n:param force: If true, the lock counter is ignored and the lock is released in every case/',\n",
       " 'summarize to docstring: Parses a custom_metric path into its component segments.',\n",
       " 'summarize to docstring: Pre-rpc interceptor for update_enhanced_measurement_settings\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'summarize to docstring: Post-rpc interceptor for update_repository\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ArtifactRegistry server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Return a callable for the list catalogs method over gRPC.\\n\\nList all catalogs in a specified project.\\n\\nReturns:\\n    Callable[[~.ListCatalogsRequest],\\n            ~.ListCatalogsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'summarize to docstring: Post-rpc interceptor for get_system_policy\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the SystemPolicyV1 server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Instantiate the transport.\\n\\nNOTE: This REST transport functionality is currently in a beta\\nstate (preview). We welcome your feedback via a GitHub issue in\\nthis library\\'s repository. Thank you!\\n\\n Args:\\n     host (Optional[str]):\\n          The hostname to connect to (default: \\'compute.googleapis.com\\').\\n     credentials (Optional[google.auth.credentials.Credentials]): The\\n         authorization credentials to attach to requests. These\\n         credentials identify the application to the service; if none\\n         are specified, the client will attempt to ascertain the\\n         credentials from the environment.\\n\\n     credentials_file (Optional[str]): A file with credentials that can\\n         be loaded with :func:`google.auth.load_credentials_from_file`.\\n         This argument is ignored if ``channel`` is provided.\\n     scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n         ignored if ``channel`` is provided.\\n     client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n         certificate to configure mutual TLS HTTP channel. It is ignored\\n         if ``channel`` is provided.\\n     quota_project_id (Optional[str]): An optional project to use for billing\\n         and quota.\\n     client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n         The client info used to send a user-agent string along with\\n         API requests. If ``None``, then default info will be used.\\n         Generally, you only need to set this if you are developing\\n         your own client library.\\n     always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n         be used for service account credentials.\\n     url_scheme: the protocol scheme for the API endpoint.  Normally\\n         \"https\", but for testing or local servers,\\n         \"http\" can be specified.',\n",
       " 'summarize to docstring: Deletes the specified target pool.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_delete():\\n        # Create a client\\n        client = compute_v1.TargetPoolsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.DeleteTargetPoolRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n            target_pool=\"target_pool_value\",\\n        )\\n\\n        # Make the request\\n        response = client.delete(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.DeleteTargetPoolRequest, dict]):\\n        The request object. A request message for\\n        TargetPools.Delete. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region scoping this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    target_pool (str):\\n        Name of the TargetPool resource to\\n        delete.\\n\\n        This corresponds to the ``target_pool`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'summarize to docstring: Call the delete method over HTTP.\\n\\nArgs:\\n    request (~.compute.DeleteVpnTunnelRequest):\\n        The request object. A request message for\\n    VpnTunnels.Delete. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'summarize to docstring: Return a callable for the set logging service method over gRPC.\\n\\nSets the logging service for a specific cluster.\\n\\nReturns:\\n    Callable[[~.SetLoggingServiceRequest],\\n            Awaitable[~.Operation]]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'summarize to docstring: Return a callable for the process open lineage run event method over gRPC.\\n\\nCreates new lineage events together with their\\nparents: process and run. Updates the process and run if\\nthey already exist. Mapped from Open Lineage\\nspecification:\\n\\nhttps://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json.\\n\\nReturns:\\n    Callable[[~.ProcessOpenLineageRunEventRequest],\\n            ~.ProcessOpenLineageRunEventResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.dataplex_v1.types.ListDataTaxonomiesRequest):\\n        The initial request object.\\n    response (google.cloud.dataplex_v1.types.ListDataTaxonomiesResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'summarize to docstring: Return a callable for the list session entity types method over gRPC.\\n\\nReturns the list of all session entity types in the\\nspecified session.\\n\\nReturns:\\n    Callable[[~.ListSessionEntityTypesRequest],\\n            ~.ListSessionEntityTypesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Bulk import of multiple\\n[Document][google.cloud.discoveryengine.v1.Document]s. Request\\nprocessing may be synchronous. Non-existing items are created.\\n\\nNote: It is possible for a subset of the\\n[Document][google.cloud.discoveryengine.v1.Document]s to be\\nsuccessfully updated.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import discoveryengine_v1\\n\\n    def sample_import_documents():\\n        # Create a client\\n        client = discoveryengine_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = discoveryengine_v1.ImportDocumentsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.import_documents(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.discoveryengine_v1.types.ImportDocumentsRequest, dict]):\\n        The request object. Request message for Import methods.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.discoveryengine_v1.types.ImportDocumentsResponse` Response of the\\n           [ImportDocumentsRequest][google.cloud.discoveryengine.v1.ImportDocumentsRequest].\\n           If the long running operation is done, then this\\n           message is returned by the\\n           google.longrunning.Operations.response field if the\\n           operation was successful.',\n",
       " 'summarize to docstring: Call the update participant method over HTTP.\\n\\nArgs:\\n    request (~.gcd_participant.UpdateParticipantRequest):\\n        The request object. The request message for\\n    [Participants.UpdateParticipant][google.cloud.dialogflow.v2.Participants.UpdateParticipant].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.gcd_participant.Participant:\\n        Represents a conversation participant\\n    (human agent, virtual agent, end-user).',\n",
       " 'summarize to docstring: Return a callable for the batch create target sites method over gRPC.\\n\\nCreates [TargetSite][google.cloud.discoveryengine.v1.TargetSite]\\nin a batch.\\n\\nReturns:\\n    Callable[[~.BatchCreateTargetSitesRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Parses a file_store_data_profile path into its component segments.',\n",
       " 'summarize to docstring: Return a callable for the list volume backups method over gRPC.\\n\\nLists the VolumeBackups for a given Backup.\\n\\nReturns:\\n    Callable[[~.ListVolumeBackupsRequest],\\n            ~.ListVolumeBackupsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    AttachedClustersAsyncClient: The constructed client.',\n",
       " 'summarize to docstring: Resets an Identity Aware Proxy (IAP) OAuth client\\nsecret. Useful if the secret was compromised. Requires\\nthat the client is owned by IAP.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import iap_v1\\n\\n    def sample_reset_identity_aware_proxy_client_secret():\\n        # Create a client\\n        client = iap_v1.IdentityAwareProxyOAuthServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = iap_v1.ResetIdentityAwareProxyClientSecretRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.reset_identity_aware_proxy_client_secret(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.iap_v1.types.ResetIdentityAwareProxyClientSecretRequest, dict]):\\n        The request object. The request sent to\\n        ResetIdentityAwareProxyClientSecret.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.iap_v1.types.IdentityAwareProxyClient:\\n        Contains the data that describes an\\n        Identity Aware Proxy owned client.',\n",
       " 'summarize to docstring: Call the list assets method over HTTP.\\n\\nArgs:\\n    request (~.migrationcenter.ListAssetsRequest):\\n        The request object. Message for requesting a list of\\n    assets.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.migrationcenter.ListAssetsResponse:\\n        Response message for listing assets.',\n",
       " 'summarize to docstring: Post-rpc interceptor for update_http_route\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the NetworkServices server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Deletes a single Exadata Infrastructure.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import oracledatabase_v1\\n\\n    def sample_delete_cloud_exadata_infrastructure():\\n        # Create a client\\n        client = oracledatabase_v1.OracleDatabaseClient()\\n\\n        # Initialize request argument(s)\\n        request = oracledatabase_v1.DeleteCloudExadataInfrastructureRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_cloud_exadata_infrastructure(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.oracledatabase_v1.types.DeleteCloudExadataInfrastructureRequest, dict]):\\n        The request object. The request for ``CloudExadataInfrastructure.Delete``.\\n    name (str):\\n        Required. The name of the Cloud Exadata Infrastructure\\n        in the following format:\\n        projects/{project}/locations/{location}/cloudExadataInfrastructures/{cloud_exadata_infrastructure}.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'summarize to docstring: Parses a ssh_public_key path into its component segments.',\n",
       " 'summarize to docstring: Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsRequest):\\n        The initial request object.\\n    response (google.cloud.security.privateca_v1.types.ListCertificateRevocationListsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'summarize to docstring: Create the client designed to process long-running operations.\\n\\nThis property caches on the instance; repeated calls return the same\\nclient.',\n",
       " 'summarize to docstring: Post-rpc interceptor for get_tag_value\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the TagValues server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'summarize to docstring: :param sec_key_ref:\\n    A Security framework SecKeyRef value from loading/importing the\\n    key\\n\\n:param asn1:\\n    An asn1crypto.keys.PrivateKeyInfo object',\n",
       " 'summarize to docstring: Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'run.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " 'summarize to docstring: Return a callable for the test iam permissions method over gRPC.\\n\\nReturns the permissions that a caller has on the\\nspecified source.\\n\\nReturns:\\n    Callable[[~.TestIamPermissionsRequest],\\n            ~.TestIamPermissionsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Call the list event threat\\ndetection custom modules method over HTTP.\\n\\n    Args:\\n        request (~.securitycenter_service.ListEventThreatDetectionCustomModulesRequest):\\n            The request object. Request to list Event Threat\\n        Detection custom modules.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.securitycenter_service.ListEventThreatDetectionCustomModulesResponse:\\n            Response for listing Event Threat\\n        Detection custom modules.',\n",
       " 'summarize to docstring: Post-rpc interceptor for get_company\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CompanyService server but before\\nit is returned to user code.',\n",
       " 'summarize to docstring: Searches across deployment revisions.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import telcoautomation_v1alpha1\\n\\n    def sample_search_deployment_revisions():\\n        # Create a client\\n        client = telcoautomation_v1alpha1.TelcoAutomationClient()\\n\\n        # Initialize request argument(s)\\n        request = telcoautomation_v1alpha1.SearchDeploymentRevisionsRequest(\\n            parent=\"parent_value\",\\n            query=\"query_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.search_deployment_revisions(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.telcoautomation_v1alpha1.types.SearchDeploymentRevisionsRequest, dict]):\\n        The request object. Request object for ``SearchDeploymentRevisions``.\\n    parent (str):\\n        Required. The name of parent orchestration cluster\\n        resource. Format should be -\\n        \"projects/{project_id}/locations/{location_name}/orchestrationClusters/{orchestration_cluster}\".\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    query (str):\\n        Required. Supported queries:\\n\\n        1. \"\"                       : Lists all\\n            revisions across all deployments.\\n        2. \"latest=true\"            : Lists\\n            latest revisions across all\\n            deployments.\\n        3. \"name={name}\"            : Lists all\\n            revisions of deployment with name\\n            {name}.\\n        4. \"name={name} latest=true\": Lists\\n            latest revision of deployment with\\n            name {name}\\n\\n        This corresponds to the ``query`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.telcoautomation_v1alpha1.services.telco_automation.pagers.SearchDeploymentRevisionsPager:\\n        Response object for SearchDeploymentRevisions.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'summarize to docstring: Return a callable for the list slates method over gRPC.\\n\\nLists all slates in the specified project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListSlatesRequest],\\n            ~.ListSlatesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'summarize to docstring: Cell magics declared via a class, #2',\n",
       " 'summarize to docstring: A list of the captured rich display outputs, if any.\\n\\nIf you have a CapturedIO object ``c``, these can be displayed in IPython\\nusing::\\n\\n    from IPython.display import display\\n    for o in c.outputs:\\n        display(o)',\n",
       " 'summarize to docstring: Returns an instance of a compressor file object.',\n",
       " 'summarize to docstring: preprocess old style comments.\\n\\nexample:\\n\\nfrom mako.ext.preprocessors import convert_comments\\nt = Template(..., preprocessor=convert_comments)',\n",
       " 'summarize to docstring: Return text with backslash escapes undone (backslashes are restored). ',\n",
       " 'summarize to docstring: Demonstrates displaying different variables through shade and color.',\n",
       " 'summarize to docstring: Copy properties from *other* to *self*.',\n",
       " 'summarize to docstring: Return the depth of the axis used by the picker.',\n",
       " 'summarize to docstring: Read one page from the file. Return True if successful,\\nFalse if there were no more pages.',\n",
       " \"summarize to docstring: Return this line's `~matplotlib.transforms.TransformedPath`.\",\n",
       " 'summarize to docstring: Convert the given BSON value into our own type.',\n",
       " 'summarize to docstring: Raise error on missing token.',\n",
       " 'summarize to docstring: `MeterProvider.get_meter` arguments are used to create an\\n`InstrumentationScope` object on the created `Meter`.',\n",
       " 'summarize to docstring: Register a function to convert a core foundation data type into its\\nequivalent in python\\n\\n:param type_id:\\n    The CFTypeId for the type\\n\\n:param callback:\\n    A callback to pass the CFType object to',\n",
       " 'summarize to docstring: Simple helper method to create data for to ``to_dict(orient=\"split\")``\\nto create the main output data',\n",
       " 'summarize to docstring: Return a string representation for a particular Series.',\n",
       " 'summarize to docstring: Appends lines to a buffer.\\n\\nParameters\\n----------\\nbuf\\n    The buffer to write to\\nlines\\n    The lines to append.',\n",
       " 'summarize to docstring: Check that two objects are not approximately equal.\\n\\nParameters\\n----------\\na : object\\n    The first object to compare.\\nb : object\\n    The second object to compare.\\n**kwargs\\n    The arguments passed to `tm.assert_almost_equal`.',\n",
       " 'summarize to docstring: Iterate through file entries declared in this distribution.\\n\\nFor modern .dist-info distributions, this is the files listed in the\\n``RECORD`` metadata file. For legacy setuptools distributions, this\\ncomes from ``installed-files.txt``, with entries normalized to be\\ncompatible with the format used by ``RECORD``.\\n\\n:return: An iterator for listed entries, or None if the distribution\\n    contains neither ``RECORD`` nor ``installed-files.txt``.',\n",
       " 'summarize to docstring: Dispatches a hook dictionary on a given piece of data.',\n",
       " 'summarize to docstring: Verify that installing works from a link with an extra if there is an indirect\\ndependency on that same package with the same extra (#12372).',\n",
       " 'summarize to docstring: Returns a dictionary of token types, matched to their action in the parser.\\n\\nOnly returns token types that are accepted by the current state.\\n\\nUpdated by ``feed_token()``.',\n",
       " 'summarize to docstring: Alternate implementation using /proc/cpuinfo.\\nmin and max frequencies are not available and are set to None.',\n",
       " 'summarize to docstring: Re-run tests which failed on last run.',\n",
       " 'summarize to docstring: The context argument for `PydanticKnownError` requires a number or str type, so we do a simple repr() coercion for types like timedelta.\\n\\nSee tests/test_types.py::test_annotated_metadata_any_order for some context.',\n",
       " 'summarize to docstring: Grant owner access to the current entity.',\n",
       " 'summarize to docstring: Return the more user-friendly information about the location of a warning, or None.',\n",
       " \"summarize to docstring: Regression test for importing a submodule 'foo.bar' while there is a 'bar' directory\\nreachable from sys.path -- ensuring the top-level module does not end up imported as a namespace\\npackage.\\n\\n#12194\\nhttps://github.com/pytest-dev/pytest/pull/12208#issuecomment-2056458432\",\n",
       " 'summarize to docstring: Get the next page in the iterator.\\n\\nReturns:\\n    Page: The next page in the iterator or :data:`None` if\\n        there are no pages left.',\n",
       " \"summarize to docstring: Patch a table's metadata.\",\n",
       " 'summarize to docstring: Return query result and display a progress bar while the query running, if tqdm is installed.\\n\\nArgs:\\n    query_job:\\n        The job representing the execution of the query on the server.\\n    progress_bar_type:\\n        The type of progress bar to use to show query progress.\\n    max_results:\\n        The maximum number of rows the row iterator should return.\\n\\nReturns:\\n    A row iterator over the query results.',\n",
       " 'summarize to docstring: The configuration for this copy job.',\n",
       " 'summarize to docstring: Optional[str]: Description of the destination table.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.description',\n",
       " \"summarize to docstring: Scan the input for current state's tokens starting at ``current_offset``.\\n\\nArgs:\\n    state (LexerState): The current lexer state.\\n    current_offset (int): The offset in the input text, i.e. the number\\n        of characters already scanned so far.\\n\\nYields:\\n    The next ``Token`` or ``StateTransition`` instance.\",\n",
       " 'summarize to docstring: Sets the terminating of this V1EndpointConditions.\\n\\nterminating indicates that this endpoint is terminating. A nil value indicates an unknown state. Consumers should interpret this unknown state to mean that the endpoint is not terminating.  # noqa: E501\\n\\n:param terminating: The terminating of this V1EndpointConditions.  # noqa: E501\\n:type: bool',\n",
       " 'summarize to docstring: Returns true if both objects are not equal',\n",
       " 'summarize to docstring: Returns the hyperparameters as a dictionary to use for training.\\n\\nThe :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which\\ntrains the model, calls this method to find the hyperparameters.\\n\\nReturns:\\n    dict[str, str]: The hyperparameters.',\n",
       " 'summarize to docstring: Instantiate a new SparkOutputManager for PySpark functions.\\n\\nArgs:\\n    feature_store_manager_factory (FeatureStoreManagerFactory): A factory to provide\\n        that provides a FeatureStoreManager that handles data ingestion to a Feature Group.\\n        The factory lazily loads the FeatureStoreManager.\\n\\nReturns:\\n    SparkOutputReceiver: An instance that handles outputs of the wrapped function.',\n",
       " 'summarize to docstring: Returns schema version.',\n",
       " 'summarize to docstring: Initialize the class.\\n\\nArgs:\\n    force (bool): If True, render colorizes output no matter where the\\n        output is (default: False).',\n",
       " 'summarize to docstring: Describe the latest baselining job kicked off by the suggest workflow.',\n",
       " 'summarize to docstring: Deletes the Amazon SageMaker models backing this predictor.',\n",
       " 'summarize to docstring: Validate mutually exclusive property file / s3uri',\n",
       " 'summarize to docstring: Compute gradient and hessian of loss w.r.t raw_prediction.\\n\\nParameters\\n----------\\ny_true : C-contiguous array of shape (n_samples,)\\n    Observed, true target values.\\nraw_prediction : C-contiguous array of shape (n_samples,) or array of             shape (n_samples, n_classes)\\n    Raw prediction values (in link space).\\nsample_weight : None or C-contiguous array of shape (n_samples,)\\n    Sample weights.\\ngradient_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the gradient is stored. If None, a new array\\n    might be created.\\nhessian_out : None or C-contiguous array of shape (n_samples,) or array             of shape (n_samples, n_classes)\\n    A location into which the hessian is stored. If None, a new array\\n    might be created.\\nn_threads : int, default=1\\n    Might use openmp thread parallelism.\\n\\nReturns\\n-------\\ngradient : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise gradients.\\n\\nhessian : arrays of shape (n_samples,) or (n_samples, n_classes)\\n    Element-wise hessians.',\n",
       " 'summarize to docstring: Check that expected error are raised during fit.',\n",
       " 'summarize to docstring: Time conventional Voronoi diagram calculation.',\n",
       " 'summarize to docstring: Construct the sigma matrix in SVD from singular values and size M, N.\\n\\nParameters\\n----------\\ns : (M,) or (N,) array_like\\n    Singular values\\nM : int\\n    Size of the matrix whose singular values are `s`.\\nN : int\\n    Size of the matrix whose singular values are `s`.\\n\\nReturns\\n-------\\nS : (M, N) ndarray\\n    The S-matrix in the singular value decomposition\\n\\nSee Also\\n--------\\nsvd : Singular value decomposition of a matrix\\nsvdvals : Compute singular values of a matrix.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy.linalg import diagsvd\\n>>> vals = np.array([1, 2, 3])  # The array representing the computed svd\\n>>> diagsvd(vals, 3, 4)\\narray([[1, 0, 0, 0],\\n       [0, 2, 0, 0],\\n       [0, 0, 3, 0]])\\n>>> diagsvd(vals, 4, 3)\\narray([[1, 0, 0],\\n       [0, 2, 0],\\n       [0, 0, 3],\\n       [0, 0, 0]])',\n",
       " 'summarize to docstring: Minimize a function using a nonlinear conjugate gradient algorithm.\\n\\nParameters\\n----------\\nf : callable, ``f(x, *args)``\\n    Objective function to be minimized. Here `x` must be a 1-D array of\\n    the variables that are to be changed in the search for a minimum, and\\n    `args` are the other (fixed) parameters of `f`.\\nx0 : ndarray\\n    A user-supplied initial estimate of `xopt`, the optimal value of `x`.\\n    It must be a 1-D array of values.\\nfprime : callable, ``fprime(x, *args)``, optional\\n    A function that returns the gradient of `f` at `x`. Here `x` and `args`\\n    are as described above for `f`. The returned value must be a 1-D array.\\n    Defaults to None, in which case the gradient is approximated\\n    numerically (see `epsilon`, below).\\nargs : tuple, optional\\n    Parameter values passed to `f` and `fprime`. Must be supplied whenever\\n    additional fixed parameters are needed to completely specify the\\n    functions `f` and `fprime`.\\ngtol : float, optional\\n    Stop when the norm of the gradient is less than `gtol`.\\nnorm : float, optional\\n    Order to use for the norm of the gradient\\n    (``-np.inf`` is min, ``np.inf`` is max).\\nepsilon : float or ndarray, optional\\n    Step size(s) to use when `fprime` is approximated numerically. Can be a\\n    scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\\n    floating point machine precision.  Usually ``sqrt(eps)`` is about\\n    1.5e-8.\\nmaxiter : int, optional\\n    Maximum number of iterations to perform. Default is ``200 * len(x0)``.\\nfull_output : bool, optional\\n    If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\\n    addition to `xopt`.  See the Returns section below for additional\\n    information on optional return values.\\ndisp : bool, optional\\n    If True, return a convergence message, followed by `xopt`.\\nretall : bool, optional\\n    If True, add to the returned values the results of each iteration.\\ncallback : callable, optional\\n    An optional user-supplied function, called after each iteration.\\n    Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\\nc1 : float, default: 1e-4\\n    Parameter for Armijo condition rule.\\nc2 : float, default: 0.4\\n    Parameter for curvature condition rule.\\n\\nReturns\\n-------\\nxopt : ndarray\\n    Parameters which minimize f, i.e., ``f(xopt) == fopt``.\\nfopt : float, optional\\n    Minimum value found, f(xopt). Only returned if `full_output` is True.\\nfunc_calls : int, optional\\n    The number of function_calls made. Only returned if `full_output`\\n    is True.\\ngrad_calls : int, optional\\n    The number of gradient calls made. Only returned if `full_output` is\\n    True.\\nwarnflag : int, optional\\n    Integer value with warning status, only returned if `full_output` is\\n    True.\\n\\n    0 : Success.\\n\\n    1 : The maximum number of iterations was exceeded.\\n\\n    2 : Gradient and/or function calls were not changing. May indicate\\n        that precision was lost, i.e., the routine did not converge.\\n\\n    3 : NaN result encountered.\\n\\nallvecs : list of ndarray, optional\\n    List of arrays, containing the results at each iteration.\\n    Only returned if `retall` is True.\\n\\nSee Also\\n--------\\nminimize : common interface to all `scipy.optimize` algorithms for\\n           unconstrained and constrained minimization of multivariate\\n           functions. It provides an alternative way to call\\n           ``fmin_cg``, by specifying ``method=\\'CG\\'``.\\n\\nNotes\\n-----\\nThis conjugate gradient algorithm is based on that of Polak and Ribiere\\n[1]_.\\n\\nConjugate gradient methods tend to work better when:\\n\\n1. `f` has a unique global minimizing point, and no local minima or\\n   other stationary points,\\n2. `f` is, at least locally, reasonably well approximated by a\\n   quadratic function of the variables,\\n3. `f` is continuous and has a continuous gradient,\\n4. `fprime` is not too large, e.g., has a norm less than 1000,\\n5. The initial guess, `x0`, is reasonably close to `f` \\'s global\\n   minimizing point, `xopt`.\\n\\nParameters `c1` and `c2` must satisfy ``0 < c1 < c2 < 1``.\\n\\nReferences\\n----------\\n.. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\\n\\nExamples\\n--------\\nExample 1: seek the minimum value of the expression\\n``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\\nof the parameters and an initial guess ``(u, v) = (0, 0)``.\\n\\n>>> import numpy as np\\n>>> args = (2, 3, 7, 8, 9, 10)  # parameter values\\n>>> def f(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\\n>>> def gradf(x, *args):\\n...     u, v = x\\n...     a, b, c, d, e, f = args\\n...     gu = 2*a*u + b*v + d     # u-component of the gradient\\n...     gv = b*u + 2*c*v + e     # v-component of the gradient\\n...     return np.asarray((gu, gv))\\n>>> x0 = np.asarray((0, 0))  # Initial guess.\\n>>> from scipy import optimize\\n>>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\\nOptimization terminated successfully.\\n         Current function value: 1.617021\\n         Iterations: 4\\n         Function evaluations: 8\\n         Gradient evaluations: 8\\n>>> res1\\narray([-1.80851064, -0.25531915])\\n\\nExample 2: solve the same problem using the `minimize` function.\\n(This `myopts` dictionary shows all of the available options,\\nalthough in practice only non-default values would be needed.\\nThe returned value will be a dictionary.)\\n\\n>>> opts = {\\'maxiter\\' : None,    # default value.\\n...         \\'disp\\' : True,    # non-default value.\\n...         \\'gtol\\' : 1e-5,    # default value.\\n...         \\'norm\\' : np.inf,  # default value.\\n...         \\'eps\\' : 1.4901161193847656e-08}  # default value.\\n>>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\\n...                          method=\\'CG\\', options=opts)\\nOptimization terminated successfully.\\n        Current function value: 1.617021\\n        Iterations: 4\\n        Function evaluations: 8\\n        Gradient evaluations: 8\\n>>> res2.x  # minimum found\\narray([-1.80851064, -0.25531915])',\n",
       " 'summarize to docstring: This test is for backwards compatibility post scipy 1.13.\\nThe behavior observed here is what is to be expected\\nwith the older matrix classes. This test comes with the\\nexception of dok_matrix, which was not working pre scipy1.12\\n(unlike the rest of these).',\n",
       " 'summarize to docstring: Call this method once you are done using the instance. It is automatically\\ncalled on destruction, and should be called just in time to allow system\\nresources to be freed.\\n\\nOnce you called end_access, you must call begin access before reusing this instance!',\n",
       " \"summarize to docstring: When join conditions don't express the left side explicitly,\\ndetermine if an existing FROM or entity in this query\\ncan serve as the left hand side.\",\n",
       " 'summarize to docstring: Test that any link will not match in XHTML (all links are unvisited).',\n",
       " 'summarize to docstring: Test input direction when input is the root.',\n",
       " 'summarize to docstring: Runs distributed training.\\n\\nParameters\\n----------\\ntrain_object : callable object or str\\n    Either a PyTorch function, PyTorch Lightning function, or the path to a python file\\n    that launches distributed training.\\nargs :\\n    If train_object is a python function and not a path to a python file, args need\\n    to be the input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, 1e-3, 64)\\n\\n    where train is a function and 1e-3 and 64 are regular numeric inputs to the function.\\n\\n    If train_object is a python file, then args would be the command-line arguments for\\n    that python file which are all in the form of strings. An example would be\\n\\n    >>> distributor.run(\"/path/to/train.py\", \"--learning-rate=1e-3\", \"--batch-size=64\")\\n\\n    where since the input is a path, all of the parameters are strings that can be\\n    handled by argparse in that python file.\\nkwargs :\\n    If train_object is a python function and not a path to a python file, kwargs need\\n    to be the key-word input parameters to that function. It would look like\\n\\n    >>> model = distributor.run(train, tol=1e-3, max_iter=64)\\n\\n    where train is a function of 2 arguments `tol` and `max_iter`.\\n\\n    If train_object is a python file, then you should not set kwargs arguments.\\n\\nReturns\\n-------\\n    Returns the output of train_object called with args inside spark rank 0 task if the\\n    train_object is a Callable with an expected output. Returns None if train_object is\\n    a file.',\n",
       " 'summarize to docstring: Rows of the RowMatrix stored as an RDD of vectors.\\n\\nExamples\\n--------\\n>>> mat = RowMatrix(sc.parallelize([[1, 2, 3], [4, 5, 6]]))\\n>>> rows = mat.rows\\n>>> rows.first()\\nDenseVector([1.0, 2.0, 3.0])',\n",
       " \"summarize to docstring: test additional use case that wasn't considered for #8372\",\n",
       " 'summarize to docstring: Filter ``only`` nodes which do not match *tags*.',\n",
       " 'summarize to docstring: Fetch the first column of the first row, and close the result set.\\n\\nReturns ``None`` if there are no rows to fetch.\\n\\nNo validation is performed to test if additional rows remain.\\n\\nAfter calling this method, the object is fully closed,\\ne.g. the :meth:`_engine.CursorResult.close`\\nmethod will have been called.\\n\\n:return: a Python scalar value, or ``None`` if no rows remain.',\n",
       " 'summarize to docstring: Return the attribute name that should be used to refer from one\\nclass to another, for a collection reference.\\n\\nThe default implementation is::\\n\\n    return referred_cls.__name__.lower() + \"_collection\"\\n\\nAlternate implementations\\ncan be specified using the\\n:paramref:`.AutomapBase.prepare.name_for_collection_relationship`\\nparameter.\\n\\n:param base: the :class:`.AutomapBase` class doing the prepare.\\n\\n:param local_cls: the class to be mapped on the local side.\\n\\n:param referred_cls: the class to be mapped on the referring side.\\n\\n:param constraint: the :class:`_schema.ForeignKeyConstraint` that is being\\n inspected to produce this relationship.',\n",
       " 'summarize to docstring: Tries to find more initial conditions by substituting the initial\\nvalue point in the differential equation.',\n",
       " 'summarize to docstring: Converts a term in the expansion of a function from binary to its\\nvariable form (for POS).',\n",
       " \"summarize to docstring: Young's Modulus of the Beam. \",\n",
       " 'summarize to docstring: Expands the transfer function matrix',\n",
       " 'summarize to docstring: The length of the spring at which it produces no force.',\n",
       " 'summarize to docstring: Tests max degrees function.',\n",
       " 'summarize to docstring: Returns the first `n` terms of the composed formal power series.\\nTerm by term logic is implemented here.\\n\\nExplanation\\n===========\\n\\nThe coefficient sequence of the :obj:`FormalPowerSeriesCompose` object is the generic sequence.\\nIt is multiplied by ``bell_seq`` to get a sequence, whose terms are added up to get\\nthe final terms for the polynomial.\\n\\nExamples\\n========\\n\\n>>> from sympy import fps, sin, exp\\n>>> from sympy.abc import x\\n>>> f1 = fps(exp(x))\\n>>> f2 = fps(sin(x))\\n>>> fcomp = f1.compose(f2, x)\\n\\n>>> fcomp._eval_terms(6)\\n-x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\n>>> fcomp._eval_terms(8)\\nx**7/90 - x**6/240 - x**5/15 - x**4/8 + x**2/2 + x + 1\\n\\nSee Also\\n========\\n\\nsympy.series.formal.FormalPowerSeries.compose\\nsympy.series.formal.FormalPowerSeries.coeff_bell',\n",
       " 'summarize to docstring: Apply ``_hasattrs`` and ``_hastypes`` to ``expr``. ',\n",
       " 'summarize to docstring: Show that options are not passed for older versions of requests.',\n",
       " 'summarize to docstring: Test CLI --manpath',\n",
       " 'summarize to docstring: Completion hints for argcomplete',\n",
       " 'summarize to docstring: validate object to defined invariants.',\n",
       " 'summarize to docstring: Encodes the value using DER\\n\\n:param force:\\n    If the encoded contents already exist, clear them and regenerate\\n    to ensure they are in DER format instead of BER format\\n\\n:return:\\n    A byte string of the DER-encoded value',\n",
       " 'summarize to docstring: The native Python datatype representation of this value\\n\\n:return:\\n    A byte string or None',\n",
       " 'summarize to docstring: Writes the loaded data back out to the JSON index.',\n",
       " 'summarize to docstring: Should determine names and address with a hostname override.',\n",
       " 'summarize to docstring: Property suppress_warnings.',\n",
       " 'summarize to docstring: Filter only running futures.',\n",
       " 'summarize to docstring: StringTransformer instances have a call signature that mirrors that of\\nthe Transformer type.\\n\\nRaises:\\n    CannotTransform(...) if the concrete StringTransformer class is unable\\n    to transform @line.',\n",
       " \"summarize to docstring: Acquire the GIL. The thread's thread state must have been initialized\\nby a previous `put_release_gil`\",\n",
       " 'summarize to docstring: Delete a specific SLO.\\n\\n:param id: SLO id to delete\\n:type id: str\\n\\n:returns: SLO ids removed',\n",
       " \"summarize to docstring: Sets the tracing function with the pydev debug function and initializes needed facilities.\\n\\n:param host: the user may specify another host, if the debug server is not in the same machine (default is the local\\n    host)\\n\\n:param stdout_to_server: when this is true, the stdout is passed to the debug server\\n\\n:param stderr_to_server: when this is true, the stderr is passed to the debug server\\n    so that they are printed in its console and not in this process console.\\n\\n:param port: specifies which port to use for communicating with the server (note that the server must be started\\n    in the same port). @note: currently it's hard-coded at 5678 in the client\\n\\n:param suspend: whether a breakpoint should be emulated as soon as this function is called.\\n\\n:param trace_only_current_thread: determines if only the current thread will be traced or all current and future\\n    threads will also have the tracing enabled.\\n\\n:param overwrite_prev_trace: deprecated\\n\\n:param patch_multiprocessing: if True we'll patch the functions which create new processes so that launched\\n    processes are debugged.\\n\\n:param stop_at_frame: if passed it'll stop at the given frame, otherwise it'll stop in the function which\\n    called this method.\\n\\n:param wait_for_ready_to_run: if True settrace will block until the ready_to_run flag is set to True,\\n    otherwise, it'll set ready_to_run to True and this function won't block.\\n\\n    Note that if wait_for_ready_to_run == False, there are no guarantees that the debugger is synchronized\\n    with what's configured in the client (IDE), the only guarantee is that when leaving this function\\n    the debugger will be already connected.\\n\\n:param dont_trace_start_patterns: if set, then any path that starts with one fo the patterns in the collection\\n    will not be traced\\n\\n:param dont_trace_end_patterns: if set, then any path that ends with one fo the patterns in the collection\\n    will not be traced\\n\\n:param access_token: token to be sent from the client (i.e.: IDE) to the debugger when a connection\\n    is established (verified by the debugger).\\n\\n:param client_access_token: token to be sent from the debugger to the client (i.e.: IDE) when\\n    a connection is established (verified by the client).\\n\\n:param notify_stdin:\\n    If True sys.stdin will be patched to notify the client when a message is requested\\n    from the IDE. This is done so that when reading the stdin the client is notified.\\n    Clients may need this to know when something that is being written should be interpreted\\n    as an input to the process or as a command to be evaluated.\\n    Note that parallel-python has issues with this (because it tries to assert that sys.stdin\\n    is of a given type instead of just checking that it has what it needs).\\n\\n:param protocol:\\n    When using in Eclipse the protocol should not be passed, but when used in VSCode\\n    or some other IDE/editor that accepts the Debug Adapter Protocol then 'dap' should\\n    be passed.\",\n",
       " 'summarize to docstring: check version string found_version >= expected_min_or_eq_to_version\\n\\nIf dev/prerelease tags result in TypeError for string-number comparison,\\nit is assumed that the dependency is satisfied.\\nUsers on dev branches are responsible for keeping their own packages up to date.',\n",
       " 'summarize to docstring: @type  crash: L{Crash}\\n@param crash: Crash object.\\n\\n@rtype:  bool\\n@return:\\n    C{True} if a Crash object with the same key is in the container.',\n",
       " 'summarize to docstring: @rtype:  int\\n@return: Exit code of the thread.',\n",
       " 'summarize to docstring: @type  dwThreadId: int\\n@param dwThreadId: Global thread ID.\\n\\n@type  hThread: L{ThreadHandle}\\n@param hThread: (Optional) Handle to the thread.\\n\\n@type  process: L{Process}\\n@param process: (Optional) Parent Process object.',\n",
       " ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querrynew = [\"summarize to docstring: \" + doc for doc in values['docstring']]\n",
    "querrynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry = [\"summarize to docstring: \" + doc for doc in values['docstring']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:51<00:00, 1157.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 22,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 78,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 201,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 22,\n",
       " 271,\n",
       " 272,\n",
       " 20,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 78,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 198,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 201,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 22,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 78,\n",
       " 318,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 78,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 318,\n",
       " 427,\n",
       " 428,\n",
       " 63,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 331,\n",
       " 63,\n",
       " 201,\n",
       " 312,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 78,\n",
       " 78,\n",
       " 541,\n",
       " 201,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 63,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 22,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 483,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 78,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 20,\n",
       " 22,\n",
       " 726,\n",
       " 727,\n",
       " 22,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 63,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 312,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 22,\n",
       " 20,\n",
       " 829,\n",
       " 20,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 63,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 201,\n",
       " 889,\n",
       " 890,\n",
       " 198,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 78,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 232,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 20,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " 1001,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1011,\n",
       " 78,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1016,\n",
       " 1017,\n",
       " 1018,\n",
       " 1019,\n",
       " 1020,\n",
       " 1021,\n",
       " 1022,\n",
       " 1023,\n",
       " 1024,\n",
       " 1025,\n",
       " 1026,\n",
       " 89,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1031,\n",
       " 1032,\n",
       " 1033,\n",
       " 1034,\n",
       " 63,\n",
       " 1036,\n",
       " 1037,\n",
       " 1038,\n",
       " 1039,\n",
       " 1040,\n",
       " 1041,\n",
       " 1042,\n",
       " 1043,\n",
       " 1044,\n",
       " 1045,\n",
       " 1046,\n",
       " 1047,\n",
       " 1048,\n",
       " 1049,\n",
       " 1050,\n",
       " 1051,\n",
       " 1052,\n",
       " 1053,\n",
       " 1054,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1058,\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "restored_indexes = []\n",
    "\n",
    "for q in tqdm(querry):\n",
    "    # find q in querrynew\n",
    "    restored_indexes.append(querrynew.index(q))\n",
    "\n",
    "restored_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "      <th>args_types</th>\n",
       "      <th>args_defaults</th>\n",
       "      <th>body</th>\n",
       "      <th>docstring</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>54432</td>\n",
       "      <td>179</td>\n",
       "      <td>service_logs</td>\n",
       "      <td>{self,service,details,follow,stdout,stderr,sin...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{False,False,False,False,0,False,'all',None}</td>\n",
       "      <td>params = {'details': details, 'follow': follow...</td>\n",
       "      <td>Get log stream for a service.\\nNote: This endp...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>56509</td>\n",
       "      <td>138</td>\n",
       "      <td>delta_list_apply</td>\n",
       "      <td>{dcl,bbuf,write}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...</td>\n",
       "      <td>Apply the chain's changes and write the final ...</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>56635</td>\n",
       "      <td>169</td>\n",
       "      <td>to_json</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self._to_json()</td>\n",
       "      <td>Create a JSON representation of an instance of...</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>57434</td>\n",
       "      <td>276</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self,request}</td>\n",
       "      <td>{analytics_admin.CreateAdSenseLinkRequest}</td>\n",
       "      <td>{}</td>\n",
       "      <td>http_options = _BaseAnalyticsAdminServiceRestT...</td>\n",
       "      <td>Call the create ad sense link method over HTTP...</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>125</td>\n",
       "      <td>276</td>\n",
       "      <td>_get_universe_domain</td>\n",
       "      <td>{client_universe_domain,universe_domain_env}</td>\n",
       "      <td>{Optional[str],Optional[str]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>universe_domain = ApiKeysClient._DEFAULT_UNIVE...</td>\n",
       "      <td>Return the universe domain used by the client....</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60541</th>\n",
       "      <td>46700</td>\n",
       "      <td>286</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>orders = (covariant_order(e) + contravariant_o...</td>\n",
       "      <td>Apply on a list of vector_fields.\\nThe express...</td>\n",
       "      <td>275727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60542</th>\n",
       "      <td>47080</td>\n",
       "      <td>286</td>\n",
       "      <td>implicit_application</td>\n",
       "      <td>{tokens,local_dict,global_dict}</td>\n",
       "      <td>{List[TOKEN],DICT,DICT}</td>\n",
       "      <td>{}</td>\n",
       "      <td>res1 = _group_parentheses(implicit_application...</td>\n",
       "      <td>Makes parentheses optional in some cases for f...</td>\n",
       "      <td>275737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60543</th>\n",
       "      <td>47110</td>\n",
       "      <td>286</td>\n",
       "      <td>plot_bending_moment</td>\n",
       "      <td>{self,subs}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{None}</td>\n",
       "      <td>bending_moment = self.bending_moment()\\nif sub...</td>\n",
       "      <td>Returns a plot for Bending moment present in t...</td>\n",
       "      <td>275739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60544</th>\n",
       "      <td>47248</td>\n",
       "      <td>286</td>\n",
       "      <td>sT</td>\n",
       "      <td>{expr,string}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>assert srepr(expr) == string\\nassert eval(stri...</td>\n",
       "      <td>sT := sreprTest\\nfrom sympy/printing/tests/tes...</td>\n",
       "      <td>275740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51519</th>\n",
       "      <td>47479</td>\n",
       "      <td>286</td>\n",
       "      <td>cofactors</td>\n",
       "      <td>{f,g}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>(F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)</td>\n",
       "      <td>Returns GCD of ``f`` and ``g`` and their cofac...</td>\n",
       "      <td>234415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id  repo                  name  \\\n",
       "448      54432   179          service_logs   \n",
       "434      56509   138      delta_list_apply   \n",
       "437      56635   169               to_json   \n",
       "449      57434   276              __call__   \n",
       "451        125   276  _get_universe_domain   \n",
       "...        ...   ...                   ...   \n",
       "60541    46700   286              __call__   \n",
       "60542    47080   286  implicit_application   \n",
       "60543    47110   286   plot_bending_moment   \n",
       "60544    47248   286                    sT   \n",
       "51519    47479   286             cofactors   \n",
       "\n",
       "                                                    args  \\\n",
       "448    {self,service,details,follow,stdout,stderr,sin...   \n",
       "434                                     {dcl,bbuf,write}   \n",
       "437                                               {self}   \n",
       "449                                       {self,request}   \n",
       "451         {client_universe_domain,universe_domain_env}   \n",
       "...                                                  ...   \n",
       "60541                                             {self}   \n",
       "60542                    {tokens,local_dict,global_dict}   \n",
       "60543                                        {self,subs}   \n",
       "60544                                      {expr,string}   \n",
       "51519                                              {f,g}   \n",
       "\n",
       "                                       args_types  \\\n",
       "448                                            {}   \n",
       "434                                            {}   \n",
       "437                                            {}   \n",
       "449    {analytics_admin.CreateAdSenseLinkRequest}   \n",
       "451                 {Optional[str],Optional[str]}   \n",
       "...                                           ...   \n",
       "60541                                          {}   \n",
       "60542                     {List[TOKEN],DICT,DICT}   \n",
       "60543                                          {}   \n",
       "60544                                          {}   \n",
       "51519                                          {}   \n",
       "\n",
       "                                      args_defaults  \\\n",
       "448    {False,False,False,False,0,False,'all',None}   \n",
       "434                                              {}   \n",
       "437                                              {}   \n",
       "449                                              {}   \n",
       "451                                              {}   \n",
       "...                                             ...   \n",
       "60541                                            {}   \n",
       "60542                                            {}   \n",
       "60543                                        {None}   \n",
       "60544                                            {}   \n",
       "51519                                            {}   \n",
       "\n",
       "                                                    body  \\\n",
       "448    params = {'details': details, 'follow': follow...   \n",
       "434    for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...   \n",
       "437                               return self._to_json()   \n",
       "449    http_options = _BaseAnalyticsAdminServiceRestT...   \n",
       "451    universe_domain = ApiKeysClient._DEFAULT_UNIVE...   \n",
       "...                                                  ...   \n",
       "60541  orders = (covariant_order(e) + contravariant_o...   \n",
       "60542  res1 = _group_parentheses(implicit_application...   \n",
       "60543  bending_moment = self.bending_moment()\\nif sub...   \n",
       "60544  assert srepr(expr) == string\\nassert eval(stri...   \n",
       "51519    (F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)   \n",
       "\n",
       "                                               docstring      id  \n",
       "448    Get log stream for a service.\\nNote: This endp...     258  \n",
       "434    Apply the chain's changes and write the final ...     264  \n",
       "437    Create a JSON representation of an instance of...     266  \n",
       "449    Call the create ad sense link method over HTTP...     269  \n",
       "451    Return the universe domain used by the client....     279  \n",
       "...                                                  ...     ...  \n",
       "60541  Apply on a list of vector_fields.\\nThe express...  275727  \n",
       "60542  Makes parentheses optional in some cases for f...  275737  \n",
       "60543  Returns a plot for Bending moment present in t...  275739  \n",
       "60544  sT := sreprTest\\nfrom sympy/printing/tests/tes...  275740  \n",
       "51519  Returns GCD of ``f`` and ``g`` and their cofac...  234415  \n",
       "\n",
       "[60000 rows x 9 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.iloc[restored_indexes]\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(querry, return_tensors='pt', max_length=1024, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[24710,   112, 17851,  ...,     0,     0,     0],\n",
       "        [24710,   112, 17851,  ...,     0,     0,     0],\n",
       "        [24710,   112, 17851,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [24710,   112, 17851,  ...,     0,     0,     0],\n",
       "        [24710,   112, 17851,  ...,     0,     0,     0],\n",
       "        [24710,   112, 17851,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433025c340874132bf354929895c5610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# generated_ids = model.generate(**tokenized, max_length=100, length_penalty=2.0, early_stopping=True, batch_size=1)\n",
    "batch_size = 2\n",
    "outputs = []\n",
    "for i in tqdm(range(0, len(tokenized['input_ids']), batch_size)):\n",
    "    batch_input_ids = tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "    batch_attention_mask = tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=batch_input_ids,\n",
    "        attention_mask=batch_attention_mask,\n",
    "        max_length=100,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    outputs.extend([tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"docsstring gets log stream for a service .<n>bool: Whether the service's :py:class:ContainerSpec enables the TTY option .<n>bool: Add timestamps to every log line .\",\n",
       " \"Apply the chain's changes and write the final result using the passed write function .<n>Base buffer contains the base of all deltas contained in this list .<n>Writer takes a string of bytes to write to the output .\",\n",
       " 'docstring: Create a representation of an instance of Media.<n> Returns string, a representation of this instance, suitable to pass to from_Upload .<n>Media.Upload is an instance of Media.',\n",
       " '.resources.AdSenseLink: A link between a GA4 Property and an AdSense for Content ad client .<n>CreateAdSenseLink: A link between a GA4 Property and an AdSense for Content ad client .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'Retrieves the Settings for the Project .<n>Callable[.GetProjectSettingsRequest], .ProjectSettings]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Sleep *ds* seconds.<n>Sleep *ds* seconds.<n>Sleep *ds* seconds.<n>Sleep *ds* seconds.<n>Sleep *ds* seconds.',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file .<n>args: Additional arguments to pass to the constructor .<n> kwargs: Additional arguments to pass to the constructor .',\n",
       " 'Merges capacity commitments of the same plan into a single commitment .<n>Attempting to merge capacity commitments of different plan will fail with the error code google.rpc.Code.FAILED_PRECONDITION .',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object. retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request constructors .<n>Returns the specified network firewall policy .',\n",
       " 'docstring: Sets the port of this CoreV1EndpointPort .<n>param port: The port of this CoreV1EndpointPort .',\n",
       " 'Post-rpc interceptor for export_deployment_statefile .<n>Uses a subclass to manipulate the response after it is returned by the Config server .',\n",
       " 'Return a callable for the read repository file method over gRPC .<n>The Repository must not have a value for git_remote_settings.url .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .',\n",
       " 'This resource represents a long-running operation that is the result of a network API call .<n>Call the create connection method over HTTP to create a connection .<n>Message for creating a Connection retry .',\n",
       " 'Post-rpc interceptor for list_participants .<n> summarize in a subclass to manipulate the response after it is returned by the Participants server but before it is returned to user code.',\n",
       " 'Returns a callable for the check grounding method over gRPC .<n>A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Permanently deletes all [SuggestionDenyListEntry] for a DataStore .<n>Returns[.PurgeSuggestionDenyListRequest], .Operation]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'Returns: Callable[.ListSitesRequest], .ListSitesResponse]: A function that, when called, will call the underlying RPC on the server .<n>Lists sites in a given project and location .',\n",
       " 'docstring: Creates an instance of this client using the provided credentials info .<n>args: Additional arguments to pass to the constructor .<n>AsyncClient: The constructed client .',\n",
       " 'verbose: .operations_pb2.Operation: This resource represents a long-running operation that is the result of a network API call .<n> verbose: .operations_pb2.Retry: Designation of what errors, if any, should be retried .',\n",
       " 'summarize to docstring: Return if pages/frames are currently being cached .<n>Returns if pages/frames are currently being cached .<n>Returns if pages/frames are currently being cached .',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'Callable[.ListRelatedAccountGroupsRequest], .ListRelatedAccountGroupsResponse]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'Returns a callable for the add control method over gRPC .<n>Callable[.AddControlRequest], .ServingConfig]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns: Callable[.ListAcceleratorTypesRequest], .ListAcceleratorTypesResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " '.securitycenter_service.UpdateOrganizationSettingsRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n> metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'This method returns the API endpoint and client cert source for mutual TLS .<n>Only the api_endpoint and client_cert_source properties may be used in this method .',\n",
       " 'Get_management_dns_zone_binding manipulates the request or metadata before they are sent to the VmwareEngine server.<n>Pre-rpc interceptor for get_management_dns_zone_binding .',\n",
       " '.scan_run.ScanRun: A ScanRun is a output-only resource representing an actual run of the scan .<n>param .scan_run.ScanRun: The start scan run method over HTTP .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'Forward latex should really return nothing in either field if nothing is found .<n>The only thing that could change this is if there is a change in the way latex works .<n>The only thing that could change this is if there is a change in the way latex works .',\n",
       " 'Method computes the quantiles of the array for the given axis .<n>By default, the method is \"linear\" where alpha == beta == 1 which performs the 7th method of Hyndman&Fan .',\n",
       " '- is a backport the numpy memmap offset fix .<n>The numpy fix is available starting numpy 1.13)<n>If set to True, it adds a finalizer to the newly-created memmap .',\n",
       " 'If the most relevant error is an anOf, then we traverse its context and select the otherwise *least* relevant error .<n>I.e. since only one of the schemas must match, we look for the most relevant one .',\n",
       " 'Test that a nested AtomicString is not parsed.<n>Test that a summary of a nested AtomicString is not parsed.<n>Test that a nested AtomicString is not parsed.',\n",
       " 'summarize to docstring: Recall the first view and position from the stack.<n>In this example, the first view is the position from the stack.<n>If the first view is the position from the stack, then summarize to docstring.',\n",
       " 'Remove this colorbar from the figure.<n>If the colorbar was created with use_gridspec=True the previous gridspec is restored.',\n",
       " 'Tests that for loops at deeper levels are picked up .<n>Tests that for loops at deeper levels are picked up .<n>Tests that for loops at deeper levels are picked up .',\n",
       " 'Parameters ---------- mappable The matplotlib.cm.ScalarMappable (i.e., .AxesImage, .ContourSet, etc.) described by this colorbar .<n>If None, then a new Axes is created and the space for it will be stolen from the Axes(s) specified in *ax* .',\n",
       " 'Parameters ---------- val : float .<n>Set slider value to *val* to summarize to docstring .<n>Returns true if slider is set to .',\n",
       " 'Use the ZAxis (.axis3d.Axis) instance to return the ZAxis (.axis3d.Axis) instance.<n>Return the ZAxis (.axis3d.Axis) instance.',\n",
       " 'summarize to docstring: Whether the returned results should be verbose .<n>This page includes a summary of the results returned by the .<n>docstring function.',\n",
       " 'summarize to docstring .<n>test scale function with non-array .<n>test scale function with non-array .<n>test scale function with non-array .',\n",
       " \"If our message did not include a digest_name, the client is allowed to select a stronger digest_name from _ALLOWED_DIGESTS.<n>In case our message is prefixed, a client cannot downgrade to a weaker algorithm, because the MAC is calculated over the entire message including the 'digest_name' prefix.\",\n",
       " 'Given a non-complete graph G, returns a missing edge .<n>Returns a missing edge if there is no non-complete graph G .<n>Returns a missing edge if there is no complete graph G .',\n",
       " 'summarize to docstring: Make X orthogonal to the nullspace of L .<n>Use this to make X equal to the nullspace of L .<n>Use the following code to make X equal to the nullspace of L.',\n",
       " \"This function only supports built-in NumPy's data types .<n>Third-party dtypes are not yet supported .<n>bool : integer data types .<n>bool : real-valued floating-point data types .<n>bool : numeric data types .\",\n",
       " 'Sets the pod_selector of this V1NetworkPolicySpec.<n>param pod_selector: The pod_selector of this V1NetworkPolicySpec.',\n",
       " 'Silent OpenID authorization allows access tokens and id tokens to be granted to clients without any user prompt or interaction .<n> Method is used by: - OpenIDConnectAuthCode - OpenIDConnectImplicit - OpenIDConnectHybrid .',\n",
       " 'This property can be used as a prefix for any HTTP method call to return the the raw response object instead of the parsed content .<n>For more information, see https://www.openai.com/openai- python#accessing-raw-response-data-eg-headers .',\n",
       " 'Test that the meter provides a function to create a new ObservableCounterCounter .<n>The meter provides a function to create a new ObservableCounterCounter .<n>Use the following code to create a new ObservableCounterCounter .',\n",
       " 'Loads an ASN.1 object of an x509 certificate into a Certificate object .<n>param certificate: An asn1crypto.x509.Certificate object :return: A Certificate object .',\n",
       " 'Convert a native series structure to a Series object.<n>Use the following code to convert a native series structure to a Series object.<n>Use the following code to convert a native series structure to a Series object.',\n",
       " 'Parameters ---------- table : a table element that contains zero or more thead elements.<n>Returns ------- list of node-like These are the tr> row elements of a table.',\n",
       " 'Try to summarize axes if they are datelike.<n>Add a line in the beginning of each line to make it easier to read.<n>Add a line in the end of each line to make it easier to read.',\n",
       " 'summarize to docstring: return the root node .<n>Opens in .<n>closes in .<n>closes in .<n>closes in .',\n",
       " 'Convert the data from this selection to the appropriate pandas type .<n>Use docstring to convert the data from this selection to the appropriate pandas type .<n>bool np.ndarray nan_rep : str encoding : str errors : str',\n",
       " 'Return different versions of data for count times .<n>Use this code to return different versions of data for count times .<n>Use this code to return different versions of data for count times .',\n",
       " 'Check if we have a not-outdated version loaded already.<n>Check if we have a not-outdated version loaded already.<n>Check if we have a not-outdated version loaded already.<n>Check if we have a not-outdated version loaded already.',\n",
       " 'Remove a number of characters from the end of the text .<n>Add a line or two to the end of the docstring .<n>Add a line or two to the end of the text .',\n",
       " 'This method should only be called once, before the connection is used.<n>It should only be called once, before the connection is used.<n>This method should only be called once, before the connection is used.',\n",
       " 'Examples .<n>from sympy.polys import ring, ZZ .<n>R, x = ring(\"x\", ZZ) .dup_neg(x**2 - 1) -x**2 + 1 .',\n",
       " 'docstring: Test Subversion.get_vcs_version() with previously cached result.<n>Test Subversion.get_vcs_version() with previously cached result.',\n",
       " 'Executes a statement using :sql:cute (1S) with a sequence of parameters .<n>It must contain a single %s placeholder, which will be replaced by a S list__ Example: \"INSERT into mytable (id, f1, f2) VALUES %s\"',\n",
       " 'summarize to docstring: Finalize build system configuration on win32 platform.<n>Finalize build system configuration on win32 platform.<n>Finalize build system configuration on win32 platform.',\n",
       " 'docstring: Instantiate a cipher object that performs ECB encryption/decryption.<n>All keywords are passed to the underlying block cipher .<n>See the relevant documentation for details .',\n",
       " 'version is the mypy version string.<n>We might want to use this to print a warning if the mypy version being used is newer, or especially older, than we expect (or need)',\n",
       " 'Returns a dict of config keys to values .<n>Reads configs from toml file and returns None if the file is not a toml file .',\n",
       " 'docstring: :calls: POST /repos/owner/repo/forks .<n>bool :rtype: :class:github.Repository.Repository',\n",
       " 'Issue the warning :param:message for the definition of the given .<n>This helps to log warnings for functions defined prior to finding an issue with them (like hook wrappers being marked in a legacy mechanism)',\n",
       " \"Check whether 'value' should be coerced to 'field' type.<n>Use docstring to show whether 'value' should be coerced to 'field' type.<n>Check whether 'value' should be coerced to 'field' type.\",\n",
       " 'Error 301 -- also relocated (permanently)<n>Error 301 -- also relocated (permanently)<n>Error 301 -- also relocated (permanently)<n>Error 301 -- also relocated (permanently)',\n",
       " 'Issue #96 summarize to docstring .<n>Issue #96 summarize to docstring .<n>Issue #96 summarize to docstring .<n>Issue #96 summarize to docstring .',\n",
       " 'docstring: Polls a Cloud Pub/Sub subscription for new GCS events for display.<n>Polls summarize to a Cloud Pub/Sub subscription for new GCS events for display.',\n",
       " 'Sets the api_server_id of this V1alpha1ServerStorageVersion .<n>param api_server_id: The api_server_id of this V1alpha1ServerStorageVersion .',\n",
       " \"In the 2-D case, each column gives the coefficients of a; the first row holds the coefficients associated with the highest power m.<n>In thenumpypolyder doesn't handle the 2-D case.\",\n",
       " 'MapperOption will indicate to the :class:.MapperOption that the main table has been aliased .<n>Return a :class:.MapperOption that will indicate to the :class:_Query .',\n",
       " 'The AlgorithmEstor instance creates a default AWS instance of the AlgorithmEstor algorithm .<n>If not specified, the estimator generates a default job name based on the training image name and current timestamp .<n>The Amazon SageMaker training jobs and APIsthat create Amazon SageMaker use this role to access training data .',\n",
       " 'docstring: fields in object based on  Sets of header.<n>Args: _obj (Dict[str, Any]): Dictionary representation of spec.',\n",
       " 'The statement by default will not have disambiguating labels applied to the construct unless with_labels(True) is called first .<n>The full SELECT statement represented by this Query .',\n",
       " 'Set Additional Model Source to this model.<n> speculative_decoding_config (Optional[Dict[str]): Speculative decodingconfig. accept_eula (Optional[bool]): For models that require a Model Access Config.',\n",
       " 'Decorator to emit telemetry logs for SageMaker Python SDK functions .<n>Use this code to emit telemetry logs for SageMaker Python SDK functions .<n>Use this code to emit telemetry logs for SageMaker Python SDK functions .',\n",
       " 'Memoized fast path for _get_funcs instances .<n>Uses a modified version of python .<n> python is one of the most widely used python programs in the world .',\n",
       " 'Parameters ---------- s : sparse array Input sparse array .<n>Returns ------- data: ndarray Nonzero values of the array, with shape .',\n",
       " 'Compute matrices to transform rot. vector derivatives to angular rates .<n>The matrices depend on the current attitude represented as a rotation vector .<n>Returns ------- ndarray, shape (n, 3, 3)',\n",
       " 'Return a GeoJSON-like mapping of the LineString geometry .<n>Returns true if the geometry is mapped in a GeoJSON-like way .',\n",
       " 'summarize to docstring: Unsupported .<n>This page includes a summary of the words summarize and docstring .<n>This page was last modified on 23 June.',\n",
       " 'Does s3 multipart chunking work correctly?<n>What is the difference between s3 multipart chunking and s3 multipart chunking?<n>What is the difference between s3 multipart chunking and s3 multipart chunking?',\n",
       " 'summarize to docstring .<n>test #6696 .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .<n>Follow us on Facebook and Twitter .',\n",
       " 'summarize to docstring: Gets the value of withCentering or its default value.<n>Gets the value of withCentering or its default value.',\n",
       " \"Read HTML tables into a list of DataFrame objects .<n>This is a dictionary of attributes that you can pass to identify the table in the HTML .<n>For example, attrs is a valid dictionary because 'id' is a valid HTML tag .\",\n",
       " 'summarize to docstring: Truncate the timestamps for nanoseconds .<n>Use truncate to organize timestamps by type .<n>Use truncate to organize timestamps by date .',\n",
       " 'summarize to docstring: Test the different timestamp, date, and timedelta types .<n>Test the different timestamp, date, and timedelta types .',\n",
       " 'Return the \"comment\" for the table identified by table_name .<n>This is an internal dialect method.<n> Applications should use :meth:.Inspector.get_table_comment .',\n",
       " 'Cursor.fetchall() will work even if the connection has been rolled back .<n>This generally refers to buffered cursors but also seems to work with _oracle, for example .',\n",
       " 'Target must support simultaneous, independent database connections that will be used in a readonly fashion .<n>Attempt to use database connections in readonly fashion fails .<n>Attempt to use database connections in readonly fashion succeeds .',\n",
       " 'summarize to docstring: Start date column, if not present already.<n>End date column, if not present already.<n>Close date column, if not present already.<n>End date column, if not present already.',\n",
       " 'Returns the typecast or None of this object as a string .<n>Returns the typecast or None of this object as a string .',\n",
       " 'Target must support simultaneous, independent database connections.<n>Uses a comma to indicate whether the target is in English or Spanish.<n>Uses a hyphen to indicate whether the target is in English or Spanish.',\n",
       " 'If key is in dictionary, set the new value of key to be the union between the old value and value.<n> Otherwise, set the value of key to value.',\n",
       " 'If expr1 and expr2 are numerically close, they are compared using rtol and atol .<n>Relative tolerance is measured with respect to expr2 so when used in testing expr2 should be the expected answer .',\n",
       " 'This function does not promote exp(x) to exppolar(x).<n>If lift is True, both addition statements and non-polar symbols are changed to their polar_lift()ed versions.',\n",
       " 'If everything else fails, it tries to integrate.<n>In inverse Laplace transform, the front-end function tries to apply all known rules .<n>If everything else fails, it tries to integrate.',\n",
       " 'summarize to docstring: Returns denominator of a.<n>a a .<n>a .<n>a .',\n",
       " 'summarize to docstring: Returns a field associated with self.<n>Returns a field associated with self.<n>Returns a field associated with self.',\n",
       " 'Return the Smith Normal Form of a matrix m over the ring domain .<n>This will only work if the ring is a principal ideal domain .<n> print(smith_normal_form(m).to_Matrix())',\n",
       " 'Returns True if f is a cyclotomic.<n>Returns True if f is a cyclotomic.',\n",
       " 'This method calls the given callback on the next IOLoop iteration .<n>As of Tornado 6.0, this method is equivalent to add_callback.',\n",
       " 'Put a connection back into the pool .<n>If the pool is already full, the connection is closed and discarded .<n>If the pool is closed, the connection will be closed and discarded .',\n",
       " \"This function creates a WebSocket server listening on a Unix socket .<n>It's only available on Unix .<n>It's useful for deploying a server behind a reverse proxy such as nginx .\",\n",
       " 'summarize to docstring: Handshake succeeds without subprotocols .<n>Handshake succeeds without subprotocols .<n>This article was first published in The Conversation, a free, peer-to-peer, online, forum for exchanging ideas about politics and the Internet.',\n",
       " 'Creates datatype test schema, tables, and inserts test data.<n>Creates datatype test schema, tables, and inserts test data.<n>Uses docstring to summarize to docstring .',\n",
       " 'summarize to docstring: Write File.<n>Opens in a new window.<n>Closes in a new window.<n>Saved in a new window.<n>Opens a new window.',\n",
       " \"Locale display names for months.<n>Months['wide'] u'Oktober'<n>Months['de'', 'DE'] months['wide'] u'Oktober'\",\n",
       " 'Return the era names used by the locale for the specified format .<n>param width: the width to use, either \"wide\", \"abbreviated\", or \"narrow\"',\n",
       " 'docstring represents the (type) result of an indexing operation, e.g.<n>Use this to return a result of an unpacking operation, e.g.<n>Or a result of an iteration .',\n",
       " 'summarize to docstring .<n>Uses this to compact the frames to deduplicate calls .<n>Uses this to specify the type of call to be deduplicated .',\n",
       " 'data_file is the base name of the data file to use .<n>concurrency is a parameter determining whether Python code installed with the Python interpreter is measured .<n>source_preimported is a list of file paths or package names .',\n",
       " 'Make sure our summarizecov directory exists.<n>Use the following code to send an email to all your contacts with a summary of your work.<n>Use the following code to send an email to all your contacts with a summary of your work.',\n",
       " 'summarize to docstring: Assert that numbits is good.<n>Assert that numbits is good.<n> summarize to docstring: Assert that numbits is good.',\n",
       " 'Makes width directories, named d0 .. dwidth-1.<n>Each directory has __init__.py, and width files, named f0.py ..<n>fwidth-1.py.',\n",
       " 'summarize to docstring: Changes the value of a variable .<n>Uses docstring to specify the type of variable to use .<n>Uses docstring to specify the value of a variable .',\n",
       " '@type address: int @param address: Memory address.<n> @rtype: int @return: Aligned memory address.<n>align the given address to the end of the page it occupies. That is, to point to the start of the next page.',\n",
       " '@rtype: bool @return True if the memory in this region belongs to a mapped file.<n> summarize to docstring: @rtype: bool @return True if the memory in this region belongs to a mapped file.',\n",
       " 'docstring: Set a the value for key to value inside the config dict.<n>In the config dict, set the value for key to value.',\n",
       " 'Unpack the crc32 summarize for the ith object from the index file.<n>Unpack the crc32 summarize for the ith object from the index file.',\n",
       " 'summarize to docstring: All refs present in this container .<n>This page includes all refs present in this container .<n>This page was last edited on 18 June, 2014.',\n",
       " \"The first week of the year is the (Mon-Sun) week containing the year's first Thursday; everything else derives from that.<n>The first week is 1; Monday is 1; Sunday is 7.\",\n",
       " 'Mirror attributes and methods from the given origin_name attribute of the instance to the decorated class .<n>Use the following code to show each attribute of an instance in a decorated class .',\n",
       " 'The post-rpc interceptor for create_analytics_account_link manipulates the response after it is returned by the MarketingplatformServiceAdmin server but before it is returned to user code .',\n",
       " 'verbose: .analytics_admin.ListGoogleAdsLinksResponse: Response message for ListGoogleAdsLinks RPC.<n>paramListGoogleAdsLinks, .analytics_admin.ListGoogleAdsLinks, .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>Iterating over this object will yield results and resolve additional pages automatically .',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file.<n>args: Additional arguments to pass to the constructor.<n>Returns: ApiHubAsyncClient: The constructed client.',\n",
       " 'verbose: .registry_service.GetApiSpecRequest .<n> verbose: .registry_models.ApiSpec .<n> verbose: .registry_models.ApiSpec .',\n",
       " 'docstring: return a summarize if I am possibly a view .<n>Uses the expression \"I am possibly a view\" to mean \"I am possibly a view\"<n>Returns \"I am possibly a view\" as a return .',\n",
       " 'docstring: Call the get location method over HTTP.<n>Args: request (locations_pb2.GetLocationRequest): The request object for GetLocation method.<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata: Strings which should be sent along with the request as metadata.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request constructors .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'Python docstring: Retrieves a list of resize requests that are contained in the managed instance group .<n>Iterating over this object yield results and resolve additional pages automatically .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " 'Python code-block:: python # This snippet has been automatically generated and should be regarded as a # code template only.<n>It will require modifications to work: # - It may require correct/inrange values for request initialization.<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>Use this method to delete a private connectivity configuration .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'fetch git hub installations method over HTTP .<n>developer.FetchGitHubInstallationsRequest .<n>Retry (google.api_core.retry): Designation of what errors, if any, should be retried .',\n",
       " '.gcd_conversation_profile.ConversationProfile: Defines the services to connect to incoming Dialogflow conversations .<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .',\n",
       " 'Dialogflow provides predefined system entities that can match many common types of data .<n>Each parameter has a type, called the entity type, which dictates exactly how data from an end-user expression is extracted .<n>You can also create your own custom entities for matching custom data .',\n",
       " '.completion_service.CompleteQueryRequest: The request object.<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It may require specifying regional endpoints when creating the service # client .<n>Iterating over this object will yield results and resolve additional pages automatically .',\n",
       " 'Return GeoTIFF metadata from first page as dict.<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .',\n",
       " 'Call the list crypto keys method over HTTP .<n>Sends a response message for [KeyManagementService.ListCryptoKeys] .<n>Retry what errors, if any, should be retried .',\n",
       " 'python # This snippet has been automatically generated and should be regarded as a # code template only .<n>It may require specifying regional endpoints when creating the service # client .<n>Iterating over this object yield results and resolve pages automatically .',\n",
       " 'The post-rpc interceptor for update_certificate_template manipulates the response after it is returned by the CertificateAuthorityService server but before it is returned to user code .<n>This interceptor is used to manipulate the response after it is returned by the CertificateAuthorityService server but before it is returned to user code .',\n",
       " 'Deletes permanently all user events specified by the filter provided .<n>To test a filter, use the list command first .<n>Code-block:: python # This snippet has been automatically generated .',\n",
       " 'Returns a callable for the pause job method over gRPC .<n>Callable[.JobRequest], .Job]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'This resource represents a long-running operation that is the result of a network API call .<n>Call the delete series method over HTTP to delete a series .<n>Message for deleting a Series. retry .',\n",
       " 'docstring: Return whether the supplied file name fn matches pattern .<n>Return whether the supplied file name fn matches pattern .<n>Use this to test if the supplied file name matches a pattern .',\n",
       " 'This method is called by .FigureManagerBase.pyplot_show, which is the implementation of .pyplot.show.<n>If more customized logic is necessary, .FigureManagerBase.pyplot_show can also be overridden.',\n",
       " 'Process every newly added tool.<n>Sum up every word in docstring.<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .',\n",
       " \"This magic replaces IPython's threaded shells that were activated using the (pylab/wthread/etc.) command line flags .<n>Gui toolkits can now be enabled at interrupts and keyboard runtime should work without any problems .\",\n",
       " 'docstring: Run the conda package manager within the current kernel .<n>Use this to run the conda package manager within the current kernel .<n>Use the following command to run the conda package manager .',\n",
       " 'Sagemath use custom prompt and we broke them in 8.19 .<n>This article was first published in The Conversation on October 2, 2013 .<n>This article was amended on October 2, 2013 to clarify that Sagemath use custom prompt and we broke them in 8.19 .<n>For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.',\n",
       " 'The table can be created empty and manually filled or it can be created with a list of valid color schemes AND the specification for the default active scheme.<n>The table can be created with a list of valid color schemes AND the specification for the default active scheme.',\n",
       " 'Some functions use type vars not defined by the class, but rather only defined in the function.<n>Search for undefined type vars.<n>Infer type vars with the execution state we have.<n>Return the union of all type vars that have been found.',\n",
       " 'See SSHConfig.from_file for more information.<n>Use SSHConfig.from_file instead.<n>For more information, see SSHConfig.from_file.',\n",
       " 'Binds the app context to the current context .<n>Uses this to Bind the app context to the current context .<n>Uses this to Bind the app context to the current context .',\n",
       " 'Return the corners of the rectangle, moving anti-clockwise from (x0, y0).<n>Return the corners of the rectangle, moving anti-clockwise from (x0, y0).',\n",
       " 'Check that the animated artists changed in callbacks are updated.<n>Use the following code to check if the animated artists changed in callbacks.<n>Use the following code to check if the animated artists changed in callbacks.',\n",
       " \"Parameters ---------- nbins : int or 'auto', optional Number of ticks.<n>Only used if minor is False.<n>bool, default: False Indicate if this locator is for minor ticks or not.\",\n",
       " 'Set list of module names to try to load in forkserver process .<n>Use docstring to set the list of module names to try to load .<n>Use the following code to set the name of a module .',\n",
       " 'convert the given BSON value into our own type.<n>Use the following code to convert the given BSON value into our own type.<n>Use the following code to convert the given BSON value into our own type.',\n",
       " 'summarize to docstring: Raise a BulkWriteError from the full bulk api result .<n>Use this method to raise a BulkWriteError from the full bulk api result .',\n",
       " 'docstring: Discover python modules and packages in sub-directory.<n>Iterator of paths to discovered modules and packages .<n>Returns iterator of paths to discovered modules and packages .',\n",
       " 'This calculates the load of each node for paths from a single source.<n>To get the load for a node you need to do all-pairs shortest paths.<n>If weight is not None then use Dijkstra for finding shortest paths.',\n",
       " 'Tests for providing an alternate distance metric to the generator .<n>Tests for providing an alternate distance metric to the generator .<n>Tests for providing an alternate distance metric to the generator .',\n",
       " 'Write a graph G in GML format to the file or file handle path .<n>stringizer : callable, optional A stringizer which converts non-int/non-float/non-dict values into strings .',\n",
       " \"Get information about the arguments accepted by a code object .<n>'args' is a list of argument names .<n>'varargs' and 'varkw' are the names of the * and ** arguments or None .\",\n",
       " 'validate multi targets that defined between parentheses() .<n>render in .<n>renderConfig .<n>renderConfig .<n>renderConfig .<n>renderConfig .',\n",
       " 'This function uses charset_normalizer package, when available, for determining the encoding of the file to be opened.<n>When charset_normalizer is not available, the function detects only ASCII encodings, otherwise, ASCII is used as fallback.',\n",
       " 'Coefficients should be modifiable .<n>Use the canonical form to help you understand this article .<n>In the U.S., the canonical form is Merriam-Websterâ€™s definitions of the term .',\n",
       " 'function returns the values .<n> math:: p(a,b,c) = sum_i,j,k c_i,j,k .<n>The Chebyshev series is evaluated at the points in the Cartesian product of x, y, and z .',\n",
       " 'What do you think? Share your thoughts in the comments below or send us a video to iReport.com .<n>Follow us on Twitter .<n>Follow us on Facebook .',\n",
       " 'docstring:  summarizeContext Returns an instance ofContextContext.<n>An instance ofContextContext.<n>An example of an argument .<n>An instance of an argument .',\n",
       " 'This function is a convenience wrapper for opentelemetry.trace.TracerProvider.get_tracer .<n>If tracer_provider is omitted the current configured one is used .',\n",
       " 'summarize to docstring: Test inject() method for Format.BINARY .<n>Test inject() method for Format.BINARY .',\n",
       " 'Preloads asn1crypto and oscrypto from a local source checkout .<n>bool if oscrypto needs to be preloaded .<n>bool if info about asn1crypto and oscrypto should be printed .',\n",
       " 'Pairwise frames test_pairwise .<n>Opens in .<n>Closes in .<n>Opens in .<n>Closes in .',\n",
       " 'Use constants from the :mod:signal module to specify which signal.<n>Sends a Unix signal to the subprocess.<n>Use constants from the :mod:signal module to specify which signal.',\n",
       " 'Example:: Expected sha256 abcdeabcdeabcdeabcdeabcde or 12345123451234512345123451234512345 Got bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef .',\n",
       " 'Return true if name is a considered as an archive file.<n>Returns true if name is considered as an archive file.<n>Returns true if name is a considered as an archive file.',\n",
       " 'summarize to docstring: Replace the password in a given URL with ****.<n>In a given URL, replace the password with ****.<n>In a given URL, replace the password with .',\n",
       " 'Return all the distribution names known to this locator.<n>Return all the distribution names known to this locator.<n>Use this locator to find out where to find distribution names in your area.',\n",
       " ':func:_rec_strip.<n> :func:_rec_strip.<n> :func:_rec_strip.',\n",
       " 'Test that Link.from_() produces Links with consistent cache locations .<n>Use this test to verify that your links are working properly .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on this site .',\n",
       " 'Test deprecated default_value=None behavior for Container subclass traits .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .',\n",
       " 'See 2.3.3 in RFC6979 for details .<n>Use the canonical form to help with reading comprehension and vocabulary .<n>Use the canonical form to help with reading comprehension and vocabulary .<n>See 2.3.3 for more details .',\n",
       " \"Parse the input into a new type, deferring resolution of the type until the current class is fully defined.<n>This is useful when you need to reference the class in it's own type annotations.\",\n",
       " \"docstring: Don't die on unary +.<n> docstring: Don't die on unary .<n> docstring: Don't die on unary .\",\n",
       " \"This implementation runs setup() on item and all of its parents .<n>For a given item, only conftest files in the item's directory and its parent directories are consulted .\",\n",
       " \"[tool:pytest] section is read correctly .<n>Use the [tool:pytest] section to summarize files .<n>Use the [tool:pytest] section to test a tool's accuracy .\",\n",
       " 'Pythonpath kicks early enough to load plugins via -p (#11118)<n>Pythonpath kicks early enough to load plugins via -p (#11118)',\n",
       " 'Additional properties to set if sourceFormat is set toO.<n> See at https://cloud.google.com/bigQuery/docs/reference/v2tables#ExternalDataConfiguration.FIELDS.avro_options .',\n",
       " 'Known resources are set by regrtest.py .<n>Test whether a resource is enabled .<n>Known resources are set by regrtest.py .',\n",
       " 'docstring: Saves the private key in PKCS#1 DER format.<n>:returns: the DER-encoded private key.<n>:rtype: bytes',\n",
       " 'Computes minimum distances between one point and a set of points .<n>This function computes for each row in X, the index of the row of Y which is closest .<n>Uses much less memory, and is faster for large arrays .',\n",
       " 'Internal: Align multiline celltext text to bottom .<n>Crowded text: .<n>Crowded text: .<n>Crowded text: .',\n",
       " 'This method makes a synchronous HTTP request by default .<n>param str _continue: The continue option should be set when retrieving more results from the server .<n>param str field_selector: A selector to restrict the list of returned objects by their fields .',\n",
       " 'This method makes a synchronous HTTP request by default .<n>param str _continue: The continue option should be set when retrieving more results from the server .<n>param str label_selector: A selector to restrict the list of returned objects by their fields .',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'summarize to docstring: override if setattr should do something other than call self.set.<n>Setattr should do something other than call self.set.',\n",
       " '_continue may be set if the user set a limit on the number of items returned, and indicates that the server has more data available .<n>Continuing a consistent list may not be possible if the server configuration has changed or more than a few minutes have passed .',\n",
       " 'summarize to docstring: Returns true if both objects are not equal .<n>Returns true if both objects are not equal .<n>Returns true if both objects are not equal .',\n",
       " 'MaxSkew describes the degree to which pods may be unevenly distributed .<n>Global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains .',\n",
       " 'Append SCORE_FIELD and SCORE .<n>Use the canonical form to add this entry to your page .<n>Follow the instructions below to add this entry to your page .',\n",
       " 'It references the latest s3 model data produced by this Estimator .<n>Use constructors and security groups from this Estimator .<n>Additional kwargs passed to the LinearLearnerModel .',\n",
       " 'defines if the ast.Call node points to an ast.Name node with a matching name .<n>For more, see https://docs. python.org/3/library/ast.html#abstract-grammar. name (str): the function name .',\n",
       " 'Returns: dict represents the attributes.<n>constructs a dictionary based on the attributes .<n>Uses dict to represent the attributes in a dictionary .',\n",
       " 'docstring:  source algorithm object Args: algorithm_name (str): The ARN of an algorithm resource that was used to create the model package.<n>model_data_url: The Amazon S3 path where the model artifacts, which result from model training, are stored.',\n",
       " 'docstring:  summarize a TrainingDetails object.<n>Training_observations (str, optional): Any observations about training (default: None).<n>TrainingJobDetails (TrainingJobDetails, optional): Details about any associated training jobs.',\n",
       " 'Generates indices to split data into training and test set .<n> Parameters ---------- X : array-like of shape (n_samples, n_features)<n>Group labels for the samples used while splitting the dataset into train/test set .',\n",
       " 'Parameters ---------- X : ndarray, dataframe of shape (n_samples, n_features)<n>If the dataframe contains non-string feature names, None is returned.',\n",
       " 'x array split into knot intervals, t(j+k) = x(i) = t(j+k+1) .<n>Find interval with maximum sum of residuals .<n> insert a new knot into the middle of that interval .',\n",
       " 'summarize to docstring: Encode bson.objectid.ObjectId.<n>render .<n>render .<n>render .<n>render .<n>render .',\n",
       " 'summarize to docstring: Sets the value of :py:attr:labelType .<n>Uses :py:attr:labelType to specify a label type .',\n",
       " 'summarize to docstring: Returns R2, the coefficient of determination .<n>Returns R2, the coefficient of determination .<n>Returns R2, the coefficient of determination .',\n",
       " 'Returns a rotation matrix for a rotation of theta (in radians) about the 3-axis .<n>For a right-handed coordinate system, this corresponds to a clockwise rotation around the z-axis .',\n",
       " 'Apply \"render derived\" to this :class:sql.TableValuedAlias .<n>The with_types keyword will render column types within the alias expression .<n>If left as None, a unique anonymizing name will be used .',\n",
       " 'matches any differential equation that nth_algebraic can solve .<n>Uses sympy.solve but teaches it how to integrate derivatives .',\n",
       " \"By default, this function will create basic* strings .<n>Common escaping rules will be applied for basic strings .<n>If you disable escaping you will have to make sure that the given strings don't contain any forbidden character or sequence .\",\n",
       " 'Return a decorated class with a constructor signature that contain Trait names as kwargs .<n>Use this to return a decorated class with a constructor signature that contain Trait names as kwargs .',\n",
       " 'Figure out the full host name for the given domain part.<n>The domain part is a subdomain in case host matching is disabled or a full host name.',\n",
       " 'Check if the mimetype indicates data, either :mimetype:application/ or :mimetype:application/*+ .',\n",
       " 'Sets Content-Disposition header.<n>Content-Disposition header .<n>Content-Disposition header .<n>Content-Disposition header .',\n",
       " 'Determines if the labels in a domain are a valid match for labels from a wildcard .<n>Return with a valid domain: ADN: A domain with a valid name .',\n",
       " 'Indicates whether the :class:Arrow arrow.arrow.Arrow> object is a repeated wall time in the current timezone.<n>Arrow> object is a repeated wall time in the current timezone.',\n",
       " ':return: summarize.<n> :length: .<n> : width: .<n> : height: .<n> : depth: .',\n",
       " 'summarize to docstring: Upload Part.<n>This page includes a summary of all the words that have been added to this page.<n>For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.',\n",
       " 'Parse the .c file written by pgen. (Internal)<n>The first two lines are always this: #include \"pgenheaders.h\" #include \"grammar.h\"',\n",
       " \"Check if a stream's encoding and errors attributes are compatible with the desired values .<n>Use docstring to summarize a stream's encoding and errors attributes .\",\n",
       " 'summation: Alias for :meth:main.<n> .<n> .<n> .<n> .<n> .',\n",
       " 'Returns the sample value, or None if not found.<n>This is inefficient, and intended only for use in unittests.<n>Returns the sample value, or None if not found.',\n",
       " 'py_versions: The Python versions to test .<n>cov_versions: The coverage versions to test .<n>num_runs: The number of times to run each matrix element .',\n",
       " 'summarize to docstring: Import modules randomly to stress coverage.<n>Intends to import modules randomly to stress coverage.<n>Intends to import modules randomly to stress coverage.',\n",
       " 'docstring yields a sequence of (PyObjectPtr key, PyObjectPtr value) pairs, analogous to dict.iteritems()<n>Uses (PyObjectPtr key, PyObjectPtr value) to identify the PyObjectPtr key and the PyObjectPtr value.',\n",
       " 'Main entry point to process a list of .py files and inject type inferred declarations .<n>Use this entry to process a list of .py files and inject type inferred declarations .',\n",
       " 'docstring: .<n>In_lambda_in_list_comprehension .<n>In_lambda_in_list_comprehension .<n>In_lambda_in_list_comprehension .',\n",
       " 'Builds an EDNS option object from wire format .<n> Returns an instance of adns.edns.Option .<n>Parses the option length using the dns.wire.parser class .',\n",
       " 'Raises dns.rdataset.IncompatibleTypes if the type and class do not match the type and class of the rdataset .<n>Raises dns.rdataset.DifferingCovers if the type is a signature type and the covered type does not match that of the rdataset .',\n",
       " 'Similar to the docker rm command.<n>Raises :py:class:docker.errors.APIError If the server returns an error.',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " \"s client's and credentials' universe domains are consistent .<n>bool: True iff the configured universe domain is valid .<n>Raises: ValueError: If the configured universe domain is not valid .\",\n",
       " '.compute.TestIamPermissionsFirewallPolicyRequest: The request object.<n> retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata: Strings which should be sent along with the request as metadata.',\n",
       " 'interceptor for list_terraform_versions to manipulate the request or metadata before they are sent to the Config server.<n>Pre-rpc interceptor for list_terraform_versions to manipulate the request or metadata before they are sent to the Config server.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'JobMetrics contains a collection of metrics describing the detailed progress of a Dataflow job .<n>This resource captures only the most recent values of each metric; time-series data can be queried for them from Cloud Monitoring .',\n",
       " 'verbose: .operations_pb2.Operation: This resource represents a long-running operation that is the result of a network API call .<n> verbose: .operations_pb2.Retry: Designation of what errors, if any, should be retry .',\n",
       " 'overridden the base class method to use async wrappers.<n>Precompute the wrapped methods, overriding the base class method to use async wrappers.<n>Use docstring to summarize the wrapped methods.',\n",
       " 'interceptor to manipulate the request or metadata before they are sent to the CloudDeploy server .<n>Use this interceptor to manipulate the request or metadata before they are sent to the CloudDeploy server .',\n",
       " 'interceptor to manipulate the response after it is returned by the CloudDeploy server but before it is returned to user code .<n>Before it is returned to user code .<n>Use this interceptor to manipulate the response after it is returned by the CloudDeploy server .',\n",
       " 'Gets agent validation result .<n>Callable[.GetValidationResultRequest], .ValidationResult]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'subclass manipulates the request or metadata before they are sent to the DlpService server .<n>Use this interceptor to manipulate the request or metadata before they are sent to the DlpService server .',\n",
       " 'This method updates metadata associated with a dataset .<n>It requires the documentai.googleapis.com/datasets.update permission on the project, which is highly privileged .',\n",
       " 'docstring: Call the get location method over HTTP.<n>Args: request (locations_pb2.GetLocationRequest): The request object for GetLocation method.<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata: Strings which should be sent along with the request as metadata.',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file .<n>args: Additional arguments to pass to the constructor .<n>ManagedIdentitiesServiceAsyncClient: The constructed client .',\n",
       " 'Python snippet has been automatically generated and should be regarded as a # code template only .<n>Lists the clusters in a given project and location .<n>Iterating over this object will yield results and resolve additional pages automatically .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>Creates a new GrpcRoute in a given project and location .<n>Use the following method to create a new GrpcRoute .',\n",
       " 'Quantize the bitmap to make it 8-bit (paletted)<n>Returns a new FIBitmap object.<n>Only for 24 bit images.<n>Returns a new FIBitmap object.',\n",
       " '.model_service.ListModelsRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n> metadata: Strings which should be sent along with the request as metadata .',\n",
       " 'docstring: Call the get iam policy method over HTTP .<n>arguments: request (iam_policy_pb2.GetIamPolicyRequest): The request object for GetIamPolicy method .<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>StreamingSynthesizeResponse is the only message returned to the client by StreamingSynthesize method .',\n",
       " 'interceptor for get_connector to manipulate the request or metadata before they are sent to the VpcAccessService.<n>Use this interceptor to manipulate the request or metadata before they are sent to the VpcAccessService.',\n",
       " 'Returns True for 4xx status codes, False otherwise .<n>Returns True for 4xx status codes, False otherwise .',\n",
       " 'summarize to docstring: Print the docstring for an object.<n>If the given object is a class, it will print both the class and the constructor docstrings.',\n",
       " 'docstring: Idle, the editor bundled with python Parameters ---------- exe : str, None If none, should be pretty smart about finding the executable.',\n",
       " 'Tests for definition_start_position and definition_end_position .<n>Tests for definition_end_position .<n>Tests for definition_start_position and definition_end_position .',\n",
       " 'configure null disable()  disable() Traceback (most recent call last): Refusing to overwrite...Error: Refusing to overwrite...<n>Error: Trying to use a keyring that does not exist. The keyring does not exist.',\n",
       " 'Parameters ---------- xy : (float, float) The lower left corner of the box.<n>Width : float The width of the box. height : float The height of the box.<n>BoxStyle : str or matplotlib.patches.BoxStyle The style of the fancy box.',\n",
       " 'Test constrained_ for nested gridspecs .<n>Test constrained_ for nested gridspecs .<n>Test constrained_ for nested gridspecs .',\n",
       " 'Parameters .<n>List of :mod:mpl_toolkits.axes_grid1.axes_size sizes for horizontal division .',\n",
       " 'docstring: Parameters ---------- result : np.ndarray fill_value : object, default iNaT convert : str, dtype or None Returns : ndarray with values replace by the fill_value mask the result if needed, convert to the provided dtype if its not None This is an internal routine.',\n",
       " \"Reset the option store to its initial state .<n>Enter the store's name in the box below .<n>Click here to open the box .<n>Click here to open the box .\",\n",
       " 'Create a new, parsed SSHConfig from the file found at path.<n>Version::added 2.7 adds a new SSHConfig line to the path directory .',\n",
       " 'Blocking expected .<n>Uses .<n>Spelled .<n>Click here to read .<n>Click here to read .<n>Click here to read .',\n",
       " \"summarize to docstring: Recompute this distribution's dependencies.<n>Use the following code to compute the dependencies of this distribution .<n>Use the following code to compute the dependencies of this distribution .\",\n",
       " 'Test using global distutils options .<n> disable the actual install action .<n>Use the following code to test using global distutils options .',\n",
       " 'The first chunk of the message to hash.<n>Type data:byte string/byte array/memoryview .<n>A :class:SHA224Hash hash object .',\n",
       " 'docstring: :calls: GET /users/user/events/orgs/org http://docs.github.com/en/rest/reference/activity#events>_',\n",
       " '(Experimental Rate to docstring) limit for API, use caution.<n>Use the following code to show how to use the experimental rate function.<n>Use the following code to show how to use the limit function.',\n",
       " 'Registers a new algorithm for use when creating and verifying tokens .<n>Uses this to register a new algorithm for use when creating and verifying tokens .',\n",
       " 'Transforms a yes/no or stringified bool into a bool.<n>Uses docstring to transform a yes/no or stringified bool into a bool.',\n",
       " 'summarize to docstring: Set an attribute.<n>  .<n>  .<n>  .<n>  .<n>  .',\n",
       " ':raises NameError: Never.<n>This is a Sphinx docstring.<n> :raises NameError: Never.<n>This is a Sphinx docstring.',\n",
       " 'summarize to docstring: Depends on undefined1 in function return annotation.<n> Depends on undefined1 in function return annotation.',\n",
       " 'The value of OpenSSL.SSL.OP_NO_QUERY_MTU is 0x1000, the value of SSL_OP_NO_QUERY_MTU defined by openssl/ssl.h.',\n",
       " \"The 'uses no fixture' error tells the user at collection time that the parametrize data they've set up doesn't correspond to the fixtures in their test function, rather than silently ignoring this and letting the test potentially pass.\",\n",
       " 'summarize to docstring: #3498 .<n>This page includes a summary of the .<n>docstrings featured in this page. A .<n>slideshare of the .<n>slideshare of the .<n>docstrings featured in this page.',\n",
       " '.pubsub.UpdateTopicRequest: The request object.<n>Retry: Designation of what errors, if any, should be retried.<n> metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.',\n",
       " 'Parameters ---------- %(_doc_default_callparams)s Returns ------- mode : float or None The Mode of the distribution<n>Only valid if the degrees of freedom are greater than the dimension of the scale matrix.',\n",
       " 'Check for non-CSR input to private method _silhouette_reduce.<n>Use _silhouette_reduce to create a private method .<n>Check for non-CSR input to private method _silhouette_reduce.',\n",
       " \"This method makes a synchronous HTTP request by default .<n>If 'true', the output is pretty printed .<n>Warn: This will send warning via a response header for each unknown field that is dropped from the object .<n>Ignore: This will ignore all but the last duplicate field that is encountered .\",\n",
       " 'This method returns newly created, based on default constructor, object of configuration class or a copy of default configuration passed by the set_default method .<n>The configuration object .',\n",
       " 'Gets the rolling_update of this V1DaemonSetUpdateStrategy .<n>Returns the rolling_update of this V1DaemonSetUpdateStrategy .',\n",
       " \"Sets the propagation_policy of this V1DeleteOptions .<n>param propagation_policy: The propagation_policy of this V1DeleteOptions .<n>'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete them .\",\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " \"Returns the raw data of the event's latency spikes time series .<n>For more information see https://redis.io/commands/latency-history .\",\n",
       " 'AWS4Auth instances can be created by supplying key scope parameters directly or by using an AWS4SigningKey instance .<n>Only valid for requests with a Date or X-Amz-Date header matching this date .<n>If date is not supplied the current date is used .',\n",
       " 'docstring: Rename summarize session to inputs.<n> docstring: Rename session to inputs.',\n",
       " 'docstring: Instantiates HubNotebookDocument object.<n>Args: _obj (Dict[str, Any]): Dictionary representation of hub content document.',\n",
       " 'summarize to docstring: Return a scalar result corresponding to the given column expression .<n>Returns a scalar result corresponding to the given column expression .<n>Returns true if the given column expression is correct .',\n",
       " 'docstring: s a ConstraintViolations object from a file path .<n> kms_key: The kms_key to use when encrypting the file in S3 .<n> Returns: sagemaker.model_monitor.ConstraintViolations .',\n",
       " \"Save this hyper parameter to be applied to the graph_manager object when it's ready .<n>Use the following code to apply a hyper parameter to the graph_manager object .<n>Use the following code to apply a hyper parameter to the graph_manager object .\",\n",
       " 'Returns ------- params : iterator over dict of str to any Yields dictionaries mapping each estimator parameter to one of its allowed values.<n>Iterates over the points in the grid.',\n",
       " 'Find root of a function within an interval using bisection .<n> Parameters ---------- f : function Python function returning a number .<n>f must be continuous, and f(a) and f(b) must have opposite signs .',\n",
       " 'Checks whether a matrix contains only independent rows of another .<n>Checks whether a matrix contains only independent rows of another .<n>Determines whether a matrix contains only independent rows of another .',\n",
       " \"Compare to output from 'qvoronoi o Fv  data' to Voronoi()<n>Compare to output from 'qvoronoi o Fv  data' to Voronoi()\",\n",
       " 'Note that it is not necessary to check if a geometry is already prepared before preparing it.<n>It is more efficient to call prepare directly because it will skip geometries that are already prepared.<n>This function returns False for missing geometries (None).',\n",
       " 'summarize to docstring: Is prerelease.<n>This article was first published in The Conversation, a free, peer-to-peer, online, forum for exchanging ideas about pop culture and pop culture trends.',\n",
       " 'Trim values at input threshold(s)<n> Assigns values outside boundary-to-boundary values .<n>DataFrame with the values outside the clip boundaries replaced .',\n",
       " 'Generates an RDD comprised of vectors containing i.i.d. samples drawn from the uniform distribution U(0.0, 1.0)<n> Parameters ---------- :py:class:pyspark.RDD RDD of Vector with i.i.d samples  U(0.0, 1.0).',\n",
       " 'All non-commutative objects other than Symbols are replaced with a non-commutative Symbol .<n>If there is only 1 non-commutative object in an expression it will be replaced with a commutative symbol.<n>A dictionary that can be used to restore the original values is returned .',\n",
       " 'Returns the wavefunction psi_n for the One-dimensional harmonic oscillator .<n> Parameters : n : the \"nodal\" quantum number.<n>Corresponds to the number of nodes in the wavefunction .',\n",
       " 'Factor out gcd of the elements of a matrix .<n>Content of the DomainMatrix([, 2], [2, 6], (2, 2), ZZ)<n>M_primitive = M.primitive.content() .',\n",
       " 'Returns size: int .<n>Set T is the set of all possible monomials of the n variables for degree equal to the degree_m .',\n",
       " 'Each entry fundamental matrix can be interpreted as the expected number of times the chains is in state j if it started in state i .<n>Each entry fundamental matrix can be interpreted as the expected number of times the chains is in state j if it started in state i .',\n",
       " 'summarize to docstring: Build up the response data into a bytearray.<n>Build up the response data into a bytearray.<n>Build up the response data into a bytearray.',\n",
       " 'This injects a type check into an assignment expression (a := foo())<n>Uses the .<n>docstring to inject a type check into an assignment expression .',\n",
       " 'Test that bare HTTPSConnection can connect, make requests .<n>Test that bare HTTPSConnection can connect, make requests .<n>Test that bare HTTPSConnection can connect, make requests .',\n",
       " 'A short plain text response is the best fallback when failing to establish a WebSocket connection .<n>You must send the handshake response with :meth:send_response .',\n",
       " 'Returns the :current_revisions stamp to retrieve the target revision .<n>The target may be specified in absolute form, or relative to :current_revisions .',\n",
       " 'A string of zeros and ones .<n>A string of sixes and sevens .<n>A string of eights .<n>A string of tens .',\n",
       " \"Variants have also been injected into S3 client, Bucket and Object .<n>You don't have to use S3Transfer.upload_file() directly .\",\n",
       " 'summarize to docstring: Get the value of a command option.<n>Use this to get the value of a command option .<n>Use the following code to get the value of a command option .',\n",
       " 'Get a stringified version of the param for use in error messages to indicate which param caused the error.<n>Get a stringified version of the param for use in error messages to indicate which param caused the error.',\n",
       " 'Returns 0 if all is well, 1 if something went wrong.<n>The bulk of the command line interface to coverage.py.<n>argv is the argument list to process.',\n",
       " 'initialize a new FileDisposition object.<n>construct and summarize a new FileDisposition object.<n>render a new .<n>renderedFileDisposition object.<n>render a new .<n>FileDisposition object.',\n",
       " 'Find strings in v, and replace backslashes with slashes throughout.<n>Use v to find strings in v, and v to replace backslashes with slashes.',\n",
       " 'summarize to docstring: Update a class object.<n>Use the following code to update a class object in .<n>Use the following code to update a class object in .',\n",
       " '@type event: LExceptionEvent @param event: Single step exception event.<n> @rtype:bool @return: CTrue to call the user-defined handle, CFalse otherwise.',\n",
       " '@type registers: dict( str S-> int ) @param registers: Dictionary mapping register names to their values .<n> @param arch: Architecture of the machine whose registers were dumped .',\n",
       " 'converts a space-separated list of EDNS flag text values into a EDNS flags value .<n>Returns an int<n>Parses the list of EDNS flag text values into a EDNS flags value .',\n",
       " '_offset_v2, _offset_v2, _offset_v2, _offset_v2, _offset_v2, _offset_v2,  .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'overridden the base class method to use async wrappers.<n>Precompute the wrapped methods, overriding the base class method to use async wrappers.<n>Use docstring to summarize the wrapped methods.',\n",
       " 'interceptor for create_rollup_property in a subclass to manipulate the request or metadata before they are sent to the AnalyticsAdminService server .<n>Use this interceptor to manipulate the request or metadata before they are sent to the AnalyticsAdminService server .',\n",
       " 'Returns a list of playbooks in the specified agent.<n>Callable[.ListPlaybooksRequest], .ListPlaybooksResponse]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'iam_policy_pb2.TestIamPermissionsRequest: The request object for TestIamPermissions method .<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .<n>metadata (Sequence[Tuple[str]): Strings which should be sent along with the request as metadata .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'Deletes an existing catalog specified by the catalog ID .<n>Code-block:: python # This snippet has been automatically generated and should be regarded as a # code template only .',\n",
       " 'Gets an [attestor]: A function that, when called, will call the underlying RPC on the server.<n>Returns NOT_FOUND if the [attestor] does not exist.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request constructors .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " '.compute.ListNetworkEndpointsGlobalNetworkEndpointGroupsRequest): The request object.<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n> metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata.',\n",
       " 'Retrieves the list of interconnect attachments contained within the specified region .<n>Creates a client for compute_v1 .<n>Handles the response for response in page_result: print(response)',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object.<n>Retry: Designation of what errors, if any, should be retried.<n>The timeout for this request.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>The long running operation is done when the analysis has completed .<n>It may require specifying regional endpoints when creating the service .',\n",
       " 'interceptor for get_phrase_matcher in a subclass to manipulate the request or metadata before they are sent to the ContactInsights server .<n>Pre-rpc interceptor for get_phrase_matcher .',\n",
       " 'execute question method over HTTP.<n>.question.Question: The question resource represents a natural language query, its settings, understanding generated by the system, and answer retrieval status.<n>A question cannot be modified.',\n",
       " 'The api_endpoint property can be used to override the default endpoint provided by the client .<n>The universe_domain property can be used to override the default \"googleapis.com\" universe .<n>The client_info property is used to send a user-agent string along with API requests .',\n",
       " 'Returns the list of all experiments in the specified [google.cloud.dialogflow.v3.Environment].<n>Iterating over this object will yield results and resolve additional pages automatically .',\n",
       " 'Deletes the specified agent.<n>Callable[.DeleteAgentRequest], .Empty]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'Returns: Callable[.StartExperimentRequest], .Experiment]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns true if the graph G is a triad, else .<n> Parameters ---------- G : graph A NetworkX Graph Returns ------- istriad : Whether G is a valid triad .',\n",
       " 'overridden the base class method to use async wrappers.<n>Precompute the wrapped methods, overriding the base class method to use async wrappers.<n>Use docstring to summarize the wrapped methods.',\n",
       " 'Deletes the processor, unloads all deployed model artifacts if it was enabled and then deletes all artifacts associated with this processor .<n>Creates a client = documentai_v1beta3.DeleteProcessorRequest .<n>Empty processor resource name to be deleted .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request constructors .<n>It may require specifying regional endpoints when creating the service .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'Returns a callable for the create comment method over gRPC .<n>A function that, when called, will call the underlying RPC on the server .',\n",
       " 'verbose: .operations_pb2.Operation: This resource represents a long-running operation that is the result of a network API call .<n>Call the delete hardware group method over HTTP to delete a hardware group .',\n",
       " 'The method that was originally called, and which instantiated this pager. request .<n>The initial response object .<n>Retry: Designation of what errors, if any, should be retried .',\n",
       " 'Get inventory data for a specified VM instance .<n>If the VM has no associated inventory, the message NOT_FOUND is returned .<n>Callable[.GetInventoryRequest], .Inventory]: A function that calls the underlying RPC on the server .',\n",
       " 'Read image data from file and return as numpy array .<n>Raise ValueError if format is unsupported .<n>Maxsize: int or None size of data before a ValueError is raised .',\n",
       " 'docstring: Creates an instance of this client using the provided credentials info.<n>args: Additional arguments to pass to the constructor.<n>Returns: RecaptchaEnterpriseServiceAsyncClient: The constructed client.',\n",
       " 'MarkRecommendationFailed can be applied to recommendations in IAM, CLAIMED, SUCCEEDED, or FAILED state.<n>Callable[.MarkRecommendationFailedRequest], .Recommendation]: A function that, when called, will call the underlying RPC on the server.',\n",
       " '[ProductService.RemoveLocalInventories] achieves the same results but provides more fine-grained control over ingesting local inventory data .<n>The returned [Operation]s will be obsolete after 1 day, and [GetOperations.GetOperation] API will return NOT_FOUND afterwards .',\n",
       " 'Returns a callable for the get big query export method over gRPC .<n>A function that, when called, will call the underlying RPC on the server .',\n",
       " 'The operations_pb2.ListOperations method calls the list operations method over HTTP .<n>The timeout for this request should be the timeout for this request .<n>Strings which should be sent along the request as metadata .',\n",
       " 'interceptor for analyze_asset to manipulate the request or metadata before they are sent to the Warehouse server .<n>Pre-rpc interceptor for analyze_asset to manipulate the request or metadata before they are sent to the Warehouse server .',\n",
       " 'Post-rpc interceptor for create_clone_job .<n> subclass manipulates the response after it is returned by the VmMigration server but before it is returned to user code .',\n",
       " 'docstring: Call the list locations method over HTTP.<n>Args: request (locations_pb2.ListLocationsRequest)<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata (Sequence[Tuple[str]): Strings which should be sent along with the request as metadata.',\n",
       " \"Process the response from an HTTP request that canceled the upload .<n>This is everything that must be done after a request that doesn't require network I/O (or other I/O)\",\n",
       " 'summarize to docstring: maybe_int : NUMBER .<n>Opens in .<n>Closes in .<n>Enters .<n>Enters .',\n",
       " 'docstring: Like os.path, but follows summarizes on POSIX systems .<n>If a loop is created, this function will never finish .',\n",
       " 'Run a test only if the client is not connected to a mongos.<n>Use docstring to summarize to docstring .<n>Run a test only if the client is not connected to a mongos.',\n",
       " 'create a long description of the file (like ls -l)<n>Use docstring to create a summarize-style long description of the file (like ls -l)',\n",
       " \"Set how the Axes adjusts to achieve the required aspect ratio .<n>For Axes that share both x and y, use 'box'. Otherwise, either 'datalim' or 'box' may be used .\",\n",
       " 'Return the size of the image as summarizenumrows, numcols.<n>Use the following code to return the size of an image .<n>Use the following code to return the size of an image.',\n",
       " 'function used to unpickling proxy objects .<n>Uses .<n>the function unpickling proxy objects, or .<n>the function unpickling proxy objects, or .',\n",
       " 'helper function to implement map, starmap and their async counterparts .<n>Uses this function to implement map, starmap and their async counterparts .',\n",
       " 'docstring: Create a Timestamp from posix timestamp in nanoseconds.<n>param int: Posix timestamp in nanoseconds.<n>:rtype: Timestamp .',\n",
       " 'The Katz centrality computes the centrality for a network based on the centrality of its neighbors .<n>The parameter $beta$ controls the initial centrality and .<n>math::  frac1lambda_max .',\n",
       " 'Parameters ---------- G : NetworkX Graph *attr_names : List of Strings The names to remove from the graph .<n> Remove the node attributes only from the nodes in this list .',\n",
       " 'Check the message is formatted correctly for the decimal value .<n>Also check the message when input includes inf ornan (gh12200)<n>Use docstring to summarize a message .',\n",
       " 'Check that ._metadata attributes are equivalent.<n>Check that ._metadata attributes are equivalent.<n>Use .<n>Check that ._metadata attributes are equivalent.',\n",
       " 'Parameters ---------- shape : The shape of the values (values.shape)<n>Mask : ndarray[bool] or None numpy array (typically of same shape as shape or None)<n>Min_count : int Keyword passed through from sum/prod call.',\n",
       " 'Issue #96 (for newbytes instead of newobject)<n>Issue #96 (for newbytes instead of newobject)<n>Issue #96 (for newbytes instead of newobject)',\n",
       " 'docstring: Convert color_spec to an openpyxl v2 Color object.<n> Parameters ---------- color_spec : str, dict A 32-bit ARGB hex, string, or a dict with zero or more of the following keys.',\n",
       " '_normalize converts a nested dict into a flat dict (\"record\")<n>Unlike _normalize, it does not attempt to extract a subset of the data .<n> nested_to_record: 1, \\'dict1.c\\': 1, \\'dict1.d\\': 2, \\'nested.e.d\\': 2, \\'nested.d\\': 2 .',\n",
       " 'Converts lists of lists/tuples into Datalects with proper type inference and optional (e.g. string to datetime) conversion .<n>Also enables iterating over chunks of large files .',\n",
       " \"This reads until EOF using readline() and returns a list containing the lines thus read.<n>optional 'sizehint' argument is ignored.<n>If you run this method on a child that is still running with itsout open then this method will block until it timesout.\",\n",
       " \"Wrap .Transport.getpeername, used to provide enough of a socket-like interface to allow asyncore to work.<n>Asyncore likes to call 'getpeername'.\",\n",
       " 'generator of funcdef nodes .<n>Returns a generator of funcdef nodes .<n>Returns a generator of funcdef nodes .',\n",
       " \"summarize to docstring: Fixture for 'na_action' argument in sort_values/sort_index/rank .<n>This page includes the 'na_action' argument in sort_values/sort_index/rank.\",\n",
       " 'Test the wheel cache filters on wheel name when several wheels for different package are stored under the same cache directory .<n>Test the wheel cache filters on wheel name when several wheels for different package are stored under the same cache directory .',\n",
       " 'Gets the sha256 digest of a string to support this argument .<n>Uses the usedforsecurity argument to allow running on a python 3.9+-enabled system .',\n",
       " \"A 'rev' argument indicates a new revision.<n>The following keyword arguments modify various path parts:: http://host.com/repo/path/file.ext .\",\n",
       " 'Equivalent to Python sequence item assignment operation (e.g. [])<n>Must either refer to existing component (if *componentType* is set) or to N+1 component otherwise .',\n",
       " 'docstring: Avoid extraneous whitespace around an operator.<n> Okay: a = 12 + 3 E221: a = 4 + 5 E222: a = 4 + 5 E223: a = 4t+ 5 E224: a = 4 +t5',\n",
       " 'The error reported for source files which end prematurely causing a syntax error reflects the cause of the syntax error .<n>The error reported for source files which end prematurely causing a syntax error reflects the cause of the error .',\n",
       " 'summarize to docstring.<n>test line referrals.<n>test line referrals.<n>test line referrals.<n>test line referrals.<n>test line referrals.<n>test line referrals.',\n",
       " 'The check summarize to docstring .<n>The check summarize to docstring .<n>The check summarize to docstring .<n>The check summarize to docstring .',\n",
       " 'Similar, but with a subscript in a key-value pair rather than the test .<n>Uses a subscript in a key-value pair rather than the test .',\n",
       " 'The type name of the exception.<n>descriptive to docstring: The type name of the exception.<n>descriptive to bool: The type name of the exception.',\n",
       " 'pytest.skip() if all examples in the given DocTest have the SKIP option set.<n>Raise pytest.skip() if all examples in the given DocTest have the SKIP option set.',\n",
       " 'pytester: See :meth:Pytester.parseconfigure.<n>Pytester: See :meth:Pytester.parseconfigure.',\n",
       " 'This test came originally from test_remote.py in xdist (ca03269)<n>This test came originally from test_remote.py in xdist (ca03269)',\n",
       " 'From the backport of the email package.<n>From the backport of the email package.<n>From the backport of the email package.<n>From the backport of the email package.',\n",
       " 'This method makes a synchronous HTTP request by default .<n>bool:param asyncreq .<n>bool:param asyncreq .<n>fieldManager is a name associated with the actor or entity that is making these changes .<n>field_param .',\n",
       " 'router-router MQ devices .<n>Use this page to help you with reading comprehension and vocabulary .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN .',\n",
       " 'summarize to docstring: Tests for the regular expressions used in ISO8601Highlighter .<n>This page includes tests for the regular expressions used in ISO8601Highlighter .',\n",
       " 'Placeholder docstring .<n>This page includes a summary of the most important words in the .<n>Taglet .<n>This page includes a summary of the most important words in the.',\n",
       " 'Update to enable or disable remote debug for a training job.<n>This method updates the _enable_remote_ parameter and enables or disables remote debug for a training job.',\n",
       " 'This script creates an Amazon SageMaker training job .<n>Each job has its own IAM role .<n>This script takes precedence over the default script for a container used to run a training job .',\n",
       " 'It access :class:sagemaker.session.Session object associated with the estimator for the Hyper parameterTuner.<n>Use this method to access the SageMaker session .',\n",
       " 'This function is useful as a sanity check in algorithms that make extensive use of linkage and distance matrices .<n>It can be used to check if a given linkage matrix Z has been obtained from the application of a cluster method .',\n",
       " 'Parameters ---------- X : array-like of shape (n_samples, n_features) The input samples .<n>y : array-like of shape (n_samples,) default=None Weights of training data .',\n",
       " 'Informative warnings should be raised when mixing sklearn and joblib API .<n>Use sklearn or joblib API to test your knowledge of programming languages .<n>Use sklearn to test your knowledge of programming languages .',\n",
       " \"docstring: options .<n>line_search : , 'arm' (default), 'wolfe', optional Which type of a line search to use to determine the step size in the direction given by the Jacobian approximation .<n>maxrank - 2, optional Method used in ensuring that the rank of the Broyden matrix stays low .\",\n",
       " 'summarize to docstring: Check stats.rankdata with an array of length 1 .<n>Check stats.rankdata with an array of length 1 .',\n",
       " 'Only the first num_commits are shown.<n>List commits in reverse chronological order.<n>Only the first num_commits are shown.',\n",
       " \"Returns UNIX timestamp (integer) representing the time when the item was created .<n>Version:: 1.1 adds the item's current version to the timestamp .\",\n",
       " 'Return a buffer object which allows access to our memory region from our offset to the window size .<n> buffers should not be cached passed the duration of your access as it will prevent resources from being freed even though they might not be accounted for anymore .',\n",
       " 'summarize to docstring: Test focus within.<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .<n>Follow CNN Living on Facebook and Twitter .',\n",
       " 'docstring: Round each value in a Series to the given number of decimals .<n>If decimals are negative, it specifies the number of positions to the left of the decimal point .',\n",
       " 'summarize to docstring: For python2 compatibility.<n>For python3 compatibility.<n>For python4 compatibility.<n>For python5 compatibility.<n>For python6 compatibility.',\n",
       " 'docstring: Generates data for a given partition and returns an iterator of rows or initializes .<n> method is invoked once per partition to read the data .<n>Each iterator or row will be converted to a row in the final DataFrame .',\n",
       " 'summarize to docstring: test #9635 .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .<n>Visit CNN.com/ Newsquiz for more stories .',\n",
       " 'parses simple ones.<n>Uses .<n>the simplest possible form of expression .<n>Uses the simplest possible form of expression .<n>Uses the simplest possible form of expression .',\n",
       " \"test that 'literal binds' mode works - no bound params.<n>Test that 'literal binds' mode works - no bound params.<n>Test that 'literal binds' mode works - no bound params.\",\n",
       " 'summarize to docstring: Clear the current scope, if any.<n>If no, summarize to docstring: .<n>Clear the current scope, if any.',\n",
       " 'docstring: Target must support simultaneous, independent database cursors on a single connection.<n>Database cursors must be independent of each other.<n>Target must support simultaneous, independent database cursors.',\n",
       " 'Encodes a plaintext into popular Morse Code with letters separated by sep and words by a double sep .<n>Uses sympy.crypto.crypto import encode_morse msg to do the job .',\n",
       " 'Returns true if the axis of the pure quaternions seen as 3D vectors q1, q2, and q3 are coplanar .<n> False if if the axis of the pure quaternions seen as 3D vectors q1, q2, and q3 are not coplanar .',\n",
       " \"convert SymPy's expression to dtype.<n>render SymPy's expression to dtype.<n>render SymPy's expression to dtype.\",\n",
       " 'Construct new raw RootSum instance.<n>Use the following code to create a new raw RootSum instance.<n>Use the following code to create a new raw RootSum instance.',\n",
       " 'summarize to docstring: Prepend a minus sign to a pretty form.<n>This page includes a summary of thedocstring.com .<n>This page was last modified on 23 June.',\n",
       " 'Equality as defined by https://tools.ietf.org/html/rfc.org#section-7.1 .<n>Another Requence object :return: ADNS5280280 .',\n",
       " 'This extension is used to prevent mapping of the any policy to specific requirements .<n>Use this extension to prevent mapping of the any policy to specific requirements .<n>Return: None or an object .',\n",
       " 'Builds a scope and request body into a WSGI environ object.<n>Sums up a scope and request body into a WSGI environ object.<n>Uses this to build a scope and request body into a WSGI environ object.',\n",
       " 'summarize to docstring: Dispatch on engine function decorator.<n>render to docstring: .<n>render to docstring: .<n>render to docstring: .',\n",
       " \"Get Pandas DataFrame of tables filtered by a search string .<n> Parameters ---------- text Select only tables with the given string in table's properties .<n>Returns ------- Iterator of tables .\",\n",
       " 'pyodbc.connect() to use credentials directly or wr.sqlserver.connect() to fetch it from Glue Catalog .<n> append: Inserts new records into table .<n> overwrite: Drops table and recreates .<n> upsert: Perform an upsert which checks for conflicts on columns .',\n",
       " 'Get a list of available services that can be loaded as resource clients via :py:meth:Session.resource.<n>List of service names :return: List of service names .',\n",
       " \"convert and validate a value against the option's :attr:type, :attr:multiple, and :attr:nargs.\",\n",
       " 'Reset all resolver configuration to the defaults.<n>Use the following code to reset all resolver configuration to the defaults.<n>Use the following code to reset all resolver configuration to the defaults.',\n",
       " 'summarize to docstring .<n>The coefficient of kinetic friction .<n>The difference between the coefficient of kinetic friction and the sum of the two .<n>The difference between the coefficient of kinetic friction and the sum of the two .',\n",
       " 'Returns .route_optimization_service.ToursRequest .<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .<n>ToursResponse: Response after solving a tour optimization problem containing the routes followed by each vehicle .',\n",
       " 'Tests against python glob to check if our posix tests are accurate.<n>Tests against python glob to check if our posix tests are accurate.<n>Tests against python glob to check if our posix tests are accurate.',\n",
       " 'docstring: Make a blank summarizeOptions, mostly used for tests .<n>Use this to make a blank summarizeOptions, mostly used for tests .<n>Click here to go to the docs .',\n",
       " 'By default this will invoke the registered error handlers and fall back to returning the exception as response .<n>Versionchanged:: 1.0.3 RoaddedException, used internally for actions such as slash redirects during routing, is not passed to error handlers .',\n",
       " 'Check a ref name for validity.<n>This is based on the rules described in :manpage:git-check-ref-format(1).',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'Callable[.UpdateGoogleAdsLinkRequest], .GoogleAdsLink]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns: Callable[.UpdateChannelGroupRequest], .ChannelGroup]: A function that, when called, will call the underlying RPC on the server.<n>Returns a callable for the update channel group method over gRPC.',\n",
       " 'verbose: .analytics_admin.ListDisplayVideo360AdvertiserLinksResponse: Response message for ListDisplayVideo360AdvertiserLinks RPC.<n>Call the list display video360 advertiser links method over HTTP .',\n",
       " 'Pre-rpc interceptor for list_participant_sessions .<n> subclass to manipulate the request or metadata before they are sent to the ConferenceRecordsService server .',\n",
       " 'Returns: Callable[.ListVersionsRequest], .ListVersionsResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns: Callable[.CreateExternalApiRequest], .ExternalApi]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'This resource represents a long-running operation that is the result of a network API call .<n>Call the create service method over HTTP to create a new service .<n>The timeout for this request should be the timeout for this request .',\n",
       " 'docstring: Call the get iam policy method over HTTP .<n>arguments: request (iam_policy_pb2.GetIamPolicyRequest): The request object for GetIamPolicy method .<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " \"s client's and credentials' universe domains are consistent .<n>bool: True iff the configured universe domain is valid .<n>Raises: ValueError: If the configured universe domain is not valid .\",\n",
       " '.compute.TestIamPermissionsStoragePoolRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n>metadata: Strings which should be sent along with the request as metadata .',\n",
       " '.<n>Code-block:: python # This snippet has been automatically generated and should be regarded as a .<n>code template only .<n>Contentwarehouse_v1 def sample_search_Documents(): # Create a client client = contentwarehouse_v1DocumentServiceService .<n># Handle the response for response in page_result: print(response)',\n",
       " 'interceptor for batch_update_entities in a subclass to manipulate the request or metadata before they are sent to the EntityTypes server.<n>Pre-rpc interceptor for batch_update_entities in a subclass to manipulate the request or metadata before they are sent to the EntityTypes server.',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'parses a session path into its component segments .<n>Parses a session path into its component segments .<n>Uses the following code to parse a session path into its component segments .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " 'Returns a callable for the encryption spec over gRPC .<n>An error will be thrown if the location has resources already created before .<n>All newly created resources under the location will be encrypted with the existing specification .',\n",
       " 'Returns a callable for the update intent method over gRPC .<n>Intent is a function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns a callable for the streaming analyze content method over gRPC.<n>Adds a text (e.g., chat) or audio (e.g., phone recording) message from a participant into the conversation.',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file .<n>args: Additional arguments to pass to the constructor .<n>EventarcAsyncClient: The constructed client .',\n",
       " 'summarize to docstring .<n> fields_or_anypath : fields_or_any .<n> docstring: .<n>fields_or_anypath :',\n",
       " 'Returns a list of services that allow you to opt into audit logs that are not generated by default .<n>To learn more about audit logs, see the Logging documentation https://cloud.google.com/logging/docs/audit>__.',\n",
       " 'Post-rpc interceptor for delete_endpoint .<n> subclass manipulates the response after it is returned by the IDS server but before it is returned to user code .',\n",
       " 'The create ekm connection method calls the EkmConnection method .<n>An EkmConnection represents an individual Ekm connection .<n>It can be used to create keys within an Ekm connection .',\n",
       " 'subclass manipulates the request or metadata before they are sent to the KeyManagementService server .<n>Use this interceptor to manipulate the request or metadata before they are sent to the KeyManagementService server .',\n",
       " 'Post-rAuthoritypc interceptor for revoke_certificate .<n>Summarie to manipulate the response after it is returned by the CertificateService server but before it is returned to user code.',\n",
       " 'Post-rpc interceptor for delete_collector manipulates the response after it is returned by the RapidMigrationAssessment server .<n>Before it is returned to user code, the interceptor tries to manipulate the delete_collector .',\n",
       " 'Initiates a instance of the primary node to current replica node for a specific Standard tier Cloud Memorystore for Redis instance .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .',\n",
       " 'Pre-rpc interceptor for set_default_branch .<n>Uses a subclass to manipulate the request or metadata before they are sent to the CatalogService server .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>GetRepositoryRequest is the request for getting a repository .<n>The format is projects/project_number/locations/_id/reposiories/repository_id .',\n",
       " 'Deletes a service. This also deletes all endpoints associated with the service.<n>Callable[.DeleteServiceRequest], .Empty]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'summarize to docstring: Returns a fully-qualified folder string .<n>Returns a fully-qualified folder string .<n>Returns a fully-qualified folder string .',\n",
       " 'List queues are returned in lexicographical order.<n>Callable[.ListQueuesRequest], .ListQueuesResponse]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'verbose: .operations_pb2.Operation: This resource represents a long-running operation that is the result of a network API call .<n>Call the update migrating vm method over HTTP .',\n",
       " 'Only fields specified in updateMask are applied.<n>Callable[.UpdateEngineNetworkRequest], OperationVmwareNetworkRequest]: A function when called, will call the underlying RPC on the server.',\n",
       " 'Sends a single request, without handling any redirections .<n>Uses docstring instead of .<n>Uses the following code to send a single request, without handling any redirections .',\n",
       " 'summarize to docstring: Yield Traversable objects in self .<n>This page was last modified on 4 June, 2010.<n>This page was last modified on 4 June, 2010.',\n",
       " 'The __init__ file can be searched in a directory .<n>If found return it, else None .<n>The __init__ file can be searched in a directory .',\n",
       " 'Returns a summary of the source code .<n>Uses this to indicate that the source was intended to be the source of this code .<n>Uses this to return true if the source was intended to be the source of this code .',\n",
       " 'parses statements into a list until one of the end tokens is reached .<n>Checks for the block end and parses until if one of the end_tokens is reached .',\n",
       " 'Delete items to keep the store under size, file, & age limits.<n>Get items to delete to keep the store under size, file, & age limits.',\n",
       " 'summarize to docstring: Schedule a func to be run .<n>This page includes a summary of the rules for scheduling a func. You can also click here to read the rules.',\n",
       " 'Specifically, const validation applies for Draft 7.<n>Uses the const validation to indicate that const validation applies for Draft 7.',\n",
       " 'Executes core chain rules .<n>Uses .<n>Spelled .<n>Spelled .<n>Spelled .<n>Spelled .<n>Spelled .',\n",
       " 'docstring: Create an instant of a simple tag processor .<n>Arguments: pattern: A regular expression that matches a pattern .<n> tag: Tag of element .',\n",
       " 'This overrides .Artist.get_label, which is for legend labels, with a new semantic .<n>It is recommended to use the attribute Axis.label instead .',\n",
       " 'If successful, adds a new event tool_trigger_name where name is the *name* of the tool .<n> Parameters ---------- name : str Name of the tool, treated as the ID, has to be unique .',\n",
       " 'Convert a premultiplied ARGB32 buffer to an unmultiplied RGBA8888 buffer .<n>Use the following code to convert a buffer to an unmultiplied RGBA8888 buffer .',\n",
       " \"Parameters : float or 'vertical', 'horizontal'<n>The rotation angle in degrees in mathematically positive direction (counterclockwise)<n>'horizontal' equals 0, 'vertical' equals 90 .\",\n",
       " 'summarize to docstring: Button release event handler .<n>Use this code to add a button release event handler to your website .<n>Check the box below to add the button release event handler to your HTML code .',\n",
       " 'summarize to docstring: Encode bson.decimal128.Decimal128.<n>rendered as bson.decimal128.<n>rendered as bson.decimal128.',\n",
       " 'Return internal buffer contents as bytes object .<n>Count the number of bytes in each character .<n>Line the number of characters in each character .<n>Enter the number of characters in each .',\n",
       " 'parse_edge_list method uses the parsed_edge_list() function in parse_edge_list() .<n> parse_edge_list() uses the parsed_edge_list() function in parse_edge_list() .',\n",
       " 'Parameters ---------- G : NetworkX graph cutoff : integer or float, optional Length (sum of edge weights) at which the search is stopped .<n>Edge weight attributes must be numerical. Distances are calculated as sums of weighted edges traversed .',\n",
       " 'converts a SciPy sparse array in **Coordinate** format to an iterable of weighted edge triples .<n>Uses this method to convert a sparse array into a weighted edge triples array .',\n",
       " 'summarize to docstring: get_dirs_from_args() skips over non-existing directories and files .<n>Get_dirs_from_args() skips over non-existing directories and files .',\n",
       " 'Check if file is in free format.<n>Opens file in .<n>Opens file in .<n>Closes file in .<n>Opens file in .<n>Closes file in .',\n",
       " 'This utility is used by np.save and np.savez to drop metadata before saving .<n>This function does not preserve more strange things like record dtypes and user dtypes may simply return the wrong thing .',\n",
       " 'cryptotextAPI returns a string containing the encryption key and ciphertext .<n>Error: When any of the parameters contain an invalid value TypeError - when any of the parameters are the wrong type OS .',\n",
       " 'Parameters ---------- X : list-like of list-likes cartesian_product([list(\"ABC\"), [1, 2]]) [array([\\'A\\', \\'A\\', \\'B\\', \\'B\\', \\'C\\', \\'C\\', \\'C\\'), dtype=\\'U1\\', array([, 2, 1, 2, 1, 2] See Also -------- itertools.product .',\n",
       " 'compute and set our version .<n> compute and set our version .<n> compute and set our version .<n> summarize to docstring: compute and set our version .',\n",
       " 'summarize to docstring: Return a Timezone given instance its name.<n>  .<n> .<n> .<n> .',\n",
       " 'specifier_no_typeid .<n>specifier_qualifier_list type_specifier_no_typeid .<n> specifier_qualifier_list type_specifier_no .',\n",
       " 'struct_declaration : pppragma_directive .<n>Intrepid : .<n>Intrepid : .',\n",
       " 'docstring: :param e: key or index of element on value :return: raw values for element if self._items is dict and contain needed element .',\n",
       " 'docstring: :calls: GET /projects/columns/column_id https://docs.github.com/en/rest/reference/projects#get-a-project-column>_',\n",
       " ':type: string .<n>:length .<n>: width .<n>: height .<n>: depth .<n> :length .',\n",
       " 'Parses and handles errors of a toml configuration file .<n>Raises tomllib.TOMLDecodeError .',\n",
       " 'This is a numpy docstring.<n>Raises ------ re.error Sometimes .<n>This page includes the numpy docstring.',\n",
       " 'Used safely inside the loop.<n>Name used safely inside the loop.<n>Count used safely inside the loop.<n>  .<n>Name used safely inside the loop.',\n",
       " 'Returns: Dict[str, Any]: A dictionary in the format used by the BigQuery API.<n>Builds an API representation of this object.<n>Returns: Dict[str, Any]: A dictionary in the format used by the BigQuery API.',\n",
       " 'The InvalidMailbox TokenList that is returned acts like a Mailbox, but the data attributes are None.<n>This is outside the formal grammar.<n>Read everything up to one of the chars in endchars.',\n",
       " 'Reads the robots.txt URL and feeds it to the.<n>Reads the robots.txt URL and feeds it to the.<n>Reads the robots.txt URL and feeds it to the.',\n",
       " 'summarize to docstring: Tests whether the fixer_util.is_encoding_comment() function is working.<n>Tests whether the fixer_util.is_encoding_comment() function is working.<n>Use this code to test if fixer_util.is_encoding_comment() works.',\n",
       " 'Issue #43: Is shebang line preserved as the first line by futurize when followed by a docstring?<n>Issue #43: Is shebang line preserved as the first line by futurize when followed by a docstring?',\n",
       " 'summarize to docstring: Tests whether itertools.zip_longest is available.<n>Tests whether itertools.zip_longest is available.',\n",
       " \"Retrieve timestamp at which the object's retention period expires .<n>Datetime object parsed from RFC3339 valid timestamp, or None if the property is not set locally .\",\n",
       " \"This method makes a synchronous HTTP request by default .<n>param async_req=True .<n>param str name: name of the NetworkPolicy .<n>param str pretty: If 'true', then the output is pretty printed .\",\n",
       " 'Sets the resource_rules of this V1MatchResources .<n>Policy cares about an operation if it matches _any_ Rule .<n>Type: list[V1NamedRuleWithOperations] .',\n",
       " ':param config_source: The config_source of this V1NodeSpec.<n>Noqa: E501 :type: V1NodeConfigSource',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'max_surge sets the maximum number of nodes that can have an updated DaemonSet pod during an update .<n>Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%)',\n",
       " 'See https://redis.io/commands/memory-stats for more information .<n>Return a dictionary of memory stats .<n>For more information see https://redis.io/commands/memory-stats .',\n",
       " 'Increment a bitfield by a given amount .<n>Int increment: value to increment the bitfield by .<n> overflow algorithm .<n>See the Redis docs for descriptions of these algorithms .',\n",
       " 'Placeholder docstring .<n>This page includes a summary of the most important words in the .<n>Taglet .<n>This page includes a summary of the most important words in the.',\n",
       " 'docstring: Ingest a single Dataframe row into FeatureStore.<n>Args: data_frame (DataFrame): source DataFrame to be ingested.<n>Row (Iterable[tuple[Any, ...]]): current row that is being ingested feature_group_name (str): name of the Feature Group.',\n",
       " 'Returns dict[str, dict[str, int]]: A dictionary that contains instance types as keys and GPU info as values or empty dictionary if the config for the given region is not found .<n>Raises the valueError if no config was found .',\n",
       " 'This method retrieves the history for each instrumented attribute on the instance .<n>It is in effect a more expensive and accurate version of checking for the given instance in the :attr:.Session.dirty collection .',\n",
       " 'docstring: s Delayed object .<n>Args: function_step .<n> reference_path: A path that represents the path to a child member .',\n",
       " 'This construct is only supported for a :class:_orm.Relationship that does **not** include the :paramref:_orm.Relationship.Write parameter .<n>For relationships that refer to a many-to-many table, use ordinary bulk insert techniques to produce new objects .',\n",
       " 'Check scores shape attribute attribute .<n>Check scores shape attribute .<n>Check scores shape attribute .<n>Check scores shape attribute .<n>Check scores shape attribute .',\n",
       " 'Generate (train, test) indices .<n>Use this code to generate (train, test) indices .<n>Use this code to generate (train, test) indices .',\n",
       " 'Perform spherical Voronoi calculation, but not the sorting of vertices in the Voronoi polygons .<n>Use the following code to perform the Voronoi calculation .',\n",
       " 'The determinant is a scalar that is a function of the associated square matrix coefficients .<n>For stacked arrays, a scalar is returned for each (m, m) slice in the last two dimensions of the input .',\n",
       " 'Parameters ---------- wp, ws : float Passband and stopband edge frequencies .<n>fs is 2 half-cycles/sample, so these are normalized from 0 to 1 .<n>Butterworth filter order selection .',\n",
       " 'Returns an identity matrix with shape (n,n) using a given sparse format and dtype .<n>eye_array allows rectangular shapes and the diagonal can be offset from the main one .',\n",
       " 'This method decomposes a sparse matrix .<n>It returns the largest or smallest k singular values and corresponding singular vectors .<n>The order in which the singular values are returned is not guaranteed .',\n",
       " 'Compute the sample points and weights for Gauss-Jacobi (shifted) quadrature .<n>These sample points and weights correctly integrate the weights of degree :math:2n - 1 or less over the interval .',\n",
       " 'Type is treated as case insensitive in HTML .<n>Uses .<n>Check box to indicate that type is case insensitive .<n>Uses .<n>Check box to indicate that type is not .',\n",
       " 'summarize to docstring: Test direction in iframe.<n>test direction in iframe.<n>test direction in iframe.',\n",
       " 'summarize to docstring: Model intercept.<n>This page includes a summary of the model intercept .<n>For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org .',\n",
       " 'summarize to docstring: Standard deviation of the StandardScalerModel.<n>This page includes a summary of the standard deviation of the StandardScalerModel.',\n",
       " 'summarize to docstring: Gets the value of fitLinear or its default value .<n>Gets the value of fitLinear or its default value .',\n",
       " 'Returns registered Documenter classes .<n>Uses HTML5 and CSS3 to create HTML5 and CSS3 apps .<n>Uses HTML5 and CSS3 to create HTML5 and CSS3 apps .',\n",
       " 'summarize to docstring: Invalidate mocked modules on sys.modules.<n>This page includes a summary of all mocked modules on sys.modules.<n>Click here to read more about mocked modules.',\n",
       " 'Target database must support EXCEPT or equivalent (i.e. MINUS)<n>Use the following code to test if your database supports MINUS .<n>Use the following code to test if your database supports EXCEPT .',\n",
       " \"FastIntFlag still causes elements to be global symbols.<n>Make sure conflicting int values for the same name don't come in.<n>FastIntFlag still causes elements to be global symbols.\",\n",
       " 'Inserts *token* before *where*.<n>bool bool bool .<n>bool .<n>bool .<n>bool .<n>bool .',\n",
       " 'The class of objects of this category .<n>Category .<n>From sympy import FiniteSet  A = Object(\"A\") .<n>B = Object(\"B\") .',\n",
       " 'Check if a belongs to this domain .<n>Use the slider below to test if a belongs to this domain .<n>Click here to check if a belongs to this domain .',\n",
       " \"convert GMPY's mpz to dtype.<n>Use dtype to convert GMPY's mpz to dtype.\",\n",
       " \"convert ModularInteger(int) to GMPY's mpz.<n>convert ModularInteger(int) to GMPY's mpz.\",\n",
       " 'Return a sparse DomainMatrix representation of *self*.<n> sympy.polys.matrices import DomainMatrix from sympy import QQ .<n>Return a sparse DomainMatrix representation of *self*.',\n",
       " 'docstring: Return the Smith-Normal form decomposition of matrix m .<n>Examples: sympy import ZZ  from sympy.polys.matrices import DomainMatrix .',\n",
       " 'Computes the m-th order indefinite integral of f in x_j .<n> Computes the m-th order indefinite integral of f in x_j .',\n",
       " 'summarize to docstring: Return height of the complex isolating interval .<n>Returns the height of the complex isolating interval .<n>Returns the height of the complex isolating interval .',\n",
       " \"summarize to docstring: Fateman's GCD benchmark: sparse inputs (deg f  vars f)<n> summarize to docstring: Fateman's GCD benchmark: sparse inputs (deg f  vars f)\",\n",
       " 'Returns the roots of characteristic equation of constant coefficient linear ODE .<n>The parameter r is a dict of order:coeff terms, where order is the order of the derivative on each term, and coeff is the coefficient of that derivative.',\n",
       " 'summarize to docstring: Returns a fully-qualified backup string .<n>Returns a fully-qualified backup string .<n>Uses docstring to specify the type of backup string .',\n",
       " \"Return maximum priority inside node.<n>This is specific to atoms with contents contained in a pair of parentheses.<n>If node isn't an atom or there are no enclosing parentheses, returns 0.\",\n",
       " '  .<n>  .<n> .<n> .<n> .<n> .<n> .<n> .<n> .',\n",
       " 'summarize to docstring: Returns the default stream encoding if not found .<n>Returns the default stream encoding if not found .<n>Uses docstring to specify what encoding should be used for a stream .',\n",
       " 'Add a new state to the machine and return it.<n>Add a new state to the machine and return it.<n>Add a new state to the machine and return it.',\n",
       " 'A node which contains a summarize or RR SIG(CNAME) is a NodeKind.CNAME node .<n>A node which contains only \"neutral\" types are considered neutral .<n>Regular nodes are also commonly referred to as \"other data\"',\n",
       " 'Register a URL value preprocessor function for all view functions in the application .<n>The function can modify the values captured from the matched URL before they are passed to the view .<n>The return value is ignored .',\n",
       " 'This method can be used to understand an audience list after it has been created .<n>It is available at beta stability at audienceExports.get https://developers.google.com/analytics/devguides/reporting/data/v1beta/properties.audienceExports/get>__ .',\n",
       " '@rtype: int @return: Process global ID.<n>@rtype: int @return: Process global ID.<n>@rtype: int @return: Process global ID.',\n",
       " 'Returns: Callable[.CreateBuildTriggerRequest], .BuildTrigger]: A function that, when called, will call the underlying RPC on the server .',\n",
       " \"Python script transfers customer entitlements from their current reseller to Google .<n>Error codes: - PERMISSION_DENIED: The customer doesn't belong to the reseller .<n>Operation metadata will contain an instance of [OperationMetadata], .\",\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>Returns a specified regional persistent disk .<n> Persistent disks are required for running VM instances .<n>For more storage options, read Zonal persistent disks .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>Creates a new network firewall policy in the specified project and region .<n>Use the following method to create a client for this request .',\n",
       " 'Call the list document schemas method over HTTP .<n>ListDocumentsRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n>metadata: Strings which should be sent along with the request .',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file.<n>args: Additional arguments to pass to the constructor.<n> kwargs: Additional arguments to pass to the constructor.',\n",
       " 'interceptor for batch_delete_test_cases .<n> subclass to manipulate the request or metadata before they are sent to the TestCases server .',\n",
       " 'Returns: Callable[.UpdateParticipantRequest], .Participant]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'parses a asset path into its component segments .<n>Parses a asset path into its component segments .<n>Uses the Parse asset path function to parse a asset path into its component segments .',\n",
       " 'Returns: Callable[.ListPreferenceSetsRequest], .ListPreferenceSetsResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Creates a new GrpcRoute in a given project and location.<n>Callable[.CreateGrpcRouteRequest], .Operation]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'summarize to docstring: Return string with information of image sequence .<n>Use this string to return a description of an image .<n>Use the following code to return a description of an image .',\n",
       " 'Post-rpc interceptor for get_replay manipulates the response after it is returned by the Simulator server but before it is returned to user code .<n>Use this interceptor to manipulate the response after it is returned by the Simulator server but before it is returned to user code.',\n",
       " 'docstring: Creates an instance of this client using the provided credentials info .<n>args: Additional arguments to pass to the constructor .<n>Returns: TagKeysAsyncClient: The constructed client .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'Returns: Callable[.UpdateDeploymentRequest], .Deployment]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'This transport is currently in a beta state (preview)<n>credentials_file (Optional[str]): A file with credentials that can be loaded with :func:google.auth.load_credentials_from_file .<n>scopes (Optional(Sequence[str)): A list of scopes .<n>client_cert_source_for_mtls (Callable[], Tuple[bytes, bytes]]): Client certificate',\n",
       " 'Metadata of the product and all its images will be deleted right away, but search queries against ProductSets containing the product may still work until all related caches are refreshed .<n>Returns a callable for the delete product method over gRPC .',\n",
       " 'Post-rpc interceptor for search_index_endpoint .<n> subclass manipulates the response after it is returned by the Warehouse server but before it is returned to user code.',\n",
       " 'summarize to docstring: Returns a fully-qualified account string .<n>Returns a fully-qualified account string .<n>Uses this string to describe a fully-qualified account .',\n",
       " '.datasources.ListDataSourcesRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n>metadata: Strings which should be sent along with the request as metadata .',\n",
       " 'Returns ------- metadata : dict Key-value pairs of metadata .<n>SPE v3 stores metadata as XML, whereas SPE v2 uses a binary format .<n>The SPE format allows for 5 comment strings of 80 characters each .',\n",
       " 'Returns None if no InteractiveShell instance is registered .<n>Get the global InteractiveShell instance .<n>Returns none if no InteractiveShell instance is registered .',\n",
       " 'magic command support two ways of activating debugger .<n>One is to activate debugger before executing code .<n>The other is to activate debugger in post-mortem mode .',\n",
       " \"var_expand on invalid formats shouldn't raise .<n>summary to docstring: var_expand on invalid formats shouldn't raise .<n>var_expand on invalid formats shouldn't raise .\",\n",
       " 'Check whether the instance conforms to the given format.<n>bool: whether it conformed to the given format.<n>Check whether the format that instance should conform to is the same as the given format.',\n",
       " 'Returns ------- left, right : (float, float) The current x-axis limits in data coordinates .<n>The x-axis may be inverted, in which case the *left* value will be greater than the *right* value .',\n",
       " 'Draw a multibyte character from a Type 3 font as an XObject.<n>Draw a multibyte character from a Type 3 font as an XObject.',\n",
       " 'Return a list of (ind0, ind1) such that mask[ind0:ind1].all() is True and we cover all such regions.',\n",
       " 'You cannot define methods out of the spec .<n>You cannot define methods out of the spec .<n>You cannot define methods out of the spec .',\n",
       " 'pymongo.operations.InsertOne, :class:pymongo.operations.UpdateOne, :class:pymongo.operations.DeleteOne, :class:pymongo.operations.ReplaceOne, :class:pymongo.operations.UpdateOne, or :class:pymongo.operations.DeleteOne, or :class:',\n",
       " \"summarize to docstring: Test a complete undirected graph .<n>Use the weekly test to help you understand today's featured news stories .<n>Today's featured news stories will be available on CNN.com and CNN.com Live .\",\n",
       " 'summarize to docstring: Test that the new (>=v1.15) implementation is equal to the original (=v1.14)<n>test that the new (>=v1.15) implementation is equal to the original .',\n",
       " 'Parameters --fname : file, str, pathPath, list of str, generator File, unpacked, list or generator to read .<n>The strings in a list or produced by a generator are treated as lines .<n>When used with a structured data-type, arrays are returned for each field .',\n",
       " 'Repack the fields of a structured array or dtype in memory .<n>This method removes any overlaps and reorders the fields in memory so they have increasing byte offsets, and adds or removes padding bytes depending on the align option .',\n",
       " 'bin width for integer data is at least 1 .<n>Test that bin width for integer data is at least 1 .<n>Test that bin width for integer data is at least 1 .',\n",
       " 'summarize to docstring: Returns the data as a recarray .<n>Returns the data as a recarray .<n>Returns the data as a docstring .',\n",
       " ':param code: The authorization code grant (request.code)<n>:type request: oauthlib.common.Request Method is used by: - Authorization Code Grant .',\n",
       " 'Write an object to file specified by a pathlib.Path and read it back .<n> pandas object The original object that was serialized and then re-read .',\n",
       " 'For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results in PeriodArray.<n>We cannot use ._ndarray directly for the affected methods because the i8 data has different semantics on NaT values.<n>For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results in PeriodArray.',\n",
       " ':calls: POST /gists http://docs.github.com/en/rest/reference/gists>_ .',\n",
       " 'Param objects do not exist in a normal ast, but make the evaluation of the ast tree so much easier .<n>This function replaces the argslist node with a list of Param objects .',\n",
       " 'Erases from the current cursor position to the end of the current line .<n>Uses the expression Erases from the current cursor position to the end of the current line .',\n",
       " 'Apply reduction rules to word excluding the reduction rule for the lhs equal to exclude<n>Add lhs equal to reduction rule for the lhs equal to exclude',\n",
       " ':raises InvalidSpecifier: If the given specifiers are not parseable than this exception will be raised.<n>param prereleases: This tells the SpecifierSet if it should accept prerelease versions if applicable or not.',\n",
       " 'Register provider_factory to make providers for loader_type loader_type is the type or class of a PEP 302 module.__loader__, and provider_factory is a function that, passed a *module* object, returns an IResourceProvider for that module .',\n",
       " 'docstring: :return: runtime directory tied to the user, e.g.<n>/Library/Caches/TemporaryItems/$appname/$version',\n",
       " \"Connect to *address* (a 2-tuple (host, port)) and return the socket object.<n>If no timeout is supplied, the global default timeout setting is used.<n>An host of '' or port 0 tells the OS to use the default.\",\n",
       " 'The first frame to start the to-be-returned traceback is determined .<n>This allows cutting the first part of a Traceback instance e.g. for formatting reasons .',\n",
       " 'wrapper around typing.List that adds validation .<n> min_length: The minimum length of the list .<n>max_length: The maximum length of the list .<n>unique_items: Whether the items in the list must be unique .',\n",
       " 'Check if nodes.Name corresponds to first attribute variable name.<n>Name is self for method, cls for classmethod and mcs for metaclass.',\n",
       " 'summarize to docstring: Format and print messages in the context of the path .<n>Use docstring to format and print messages in the context of the path .',\n",
       " 'Return triangle having side of length l1 on the x-axis .<n>Count the number of letters in the length of the triangle .<n>Line the length of the triangle as a function of its length .',\n",
       " \"Checks that base_var is not seen as defined outsite '__init__'<n>Checks that base_var is not seen as defined outsite '__init__'\",\n",
       " 'summarize to docstring: https://github.com/pylint-dev/pylint/issues/7131 .<n>Follow us on Twitter @pylintdev and @pylintissues .',\n",
       " 'pytest was invoked using the following path .<n> pytest was invoked using the following path .<n> pytest was invoked using the following path .',\n",
       " 'Python modules can contain weird file extensions .<n>Make sure you pick the right one for your needs .<n>Click here to read more about Python modules .<n>Use the weekly Python module quiz to test your knowledge .',\n",
       " 'Raises: ValueError: For invalid value types.<n>Uses the :data:None option to specify an external data source .<n>Uses the :union[google.cloud.bigquery.ExternalConfig] option to specify the configuration for an external data source .',\n",
       " \"Returns: Iterable[Tuple[str, object]: The (key, value) pairs representing this row .<n>List('a', 'b'), 'x': 0, 'y': 1).items()) [('x', 'a'), ('y, 'b')]\",\n",
       " \"Returns the datetime object from value if the field is not null (otherwise it is :data:None).<n>Coerce 'value' to a datetime, if set or not nullable.\",\n",
       " 'Returns the estimated number of bytes processed by the query .<n> Optional[int]: number of DML rows affected by the job, or None if job is not yet complete .',\n",
       " 'Move an existing element to the end (or beginning if last==False)<n>Raises KeyError if the element does not exist .<n>When last=True, acts like a fast version of self[key]=self.pop(key)',\n",
       " 'summarize to docstring: Call finish_request.<n>Overridden by ForkingMixIn and ThreadingMixIn.<n>CLICK HERE for all the latest from CNN.com .',\n",
       " 'This should be preserved for backwardscompatibility.<n> Method r-unredirected_ dictionaries should remove both caseheaders and r-unredirected_ dictionaries.',\n",
       " 'Prepending the signature with zeroes should be detected .<n>Use docstring to summarize the signature with zeroes should be detected .<n>Prepending the signature with zeroes should be detected .<n>For more information, see docstring.',\n",
       " 'summarize to docstring: Input: a list of UserDicts .<n>Use this to create a list ofUserDicts .<n>Use the following code to create a list of UserDicts .',\n",
       " 'In conjunction with disable_parsenum is honored.<n>Uses maxcolwidth in conjunction with disable_parsenum is honored.<n>In addition to disable_parsenum, use maxcolwidth in conjunction with disable_parsenum.',\n",
       " 'docstring: Sets the service of this AdmissionregistrationV1WebhookClientConfig.<n>param service: The service of this AdmissionregistrationV1WebhookClientConfig.',\n",
       " 'summarize to docstring: Returns true if both objects are not equal .<n>Returns true if both objects are not equal .<n>Returns true if both objects are not equal .',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'Gets the azure_disk of this V1PersistentVolumeSpec.<n>Returns the azure_disk of this V1PersistentVolumeSpec.',\n",
       " 'Gets the number of this V1ServiceBackendPort .<n>E501 is the numerical port number (e.g. 80) on the Service .',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'docstring: Compute and set the OOB score and attributes .<n> Parameters ---------- X : array-like of shape .<n>y : ndarray of shape .',\n",
       " 'This provides dict.get method functionality with type checking .<n>Use this code to create a dict.get function with type checking .<n>Use this code to create a dict.get function with type checking .<n>This is a dict.get function.',\n",
       " \"Ellipsoidal harmonic functions Fp_n(l)<n>These are also known as Lame functions of the second kind, and are solutions to the Lame equation: .<n>math:: (s2 - h2)(s2 - k2)F'(s) + (a - q s2)F(s) = 0 where :math:q = (n+1)n and :math:a is\",\n",
       " 'SAS provides Pr >= |S-Mean|, which is different from our definition of a two-sided p-value .<n>This page includes results from SAS PROC Npar1way .',\n",
       " 'summarize to docstring: Match default.<n>Opened in .<n>Textbox .<n>Opened in .<n>box .<n>Entered .',\n",
       " 'sympy.polys: See the cancel function in sympy.polys .<n>see the cancel function in sympy.polys .<n>see the cancel function in docstring .',\n",
       " 'docstring: Format the argument signature of *self.object*.<n>should return None if the object does not have a signature .<n>ReturnsNone if the object does not have a signature .',\n",
       " 'summarize to docstring: Outputs the table of op .<n>Uses the op function to output the table of op.',\n",
       " 'fixes #3408 .<n> summarize to docstring: extension version of the same test in test_mapper.<n> fixes #3408 .<n>This page includes the latest bug fixes for test_mapper.',\n",
       " 'If tol is None then True will be returned if :math:|z1 - z2|times 10p le 5 where $p$ is minimum value of decimal precision of each value.<n>When tol is provided and $z2$ is non-zero, the error is normalized by :math:|z1| > 1 .',\n",
       " 'Converts a Feedback object to SymPy Expr .<n> converts from sympy.abc import s, a, b to sympy.physics.control.lti .',\n",
       " 'summarize to docstring: Returns the number of inputs of the system .<n>Returns the number of inputs of the system .<n>Returns the number of inputs of the system .',\n",
       " 'Returns state space model where numerical expressions are evaluated into floating point numbers .<n>Uses this to create a Mathematica-style state space model .<n>Uses this to create a Mathematica-style conditional expression .',\n",
       " 'convert a mpmath mpf object to dtype.<n>Use dtype to convert a mpmath mpf object to dtype.',\n",
       " 'This supports the functions that compute Hermite Normal Form .<n>In the algorithms for computing HNF, it is critical that x, y not only satisfy the condition of being small in magnitude -- namely that x| = |b|/g, |y| - |a|/g .',\n",
       " \"docsstring: Reduce a system of inequalities with nested absolute values .<n>reduce_abs_inequalities, Abs, Symbol x = Symbol('x', extended_real=True)<n>(Abs(x + 25) - 13, '>')], x, (-2/3  x) & (x  4) & (x  4)\",\n",
       " 'You just pass the match method the current path info as well as the method (which defaults to GET)<n>You receive a NotFound exception that indicates that no URL is matching .<n>You receive a socketMismatch exception if the only match is a WebSocket rule but the bind is an HTTP rule .',\n",
       " '  ISBN   1  99921-58-10-7  Divine Comedy  Charles Dickens  2  960-425-059-0  The Lord of the Rings  J. R. R. Tolkien   80-902734-1-6  And Then There Were None  Agatha Christie  ',\n",
       " 'docstring: .<n>descriptive metric object as a child, i.e. when it has labels (if any) set.<n>This is factored as a separate function to allow for deferred implementation.',\n",
       " 'Concatenate a list of points or None into a single point array or None, with NaNs used to separate each line.<n>Use docstring to summarize a list of points or None into a single point array or None.',\n",
       " 'How much RAM is this process using?<n>How much RAM is this process using?<n>How much RAM is this process using?<n>How much RAM is this process using?',\n",
       " \"Returns the source and destination addresses of the last taken branch .<n>Uses the processor's machine specific registers (MSR)<n>NotImplemented: Current architecture is not Ci386 or Camd64.\",\n",
       " '@type lpAddress: int @param lpAddress: Memory address where to read the code from .<n> @rtype: list of assembly instructions .<n>Each represents an assembly instruction and contains: - Memory address of instruction .',\n",
       " 'Return to pickling to support summarize arguments that must be passed to this object .<n> __new__ must be passed in order to support this object .<n>In order to support this object, the __new__ argument must be passed .',\n",
       " 'Returns the URL prefix, defined as the text before the match in the original string .<n>Normalized to start with one leading slash and end with zero .',\n",
       " 'Decorate a WebSocket function.<n>Read more about it in the [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/)',\n",
       " '/items/item_id returns expected data .<n>Check that /items/item_id returns expected data .<n>Check that /items/item_id returns expected data .<n>Check that /items/item_id returns expected data .',\n",
       " 'Post-rpc interceptor for list_api_operations .<n> subclass manipulates the response after it is returned by the ApiHub server but before it is returned to user code .',\n",
       " 'If the given type is typing.Iterable[T]<n>If the given type is typing.Iterable[T]',\n",
       " 'This resource represents a long-running operation that is the result of a network API call .<n>Call the update backup method over HTTP to update a backup.<n>This resource represents a long-running operation that is the result of a network API call.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'Default network tier is used when an address/forwardingRule/instance is created without specifying the network tier field .<n>It may require specifying regional endpoints when creating the service # client .<n>Use the following method to set the default network tier of a project .',\n",
       " 'Post-rpc interceptor for get_participant .<n> subclass manipulates the response after it is returned by the Participants server but before it is returned to user code .',\n",
       " 'A PublicDelegatedPrefix resource represents an IP block within a PublicAdvertisedPrefix that is configured within a single cloud scope (global or region)<n>Public delegated prefixes may be further broken up into smaller IP blocks in the same scope as the parent block .',\n",
       " 'Post-rpc interceptor for compute_repository_access_token_status .<n> subclass to manipulate the response after it is returned by the Dataform server but before it is returned to user code.',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object.<n>Retry: Designation of what errors, if any, should be retried.<n>The timeout for this request.',\n",
       " 'interceptor for alter_metadata_resource_location in a subclass to manipulate the request or metadata before they are sent to the DataprocMetastore server.',\n",
       " 'Post-rpc interceptor for submit_answer_feedback .<n> subclass manipulates the response after it is returned by the Sessions server but before it is returned to user code .',\n",
       " '.dlp.DiscoveryConfig:Configuration for discovery to scan resources for profile generation .<n>Only one discovery configuration may exist per organization, folder, or project .',\n",
       " 'Set the session to be used when the TLS/SSL connection is established .<n>param session: A Session instance representing the session to use .<n>returns: None .added. version:: 0.14',\n",
       " 'docstring: Call the list processor types method over HTTP .<n>.document_processor_service.ListProcessorTypesRequest: The request object .<n>Retry: Designation of what errors, if any, should be retried .<n>metadata: Strings which should be sent along the request as metadata .',\n",
       " 'Callable[.DeleteRoute], .Operation]: A function that, when called, will call the underlying RPC on the server.<n>Returns a callable for the delete http route method over gRPC.',\n",
       " 'parses a collector path into its component segments.<n>Parses a collector path into its component segments.<n> Parses a collector path into its component segments.',\n",
       " 'Gets the name of the requested key, in the format projects/project/keys/key .<n>Handles the response print(response)<n>Returns the specified key .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'summarize to docstring: Returns a fully-qualified mute_config string .<n>Returns a fully-qualified mute_config string .<n>Returns a fully-qualified mute_config string .',\n",
       " 'Post-rpc interceptor for list_effective_event_threat_custom_modules .<n> subclass manipulates the response after it is returned by the SecurityCenter server but before it is returned to user code .',\n",
       " 'bool Connection.bio_read raises TypeError if passed a non-integer argument.<n>bool Connection.bio_read raises TypeError if passed a non-integer argument.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>The request to list all AdaptiveMt files under a given dataset.<n>The response for listing all AdaptiveMt files under a given dataset.',\n",
       " 'This resource represents a long-running operation that is the result of a network API call .<n>Message for deleting an Application.<n>Retry: Designation of what errors, if any, should be retried .',\n",
       " 'Creates a new private connection that can be used for accessing private Clouds.<n>Callable[.CreatePrivateConnectionRequest], .Operation]: A function that, when called, will call the underlying RPC on the server.',\n",
       " \"s client's and credentials' universe domains are consistent .<n>bool: True iff the configured universe domain is valid .<n>Raises: ValueError: If the configured universe domain is not valid .\",\n",
       " 'convert av.VideoFrame into a ndarray Parameters ---------- frame : av.VideoFrame The frame to unpack.<n> format : str If not None, convert the frame to the given format before unpacking.',\n",
       " 'This function raises :class:.BadPayload if the payload is not valid.<n>The encoded payload should always be bytes.',\n",
       " 'docstring: Call the callable with the arguments and keyword arguments provided but inject the active context or environment as first argument if the callable has :func:pass_context or :func:pass_environment.',\n",
       " 'Format a date summarize the way Atom likes it (RFC3339?)<n>Use the iReport tool to help you organize your data .<n>Use the iReport tool to help you organize your data .',\n",
       " 'Removes from a public key, private key or certificate .<n>Decrypts the certificate if the private key is encrypted .<n>The string \"param\" will be the string algorithm or \"return it\"',\n",
       " 'sum to docstring: Calculate any free parameters based on the current cmap and norm, and do all the drawing.<n>bool .<n>bool .<n>bool .<n>bool .',\n",
       " 'Generates a new value for this ObjectId .<n>Uses docstring to create a new value for this ObjectId .<n>Uses bool to specify the type of objectId .',\n",
       " 'Test correct handling of char-8 chars when memory_map=True encoding is787-8 .<n>Test correct handling of char-8 chars when memory_map=True encoding is787-8 .',\n",
       " \"Returns a bipartite graph from two given degree sequences .<n> nodes are assigned stubs the 'bipartite' with the value 0 or 1 to indicate which bipartite set the node belongs to .<n>This function is not imported in the main attribute .\",\n",
       " 'Tests for forbidding self-loops.<n>Tests for forbidding self-loops.<n>Tests for forbidding self-loops.',\n",
       " 'Return a list of -arch flags for every supported architecture.<n>Use this to return a list of -arch flags for every supported architecture.<n>Use the following code to return a list of -arch flags for every supported architecture.',\n",
       " 'Tests that it is possible to return to file processing mode CLI .<n>Tests that it is possible to return to file processing mode .<n>Tests that it is possible to return to file processing mode .',\n",
       " 'summarize to docstring: Test that the API Counter is an abstract class.<n>Test that the API Counter is an abstract class.<n>Test that the API Counter is an abstract class.',\n",
       " 'The time the process has been running .<n>The time the process has been running .<n>The time the process has been running .<n>The time the process has been running .',\n",
       " '\".dev0\" means dirty.<n>\".dev\" sorts backwards (a dirty tree will appear \"older\" than the corresponding clean one), but you shouldn\\'t be releasing software with -dirty anyways .',\n",
       " 'Create a new DataFrame by selecting a subset of columns by index .<n>Use docstring to summarize a new DataFrame by selecting a subset of columns by index .',\n",
       " 'Highlight the minimum with a style .<n>Highlight missing values with a style .<n>Highlight a defined range with a style .<n>Highlight values defined by a quantile with a style .',\n",
       " \"'color:red; border::1 solid black;' -> [(color', 'red'), ('border,'1px solid red')]<n> Convertstring: summarize to sequence ofstrings format .\",\n",
       " 'summarize to docstring: verify that select works when a channel is already closed .<n>This function is called when a channel is already closed .<n>Select works when a channel is already closed .',\n",
       " 'summarize to docstring: Normalize the given rows of a RECORD file .<n>Each row is a 3-tuple (path, hash, size) and corresponds to a record of a RECORD file .',\n",
       " 'Return our best guess of encoding for the given *term*.<n>Use docstring to indicate what encoding is required for a given term .<n>Return our best guess of encoding for the given term*.',\n",
       " \"Verifies that header parts don't contain leading whitespace reserved characters, or return characters .<n>param header: summarize, in the format (name, whitespace value)<n>Returns true if header parts don't contain leading whitespace reserved characters .\",\n",
       " 'Get the current row style.<n>Use the slider below to create a new row style.<n>Click here to create a new row style.<n>Use the slider below to create a new row style.',\n",
       " 'Test that wrong hash in direct URL dependency stops installation .<n>Test that wrong hash in direct URL dependency stops installation .<n>Test that wrong hash in direct URL dependency stops installation .',\n",
       " 'Test that we fall back to setuptools develop when using a backend that does not support build_editable.<n>Since there is a pyproject.toml, the prepare_metadata_for_build_wheel hook is called.',\n",
       " 'summarize to docstring: Test the log message for an invalid Requires-Python .<n>Test the log message for an invalid Requires-Python .<n>Test the log message for an invalid Python.',\n",
       " 'docstring: Evaluate a summarize at x_0 = a in K[X] using the scheme.<n>Examples .<n>from sympy.polys import ring, ZZ .<n>R.dmp_eval (2*x*y + 3*x + y + 2, 2) 5*y + 8 .',\n",
       " 'The source type of system site packages must not be falsely identified as \"directory\"<n>The .<n>source type of system site packages must not be falsely identified as \"directory\"',\n",
       " 'summarize to docstring: commit with support for non-recursive commits .<n> commit with support for non-recursive commits .<n>',\n",
       " 'summarize to docstring: Builds a function definition.<n>This page includes the definitions of the following words: .<n>This page includes the definitions of the following words: .',\n",
       " 'summarize to docstring: struct_or_union : STRUCT | UNION .<n>This page includes the struct_or_union : struct .',\n",
       " 'Returns: True if field name is in validator fields, False otherwise .<n>Checks if field name is in validator fields .',\n",
       " 'Tests that a SymilarChecker can return and reduce mapped data.<n>Tests that a SymilarChecker can reduce mapped data.<n>A SymilarChecker can return and reduce mapped data.',\n",
       " 'summarize to docstring: list index is a str constant .<n>This page includes the list index in .<n>This page includes the list index in .',\n",
       " 'Say it load .<n>Uses .<n>Spelled .<n>Spelled .<n>Spelled .<n>Spelled .<n>Spelled .<n>Spelled .',\n",
       " 'An assignment assumed to execute in one Try should continue to be assumed to execute in a consecutive Try.<n>An assignment assumed to execute in one Try should continue to be assumed to execute in a consecutive Try.',\n",
       " 'summarize to docstring: https://github.com/pylint-dev/pylint/issues/5965 .<n>This page includes a summary of each issue of thepylint open source software.',\n",
       " \"An error in the initializer for the parallel jobs doesn't lead to a deadlock .<n>Tests that an error in the initializer for the parallel jobs doesn't lead to a deadlock .\",\n",
       " 'Returns a module from the given path, which can be a file (a module) or a directory (a package)<n>importMode.append: same as prepend, but the directory will be appended to the end of sys.path .<n>Uses more fine control mechanisms provided by importlib to import modules .',\n",
       " 'Defaults to the test name with a \\'.txt\\' extension, e.g test_foobar.txt, overwriting existing files.<n> pytester.maketxtfile(custom=\"foobar\") # To create multiple files, pass kwargs accordingly.',\n",
       " 'Check asynctest support (#7110)<n>Use the weekly digest to help you with reading comprehension and vocabulary .<n>Use the weekly digest to help you with reading comprehension and vocabulary .',\n",
       " 'Note that checks like: isinstance(n, int) are fragile on Py2, because isinstance(10L, int) is False.<n> explicitly feeding in a long on Py2 .',\n",
       " \"It would nice if the b'' literal syntax could be coaxed into producing bytes objects somehow .<n>It would be nice if the b'' literal syntax could be coaxed into producing bytes objects somehow .\",\n",
       " 'Test spec addition using :data:+= operator.<n>Test spec addition using :data:+= operator.<n>Test spec addition using :data:+= operator.',\n",
       " 'This method makes a synchronous HTTP request by default .<n>param async_req=True .<n>param dry_run: When present, indicates that modifications should not be persisted .<n>param orphan_dependents: Deprecated, this field will be deprecated in 1.7 .',\n",
       " 'This method makes a synchronous HTTP request by default .<n>param _return_http_data_only: response data without head status code and headers .<n>param _preload_content: if False, the URL will be returned without reading/decoding response data .',\n",
       " 'summarize to docstring: Returns true if both objects are equal .<n>Returns true if both objects are equal .<n>Returns true if both objects are equal .',\n",
       " 'summarize to docstring: Returns true if both objects are not equal .<n>Returns true if both objects are not equal .<n>Returns true if both objects are not equal .',\n",
       " '# noqa: E501 :rtype: V1GitRepoVolumeSource .<n>Gets the git_repo of this V1Volume .',\n",
       " 'summarize to docstring: Returns true if both objects are not equal .<n>Returns true if both objects are not equal .<n>Returns true if both objects are not equal .',\n",
       " 'Re-subscribe to any channels and patterns previously subscribed to .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .<n>Visit CNN.com/Impact for stories you saw on CNN.com .',\n",
       " 'When out of connections, block until another connection is released to the pool .<n>When out of connections, block until another connection is released to the pool .',\n",
       " 'Appends an arbitrary number of strings to use as path constants .<n>Uses a comma after each string .<n>Counts the number of letters in a string before each string .',\n",
       " 'Search recommendation based on recommendation id .<n>Use the search box to open a new page .<n>Enter the name of the person you want to search for in the box below .',\n",
       " 'Args: group_name (str): The metric group name .<n>Get a metric group name .<n>Use this to get the name of the metric group you want to use .',\n",
       " 'Check that check_is_fitted passes for stateless estimators.<n>Use the following code to check that check_is_fitted passes for stateless estimators.<n>Use the following code to check that check_is_fitted passes for stateless estimates.',\n",
       " 'bootstrap script entry point .<n> bootstrap script entry point .<n> bootstrap script entry point .<n> bootstrap script entry point .<n> entry point .',\n",
       " 'Tokenize text_document into a sequence of character n-grams operating only inside word boundaries .<n> n-grams at the edges of words are padded with space .',\n",
       " 'summarize to docstring: Returns whether the kernel is stationary .<n>Returns whether the kernel is stationary .<n>Returns whether the kernel is stationary .',\n",
       " 'summarize to docstring: Is there a file at the given path?<n>Is there a file at the given path?<n>Is there a file at the given path?',\n",
       " 'summarize to docstring .<n>Opens in .<n>closes in .<n>closes in .<n>Opens in .<n>closes in .',\n",
       " 'Generate argument list for calling the C++ function .<n>Use the generated argument list to call the C++ function .<n>Use the generated argument list to call the C++ function .',\n",
       " 'This is an optimized version of union which assumes the polygons to be non-overlapping .<n> Parameters ---------- a, b : Geometry or array_like Geometry or geometries to merge (union)<n>Union with None returns same polygon .',\n",
       " 'When there is no root due to double root tags.<n>Test when there is no root due to double root tags.<n>Test when there is no root due to double root tags.',\n",
       " 'summarize to docstring: Return any actually-specified arguments .<n>Use this to return any actually-specified arguments .<n>Or use the .<n>docstring to return any actually-specified arguments.',\n",
       " 'Parameters ---------- file_path : str Path to the file to check .<n> pyspark_error_list : list of str List of PySpark-specific error names .',\n",
       " 'Gets the value of numPartitions or its default value .<n>Uses docstring instead of numPartitions .<n>Gets the value of numPartitions or its default value .',\n",
       " 'Parameters ---------- extraMetadata : dict, optional Extra metadata to be saved at same level as uid, paramMap, etc.<n> paramMap : dict, optional If given, this is saved in the \"paramMap\" field.',\n",
       " 'docstring: Convert_files summarize string styled to one .<n>css : .<n>css : .<n>css : .<n>css : .',\n",
       " 'This is func1 .<n>This is a .<n>This is a .<n>This is a .<n>This is a .<n>This is a',\n",
       " 'docstring: Remove the remote with the given name .<n>:return: The passed remote name to remove .<n>: Remove the remote with the given name .',\n",
       " 'This method will associate the given :class:_sql.CTE constructs with the parent statement .<n>Each CTE will render in a WITH clause rendered directly along with this statement .<n>This method has two general uses .',\n",
       " 'this should raise an error.<n>summary to docstring: test #8759. this should raise an error.<n>This should be the first line of the docstring .',\n",
       " 'docstring: same as precision_numerics_many_significant_digits but within the context of a CAST statement (hello)<n> summarize to docstring: same as precision_numerics_many_significant_digits but within the context of a CAST statement (hello)',\n",
       " 'Examples .<n> summarize from sympy import Q, Abs .<n>refine import refine_abs .<n>From sympy.assumptions.refine import refine_abs .<n>x .',\n",
       " 'docstring: return the non-strict version of the inequality or self .<n>bool bool weak x = 1   _weak x = 1 = 1 = 1 .',\n",
       " 'Performs the Lenstraâ€“Lenstraâ€“Lovsz (LLL) basis reduction algorithm .<n> Parameters, algorithm and basis are same as for :meth:lll except that the return value is a (B, T) with B the reduced basis .',\n",
       " 'Iterates mappings that might have a list of values, yielding all key pairs like iter_multi_items .<n>Almost so, overerates can be used for files .',\n",
       " 'Exhaust the stream by reading until the limit is reached or the client disconnects, returning the remaining data .<n>Handle case where wrapped stream returns fewer bytes than requested .',\n",
       " \"An HTTPS request to an HTTP server doesn't show a traceback.<n>An HTTPS request to an HTTP server doesn't show a traceback.<n>An HTTPS request to an HTTP server doesn't show a traceback.\",\n",
       " 'Include the given datetime instance in the recurrence set exclusion list.<n>Some inclusive rrule or rdate matches them.<n>Dates included that way will not be generated, even if some inclusive rrule or rdate matches them.',\n",
       " ':param item: the item to send :raises anyio.BrokenResourceError: if the stream has been closed :raises anyio.WouldBlock: if the buffer is full and there are no tasks waiting to receive .',\n",
       " 'summarize to docstring: Returns whether node is a statement node .<n>Returns whether node is a statement node .<n>Returns whether node is a statement node .',\n",
       " 'Handle wr.s3.read_parquet_metadata internally.<n>Uses wr.s3.read_parquet_metadata to handle wr.s3.read_parquet_metadata.',\n",
       " 'Get_aws_open_id_config manipulates the request or metadata before they are sent to the AwsClusters server .<n>Pre-rpc interceptor for get_aws_open_id_config .',\n",
       " 'If @string were to be split there, it would result in the splitting of an f-expression (which is NOT allowed)<n>All ranges of @string which, if @string were to be split there, would result in the splitting of an f-expression (which is NOT allowed)',\n",
       " 'If input lines are not newline-terminated, a newline is automatically appended .<n>Stores one or more lines of input .<n>Adds a newline to each input line .',\n",
       " \"Windows: if 'enabled_flag' is True, enable the UNICODE and _UNICODE defines in C, and declare the types like TCHAR and LPTCSTR to be (pointers to) wchar_t.<n>This is mostly for backward compatibility; you usually want True.\",\n",
       " 'docstring: Decide whether to trace execution in filename.<n>Calls _should_trace_internal, and returns the FileDisposition .',\n",
       " \"Doesn't check external links (those with a protocol)<n>Assert that the hrefs in summarizecov/*.html are valid .\",\n",
       " \"Return the base directory containing Cython's caches .<n>/Library/Caches/Cython (posix not OS X): XDG_CACHE_HOME/cython if XDG_CACHE_HOME defined .\",\n",
       " 'summarize to docstring: Post a metric.<n>post a metric to summarize.<n>post a metric to summarize.<n>post a metric to summarize.<n>post a metric to summarize.',\n",
       " 'seq can be used to order requests, responses, and events .<n>For protocol messages of type request the sequence number can be used to cancel the request .',\n",
       " ':param array breakpoints: The instruction references of the breakpoints .<n> :param array breakpoints: The instruction references of the breakpoints .',\n",
       " 'docstring: Iterates over the entries in this pack .<n>Iterates indexerators in packfile and crc32 .<n>Returns: it over indexerators with object name and crc32 .',\n",
       " \"docstring: Iterate over cached submodules .<n>Iterator: overpath store: root_tree .<n>Sha'stree: overpath store .\",\n",
       " ':param force: If true, the lock counter is ignored and the lock is released in every case .<n>Please note, that the lock is only completely released, if the lock counter is 0 .',\n",
       " 'parses a custom_metric path into its component segments.<n>Parses a custom_metric path into its component segments.<n> Parses a custom_metric path into its component segments.',\n",
       " 'Pre-rpc interceptor for update_enhanced_measurement_settings .<n> subclass to manipulate the request or metadata before they are sent to the AnalyticsServiceAdmin server.',\n",
       " 'Post-rpc interceptor for update_repository manipulates the response after it is returned by the Artifact server but before it is returned to user code .<n>Use this interceptor to manipulate the response after it is returned by the Artifact server but before it is returned to user code .',\n",
       " 'Returns: Callable[.ListCatalogsRequest], .ListCatalogsResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'Post-rpc interceptor for get_system_policy .<n> subclass to manipulate the response after it is returned by the SystemPolicyV1 server but before it is returned to user code.',\n",
       " 'This transport is currently in a beta state (preview)<n>credentials_file (Optional[str]): A file with credentials that can be loaded with :func:google.auth.load_credentials_from_file .<n> scopes (Optional(Sequence[str)): A list of scopes. This argument is ignored if channel is provided .<n>client_cert_source_for_mtls (Callable[',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/inrange values for request constructors .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'You can use an operation resource to manage asynchronous API requests .<n>Operation resources can be global, regional or zonal .<n>For more information, read Handling API responses .',\n",
       " 'Returns a callable for the set logging service method over gRPC .<n>Set the logging service for a specific cluster .<n>When called, it will call the underlying RPC on the server .',\n",
       " 'docstring: Return the API endpoint used by the client .<n>str: The API endpoint to be used by the client .<n>Use_mtls_endpoint: How to use the mTLS endpoint .',\n",
       " 'Mapped from Open Lineage specification: https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineageResponse.<n>When called, the function will call the underlying RPC on the server .',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object.<n>Retry: Designation of what errors, if any, should be retried.<n>The timeout for this request.',\n",
       " 'docstring: Call the get location method over HTTP.<n>Args: request (locations_pb2.GetLocationRequest): The request object for GetLocation method.<n>retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.<n>metadata: Strings which should be sent along with the request as metadata.',\n",
       " 'Returns the list of all session entity types in the specified session .<n>A function that, when called, will call the underlying RPC on the server .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " '.gcd_participant.UpdateParticipant: Represents a conversation participant (human agent, virtual agent, end-user)<n>.gcd_participant.UpdateParticipant: Represents a conversation participant (human agent, virtual agent, end-user)',\n",
       " 'Returns: Callable[.BatchCreateTargetSitesRequest], .Operation]: A function that, when called, will call the underlying RPC on the server.',\n",
       " 'parses a file_store_data_profile path into its component segments.<n>Parses a file_store_data_profile path into its component segments.',\n",
       " 'Returns: Callable[.ListVolumeBackupsRequest], .ListVolumeBackupsResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'docstring: Creates an instance of this client using the provided credentials file .<n>args: Additional arguments to pass to the constructor .<n>attachedClustersAsyncClient: The constructed client .',\n",
       " 'Resets an Identity Aware Proxy (IAP) client secret .<n>Needs that the client is owned by IAP .<n>Code-block:: python # This snippet has been automatically generated .',\n",
       " 'This method calls the list assets method over HTTP .<n>Message for requesting a list of assets .<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .',\n",
       " 'subclass manipulates the response after it is returned by the NetworkServices server but before it is returned to user code .<n>Uses post-rpc interceptor to manipulate the response after it is returned by the NetworkServices server .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'parses apublic_public_key path into its component segments .<n>Parses a docstring to get the public_public_key path .<n>Parses a docstring to get the component segments .',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object. retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried.',\n",
       " 'This property caches on the instance; repeated calls return the same client .<n>Creates the client designed to process long-running operations .<n>Repeated calls return the same client .',\n",
       " 'Post-rpc interceptor for get_tag_value .<n>Uses a subclass to manipulate the response after it is returned by the TagValues server but before it is returned to user code .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " ':param sec_key_ref: A Security framework SecKeyRef value from loading/importing the key :param asn1: An asn1crypto.keys.PrivateKeyInfo object',\n",
       " \"args: host (default: 'run.googleapis.com')<n>credentials: The authorization credentials to attach to requests .<n> quota_project_id: An optional project to use for billing and quota .<n>always_use_jwt_access: Whether self signed JWT should be used for service account credentials .\",\n",
       " 'Returns permissions that a caller has on the specified source .<n>Callable[.TestIamPermissionsRequest], .TestIamPermissionsResponse]: A function that will call the underlying RPC on the server .',\n",
       " '.securitycenter_service.ListEventThreatDetectionCustomModulesRequest .<n>Retry (google.api_core.retry.Retry): Designation of what errors, if any, should be retried .<n>metadata (Sequence[Tuple[str, str]]): Strings which should be sent along with the request as metadata .',\n",
       " 'Post-rpc interceptor for get_company .<n> summarize in a subclass to manipulate the response after it is returned by the CompanyService server but before it is returned to user code .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'Lists all slates in the specified project and location .<n>Callable[.ListSlatesRequest], .ListSlatesResponse]: A function that, when called, will call the underlying RPC on the server .',\n",
       " 'Cell magics declared via a class, #2.<n>Cell magics declared via a class, #3.<n>Cell magics declared via a class, #4.<n>Cell magics declared via a class,',\n",
       " 'A list of the captured rich display outputs, if any .<n>If you have a CapturedIO object c, these can be displayed in IPython using:: from IPython.display import display for o .',\n",
       " 'summarize to docstring: Returns an instance of a compressor file object .<n>Returns an instance of a compressor file object .<n>Uses this to return an instance of a compressor file object .',\n",
       " 'example: from mako.ext.preprocessors import convert_comments t = template(..., preprocessor=convert_comments)<n>preprocessor=convert_comments .',\n",
       " 'Return text with backslash escapes undone (backslashes are restored).<n>Use the .<n>docstring to return text with backslash escapes undone (backslashes are restored).',\n",
       " 'Demonstrates displaying different variables through shade and color.<n>Explains how to use the Opacity function to display different variables through shade and color.<n>Uses the Opacity function to show different variables through shade.',\n",
       " 'Copy properties from *other* to *self*.<n>Copy properties from *other* to *self*.<n>Copy properties from *other* to *self*.',\n",
       " 'Return the depth of the axis used by the picker .<n>Use docstring to summarize the depth of the axis used by the picker .<n>Return the depth of the axis used by the picker .',\n",
       " 'Read one page from the file.<n>Return True if successful, False if there are no more pages.<n>Return true if there are no more pages.',\n",
       " \"Return this line's matplotlib.transforms.TransformedPath.<n>bool matplotlib.transforms.TransformedPath .<n>bool matplotlib.transforms.TransformedPath .\",\n",
       " 'convert the given BSON value into our own type.<n>Use the following code to convert the given BSON value into our own type.<n>Use the following code to convert the given BSON value into our own type.',\n",
       " 'summarize to docstring: Raise error on missing token.<n>This page includes a summary of each item in the form of a .<n>Shorter version of the .<n>docstring.',\n",
       " 'arguments are used to create an InstrumentationScope object on the created Meter<n>MeterProvider.get_meter arguments are used to create an InstrumentationScope object on the created Meter',\n",
       " 'convert a core foundation data type into its equivalent in python .<n>param type_id: The CFTypeId for the type .<n>A callback to pass the CFType object to pass to the CFType object .',\n",
       " 'Simple helper to create data for to to_dict(orient=\"split\") to create the main output data .<n>Uses to_dict(orient=\"split) to create the main output data .',\n",
       " 'summarize to docstring: Return a string representation for a particular Series .<n>Returns a string representation for a particular Series .<n>Use this to return a string representation for a particular Series .',\n",
       " 'Appends lines to a buffer .<n> Parameters ---------- buf The buffer to write to lines .<n>Lines to append .',\n",
       " 'Check that two objects are not approximately equal.<n> Parameters ---------- a : object The first object to compare.<n>b : object The second object to compare.',\n",
       " 'Iterates through file entries declared in this distribution .<n>:return: An iterator for listed entries, or None if the distribution contains neither RECORD nor installed-files.txt.',\n",
       " 'A hook dictionary is a written version of a docstring .<n>Uses docstring to describe a given piece of data .<n>A hook dictionary is a written version of a docstring .',\n",
       " 'Check that installing works from a link with an extra if there is an indirect dependency on that same package with the same extra (#12372)<n>Also check that installing works from a link with an extra if there is an indirect dependency on that same package with the same extra.',\n",
       " 'Returns a dictionary of token types, matched to their action in the feed .<n>Only returns token types that are accepted by the current state .<n> Updated by feed_token().',\n",
       " 'min and max frequencies are not available and are set to None.<n>Use /proc/cpuinfo to alternate implementation using /proc/cpuinfo.',\n",
       " 'summarize to docstring: Re-run tests which failed on last run.<n>Re-run tests which failed on last run.<n>Re-run tests which failed on last run.',\n",
       " 'See tests/test_types.py::test_annotated_metadata_any_order for some context.<n>The context argument for PydanticKnownError requires a number or str type, so we do a simple repr() coercion for types like timedelta.',\n",
       " 'Grant owner access to the current entity.<n> summarize to docstring: Grant owner access to the current entity.<n>For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.',\n",
       " 'Return the more user-friendly information about the location of a warning, or None.<n>Return the more user-friendly information about the location of a warning, or None.',\n",
       " \"Regression test for importing a sub module 'foo.bar' while there is a 'bar directory' reachable from sys.path .<n> ensuring the top-level module does not end up imported as a abar package .\",\n",
       " 'Returns: Page: The next page in the iterator or :data:None if there is no pages left .<n>Get the next page in the iterator .',\n",
       " \"summarize to docstring: Patch a table's metadata.<n>Use docstring to describe a table's metadata.<n>Use docstring to describe a table's data type.\",\n",
       " 'docstring: Return query result and display a progress bar while the query running, if tqdm is installed.<n>max_results: The maximum number of rows the row iterator should return.<n>A row iterator over the query results.',\n",
       " \"summarize to docstring .<n>This page includes the configuration for this copy job .<n>Use the weekly digest to help you understand this week's featured news stories .<n>For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org .\",\n",
       " 'See: https://cloud.google.com/bigQuery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.description for a description of the destination table.',\n",
       " \"Scan the input for current state's tokens starting at current_offset.<n>State (LexerState): The current lexer state.<n>Current_offset (int): The offset in the input text, i.e. the number of characters already scanned so far.\",\n",
       " 'terminating indicates that this endpoint is terminating.<n>A nil value indicates an unknown state.<n>Consumers should interpret this unknown state to mean that the endpoint is not terminating.',\n",
       " 'summarize to docstring: Returns true if both objects are not equal .<n>Returns true if both objects are not equal .<n>Returns true if both objects are not equal .',\n",
       " 'The :meth:sagemaker.estimator.EstimatorBase.fit method, which trains the model, calls this method to find the hyper parameters .<n>dict[str, str]: The hyper parameters .',\n",
       " 'A factory to provide that provides a FeatureStoreManager that handles data ingestion to a Feature Group .<n>The factory lazily loads the FeatureStoreManager .<n>An instance that handles outputs of the wrapped function .',\n",
       " 'summarize to docstring: Returns schema version .<n>Uses comma to indicate if document is in .<n>Uses hyphen to indicate if document is in English or Spanish .',\n",
       " 'bool: If True, colorizes output no matter where the output is (default: False)<n>bool: If true, render colorizes output no matter where the output is (default: False)',\n",
       " 'Describe the latest baselining job kicked off by the suggest workflow.<n>Add a brief description of the latest baselining job kicked off by the suggest workflow.<n>Add a brief summary of the job.',\n",
       " 'Deletes the Amazon SageMaker models backing this predictor.<n>Adds the Amazon SageMaker models backing this predictor.<n>Adds the Amazon SageMaker models backing this predictor.',\n",
       " 'docstring:  mutually summarize exclusive property file / s3uri .<n>This page includes the s3uri file extension.<n>Use the s3uri to help you with reading comprehension and vocabulary.',\n",
       " 'raw_prediction : C-contiguous array of shape (n_samples,) or array of shape (n_classes,)<n>Returns gradient : arrays of shape (n_samples,) or (n_classes,) Element-wise gradients.<n>Hessian_out : None or C-contiguous array of shape (n_samples,) or array of shape (n_samples, n_classes,)',\n",
       " 'Check that expected error is raised during fit.<n>Check that expected error is raised during fit.<n>Check that expected error is raised during fit.<n>Check that expected error is raised during fit.',\n",
       " 'Time conventional Voronoi diagram calculation.<n>Time conventional Voronoi diagram calculation.<n>Time conventional Voronoi diagram calculation.<n>Time conventional Voronoi diagram calculation.',\n",
       " 'docstring: Construct the sigma matrix in SVD from singular values and size M, N .<n> Parameters ---------- s : (M,) or (N,) array_like Singular values M : int Size of the matrix whose singular values are s. N : int Size of the matrix whose singular values are s.',\n",
       " 'docstring: Minimize a numerically function using a nonlinear conjugate gradient .<n> Parameters ---------- f : callable, f(x, *args) Objective function to be minimized .<n>returns ------- xopt : returned if fopt is True .',\n",
       " 'This test comes with the exception of dok_matrix, which was not working pre scipy1.12 (unlike the rest of these)<n>This test is for backwards compatibility post scipy 1.13 .',\n",
       " 'It is automatically called on destruction, and should be called just in time to allow system resources to be freed.<n>Once you called end_access, you must call begin access before reusing this instance!',\n",
       " \"When join conditions don't express the left side explicitly, determine if an existing FROM or entity in this query can serve as the left hand side .\",\n",
       " \"Test that any link will not match in summarize (all links are unvisited)<n>Use the weekly test to see if your links match or don't .\",\n",
       " 'summarize to docstring: Test input direction when input is the root .<n>Use this test to help you with reading comprehension and vocabulary .<n>Test input direction when input is the root.',\n",
       " 'Parameters ---------- train_object : callable object or str Either a PyTorch function, PyTorch function, or the path to a python file that launches distributed training .<n>args : If train_object is a python function and not a path to a python file, args need to be the input parameters to that function .',\n",
       " 'docstring: Rows of the RowMatrix stored as an RDD of vectors .<n>Use this code to show how to store multiple rows as an RDD of vectors .',\n",
       " \"summarize to docstring: test additional use case that wasn't considered for #8372.<n>test additional use case that wasn't considered for #8372.\",\n",
       " 'Filter only nodes which do not match *tags*.<n>Use only nodes to filter by type .<n>Use only nodes to filter by tag .',\n",
       " 'Returns None if there are no rows to fetch .<n>No validation is performed to test if additional rows remain .<n>After calling this method, the object is fully closed .',\n",
       " 'Default implementation is:: return referred_cls.__name__.lower() + \"_collection\"<n> alternate implementations can be specified using the :paramref:.AutomapBase.prepare.name_for_collection_relationship parameter .',\n",
       " 'Tries to find more initial conditions by substituting the initial value point in the differential equation .<n>Uses the cosine function to find more initial conditions .<n>Uses the cosine function to find more initial value points .',\n",
       " 'converts a term in the expansion of a function from binary to its variable form (for POS)<n>Uses docstring to convert a term in the expansion of a function from binary to its variable form.',\n",
       " \"summarize to docstring: Young's Modulus of the Beam.<n> summarize to docstring: Young's Modulus of the Beam.<n>Young's Modulus of the Beam.\",\n",
       " 'Expands the transfer function matrix .<n>Sums up the results of a series of tests .<n>Intensifies the search for an answer .<n>Intensifies the search for an explanation .',\n",
       " 'The length of the spring at which it produces no force.<n>The length of the spring at which it produces no force.<n>The length of the spring at which it produces no force.',\n",
       " 'Tests max degrees function.<n>Tests max degrees function.<n>Tests max degrees function.<n>Tests max degrees function.<n>Tests max degrees function.',\n",
       " 'Returns the first n terms of the composed formal power series .<n>The coefficient of the :obj:FormalPowerSeries object is the generic sequence .<n>The term by term logic is implemented here .',\n",
       " 'apply _hasattrs and _hastypes to expr.<n>apply _hasattrs and _hastypes to expr.',\n",
       " 'Show that options are not passed for older versions of requests .<n>Use this box to indicate that options are not passed for older versions of requests .<n>Use the box to indicate that options are not passed for older versions of requests .',\n",
       " \"summarize to docstring: Test CLI --manpath.<n>Use the weekly test to see if you're on track to meet your goals .<n>Use the weekly test to see if you're meeting your objectives .\",\n",
       " ' .<n> .<n> .<n> .<n> .<n> .<n> .<n> .',\n",
       " 'summarize to docstring: validate object to defined invariants.<n>Opens in .<n>Coded in .<n>Coded in .<n>Coded in .',\n",
       " 'Encodes the value using DER .<n>param force: If the encoded contents already exist, clear them and regenerate to ensure they are in DER format instead of BER format .<n>A byte string of the DER-encoded value .',\n",
       " 'The native Python datatype representation of this value :return: A byte string or None .<n>This page includes the Python datatype representation of this value .',\n",
       " 'Writes the loaded data back out to the index summarize .<n>Writes the loaded data back out to the index summarize .<n>Uses docstring to write the data back out to the index .',\n",
       " 'docstring should determine names and address with an override .<n>Use this to determine names and address with an override .<n>Or use the .<n>docstring to determine names and address.',\n",
       " 'summarize to docstring: Property suppress_warnings.<n>  .<n>  .<n> .<n> .<n> .',\n",
       " 'summarize to docstring: Filter only running futures.<n>Opens in .<n>Closes in .<n>Opens in .<n>Closes in .',\n",
       " 'StringTransformer instances have a call signature that mirrors that of the Transformer type .<n>Raises: CannotTransform(...) if the concrete StringTransformer class is unable to transform @line .',\n",
       " \"The thread's thread state must have been by a previous put_release_gil<n>The thread's thread state must have been by a previous put_release_gil\",\n",
       " 'docstring: Delete a specific SLO.<n>:param id: SLO id to delete :type id: str :returns: SLO ids removed .',\n",
       " 'docsstring: Sets the tracing function with the pydev debug function and initializes needed facilities .<n>param port: specifies which port to use for communicating with the server .<n>param suspend: whether a breakpoint should be emulated as soon as this function is called .',\n",
       " 'If dev/prerelease tags result in TypeError for string-number comparison, it is assumed that the dependency is satisfied.<n>Users on dev branches are responsible for keeping their own packages up to date.',\n",
       " 'docstring: @type crash: LCrash @param crash: Crash object.<n>bool @return: CTrue if a Crash object with the same key is in the container.',\n",
       " '@rtype: int @return: Exit code of the thread.<n>Int  .<n>Int  .<n>Int  .',\n",
       " '@type hThread: LThreadHandle @param hThread: (Optional) Handle to the thread.<n> @type process: LProcess @param process: (Optional) Parent Process object.',\n",
       " 'docstring: Starts logging all messages at the specified levels to the designated file.<n>If the file with the specified or computed name is already being used as a log file, it is not overwritten, but its levels are updated as specified.<n>When the object is closed, logs are not written into that file anymore.',\n",
       " 'join_token (string): Secret token for joining this Swarm .<n> listen_addr (string): Listen address used for inter-manager communication .<n> advertise_addr (string): Externally reachable address advertised to other nodes .',\n",
       " 'docstring: Write the object as identified by type, size and source_stream into the target_stream .<n>param size: amount of bytes to write from source_stream .<n>param read: read method of a stream providing the content data .',\n",
       " 'verbose: .analytics_admin.UpdateEnhancedMeasurementSettingsRequest.<n> verbose: .resources.EnhancedMeasurementSettings.<n> verbose: .resources.EnhancedMeasurementSettings.',\n",
       " 'Return a callable for the list locations method over gRPC .<n>Use this to return a summary of the list locations method .<n>Use this to return a callable for the list locations method .',\n",
       " 'interceptor for create_nfs_share in a subclass to manipulate the request or metadata before they are sent to the BareMetalSolution server .<n>Pre-rpc interceptor for create_nfs_share .',\n",
       " 'interceptor to manipulate the response after it is returned by the BareMetalSolution server but before it is returned to user code .<n>Use this interceptor to manipulate the response after it is returned by the BareMetalSolution server but before it is returned to user code .',\n",
       " \"This snippet has been automatically generated and should be regarded as a # code template only .<n>Message for deleting an SSH key from a project's name .<n>Name of the SSH key to delete .<n>Retry (api_core.retry.Retry): Designation of what errors, if any, should be retried .\",\n",
       " 'parses a certificate_authority path into its component segments.<n>Parses a certificate_authority path into its component segments.<n>Uses docstring toParse a certificate_authority path into its component segments.',\n",
       " 'Gets details about a [Revision],google.cloud.config.v1.Revision].<n>Callable[.GetRevisionRequest], .Revision]: A function that will call the underlying RPC on the server.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>To prevent failure, Google recommends that you set the returnSuccess parameter to true .<n>Use this method to retrieve an aggregated list ofworks .',\n",
       " 'bool: True iff client_universe matches the universe in credentials .<n>Raises: ValueError: when client_universe does not match the universe in credentials .',\n",
       " 'Returns a fully-qualified encryption_spec string .<n>Uses the encryption_spec function to return a fully-qualified encryption_spec string .<n>Uses the encryption_spec function to return an encryption_spec string .',\n",
       " 'Gets a list of suggestions based on a prefix string .<n>AutoSuggestion tolerance should be less than 1 second .<n>Returns Google.cloud.dataqna_v1alpha.types.SuggestQueriesRequest .',\n",
       " 'The method that was originally called, and which instantiated this pager. request.<n>The initial response object.<n>Retry: Designation of what errors, if any, should be retried.',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/inrange values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/inrange values for request constructors .<n>Iterating over this object will yield results and resolve additional pages automatically .',\n",
       " 'api_endpoint can be used to override the default endpoint provided by the client when transport is not explicitly provided .<n>The universe_domain property can be used to override the default \"googleapis.com\" universe .<n>If None, then default info will be used .',\n",
       " 'Returns a callable for the batch update entity types method over gRPC.<n>This method is a long-running operation https://cloud.google.com/dialogflow/es/docs/how/long-running-operations>__.',\n",
       " 'Returns a callable for the create document method over gRPC.<n>This method is a long-running operation https://cloud.google.com/dialogflow//docs/how/long-running-operation>__.',\n",
       " 'Reloads the specified document from its specified source, content_uri or content.<n>The previously loaded content of the document will be deleted.<n>If the document source is Google Cloud Storage, its metadata will be replaced with the custom metadata from Google Cloud Storage.',\n",
       " 'summarize to docstring: Returns a fully-qualified finding string .<n>Returns a fully-qualified finding string .<n>Uses docstring to specify the type of finding string .',\n",
       " 'interceptor for configure_dns_settings in a subclass to manipulate the request or metadata before they are sent to the server.<n>Pre-rpc interceptor for configure_dns_settings .',\n",
       " 'The RetrieveAuthorizationCode method retrieves an authorization code over HTTP .<n>The timeout for this request should be .<n>Sequence[Tuple[str] Strings, which should be sent along with the request as metadata .',\n",
       " 'Represents a request to perform a single point-in-time capture of some portion of the state of a GKE cluster, the record of the backup operation itself, and an anchor for the underlying artifacts .',\n",
       " 'This snippet has been automatically generated and should be regarded as a # code template only .<n>It will require modifications to work: # - It may require correct/in-range values for request initialization .<n>It may require specifying regional endpoints when creating the service # client .',\n",
       " 'This method returns the API endpoint and client cert source for mutual TLS .<n>Only the api_endpoint and client_cert_source properties may be used in this method .',\n",
       " \"empty TIFF file is created if the file does not exist, else the file is overwritten with an empty TIFF file unless 'append' is true .<n>ImageJ does not support BigTIFF format or LZMA compression .<n>The ImageJ file format is undocumented .\",\n",
       " 'interceptor for destroy_secret_version .<n> subclass to manipulate the response after it is returned by the SecretManagerService server but before it is returned to user code .',\n",
       " 'Post-rpc interceptor for get_event_threat_custom_module .<n> subclass to manipulate the response after it is returned by the SecurityCenter server .',\n",
       " 'Pre-rpc interceptor for get_notification_config to manipulate the request or metadata before they are sent to the SecurityCenter server .<n>Use this interceptor to manipulate the request or metadata before they are sent to the SecurityCenter server .',\n",
       " 'Gets a custom class that represents a common concept likely to appear in your audio .<n>Message sent by the client for the GetCustomClass method .<n>Name of the custom class to retrieve .',\n",
       " 'subclass manipulates the request or metadata before they are sent to the App server .<n>Pre-rpc interceptor for get_draft manipulates the request or metadata before they are sent to the App server .',\n",
       " 'summarize to docstring: Returns a fully-qualified index_endpoint string .<n>Returns a fully-qualified index_endpoint string .<n>Returns a fully-qualified index_endpoint string .',\n",
       " 'Returns the universe domain used by the client .<n>The universe domain is configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable .<n>Raises if the universe domain is an empty string .',\n",
       " 'Set a cookie summarize value by name.<n>May also include domain and path.<n>Use docstring to set a cookie summarize value by name.<n>Set a cookie summarize value by name.',\n",
       " 'docstring name for code for caching now expects to be unique .<n>Code will be unique if Compute is called with a unique name .<n>Code will be unique if Compute is called with a unique number .',\n",
       " \"Used by dict_key_matches, matching the prefix to a list of keys .<n>Part of the text already typed by the user. E.g. for mydict[foo'', 'b', this would be ('foo', 'bar').\",\n",
       " \"Check that the type of __builtins__ doesn't change with %run .<n>If it was already modified to be a dict by a previous use of %run, it should be a module .\",\n",
       " \"Cleanup collections faster than drop_collection .<n>Use the weekly roundup to help you understand today's featured news stories .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on MailOnline .\",\n",
       " 'Update the title position based on the bounding box enclosing all the ticklabels and x-axis spine and xlabel .<n>Update the title position based on the bounding box enclosing all the ticklabels and x-axis spine and xlabel .',\n",
       " 'Set the left and bottom coordinates of the rectangle.<n> Parameters ---------- xy : (float, float)<n> summarize to docstring: Set the left and bottom coordinates of the rectangle.',\n",
       " 'This changes the default colormap as well as the colormap of the current image if there is one.<n>See help(colormaps) for more information.',\n",
       " 'mongos mode of this read preference.<n>The mongos mode of this read preference.<n>The mongos mode of this read preference.<n>The mongos mode of this read preference.',\n",
       " 'summarize to docstring: Internal update / replace helper.<n>This page includes a summary of the definitions provided by the definitions provided by the definitions provided by the definitions provided by the search engines below.',\n",
       " 'A matching is a subset of edges in which no node occurs more than once .<n>The cardinality of a matching is the number of matched edges .<n> Method replaces the edge weights with 1 plus the maximum edge weight .',\n",
       " 'Ensures that truncated documentation is written out .<n>Test to ensure this has no effect without --latex-doc .<n>Test to ensure this has no effect without --latex-doc .',\n",
       " 'The usual companion matrix of the Laguerre summarizes is already symmetric when c is a basis Laguerre, so no scaling is applied .<n>The following code returns the Laguerre series of coefficients ordered from low to high degree .',\n",
       " 'summarize to docstring: Shuts down the log processors.<n>This page includes a summary of each line in the docstring .<n>Use the end of the page to help you with reading comprehension and vocabulary.',\n",
       " 'construct a DatetimeTZDtype from a string .<n>should be formatted like datetime64[ns, tz, where tz> is the timezone name .',\n",
       " 'Headers names of the columns in verbose table.<n>Click here to read the verbose table.<n>Click here to read the header names of the columns in verbose table.',\n",
       " 'pip wheel -e giturl preserves a git clone in src.<n>Test that pip wheel -e giturl preserves a git clone in src.',\n",
       " 'Return a new !Composed interposing the *joiner* with the !Composed items.<n>The *joiner* must be a  or the string which will be interpreted as an fields.',\n",
       " 'For SIV, encryption and MAC authentication must take place at the same point.<n>Use encrypt_and_digest instead.<n>For SIV, encryption and MAC authentication must take place at the same point.',\n",
       " 'Returns: a new :class:EccKey object .<n>Uses a matching ECC public key .<n>Spells the name of the public key .',\n",
       " 'Check converting keys to English .<n>Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com .<n>Follow us on Twitter @CNNOpinion and @cnnOpinion .',\n",
       " 'Wrap a V1 style field validator for V2 compatibility .<n>Raises if the signature is not supported or the parameters are not available in Pydantic V2 .',\n",
       " 'summarize to docstring: :calls: GET /repos/owner/repo/statuses/ref https://docs.github.com/en/rest/reference/repos#statuses>_',\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convert a file path into a regex that will match that path on any OS.',\n",
       " 'Compacts the frames to deduplicate recursive calls.',\n",
       " '>>> test_positional_only_with_optional_invalid_calls(1)\\n>>> test_positional_only_with_optional_invalid_calls()  # doctest: +ELLIPSIS\\nTraceback (most recent call last):\\nTypeError: test_positional_only_with_optional_invalid_calls() ... positional argument...\\n>>> test_positional_only_with_optional_invalid_calls(1, 2, 3)  # doctest: +ELLIPSIS\\nTraceback (most recent call last):\\nTypeError: test_positional_only_with_optional_invalid_calls() takes ... positional arguments ...3 ...given...',\n",
       " 'Scopes are the top-level items which appear for a frame (so, we receive the frame id\\nand provide the scopes it has).\\n\\n:param ScopesRequest request:',\n",
       " 'Retrieves the environment variables with wich the program is running.\\n\\n@rtype:  list of tuple(compat.unicode, compat.unicode)\\n@return: Environment keys and values as found in the process memory.\\n\\n@raise WindowsError: On error an exception is raised.',\n",
       " 'Dump a stack trace, as returned by L{Thread.get_stack_trace} with the\\nC{bUseLabels} parameter set to C{False}.\\n\\n@type  stack_trace: list( int, int, str )\\n@param stack_trace: Stack trace as a list of tuples of\\n    ( return address, frame pointer, module filename )\\n\\n@type  bits: int\\n@param bits:\\n    (Optional) Number of bits of the target architecture.\\n    The default is platform dependent. See: L{HexDump.address_size}\\n\\n@rtype:  str\\n@return: Text suitable for logging.',\n",
       " 'Return an iterator over all :class:`LoadedPlugin`s.',\n",
       " \"Make it easier to debug which StyleGuide we're using.\",\n",
       " \"Run the flow using the server strategy.\\n\\nThe server strategy instructs the user to open the authorization URL in\\ntheir browser and will attempt to automatically open the URL for them.\\nIt will start a local web server to listen for the authorization\\nresponse. Once authorization is complete the authorization server will\\nredirect the user's browser to the local web server. The web server\\nwill get the authorization code from the response and shutdown. The\\ncode is then exchanged for a token.\\n\\nArgs:\\n    host (str): The hostname for the local redirect server. This will\\n        be served over http, not https.\\n    bind_addr (str): Optionally provide an ip address for the redirect\\n        server to listen on when it is not the same as host\\n        (e.g. in a container). Default value is None,\\n        which means that the redirect server will listen\\n        on the ip address specified in the host parameter.\\n    port (int): The port for the local redirect server.\\n    authorization_prompt_message (str | None): The message to display to tell\\n        the user to navigate to the authorization URL. If None or empty,\\n        don't display anything.\\n    success_message (str): The message to display in the web browser\\n        the authorization flow is complete.\\n    open_browser (bool): Whether or not to open the authorization URL\\n        in the user's browser.\\n    redirect_uri_trailing_slash (bool): whether or not to add trailing\\n        slash when constructing the redirect_uri. Default value is True.\\n    timeout_seconds (int): It will raise an error after the timeout timing\\n        if there are no credentials response. The value is in seconds.\\n        When set to None there is no timeout.\\n        Default value is None.\\n    token_audience (str): Passed along with the request for an access\\n        token. Determines the endpoints with which the token can be\\n        used. Optional.\\n    browser (str): specify which browser to open for authentication. If not\\n        specified this defaults to default browser.\\n    kwargs: Additional keyword arguments passed through to\\n        :meth:`authorization_url`.\\n\\nReturns:\\n    google.oauth2.credentials.Credentials: The OAuth 2.0 credentials\\n        for the user.\",\n",
       " 'Updates a ``Corpus``.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.ai import generativelanguage_v1beta\\n\\n    def sample_update_corpus():\\n        # Create a client\\n        client = generativelanguage_v1beta.RetrieverServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = generativelanguage_v1beta.UpdateCorpusRequest(\\n        )\\n\\n        # Make the request\\n        response = client.update_corpus(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.ai.generativelanguage_v1beta.types.UpdateCorpusRequest, dict]):\\n        The request object. Request to update a ``Corpus``.\\n    corpus (google.ai.generativelanguage_v1beta.types.Corpus):\\n        Required. The ``Corpus`` to update.\\n        This corresponds to the ``corpus`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        Required. The list of fields to update. Currently, this\\n        only supports updating ``display_name``.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.ai.generativelanguage_v1beta.types.Corpus:\\n        A Corpus is a collection of Documents.\\n           A project can create up to 5 corpora.',\n",
       " 'Pick first non-None style.',\n",
       " 'Call the list operations method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.ListOperationsRequest):\\n        The request object for ListOperations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    operations_pb2.ListOperationsResponse: Response from ListOperations method.',\n",
       " 'Return a callable for the get application method over gRPC.\\n\\nGets information about an application.\\n\\nReturns:\\n    Callable[[~.GetApplicationRequest],\\n            ~.Application]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Renames the id (display name) of the specified data\\npolicy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_datapolicies_v1\\n\\n    def sample_rename_data_policy():\\n        # Create a client\\n        client = bigquery_datapolicies_v1.DataPolicyServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_datapolicies_v1.RenameDataPolicyRequest(\\n            name=\"name_value\",\\n            new_data_policy_id=\"new_data_policy_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.rename_data_policy(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_datapolicies_v1.types.RenameDataPolicyRequest, dict]):\\n        The request object. Request message for the\\n        RenameDataPolicy method.\\n    name (str):\\n        Required. Resource name of the data policy to rename.\\n        The format is\\n        ``projects/{project_number}/locations/{location_id}/dataPolicies/{data_policy_id}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    new_data_policy_id (str):\\n        Required. The new data policy id.\\n        This corresponds to the ``new_data_policy_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_datapolicies_v1.types.DataPolicy:\\n        Represents the label-policy binding.',\n",
       " 'Pre-rpc interceptor for list_certificate_map_entries\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CertificateManager server.',\n",
       " 'Call the update certificate map method over HTTP.\\n\\nArgs:\\n    request (~.certificate_manager.UpdateCertificateMapRequest):\\n        The request object. Request for the ``UpdateCertificateMap`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the get entry group method over gRPC.\\n\\nGets an EntryGroup.\\n\\nReturns:\\n    Callable[[~.GetEntryGroupRequest],\\n            ~.EntryGroup]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the aggregated list method over HTTP.\\n\\nArgs:\\n    request (~.compute.AggregatedListMachineTypesRequest):\\n        The request object. A request message for\\n    MachineTypes.AggregatedList. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.MachineTypeAggregatedList:',\n",
       " 'Retrieves a list of Router resources available to the\\nspecified project.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_list():\\n        # Create a client\\n        client = compute_v1.RoutersClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.ListRoutersRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.ListRoutersRequest, dict]):\\n        The request object. A request message for Routers.List.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.routers.pagers.ListPager:\\n        Contains a list of Router resources.\\n\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Sets the access control policy on the specified\\nresource. Replaces any existing policy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_iam_policy():\\n        # Create a client\\n        client = compute_v1.ServiceAttachmentsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetIamPolicyServiceAttachmentRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n            resource=\"resource_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_iam_policy(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetIamPolicyServiceAttachmentRequest, dict]):\\n        The request object. A request message for\\n        ServiceAttachments.SetIamPolicy. See the\\n        method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        The name of the region for this\\n        request.\\n\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    resource (str):\\n        Name or id of the resource for this\\n        request.\\n\\n        This corresponds to the ``resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region_set_policy_request_resource (google.cloud.compute_v1.types.RegionSetPolicyRequest):\\n        The body resource for this request\\n        This corresponds to the ``region_set_policy_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.Policy:\\n        An Identity and Access Management (IAM) policy, which\\n        specifies access controls for Google Cloud resources. A\\n        Policy is a collection of bindings. A binding binds one\\n        or more members, or principals, to a single role.\\n        Principals can be user accounts, service accounts,\\n        Google groups, and domains (such as G Suite). A role is\\n        a named list of permissions; each role can be an IAM\\n        predefined role or a user-created custom role. For some\\n        types of Google Cloud resources, a binding can also\\n        specify a condition, which is a logical expression that\\n        allows access to a resource only if the expression\\n        evaluates to true. A condition can add constraints based\\n        on attributes of the request, the resource, or both. To\\n        learn which resources support conditions in their IAM\\n        policies, see the [IAM\\n        documentation](\\\\ https://cloud.google.com/iam/help/conditions/resource-policies).\\n        **JSON example:**\\n        :literal:`\\\\` { \"bindings\": [ { \"role\": \"roles/resourcemanager.organizationAdmin\", \"members\": [ \"user:mike@example.com\", \"group:admins@example.com\", \"domain:google.com\", \"serviceAccount:my-project-id@appspot.gserviceaccount.com\" ] }, { \"role\": \"roles/resourcemanager.organizationViewer\", \"members\": [ \"user:eve@example.com\" ], \"condition\": { \"title\": \"expirable access\", \"description\": \"Does not grant access after Sep 2020\", \"expression\": \"request.time < timestamp(\\'2020-10-01T00:00:00.000Z\\')\", } } ], \"etag\": \"BwWWja0YfJA=\", \"version\": 3 }`\\\\ \\\\`\\n        **YAML example:**\\n        :literal:`\\\\` bindings: - members: - user:mike@example.com - group:admins@example.com - domain:google.com - serviceAccount:my-project-id@appspot.gserviceaccount.com role: roles/resourcemanager.organizationAdmin - members: - user:eve@example.com role: roles/resourcemanager.organizationViewer condition: title: expirable access description: Does not grant access after Sep 2020 expression: request.time < timestamp(\\'2020-10-01T00:00:00.000Z\\') etag: BwWWja0YfJA= version: 3`\\\\ \\\\`\\n        For a description of IAM and its features, see the [IAM\\n        documentation](\\\\ https://cloud.google.com/iam/docs/).',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Updates a DataTaxonomy resource.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataplex_v1\\n\\n    def sample_update_data_taxonomy():\\n        # Create a client\\n        client = dataplex_v1.DataTaxonomyServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = dataplex_v1.UpdateDataTaxonomyRequest(\\n        )\\n\\n        # Make the request\\n        operation = client.update_data_taxonomy(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataplex_v1.types.UpdateDataTaxonomyRequest, dict]):\\n        The request object. Update DataTaxonomy request.\\n    data_taxonomy (google.cloud.dataplex_v1.types.DataTaxonomy):\\n        Required. Only fields specified in ``update_mask`` are\\n        updated.\\n\\n        This corresponds to the ``data_taxonomy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        Required. Mask of fields to update.\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.dataplex_v1.types.DataTaxonomy` DataTaxonomy represents a set of hierarchical DataAttributes resources,\\n           grouped with a common theme Eg:\\n           \\'SensitiveDataTaxonomy\\' can have attributes to manage\\n           PII data. It is defined at project level.',\n",
       " 'Call the set iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.SetIamPolicyRequest):\\n        The request object for SetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from SetIamPolicy method.',\n",
       " 'Updates the specified agent.\\n\\nNote: You should always train flows prior to sending them\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/cx/docs/concept/training>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflowcx_v3\\n\\n    def sample_update_agent():\\n        # Create a client\\n        client = dialogflowcx_v3.AgentsClient()\\n\\n        # Initialize request argument(s)\\n        agent = dialogflowcx_v3.Agent()\\n        agent.display_name = \"display_name_value\"\\n        agent.default_language_code = \"default_language_code_value\"\\n        agent.time_zone = \"time_zone_value\"\\n\\n        request = dialogflowcx_v3.UpdateAgentRequest(\\n            agent=agent,\\n        )\\n\\n        # Make the request\\n        response = client.update_agent(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflowcx_v3.types.UpdateAgentRequest, dict]):\\n        The request object. The request message for\\n        [Agents.UpdateAgent][google.cloud.dialogflow.cx.v3.Agents.UpdateAgent].\\n    agent (google.cloud.dialogflowcx_v3.types.Agent):\\n        Required. The agent to update.\\n        This corresponds to the ``agent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        The mask to control which fields get\\n        updated. If the mask is not present, all\\n        fields will be updated.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflowcx_v3.types.Agent:\\n        Agents are best described as Natural Language Understanding (NLU) modules\\n           that transform user requests into actionable data.\\n           You can include agents in your app, product, or\\n           service to determine user intent and respond to the\\n           user in a natural way.\\n\\n           After you create an agent, you can add\\n           [Intents][google.cloud.dialogflow.cx.v3.Intent],\\n           [Entity\\n           Types][google.cloud.dialogflow.cx.v3.EntityType],\\n           [Flows][google.cloud.dialogflow.cx.v3.Flow],\\n           [Fulfillments][google.cloud.dialogflow.cx.v3.Fulfillment],\\n           [Webhooks][google.cloud.dialogflow.cx.v3.Webhook],\\n           [TransitionRouteGroups][google.cloud.dialogflow.cx.v3.TransitionRouteGroup]\\n           and so on to manage the conversation flows.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Pre-rpc interceptor for delete_logging_server\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VmwareEngine server.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.clouddms_v1.types.ListMigrationJobsRequest):\\n        The initial request object.\\n    response (google.cloud.clouddms_v1.types.ListMigrationJobsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Create a new [CryptoKey][google.cloud.kms.v1.CryptoKey] within a\\n[KeyRing][google.cloud.kms.v1.KeyRing].\\n\\n[CryptoKey.purpose][google.cloud.kms.v1.CryptoKey.purpose] and\\n[CryptoKey.version_template.algorithm][google.cloud.kms.v1.CryptoKeyVersionTemplate.algorithm]\\nare required.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import kms_v1\\n\\n    def sample_create_crypto_key():\\n        # Create a client\\n        client = kms_v1.KeyManagementServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = kms_v1.CreateCryptoKeyRequest(\\n            parent=\"parent_value\",\\n            crypto_key_id=\"crypto_key_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_crypto_key(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.kms_v1.types.CreateCryptoKeyRequest, dict]):\\n        The request object. Request message for\\n        [KeyManagementService.CreateCryptoKey][google.cloud.kms.v1.KeyManagementService.CreateCryptoKey].\\n    parent (str):\\n        Required. The [name][google.cloud.kms.v1.KeyRing.name]\\n        of the KeyRing associated with the\\n        [CryptoKeys][google.cloud.kms.v1.CryptoKey].\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    crypto_key_id (str):\\n        Required. It must be unique within a KeyRing and match\\n        the regular expression ``[a-zA-Z0-9_-]{1,63}``\\n\\n        This corresponds to the ``crypto_key_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    crypto_key (google.cloud.kms_v1.types.CryptoKey):\\n        Required. A [CryptoKey][google.cloud.kms.v1.CryptoKey]\\n        with initial field values.\\n\\n        This corresponds to the ``crypto_key`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.kms_v1.types.CryptoKey:\\n        A [CryptoKey][google.cloud.kms.v1.CryptoKey] represents a logical key that\\n           can be used for cryptographic operations.\\n\\n           A [CryptoKey][google.cloud.kms.v1.CryptoKey] is made\\n           up of zero or more\\n           [versions][google.cloud.kms.v1.CryptoKeyVersion],\\n           which represent the actual key material used in\\n           cryptographic operations.',\n",
       " 'Serialize obj as bytes streamed into file\\n\\nprotocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\\npickle.HIGHEST_PROTOCOL. This setting favors maximum communication\\nspeed between processes running the same Python version.\\n\\nSet protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure\\ncompatibility with older versions of Python (although this is not always\\nguaranteed to work because cloudpickle relies on some internal\\nimplementation details that can change from one Python version to the\\nnext).',\n",
       " 'Gets the value of nonnegative or its default value.',\n",
       " 'Call the delete dashboard method over HTTP.\\n\\nArgs:\\n    request (~.dashboards_service.DeleteDashboardRequest):\\n        The request object. The ``DeleteDashboard`` request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the list instances method over HTTP.\\n\\nArgs:\\n    request (~.parallelstore.ListInstancesRequest):\\n        The request object. List instances request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.parallelstore.ListInstancesResponse:\\n        Response from\\n    [ListInstances][google.cloud.parallelstore.v1.Parallelstore.ListInstances].',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    RecaptchaEnterpriseServiceAsyncClient: The constructed client.',\n",
       " 'Call the list attachments method over HTTP.\\n\\nArgs:\\n    request (~.attachment_service.ListAttachmentsRequest):\\n        The request object. The request message for the\\n    ListAttachments endpoint.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.attachment_service.ListAttachmentsResponse:\\n        The response message for the\\n    ListAttachments endpoint.',\n",
       " 'Returns the job template data.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.video import transcoder_v1\\n\\n    def sample_get_job_template():\\n        # Create a client\\n        client = transcoder_v1.TranscoderServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = transcoder_v1.GetJobTemplateRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_job_template(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.video.transcoder_v1.types.GetJobTemplateRequest, dict]):\\n        The request object. Request message for\\n        ``TranscoderService.GetJobTemplate``.\\n    name (str):\\n        Required. The name of the job template to retrieve.\\n        Format:\\n        ``projects/{project}/locations/{location}/jobTemplates/{job_template}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.video.transcoder_v1.types.JobTemplate:\\n        Transcoding job template resource.',\n",
       " 'Return all the keys in the query params.\\n\\nUsage:\\n\\nq = httpx.QueryParams(\"a=123&a=456&b=789\")\\nassert list(q.keys()) == [\"a\", \"b\"]',\n",
       " 'Workaround for https://github.com/python/cpython/issues/84538\\nto add backward compatibility for walk_up=True.\\nAn example affected package is dask-labextension, which uses\\njupyter-packaging to install JupyterLab javascript files outside\\nof site-packages.',\n",
       " 'Only print the exception type and message, without a traceback.\\n\\nParameters\\n----------\\netype : exception type\\nevalue : exception value',\n",
       " 'Test solving a system with unsatisfiable non-required constraint.',\n",
       " 'Draw the Artist (and its children) using the given renderer.\\n\\nThis has no effect if the artist is not visible (`.Artist.get_visible`\\nreturns False).\\n\\nParameters\\n----------\\nrenderer : `~matplotlib.backend_bases.RendererBase` subclass.\\n\\nNotes\\n-----\\nThis method is overridden in the Artist subclasses.',\n",
       " 'Test the deprecated API of FontProperties.__init__.\\n\\nThe deprecation does not change behavior, it only adds a deprecation warning\\nvia a decorator. Therefore, the purpose of this test is limited to check\\nwhich calls do and do not issue deprecation warnings. Behavior is still\\ntested via the existing regular tests.',\n",
       " 'Share the z-axis with *other*.\\n\\nThis is equivalent to passing ``sharez=other`` when constructing the\\nAxes, and cannot be used if the z-axis is already being shared with\\nanother Axes.  Note that it is not possible to unshare axes.',\n",
       " 'Cancel this context.',\n",
       " 'A :class:`~pymongo.write_concern.WriteConcern` instance.',\n",
       " 'Drop a collection.\\n\\n:param name_or_collection: the name of a collection to drop or the\\n    collection object itself\\n:param session: a\\n    :class:`~pymongo.client_session.ClientSession`.\\n:param comment: A user-provided comment to attach to this\\n    command.\\n:param encrypted_fields: **(BETA)** Document that describes the encrypted fields for\\n    Queryable Encryption. For example::\\n\\n        {\\n          \"escCollection\": \"enxcol_.encryptedCollection.esc\",\\n          \"ecocCollection\": \"enxcol_.encryptedCollection.ecoc\",\\n          \"fields\": [\\n              {\\n                  \"path\": \"firstName\",\\n                  \"keyId\": Binary.from_uuid(UUID(\\'00000000-0000-0000-0000-000000000000\\')),\\n                  \"bsonType\": \"string\",\\n                  \"queries\": {\"queryType\": \"equality\"}\\n              },\\n              {\\n                  \"path\": \"ssn\",\\n                  \"keyId\": Binary.from_uuid(UUID(\\'04104104-1041-0410-4104-104104104104\\')),\\n                  \"bsonType\": \"string\"\\n              }\\n          ]\\n\\n        }\\n\\n\\n.. note:: The :attr:`~pymongo.database.Database.write_concern` of\\n   this database is automatically applied to this operation.\\n\\n.. versionchanged:: 4.2\\n   Added ``encrypted_fields`` parameter.\\n\\n.. versionchanged:: 4.1\\n   Added ``comment`` parameter.\\n\\n.. versionchanged:: 3.6\\n   Added ``session`` parameter.\\n\\n.. versionchanged:: 3.4\\n   Apply this database\\'s write concern automatically to this operation\\n   when connected to MongoDB >= 3.4.',\n",
       " 'Trivial example',\n",
       " 'Color a graph using various strategies of greedy graph coloring.\\n\\nAttempts to color a graph using as few colors as possible, where no\\nneighbors of a node can have same color as the node itself. The\\ngiven strategy determines the order in which nodes are colored.\\n\\nThe strategies are described in [1]_, and smallest-last is based on\\n[2]_.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\nstrategy : string or function(G, colors)\\n   A function (or a string representing a function) that provides\\n   the coloring strategy, by returning nodes in the ordering they\\n   should be colored. ``G`` is the graph, and ``colors`` is a\\n   dictionary of the currently assigned colors, keyed by nodes. The\\n   function must return an iterable over all the nodes in ``G``.\\n\\n   If the strategy function is an iterator generator (that is, a\\n   function with ``yield`` statements), keep in mind that the\\n   ``colors`` dictionary will be updated after each ``yield``, since\\n   this function chooses colors greedily.\\n\\n   If ``strategy`` is a string, it must be one of the following,\\n   each of which represents one of the built-in strategy functions.\\n\\n   * ``\\'largest_first\\'``\\n   * ``\\'random_sequential\\'``\\n   * ``\\'smallest_last\\'``\\n   * ``\\'independent_set\\'``\\n   * ``\\'connected_sequential_bfs\\'``\\n   * ``\\'connected_sequential_dfs\\'``\\n   * ``\\'connected_sequential\\'`` (alias for the previous strategy)\\n   * ``\\'saturation_largest_first\\'``\\n   * ``\\'DSATUR\\'`` (alias for the previous strategy)\\n\\ninterchange: bool\\n   Will use the color interchange algorithm described by [3]_ if set\\n   to ``True``.\\n\\n   Note that ``saturation_largest_first`` and ``independent_set``\\n   do not work with interchange. Furthermore, if you use\\n   interchange with your own strategy function, you cannot rely\\n   on the values in the ``colors`` argument.\\n\\nReturns\\n-------\\nA dictionary with keys representing nodes and values representing\\ncorresponding coloring.\\n\\nExamples\\n--------\\n>>> G = nx.cycle_graph(4)\\n>>> d = nx.coloring.greedy_color(G, strategy=\"largest_first\")\\n>>> d in [{0: 0, 1: 1, 2: 0, 3: 1}, {0: 1, 1: 0, 2: 1, 3: 0}]\\nTrue\\n\\nRaises\\n------\\nNetworkXPointlessConcept\\n    If ``strategy`` is ``saturation_largest_first`` or\\n    ``independent_set`` and ``interchange`` is ``True``.\\n\\nReferences\\n----------\\n.. [1] Adrian Kosowski, and Krzysztof Manuszewski,\\n   Classical Coloring of Graphs, Graph Colorings, 2-19, 2004.\\n   ISBN 0-8218-3458-4.\\n.. [2] David W. Matula, and Leland L. Beck, \"Smallest-last\\n   ordering and clustering and graph coloring algorithms.\" *J. ACM* 30,\\n   3 (July 1983), 417â€“427. <https://doi.org/10.1145/2402.322385>\\n.. [3] Maciej M. SysÅ‚o, Narsingh Deo, Janusz S. Kowalik,\\n   Discrete Optimization Algorithms with Pascal Programs, 415-424, 1983.\\n   ISBN 0-486-45353-7.',\n",
       " 'Checks safety of getfield for object arrays.\\n\\nAs in _view_is_safe, we need to check that memory containing objects is not\\nreinterpreted as a non-object datatype and vice versa.\\n\\nParameters\\n----------\\noldtype : data-type\\n    Data type of the original ndarray.\\nnewtype : data-type\\n    Data type of the field being accessed by ndarray.getfield\\noffset : int\\n    Offset of the field being accessed by ndarray.getfield\\n\\nRaises\\n------\\nTypeError\\n    If the field access is invalid',\n",
       " 'Helper function to cast a result back into an array\\nwith the appropriate dtype if an object array must be used\\nas an intermediary.',\n",
       " 'Assumes that q is in [0, 1], and is an ndarray',\n",
       " 'Verify fallback parameter parsing and validation for token responses. ',\n",
       " \"An alternative to `.with_raw_response` that doesn't eagerly read the response body.\\n\\nFor more information, see https://www.github.com/openai/openai-python#with_streaming_response\",\n",
       " 'Emits the :class:`LogData` by associating :class:`LogRecord`\\nand instrumentation info.',\n",
       " 'Size of HTTP server request bodies',\n",
       " 'Encrypts plaintext using AES in CBC mode with a 128, 192 or 256 bit key and\\nPKCS#7 padding.\\n\\n:param key:\\n    The encryption key - a byte string either 16, 24 or 32 bytes long\\n\\n:param data:\\n    The plaintext - a byte string\\n\\n:param iv:\\n    The initialization vector - either a byte string 16-bytes long or None\\n    to generate an IV\\n\\n:raises:\\n    ValueError - when any of the parameters contain an invalid value\\n    TypeError - when any of the parameters are of the wrong type\\n    OSError - when an error is returned by OpenSSL\\n\\n:return:\\n    A tuple of two byte strings (iv, ciphertext)',\n",
       " 'A list of asn1crypto.x509.Certificate objects that were presented as\\nintermediates by the server',\n",
       " 'Set the ordered attribute to the boolean value.\\n\\nParameters\\n----------\\nvalue : bool\\n   Set whether this categorical is ordered (True) or not (False).',\n",
       " 'Switch outbound data cipher.\\n:param etm: Set encrypt-then-mac from OpenSSH',\n",
       " \"Computes the Gauss-Legendre quadrature [1]_ points and weights.\\n\\nExplanation\\n===========\\n\\nThe Gauss-Legendre quadrature approximates the integral:\\n\\n.. math::\\n    \\\\int_{-1}^1 f(x)\\\\,dx \\\\approx \\\\sum_{i=1}^n w_i f(x_i)\\n\\nThe nodes `x_i` of an order `n` quadrature rule are the roots of `P_n`\\nand the weights `w_i` are given by:\\n\\n.. math::\\n    w_i = \\\\frac{2}{\\\\left(1-x_i^2\\\\right) \\\\left(P'_n(x_i)\\\\right)^2}\\n\\nParameters\\n==========\\n\\nn :\\n    The order of quadrature.\\nn_digits :\\n    Number of significant digits of the points and weights to return.\\n\\nReturns\\n=======\\n\\n(x, w) : the ``x`` and ``w`` are lists of points and weights as Floats.\\n         The points `x_i` and weights `w_i` are returned as ``(x, w)``\\n         tuple of lists.\\n\\nExamples\\n========\\n\\n>>> from sympy.integrals.quadrature import gauss_legendre\\n>>> x, w = gauss_legendre(3, 5)\\n>>> x\\n[-0.7746, 0, 0.7746]\\n>>> w\\n[0.55556, 0.88889, 0.55556]\\n>>> x, w = gauss_legendre(4, 5)\\n>>> x\\n[-0.86114, -0.33998, 0.33998, 0.86114]\\n>>> w\\n[0.34785, 0.65215, 0.65215, 0.34785]\\n\\nSee Also\\n========\\n\\ngauss_laguerre, gauss_gen_laguerre, gauss_hermite, gauss_chebyshev_t, gauss_chebyshev_u, gauss_jacobi, gauss_lobatto\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Gaussian_quadrature\\n.. [2] https://people.sc.fsu.edu/~jburkardt/cpp_src/legendre_rule/legendre_rule.html\",\n",
       " 'Return URL of the first remote encountered.\\n\\nRaises RemoteNotFoundError if the repository does not have a remote\\nurl configured.',\n",
       " 'This override allows the auth information to be passed to svn via the\\n--username and --password options instead of via the URL.',\n",
       " 'An internal helper for Literal creation: flatten Literals among parameters',\n",
       " 'Check builds with a custom backends work, even with no cache.',\n",
       " 'Test basic install and uninstall.',\n",
       " 'Create a new EKSBlowfish cipher\\n\\nArgs:\\n\\n  key (bytes, bytearray, memoryview):\\n    The secret key to use in the symmetric cipher.\\n    Its length can vary from 0 to 72 bytes.\\n\\n  mode (one of the supported ``MODE_*`` constants):\\n    The chaining mode to use for encryption or decryption.\\n\\n  salt (bytes, bytearray, memoryview):\\n    The salt that bcrypt uses to thwart rainbow table attacks\\n\\n  cost (integer):\\n    The complexity factor in bcrypt\\n\\n  invert (bool):\\n    If ``False``, in the inner loop use ``ExpandKey`` first over the salt\\n    and then over the key, as defined in\\n    the `original bcrypt specification <https://www.usenix.org/legacy/events/usenix99/provos/provos_html/node4.html>`_.\\n    If ``True``, reverse the order, as in the first implementation of\\n    `bcrypt` in OpenBSD.\\n\\n:Return: an EKSBlowfish object',\n",
       " 'Encode the data using the specified encoder.\\n\\nArgs:\\n    value: The data to encode.\\n\\nReturns:\\n    The encoded data.',\n",
       " \"On method node, check if this method couldn't be a function.\\n\\nignore class, static and abstract methods, initializer,\\nmethods overridden from a parent class.\",\n",
       " 'Return something with `set_pubkey` and `sign` methods.',\n",
       " 'Read and return the captured output so far, resetting the internal\\nbuffer.\\n\\n:returns:\\n    The captured content as a namedtuple with ``out`` and ``err``\\n    string attributes.',\n",
       " 'Preconditions:\\n- our agent applies to this entry\\n- filename is URL decoded',\n",
       " 'Selectively log an accepted request.',\n",
       " 'Regression: allow rows with different number of columns (issue #85)',\n",
       " 'list_pod_template_for_all_namespaces  # noqa: E501\\n\\nlist or watch objects of kind PodTemplate  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.list_pod_template_for_all_namespaces(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param bool allow_watch_bookmarks: allowWatchBookmarks requests watch events with type \"BOOKMARK\". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server\\'s discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1PodTemplateList\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'delete_collection_namespaced_ingress  # noqa: E501\\n\\ndelete collection of Ingress  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_ingress(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1Status\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'V1LinuxContainerUser - a model defined in OpenAPI',\n",
       " 'V1VsphereVirtualDiskVolumeSource - a model defined in OpenAPI',\n",
       " 'Return maximum value from the sketch `key`. Will return DBL_MIN if the sketch is empty.\\nFor more information see `TDIGEST.MAX <https://redis.io/commands/tdigest.max>`_.',\n",
       " 'Execute after an instance is attached to a session.\\n\\nThis is called after an add, delete or merge.\\n\\n.. note::\\n\\n   As of 0.8, this event fires off *after* the item\\n   has been fully associated with the session, which is\\n   different than previous releases.  For event\\n   handlers that require the object not yet\\n   be part of session state (such as handlers which\\n   may autoflush while the target object is not\\n   yet complete) consider the\\n   new :meth:`.before_attach` event.\\n\\n.. seealso::\\n\\n    :meth:`~.SessionEvents.before_attach`\\n\\n    :ref:`session_lifecycle_events`',\n",
       " 'Bundle right_size() parameters into an endpoint configuration for Advanced job',\n",
       " 'Execute the statement represented by this\\n:class:`.ORMExecuteState`, without re-invoking events that have\\nalready proceeded.\\n\\nThis method essentially performs a re-entrant execution of the current\\nstatement for which the :meth:`.SessionEvents.do_orm_execute` event is\\nbeing currently invoked.    The use case for this is for event handlers\\nthat want to override how the ultimate\\n:class:`_engine.Result` object is returned, such as for schemes that\\nretrieve results from an offline cache or which concatenate results\\nfrom multiple executions.\\n\\nWhen the :class:`_engine.Result` object is returned by the actual\\nhandler function within :meth:`_orm.SessionEvents.do_orm_execute` and\\nis propagated to the calling\\n:meth:`_orm.Session.execute` method, the remainder of the\\n:meth:`_orm.Session.execute` method is preempted and the\\n:class:`_engine.Result` object is returned to the caller of\\n:meth:`_orm.Session.execute` immediately.\\n\\n:param statement: optional statement to be invoked, in place of the\\n statement currently represented by :attr:`.ORMExecuteState.statement`.\\n\\n:param params: optional dictionary of parameters or list of parameters\\n which will be merged into the existing\\n :attr:`.ORMExecuteState.parameters` of this :class:`.ORMExecuteState`.\\n\\n .. versionchanged:: 2.0 a list of parameter dictionaries is accepted\\n    for executemany executions.\\n\\n:param execution_options: optional dictionary of execution options\\n will be merged into the existing\\n :attr:`.ORMExecuteState.execution_options` of this\\n :class:`.ORMExecuteState`.\\n\\n:param bind_arguments: optional dictionary of bind_arguments\\n which will be merged amongst the current\\n :attr:`.ORMExecuteState.bind_arguments`\\n of this :class:`.ORMExecuteState`.\\n\\n:return: a :class:`_engine.Result` object with ORM-level results.\\n\\n.. seealso::\\n\\n    :ref:`do_orm_execute_re_executing` - background and examples on the\\n    appropriate usage of :meth:`_orm.ORMExecuteState.invoke_statement`.',\n",
       " 'Placeholder docstring',\n",
       " 'Context manager for global scikit-learn configuration.\\n\\nParameters\\n----------\\nassume_finite : bool, default=None\\n    If True, validation for finiteness will be skipped,\\n    saving time, but leading to potential crashes. If\\n    False, validation for finiteness will be performed,\\n    avoiding error. If None, the existing value won\\'t change.\\n    The default value is False.\\n\\nworking_memory : int, default=None\\n    If set, scikit-learn will attempt to limit the size of temporary arrays\\n    to this number of MiB (per job when parallelised), often saving both\\n    computation time and memory on expensive operations that can be\\n    performed in chunks. If None, the existing value won\\'t change.\\n    The default value is 1024.\\n\\nprint_changed_only : bool, default=None\\n    If True, only the parameters that were set to non-default\\n    values will be printed when printing an estimator. For example,\\n    ``print(SVC())`` while True will only print \\'SVC()\\', but would print\\n    \\'SVC(C=1.0, cache_size=200, ...)\\' with all the non-changed parameters\\n    when False. If None, the existing value won\\'t change.\\n    The default value is True.\\n\\n    .. versionchanged:: 0.23\\n       Default changed from False to True.\\n\\ndisplay : {\\'text\\', \\'diagram\\'}, default=None\\n    If \\'diagram\\', estimators will be displayed as a diagram in a Jupyter\\n    lab or notebook context. If \\'text\\', estimators will be displayed as\\n    text. If None, the existing value won\\'t change.\\n    The default value is \\'diagram\\'.\\n\\n    .. versionadded:: 0.23\\n\\npairwise_dist_chunk_size : int, default=None\\n    The number of row vectors per chunk for the accelerated pairwise-\\n    distances reduction backend. Default is 256 (suitable for most of\\n    modern laptops\\' caches and architectures).\\n\\n    Intended for easier benchmarking and testing of scikit-learn internals.\\n    End users are not expected to benefit from customizing this configuration\\n    setting.\\n\\n    .. versionadded:: 1.1\\n\\nenable_cython_pairwise_dist : bool, default=None\\n    Use the accelerated pairwise-distances reduction backend when\\n    possible. Global default: True.\\n\\n    Intended for easier benchmarking and testing of scikit-learn internals.\\n    End users are not expected to benefit from customizing this configuration\\n    setting.\\n\\n    .. versionadded:: 1.1\\n\\narray_api_dispatch : bool, default=None\\n    Use Array API dispatching when inputs follow the Array API standard.\\n    Default is False.\\n\\n    See the :ref:`User Guide <array_api>` for more details.\\n\\n    .. versionadded:: 1.2\\n\\ntransform_output : str, default=None\\n    Configure output of `transform` and `fit_transform`.\\n\\n    See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\\n    for an example on how to use the API.\\n\\n    - `\"default\"`: Default output format of a transformer\\n    - `\"pandas\"`: DataFrame output\\n    - `\"polars\"`: Polars output\\n    - `None`: Transform configuration is unchanged\\n\\n    .. versionadded:: 1.2\\n    .. versionadded:: 1.4\\n        `\"polars\"` option was added.\\n\\nenable_metadata_routing : bool, default=None\\n    Enable metadata routing. By default this feature is disabled.\\n\\n    Refer to :ref:`metadata routing user guide <metadata_routing>` for more\\n    details.\\n\\n    - `True`: Metadata routing is enabled\\n    - `False`: Metadata routing is disabled, use the old syntax.\\n    - `None`: Configuration is unchanged\\n\\n    .. versionadded:: 1.3\\n\\nskip_parameter_validation : bool, default=None\\n    If `True`, disable the validation of the hyper-parameters\\' types and values in\\n    the fit method of estimators and for arguments passed to public helper\\n    functions. It can save time in some situations but can lead to low level\\n    crashes and exceptions with confusing error messages.\\n\\n    Note that for data parameters, such as `X` and `y`, only type validation is\\n    skipped but validation with `check_array` will continue to run.\\n\\n    .. versionadded:: 1.3\\n\\nYields\\n------\\nNone.\\n\\nSee Also\\n--------\\nset_config : Set global scikit-learn configuration.\\nget_config : Retrieve current values of the global configuration.\\n\\nNotes\\n-----\\nAll settings, not just those presently modified, will be returned to\\ntheir previous values when the context manager is exited.\\n\\nExamples\\n--------\\n>>> import sklearn\\n>>> from sklearn.utils.validation import assert_all_finite\\n>>> with sklearn.config_context(assume_finite=True):\\n...     assert_all_finite([float(\\'nan\\')])\\n>>> with sklearn.config_context(assume_finite=True):\\n...     with sklearn.config_context(assume_finite=False):\\n...         assert_all_finite([float(\\'nan\\')])\\nTraceback (most recent call last):\\n...\\nValueError: Input contains NaN...',\n",
       " \"Solve a dictionary learning matrix factorization problem online.\\n\\nFinds the best dictionary and the corresponding sparse code for\\napproximating the data matrix X by solving::\\n\\n    (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\\n                 (U,V)\\n                 with || V_k ||_2 = 1 for all  0 <= k < n_components\\n\\nwhere V is the dictionary and U is the sparse code. ||.||_Fro stands for\\nthe Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\\nwhich is the sum of the absolute values of all the entries in the matrix.\\nThis is accomplished by repeatedly iterating over mini-batches by slicing\\nthe input data.\\n\\nRead more in the :ref:`User Guide <DictionaryLearning>`.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    Data matrix.\\n\\nn_components : int or None, default=2\\n    Number of dictionary atoms to extract. If None, then ``n_components``\\n    is set to ``n_features``.\\n\\nalpha : float, default=1\\n    Sparsity controlling parameter.\\n\\nmax_iter : int, default=100\\n    Maximum number of iterations over the complete dataset before\\n    stopping independently of any early stopping criterion heuristics.\\n\\n    .. versionadded:: 1.1\\n\\nreturn_code : bool, default=True\\n    Whether to also return the code U or just the dictionary `V`.\\n\\ndict_init : ndarray of shape (n_components, n_features), default=None\\n    Initial values for the dictionary for warm restart scenarios.\\n    If `None`, the initial values for the dictionary are created\\n    with an SVD decomposition of the data via\\n    :func:`~sklearn.utils.extmath.randomized_svd`.\\n\\ncallback : callable, default=None\\n    A callable that gets invoked at the end of each iteration.\\n\\nbatch_size : int, default=256\\n    The number of samples to take in each batch.\\n\\n    .. versionchanged:: 1.3\\n       The default value of `batch_size` changed from 3 to 256 in version 1.3.\\n\\nverbose : bool, default=False\\n    To control the verbosity of the procedure.\\n\\nshuffle : bool, default=True\\n    Whether to shuffle the data before splitting it in batches.\\n\\nn_jobs : int, default=None\\n    Number of parallel jobs to run.\\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n    for more details.\\n\\nmethod : {'lars', 'cd'}, default='lars'\\n    * `'lars'`: uses the least angle regression method to solve the lasso\\n      problem (`linear_model.lars_path`);\\n    * `'cd'`: uses the coordinate descent method to compute the\\n      Lasso solution (`linear_model.Lasso`). Lars will be faster if\\n      the estimated components are sparse.\\n\\nrandom_state : int, RandomState instance or None, default=None\\n    Used for initializing the dictionary when ``dict_init`` is not\\n    specified, randomly shuffling the data when ``shuffle`` is set to\\n    ``True``, and updating the dictionary. Pass an int for reproducible\\n    results across multiple function calls.\\n    See :term:`Glossary <random_state>`.\\n\\npositive_dict : bool, default=False\\n    Whether to enforce positivity when finding the dictionary.\\n\\n    .. versionadded:: 0.20\\n\\npositive_code : bool, default=False\\n    Whether to enforce positivity when finding the code.\\n\\n    .. versionadded:: 0.20\\n\\nmethod_max_iter : int, default=1000\\n    Maximum number of iterations to perform when solving the lasso problem.\\n\\n    .. versionadded:: 0.22\\n\\ntol : float, default=1e-3\\n    Control early stopping based on the norm of the differences in the\\n    dictionary between 2 steps.\\n\\n    To disable early stopping based on changes in the dictionary, set\\n    `tol` to 0.0.\\n\\n    .. versionadded:: 1.1\\n\\nmax_no_improvement : int, default=10\\n    Control early stopping based on the consecutive number of mini batches\\n    that does not yield an improvement on the smoothed cost function.\\n\\n    To disable convergence detection based on cost function, set\\n    `max_no_improvement` to None.\\n\\n    .. versionadded:: 1.1\\n\\nReturns\\n-------\\ncode : ndarray of shape (n_samples, n_components),\\n    The sparse code (only returned if `return_code=True`).\\n\\ndictionary : ndarray of shape (n_components, n_features),\\n    The solutions to the dictionary learning problem.\\n\\nn_iter : int\\n    Number of iterations run. Returned only if `return_n_iter` is\\n    set to `True`.\\n\\nSee Also\\n--------\\ndict_learning : Solve a dictionary learning matrix factorization problem.\\nDictionaryLearning : Find a dictionary that sparsely encodes data.\\nMiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\\n    learning algorithm.\\nSparsePCA : Sparse Principal Components Analysis.\\nMiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from sklearn.datasets import make_sparse_coded_signal\\n>>> from sklearn.decomposition import dict_learning_online\\n>>> X, _, _ = make_sparse_coded_signal(\\n...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\\n...     random_state=42,\\n... )\\n>>> U, V = dict_learning_online(\\n...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\\n... )\\n\\nWe can check the level of sparsity of `U`:\\n\\n>>> np.mean(U == 0)\\nnp.float64(0.53...)\\n\\nWe can compare the average squared euclidean norm of the reconstruction\\nerror of the sparse coded signal relative to the squared euclidean norm of\\nthe original signal:\\n\\n>>> X_hat = U @ V\\n>>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\\nnp.float64(0.05...)\",\n",
       " 'Print info about the current fitting iteration.',\n",
       " 'Estimate mutual information for a continuous target variable.\\n\\nMutual information (MI) [1]_ between two random variables is a non-negative\\nvalue, which measures the dependency between the variables. It is equal\\nto zero if and only if two random variables are independent, and higher\\nvalues mean higher dependency.\\n\\nThe function relies on nonparametric methods based on entropy estimation\\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\\nmethods are based on the idea originally proposed in [4]_.\\n\\nIt can be used for univariate features selection, read more in the\\n:ref:`User Guide <univariate_feature_selection>`.\\n\\nParameters\\n----------\\nX : array-like or sparse matrix, shape (n_samples, n_features)\\n    Feature matrix.\\n\\ny : array-like of shape (n_samples,)\\n    Target vector.\\n\\ndiscrete_features : {\\'auto\\', bool, array-like}, default=\\'auto\\'\\n    If bool, then determines whether to consider all features discrete\\n    or continuous. If array, then it should be either a boolean mask\\n    with shape (n_features,) or array with indices of discrete features.\\n    If \\'auto\\', it is assigned to False for dense `X` and to True for\\n    sparse `X`.\\n\\nn_neighbors : int, default=3\\n    Number of neighbors to use for MI estimation for continuous variables,\\n    see [2]_ and [3]_. Higher values reduce variance of the estimation, but\\n    could introduce a bias.\\n\\ncopy : bool, default=True\\n    Whether to make a copy of the given data. If set to False, the initial\\n    data will be overwritten.\\n\\nrandom_state : int, RandomState instance or None, default=None\\n    Determines random number generation for adding small noise to\\n    continuous variables in order to remove repeated values.\\n    Pass an int for reproducible results across multiple function calls.\\n    See :term:`Glossary <random_state>`.\\n\\nn_jobs : int, default=None\\n    The number of jobs to use for computing the mutual information.\\n    The parallelization is done on the columns of `X`.\\n\\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\\n    for more details.\\n\\n    .. versionadded:: 1.5\\n\\nReturns\\n-------\\nmi : ndarray, shape (n_features,)\\n    Estimated mutual information between each feature and the target in\\n    nat units.\\n\\nNotes\\n-----\\n1. The term \"discrete features\" is used instead of naming them\\n   \"categorical\", because it describes the essence more accurately.\\n   For example, pixel intensities of an image are discrete features\\n   (but hardly categorical) and you will get better results if mark them\\n   as such. Also note, that treating a continuous variable as discrete and\\n   vice versa will usually give incorrect results, so be attentive about\\n   that.\\n2. True mutual information can\\'t be negative. If its estimate turns out\\n   to be negative, it is replaced by zero.\\n\\nReferences\\n----------\\n.. [1] `Mutual Information\\n       <https://en.wikipedia.org/wiki/Mutual_information>`_\\n       on Wikipedia.\\n.. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\\n       information\". Phys. Rev. E 69, 2004.\\n.. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\\n       Data Sets\". PLoS ONE 9(2), 2014.\\n.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\\n       of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\\n\\nExamples\\n--------\\n>>> from sklearn.datasets import make_regression\\n>>> from sklearn.feature_selection import mutual_info_regression\\n>>> X, y = make_regression(\\n...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42\\n... )\\n>>> mutual_info_regression(X, y)\\narray([0.1..., 2.6...  , 0.0...])',\n",
       " 'Fit the imputer on `X` and return the transformed `X`.\\n\\nParameters\\n----------\\nX : array-like, shape (n_samples, n_features)\\n    Input data, where `n_samples` is the number of samples and\\n    `n_features` is the number of features.\\n\\ny : Ignored\\n    Not used, present for API consistency by convention.\\n\\n**params : dict\\n    Parameters routed to the `fit` method of the sub-estimator via the\\n    metadata routing API.\\n\\n    .. versionadded:: 1.5\\n      Only available if\\n      `sklearn.set_config(enable_metadata_routing=True)` is set. See\\n      :ref:`Metadata Routing User Guide <metadata_routing>` for more\\n      details.\\n\\nReturns\\n-------\\nXt : array-like, shape (n_samples, n_features)\\n    The imputed input data.',\n",
       " \"Plot visualization.\\n\\nParameters\\n----------\\ninclude_values : bool, default=True\\n    Includes values in confusion matrix.\\n\\ncmap : str or matplotlib Colormap, default='viridis'\\n    Colormap recognized by matplotlib.\\n\\nxticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\\n    Rotation of xtick labels.\\n\\nvalues_format : str, default=None\\n    Format specification for values in confusion matrix. If `None`,\\n    the format specification is 'd' or '.2g' whichever is shorter.\\n\\nax : matplotlib axes, default=None\\n    Axes object to plot on. If `None`, a new figure and axes is\\n    created.\\n\\ncolorbar : bool, default=True\\n    Whether or not to add a colorbar to the plot.\\n\\nim_kw : dict, default=None\\n    Dict with keywords passed to `matplotlib.pyplot.imshow` call.\\n\\ntext_kw : dict, default=None\\n    Dict with keywords passed to `matplotlib.pyplot.text` call.\\n\\n    .. versionadded:: 1.2\\n\\nReturns\\n-------\\ndisplay : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\\n    Returns a :class:`~sklearn.metrics.ConfusionMatrixDisplay` instance\\n    that contains all the information to plot the confusion matrix.\",\n",
       " \"Fit the k-nearest neighbors classifier from the training dataset.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\\n    Training data.\\n\\ny : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\\n    Target values.\\n\\nReturns\\n-------\\nself : KNeighborsClassifier\\n    The fitted k-nearest neighbors classifier.\",\n",
       " 'Check MAE criterion produces correct results on small toy dataset:\\n\\n------------------\\n| X | y | weight |\\n------------------\\n| 3 | 3 |  0.1   |\\n| 5 | 3 |  0.3   |\\n| 8 | 4 |  1.0   |\\n| 3 | 6 |  0.6   |\\n| 5 | 7 |  0.3   |\\n------------------\\n|sum wt:|  2.3   |\\n------------------\\n\\nBecause we are dealing with sample weights, we cannot find the median by\\nsimply choosing/averaging the centre value(s), instead we consider the\\nmedian where 50% of the cumulative weight is found (in a y sorted data set)\\n. Therefore with regards to this test data, the cumulative weight is >= 50%\\nwhen y = 4.  Therefore:\\nMedian = 4\\n\\nFor all the samples, we can get the total error by summing:\\nAbsolute(Median - y) * weight\\n\\nI.e., total error = (Absolute(4 - 3) * 0.1)\\n                  + (Absolute(4 - 3) * 0.3)\\n                  + (Absolute(4 - 4) * 1.0)\\n                  + (Absolute(4 - 6) * 0.6)\\n                  + (Absolute(4 - 7) * 0.3)\\n                  = 2.5\\n\\nImpurity = Total error / total weight\\n         = 2.5 / 2.3\\n         = 1.08695652173913\\n         ------------------\\n\\nFrom this root node, the next best split is between X values of 3 and 5.\\nThus, we have left and right child nodes:\\n\\nLEFT                    RIGHT\\n------------------      ------------------\\n| X | y | weight |      | X | y | weight |\\n------------------      ------------------\\n| 3 | 3 |  0.1   |      | 5 | 3 |  0.3   |\\n| 3 | 6 |  0.6   |      | 8 | 4 |  1.0   |\\n------------------      | 5 | 7 |  0.3   |\\n|sum wt:|  0.7   |      ------------------\\n------------------      |sum wt:|  1.6   |\\n                        ------------------\\n\\nImpurity is found in the same way:\\nLeft node Median = 6\\nTotal error = (Absolute(6 - 3) * 0.1)\\n            + (Absolute(6 - 6) * 0.6)\\n            = 0.3\\n\\nLeft Impurity = Total error / total weight\\n        = 0.3 / 0.7\\n        = 0.428571428571429\\n        -------------------\\n\\nLikewise for Right node:\\nRight node Median = 4\\nTotal error = (Absolute(4 - 3) * 0.3)\\n            + (Absolute(4 - 4) * 1.0)\\n            + (Absolute(4 - 7) * 0.3)\\n            = 1.2\\n\\nRight Impurity = Total error / total weight\\n        = 1.2 / 1.6\\n        = 0.75\\n        ------',\n",
       " 'If the name provided to has_command() exists as a command, but gives a non-zero\\nreturn code, there should be a log message generated.',\n",
       " \"Plot DataFrame/Series as lines.\\n\\nThis function is useful to plot lines using DataFrameâ€™s values\\nas coordinates.\\n\\nParameters\\n----------\\nx : int or str, optional\\n    Columns to use for the horizontal axis.\\n    Either the location or the label of the columns to be used.\\n    By default, it will use the DataFrame indices.\\ny : int, str, or list of them, optional\\n    The values to be plotted.\\n    Either the location or the label of the columns to be used.\\n    By default, it will use the remaining DataFrame numeric columns.\\n**kwds\\n    Keyword arguments to pass on to :meth:`Series.plot` or :meth:`DataFrame.plot`.\\n\\nReturns\\n-------\\n:class:`plotly.graph_objs.Figure`\\n    Return an custom object when ``backend!=plotly``.\\n    Return an ndarray when ``subplots=True`` (matplotlib-only).\\n\\nSee Also\\n--------\\nplotly.express.line : Plot y versus x as lines and/or markers (plotly).\\nmatplotlib.pyplot.plot : Plot y versus x as lines and/or markers (matplotlib).\\n\\nExamples\\n--------\\nBasic plot.\\n\\nFor Series:\\n\\n.. plotly::\\n\\n    >>> s = ps.Series([1, 3, 2])\\n    >>> s.plot.line()  # doctest: +SKIP\\n\\nFor DataFrame:\\n\\n.. plotly::\\n\\n    The following example shows the populations for some animals\\n    over the years.\\n\\n    >>> df = ps.DataFrame({'pig': [20, 18, 489, 675, 1776],\\n    ...                    'horse': [4, 25, 281, 600, 1900]},\\n    ...                   index=[1990, 1997, 2003, 2009, 2014])\\n    >>> df.plot.line()  # doctest: +SKIP\\n\\n.. plotly::\\n\\n    The following example shows the relationship between both\\n    populations.\\n\\n    >>> df = ps.DataFrame({'pig': [20, 18, 489, 675, 1776],\\n    ...                    'horse': [4, 25, 281, 600, 1900]},\\n    ...                   index=[1990, 1997, 2003, 2009, 2014])\\n    >>> df.plot.line(x='pig', y='horse')  # doctest: +SKIP\",\n",
       " 'Set decay factor.',\n",
       " 'Applies standardization transformation on a vector.\\n\\n.. versionadded:: 1.2.0\\n\\nParameters\\n----------\\nvector : :py:class:`pyspark.mllib.linalg.Vector` or :py:class:`pyspark.RDD`\\n    Input vector(s) to be standardized.\\n\\nReturns\\n-------\\n:py:class:`pyspark.mllib.linalg.Vector` or :py:class:`pyspark.RDD`\\n    Standardized vector(s). If the variance of a column is\\n    zero, it will return default `0.0` for the column with\\n    zero variance.\\n\\nNotes\\n-----\\nIn Python, transform cannot currently be used within\\nan RDD transformation or action.\\nCall transform directly on the RDD instead.',\n",
       " 'Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\\n\\n.. versionadded:: 1.3.0\\n\\n.. versionchanged:: 3.4.0\\n    Supports Spark Connect.\\n\\nParameters\\n----------\\nnum : int\\n    Number of records to return. Will return this number of records\\n    or all records if the DataFrame contains less than this number of records..\\n\\nReturns\\n-------\\nlist\\n    List of rows\\n\\nExamples\\n--------\\n>>> df = spark.createDataFrame(\\n...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\\n\\nReturn the first 2 rows of the :class:`DataFrame`.\\n\\n>>> df.take(2)\\n[Row(age=14, name=\\'Tom\\'), Row(age=23, name=\\'Alice\\')]',\n",
       " 'Check the function should be unwrapped on getting signature.',\n",
       " 'Return a :class:`_engine.ScalarResult` filtering object which\\nwill return single elements rather than :class:`_row.Row` objects.\\n\\nE.g.::\\n\\n    >>> result = conn.execute(text(\"select int_id from table\"))\\n    >>> result.scalars().all()\\n    [1, 2, 3]\\n\\nWhen results are fetched from the :class:`_engine.ScalarResult`\\nfiltering object, the single column-row that would be returned by the\\n:class:`_engine.Result` is instead returned as the column\\'s value.\\n\\n.. versionadded:: 1.4\\n\\n:param index: integer or row key indicating the column to be fetched\\n from each row, defaults to ``0`` indicating the first column.\\n\\n:return: a new :class:`_engine.ScalarResult` filtering object referring\\n to this :class:`_engine.Result` object.',\n",
       " 'one to one delete_orphan relationships marked load_on_pending\\nshould be able to merge()',\n",
       " 'This is a linear time unranking algorithm that does not\\nrespect lexicographic order [3].\\n\\nExamples\\n========\\n\\n>>> from sympy.combinatorics import Permutation\\n>>> from sympy import init_printing\\n>>> init_printing(perm_cyclic=False, pretty_print=False)\\n>>> Permutation.unrank_nonlex(4, 5)\\nPermutation([2, 0, 3, 1])\\n>>> Permutation.unrank_nonlex(4, -1)\\nPermutation([0, 1, 2, 3])\\n\\nSee Also\\n========\\n\\nnext_nonlex, rank_nonlex',\n",
       " 'self >= other',\n",
       " 'Form the block matrices for composing M, A, and B.',\n",
       " 'Returns True if ``self`` is a zero module.\\n\\n(If, as this implementation assumes, the coefficient ring is not the\\nzero ring, then this is equivalent to the rank being zero.)\\n\\nExamples\\n========\\n\\n>>> from sympy.abc import x\\n>>> from sympy import QQ\\n>>> QQ.old_poly_ring(x).free_module(0).is_zero()\\nTrue\\n>>> QQ.old_poly_ring(x).free_module(1).is_zero()\\nFalse',\n",
       " 'Wang/EEZ: Test evaluation points for suitability. ',\n",
       " 'return the minimal polynomial for ``op(ex1, ex2)``\\n\\nParameters\\n==========\\n\\nop : operation ``Add`` or ``Mul``\\nex1, ex2 : expressions for the algebraic elements\\nx : indeterminate of the polynomials\\ndom: ground domain\\nmp1, mp2 : minimal polynomials for ``ex1`` and ``ex2`` or None\\n\\nExamples\\n========\\n\\n>>> from sympy import sqrt, Add, Mul, QQ\\n>>> from sympy.polys.numberfields.minpoly import _minpoly_op_algebraic_element\\n>>> from sympy.abc import x, y\\n>>> p1 = sqrt(sqrt(2) + 1)\\n>>> p2 = sqrt(sqrt(2) - 1)\\n>>> _minpoly_op_algebraic_element(Mul, p1, p2, x, QQ)\\nx - 1\\n>>> q1 = sqrt(y)\\n>>> q2 = 1 / y\\n>>> _minpoly_op_algebraic_element(Add, q1, q2, x, QQ.frac_field(y))\\nx**2*y**2 - 2*x*y - y**3 + 1\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Resultant\\n.. [2] I.M. Isaacs, Proc. Amer. Math. Soc. 25 (1970), 638\\n       \"Degrees of sums in a separable field extension\".',\n",
       " 'Clear denominators, but keep the ground domain. ',\n",
       " 'Check if a lambda is a rational function. ',\n",
       " 'Reduce the complex valued equation $f(x) = y$ to a set of equations\\n\\n$$\\\\left\\\\{g(x) = h_1(y),\\\\  g(x) = h_2(y),\\\\ \\\\dots,\\\\  g(x) = h_n(y) \\\\right\\\\}$$\\n\\nwhere $g(x)$ is a simpler function than $f(x)$.  The return value is a tuple\\n$(g(x), \\\\mathrm{set}_h)$, where $g(x)$ is a function of $x$ and $\\\\mathrm{set}_h$ is\\nthe set of function $\\\\left\\\\{h_1(y), h_2(y), \\\\dots, h_n(y)\\\\right\\\\}$.\\nHere, $y$ is not necessarily a symbol.\\n\\n$\\\\mathrm{set}_h$ contains the functions, along with the information\\nabout the domain in which they are valid, through set\\noperations. For instance, if :math:`y = |x| - n` is inverted\\nin the real domain, then $\\\\mathrm{set}_h$ is not simply\\n$\\\\{-n, n\\\\}$ as the nature of `n` is unknown; rather, it is:\\n\\n$$ \\\\left(\\\\left[0, \\\\infty\\\\right) \\\\cap \\\\left\\\\{n\\\\right\\\\}\\\\right) \\\\cup\\n                   \\\\left(\\\\left(-\\\\infty, 0\\\\right] \\\\cap \\\\left\\\\{- n\\\\right\\\\}\\\\right)$$\\n\\nBy default, the complex domain is used which means that inverting even\\nseemingly simple functions like $\\\\exp(x)$ will give very different\\nresults from those obtained in the real domain.\\n(In the case of $\\\\exp(x)$, the inversion via $\\\\log$ is multi-valued\\nin the complex domain, having infinitely many branches.)\\n\\nIf you are working with real values only (or you are not sure which\\nfunction to use) you should probably set the domain to\\n``S.Reals`` (or use ``invert_real`` which does that automatically).\\n\\n\\nExamples\\n========\\n\\n>>> from sympy.solvers.solveset import invert_complex, invert_real\\n>>> from sympy.abc import x, y\\n>>> from sympy import exp\\n\\nWhen does exp(x) == y?\\n\\n>>> invert_complex(exp(x), y, x)\\n(x, ImageSet(Lambda(_n, I*(2*_n*pi + arg(y)) + log(Abs(y))), Integers))\\n>>> invert_real(exp(x), y, x)\\n(x, Intersection({log(y)}, Reals))\\n\\nWhen does exp(x) == 1?\\n\\n>>> invert_complex(exp(x), 1, x)\\n(x, ImageSet(Lambda(_n, 2*_n*I*pi), Integers))\\n>>> invert_real(exp(x), 1, x)\\n(x, {0})\\n\\nSee Also\\n========\\ninvert_real, invert_complex',\n",
       " 'Converts an arbitrary expression to a type that can be used inside SymPy.\\nAs generally strings are unwise to use in the expressions,\\nit returns the Symbol of argument if the string type argument is passed.\\n\\nParameters\\n=========\\n\\narg: The parameter to be converted to be used in SymPy.\\n\\nReturns\\n=======\\n\\nThe converted parameter.',\n",
       " \"Returns a list with dimensions of each index.\\n\\nDimensions is a property of the array, not of the indices.  Still, if\\nthe ``IndexedBase`` does not define a shape attribute, it is assumed\\nthat the ranges of the indices correspond to the shape of the array.\\n\\n>>> from sympy import IndexedBase, Idx, symbols\\n>>> n, m = symbols('n m', integer=True)\\n>>> i = Idx('i', m)\\n>>> j = Idx('j', m)\\n>>> A = IndexedBase('A', shape=(n, n))\\n>>> B = IndexedBase('B')\\n>>> A[i, j].shape\\n(n, n)\\n>>> B[i, j].shape\\n(m, m)\",\n",
       " 'Activators used to generate activations scripts.',\n",
       " 'Test whether the specification is implemented by a class or factory.\\n\\nRaise TypeError if argument is neither a class nor a callable.',\n",
       " 'Adds parameters to the BrowserIdcAuthPlugin\\n:param info: RedshiftProperty object containing the parameters to be added to the BrowserIdcAuthPlugin.\\n:return: None.',\n",
       " 'Lists the firewall rules of an application.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import appengine_admin_v1\\n\\n    def sample_list_ingress_rules():\\n        # Create a client\\n        client = appengine_admin_v1.FirewallClient()\\n\\n        # Initialize request argument(s)\\n        request = appengine_admin_v1.ListIngressRulesRequest(\\n        )\\n\\n        # Make the request\\n        page_result = client.list_ingress_rules(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.appengine_admin_v1.types.ListIngressRulesRequest, dict]):\\n        The request object. Request message for ``Firewall.ListIngressRules``.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.appengine_admin_v1.services.firewall.pagers.ListIngressRulesPager:\\n        Response message for Firewall.ListIngressRules.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " '>>> in_mult_list()\\nFalse',\n",
       " 'Wait query ends.\\n\\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html\\n\\nParameters\\n----------\\nquery_id\\n    Query ID.\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** is ``None``.\\ncloudwatch_query_wait_polling_delay\\n    Interval in seconds for how often the function will check if the CloudWatch query has completed.\\n\\nReturns\\n-------\\n    Query result payload.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> query_id = wr.cloudwatch.start_query(\\n...     log_group_names=[\"loggroup\"],\\n...     query=\"fields @timestamp, @message | sort @timestamp desc | limit 5\",\\n... )\\n... response = wr.cloudwatch.wait_query(query_id=query_id)',\n",
       " 'Another\\nmultiline\\ndocstring',\n",
       " 'Show plots in an interactive window, in the usual Matplotlib manner.\\n        ',\n",
       " 'Print the lines in `code` with X for each line number in `linenos`.',\n",
       " 'Get the unlock key for this Swarm manager.\\n\\nReturns:\\n    A ``dict`` containing an ``UnlockKey`` member',\n",
       " 'Create an argparse.Namespace instance.',\n",
       " 'Creates a subproperty Event Filter.\\n\\nArgs:\\n    request (Union[google.analytics.admin_v1alpha.types.CreateSubpropertyEventFilterRequest, dict]):\\n        The request object. Request message for\\n        CreateSubpropertyEventFilter RPC.\\n    parent (str):\\n        Required. The ordinary property for which to create a\\n        subproperty event filter. Format: properties/property_id\\n        Example: properties/123\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    subproperty_event_filter (google.analytics.admin_v1alpha.types.SubpropertyEventFilter):\\n        Required. The subproperty event\\n        filter to create.\\n\\n        This corresponds to the ``subproperty_event_filter`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.analytics.admin_v1alpha.types.SubpropertyEventFilter:\\n        A resource message representing a GA4\\n        Subproperty event filter.',\n",
       " 'Pre-rpc interceptor for update_event_edit_rule\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AnalyticsAdminService server.',\n",
       " 'Return a callable for the create user method over gRPC.\\n\\nCreates a new User in a given project, location, and\\ncluster.\\n\\nReturns:\\n    Callable[[~.CreateUserRequest],\\n            ~.User]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get gateway method over gRPC.\\n\\nGets details of a single Gateway.\\n\\nReturns:\\n    Callable[[~.GetGatewayRequest],\\n            ~.Gateway]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'List dependencies based on the provided filter and\\npagination parameters.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import apihub_v1\\n\\n    def sample_list_dependencies():\\n        # Create a client\\n        client = apihub_v1.ApiHubDependenciesClient()\\n\\n        # Initialize request argument(s)\\n        request = apihub_v1.ListDependenciesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_dependencies(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.apihub_v1.types.ListDependenciesRequest, dict]):\\n        The request object. The\\n        [ListDependencies][google.cloud.apihub.v1.ApiHubDependencies.ListDependencies]\\n        method\\'s request.\\n    parent (str):\\n        Required. The parent which owns this collection of\\n        dependency resources. Format:\\n        ``projects/{project}/locations/{location}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.apihub_v1.services.api_hub_dependencies.pagers.ListDependenciesPager:\\n        The\\n           [ListDependencies][google.cloud.apihub.v1.ApiHubDependencies.ListDependencies]\\n           method\\'s response.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Pre-rpc interceptor for create_workload\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the AppHub server.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Access the response body from an HTTP response.\\n\\nArgs:\\n    response (object): The HTTP response object.\\n\\nRaises:\\n    NotImplementedError: Always, since virtual.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Post-rpc interceptor for get_agent\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Agents server but before\\nit is returned to user code.',\n",
       " 'Call the update session entity\\ntype method over HTTP.\\n\\n    Args:\\n        request (~.gcdc_session_entity_type.UpdateSessionEntityTypeRequest):\\n            The request object. The request message for\\n        [SessionEntityTypes.UpdateSessionEntityType][google.cloud.dialogflow.cx.v3.SessionEntityTypes.UpdateSessionEntityType].\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.gcdc_session_entity_type.SessionEntityType:\\n            Session entity types are referred to as **User** entity\\n        types and are entities that are built for an individual\\n        user such as favorites, preferences, playlists, and so\\n        on.\\n\\n        You can redefine a session entity type at the session\\n        level to extend or replace a [custom entity\\n        type][google.cloud.dialogflow.cx.v3.EntityType] at the\\n        user session level (we refer to the entity types defined\\n        at the agent level as \"custom entity types\").\\n\\n        Note: session entity types apply to all queries,\\n        regardless of the language.\\n\\n        For more information about entity types, see the\\n        `Dialogflow\\n        documentation <https://cloud.google.com/dialogflow/docs/entities-overview>`__.',\n",
       " 'Post-rpc interceptor for search_agents\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Agents server but before\\nit is returned to user code.',\n",
       " 'Post-rpc interceptor for delete_data_store\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the DataStoreService server but before\\nit is returned to user code.',\n",
       " 'Render the cell as a block of HTML',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Pre-rpc interceptor for create_http_route\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the NetworkServices server.',\n",
       " 'Register an API key for use with predict method.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import recommendationengine_v1beta1\\n\\n    def sample_create_prediction_api_key_registration():\\n        # Create a client\\n        client = recommendationengine_v1beta1.PredictionApiKeyRegistryClient()\\n\\n        # Initialize request argument(s)\\n        request = recommendationengine_v1beta1.CreatePredictionApiKeyRegistrationRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_prediction_api_key_registration(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.recommendationengine_v1beta1.types.CreatePredictionApiKeyRegistrationRequest, dict]):\\n        The request object. Request message for the\\n        ``CreatePredictionApiKeyRegistration`` method.\\n    parent (str):\\n        Required. The parent resource path.\\n        ``projects/*/locations/global/catalogs/default_catalog/eventStores/default_event_store``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    prediction_api_key_registration (google.cloud.recommendationengine_v1beta1.types.PredictionApiKeyRegistration):\\n        Required. The prediction API key\\n        registration.\\n\\n        This corresponds to the ``prediction_api_key_registration`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.recommendationengine_v1beta1.types.PredictionApiKeyRegistration:\\n        Registered Api Key.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Call the list adaptive mt\\nsentences method over HTTP.\\n\\n    Args:\\n        request (~.adaptive_mt.ListAdaptiveMtSentencesRequest):\\n            The request object. The request for listing Adaptive MT\\n        sentences from a Dataset/File.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.adaptive_mt.ListAdaptiveMtSentencesResponse:\\n            List AdaptiveMt sentences response.',\n",
       " 'Call the delete corpus method over HTTP.\\n\\nArgs:\\n    request (~.warehouse.DeleteCorpusRequest):\\n        The request object. Request message for DeleteCorpus.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the update notification\\nsubscription method over HTTP.\\n\\n    Args:\\n        request (~.notificationsapi.UpdateNotificationSubscriptionRequest):\\n            The request object. Request message for the\\n        UpdateNotificationSubscription method.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.notificationsapi.NotificationSubscription:\\n            Represents a notification\\n        subscription owned by a Merchant\\n        account.',\n",
       " 'We *do* traverse oneOf with multiple subschemas when one does not type\\nmatch.',\n",
       " 'Two SubplotSpecs are considered equal if they refer to the same\\nposition(s) in the same `GridSpec`.',\n",
       " 'Compare two \"image\" files checking differences within a tolerance.\\n\\nThe two given filenames may point to files which are convertible to\\nPNG via the `.converter` dictionary. The underlying RMS is calculated\\nwith the `.calculate_rms` function.\\n\\nParameters\\n----------\\nexpected : str\\n    The filename of the expected image.\\nactual : str\\n    The filename of the actual image.\\ntol : float\\n    The tolerance (a color value difference, where 255 is the\\n    maximal difference).  The test fails if the average pixel\\n    difference is greater than this value.\\nin_decorator : bool\\n    Determines the output format. If called from image_comparison\\n    decorator, this should be True. (default=False)\\n\\nReturns\\n-------\\nNone or dict or str\\n    Return *None* if the images are equal within the given tolerance.\\n\\n    If the images differ, the return value depends on  *in_decorator*.\\n    If *in_decorator* is true, a dict with the following entries is\\n    returned:\\n\\n    - *rms*: The RMS of the image difference.\\n    - *expected*: The filename of the expected image.\\n    - *actual*: The filename of the actual image.\\n    - *diff_image*: The filename of the difference image.\\n    - *tol*: The comparison tolerance.\\n\\n    Otherwise, a human-readable multi-line string representation of this\\n    information is returned.\\n\\nExamples\\n--------\\n::\\n\\n    img1 = \"./baseline/plot.png\"\\n    img2 = \"./output/plot.png\"\\n    compare_images(img1, img2, 0.001)',\n",
       " 'Publish a CommandStartedEvent.',\n",
       " 'Returns True if graph G is bipartite, False if not.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\nExamples\\n--------\\n>>> from networkx.algorithms import bipartite\\n>>> G = nx.path_graph(4)\\n>>> print(bipartite.is_bipartite(G))\\nTrue\\n\\nSee Also\\n--------\\ncolor, is_bipartite_node_set',\n",
       " 'Check that modules are named correctly\\n\\nCLI :: defaults',\n",
       " 'Check if types match.\\n\\nParameters\\n----------\\nother : object\\n    Class instance.\\n\\nReturns\\n-------\\nbool : boolean\\n    True if other is same class as self',\n",
       " ':param request: OAuthlib request.\\n:type request: oauthlib.common.Request',\n",
       " 'Earlier manylinuxes are compatible with later manylinuxes.',\n",
       " 'Parse ``s``, which is a ``str`` instance containing HTTP headers encoded\\nfor use in ENV variables per the W3C Baggage HTTP header format at\\nhttps://www.w3.org/TR/baggage/#baggage-http-header-format, except that\\nadditional semi-colon delimited metadata is not supported.\\nIf ``liberal`` is True we try to parse ``s`` anyway to be more compatible\\nwith other languages SDKs that accept non URL-encoded headers by default.',\n",
       " 'Measures the number of messages sent per RPC',\n",
       " 'Whether or not the index values only consist of dates.',\n",
       " 'Convert the data from this selection to the appropriate pandas type.\\n\\nParameters\\n----------\\nvalues : np.ndarray\\nnan_rep :\\nencoding : str\\nerrors : str\\n\\nReturns\\n-------\\nindex : listlike to become an Index\\ndata : ndarraylike to become a column',\n",
       " \"Construct right test DataFrame with specified levels\\n(any of 'outer', 'inner', and 'v2')\",\n",
       " 'This tests that echo may be toggled off.\\n        ',\n",
       " \"Return path's uid.\\n\\nDoes not follow symlinks:\\n    https://github.com/pypa/pip/pull/935#discussion_r5307003\\n\\nPlaced this function in compat due to differences on AIX and\\nJython, that should eventually go away.\\n\\n:raises OSError: When path is a symlink or can't be read.\",\n",
       " 'Check if the task has finished.',\n",
       " 'Total number of processes created over uptime of the host',\n",
       " 'Make sure that uninstall removes gui scripts',\n",
       " 'Adds an assignment of package as a decision\\nand increments the decision level.',\n",
       " 'Create a new hash object.\\n\\nArgs:\\n    data (bytes/bytearray/memoryview):\\n        Optional. The very first chunk of the message to hash.\\n        It is equivalent to an early call to :meth:`BLAKE2b_Hash.update`.\\n    digest_bytes (integer):\\n        Optional. The size of the digest, in bytes (1 to 64). Default is 64.\\n    digest_bits (integer):\\n        Optional and alternative to ``digest_bytes``.\\n        The size of the digest, in bits (8 to 512, in steps of 8).\\n        Default is 512.\\n    key (bytes/bytearray/memoryview):\\n        Optional. The key to use to compute the MAC (1 to 64 bytes).\\n        If not specified, no key will be used.\\n    update_after_digest (boolean):\\n        Optional. By default, a hash object cannot be updated anymore after\\n        the digest is computed. When this flag is ``True``, such check\\n        is no longer enforced.\\n\\nReturns:\\n    A :class:`BLAKE2b_Hash` hash object',\n",
       " 'Check if the  PKCS#1 v1.5 signature over a message is valid.\\n\\nThis function is also called ``RSASSA-PKCS1-V1_5-VERIFY`` and\\nit is specified in\\n`section 8.2.2 of RFC8037 <https://tools.ietf.org/html/rfc8017#page-37>`_.\\n\\n:parameter msg_hash:\\n    The hash that was carried out over the message. This is an object\\n    belonging to the :mod:`Crypto.Hash` module.\\n:type parameter: hash object\\n\\n:parameter signature:\\n    The signature that needs to be validated.\\n:type signature: byte string\\n\\n:raise ValueError: if the signature is not valid.',\n",
       " 'Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails.',\n",
       " \"Test that recursive `'definition-ref` schemas for `json_schema_input_type` are not inlined.\",\n",
       " 'Gets all organization variables :rtype: :class:`PaginatedList` of\\n:class:`github.OrganizationVariable.OrganizationVariable`',\n",
       " 'Return directories and files from a directory and handles violations.',\n",
       " \"TestClass isn't defined yet. \",\n",
       " 'Disable a global pyparsing diagnostic flag (see :class:`Diagnostics`).',\n",
       " 'Recompute names_closure from initialnames and name2fixturedefs.\\n\\nCan only reduce names_closure, which means that the new closure will\\nalways be a subset of the old one. The order is preserved.\\n\\nThis method is needed because direct parametrization may shadow some\\nof the fixtures that were included in the originally built dependency\\ntree. In this way the dependency tree can get pruned, and the closure\\nof argnames may get reduced.',\n",
       " 'Perform the main runtest loop (after collection finished).\\n\\nThe default hook implementation performs the runtest protocol for all items\\ncollected in the session (``session.items``), unless the collection failed\\nor the ``collectonly`` pytest option is set.\\n\\nIf at any point :py:func:`pytest.exit` is called, the loop is\\nterminated immediately.\\n\\nIf at any point ``session.shouldfail`` or ``session.shouldstop`` are set, the\\nloop is terminated after the runtest protocol for the current item is finished.\\n\\n:param session: The pytest session object.\\n\\nStops at first non-None result, see :ref:`firstresult`.\\nThe return value is not used, but only stops further processing.\\n\\nUse in conftest plugins\\n=======================\\n\\nAny conftest file can implement this hook.',\n",
       " 'Handle a single HTTP GET request.\\n\\nDefault implementation indicates an error because\\nXML-RPC uses the POST method.',\n",
       " 'Draw random samples from a von Mises-Fisher distribution.\\n\\nParameters\\n----------\\nmu : array_like\\n    Mean direction of the distribution. Must be a one-dimensional unit\\n    vector of norm 1.\\nkappa : float\\n    Concentration parameter. Must be positive.\\nsize : int or tuple of ints, optional\\n    Given a shape of, for example, (m,n,k), m*n*k samples are\\n    generated, and packed in an m-by-n-by-k arrangement.\\n    Because each sample is N-dimensional, the output shape\\n    is (m,n,k,N). If no shape is specified, a single (N-D)\\n    sample is returned.\\nrandom_state : {None, int, np.random.RandomState, np.random.Generator},\\n                optional\\n    Used for drawing random variates.\\n    If `seed` is `None`, the `~np.random.RandomState` singleton is used.\\n    If `seed` is an int, a new ``RandomState`` instance is used, seeded\\n    with seed.\\n    If `seed` is already a ``RandomState`` or ``Generator`` instance,\\n    then that object is used.\\n    Default is `None`.\\n\\nReturns\\n-------\\nrvs : ndarray\\n    Random variates of shape (`size`, `N`), where `N` is the\\n    dimension of the distribution.',\n",
       " 'Input: a Pandas DataFrame.',\n",
       " 'Create a Socket associated with this Context.\\n\\nParameters\\n----------\\nsocket_type : int\\n    The socket type, which can be any of the 0MQ socket types:\\n    REQ, REP, PUB, SUB, PAIR, DEALER, ROUTER, PULL, PUSH, etc.\\n\\nsocket_class: zmq.Socket\\n    The socket class to instantiate, if different from the default for this Context.\\n    e.g. for creating an asyncio socket attached to a default Context or vice versa.\\n\\n    .. versionadded:: 25\\n\\nkwargs:\\n    will be passed to the __init__ method of the socket class.',\n",
       " 'Sets the pattern of this V1JSONSchemaProps.\\n\\n\\n:param pattern: The pattern of this V1JSONSchemaProps.  # noqa: E501\\n:type: str',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Gets the validations of this V1ValidatingAdmissionPolicySpec.  # noqa: E501\\n\\nValidations contain CEL expressions which is used to apply the validation. Validations and AuditAnnotations may not both be empty; a minimum of one Validations or AuditAnnotations is required.  # noqa: E501\\n\\n:return: The validations of this V1ValidatingAdmissionPolicySpec.  # noqa: E501\\n:rtype: list[V1Validation]',\n",
       " 'Parse all YAML documents in a stream\\nand produce corresponding Python objects.\\n\\nResolve all tags, even those known to be\\nunsafe on untrusted input.',\n",
       " 'Boolean that indicates whether this session has an OAuth token\\nor not. If `self.authorized` is True, you can reasonably expect\\nOAuth-protected requests to the resource to succeed. If\\n`self.authorized` is False, you need the user to go through the OAuth\\nauthentication dance before OAuth-protected requests to the resource\\nwill succeed.',\n",
       " 'Get inference result from the output & failure Amazon S3 path',\n",
       " 'Return a :class:`_engine.Connection` object corresponding to this\\n:class:`.Session` object\\'s transactional state.\\n\\n.. container:: class_bases\\n\\n    Proxied for the :class:`_orm.Session` class on\\n    behalf of the :class:`_orm.scoping.scoped_session` class.\\n\\nEither the :class:`_engine.Connection` corresponding to the current\\ntransaction is returned, or if no transaction is in progress, a new\\none is begun and the :class:`_engine.Connection`\\nreturned (note that no\\ntransactional state is established with the DBAPI until the first\\nSQL statement is emitted).\\n\\nAmbiguity in multi-bind or unbound :class:`.Session` objects can be\\nresolved through any of the optional keyword arguments.   This\\nultimately makes usage of the :meth:`.get_bind` method for resolution.\\n\\n:param bind_arguments: dictionary of bind arguments.  May include\\n \"mapper\", \"bind\", \"clause\", other custom arguments that are passed\\n to :meth:`.Session.get_bind`.\\n\\n:param execution_options: a dictionary of execution options that will\\n be passed to :meth:`_engine.Connection.execution_options`, **when the\\n connection is first procured only**.   If the connection is already\\n present within the :class:`.Session`, a warning is emitted and\\n the arguments are ignored.\\n\\n .. seealso::\\n\\n    :ref:`session_transaction_isolation`',\n",
       " \"Return a context manager that disables autoflush.\\n\\ne.g.::\\n\\n    with session.no_autoflush:\\n\\n        some_object = SomeClass()\\n        session.add(some_object)\\n        # won't autoflush\\n        some_object.related_thing = session.query(SomeRelated).first()\\n\\nOperations that proceed within the ``with:`` block\\nwill not be subject to flushes occurring upon query\\naccess.  This is useful when initializing a series\\nof objects which involve existing database queries,\\nwhere the uncompleted object should not yet be flushed.\",\n",
       " 'Average of the decision functions of the base classifiers.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\\n    The training input samples. Sparse matrices are accepted only if\\n    they are supported by the base estimator.\\n\\nReturns\\n-------\\nscore : ndarray of shape (n_samples, k)\\n    The decision function of the input samples. The columns correspond\\n    to the classes in sorted order, as they appear in the attribute\\n    ``classes_``. Regression and binary classification are special\\n    cases with ``k == 1``, otherwise ``k==n_classes``.',\n",
       " 'Function f\\n\\nParameters\\n----------\\na : int\\n    Parameter a\\nb : float\\n    Parameter b\\n\\nReturns\\n-------\\nc : list\\n    Parameter c',\n",
       " 'Make a new function from a given template and update the signature',\n",
       " 'An int method.',\n",
       " 'Test two failures from gh-20904: int32 and indices-as-None.',\n",
       " 'Sets item secret to `secret`. If `content_type` is given,\\nalso sets the content type of the secret (``text/plain`` by\\ndefault).',\n",
       " 'Does our gzip_compress function actually produce gzipped data?',\n",
       " 'Check if the system supports symlinks (e.g. *nix) or not.',\n",
       " 'Gets summary (accuracy/precision/recall, objective history, total iterations) of model\\ntrained on the training set. An exception is thrown if `trainingSummary is None`.',\n",
       " 'Sets the value of :py:attr:`intermediateStorageLevel`.',\n",
       " 'Returns the estimated number of unique values given the binary representation\\nof a Datasketches HllSketch.\\n\\n.. versionadded:: 3.5.0\\n\\nParameters\\n----------\\ncol : :class:`~pyspark.sql.Column` or str\\n\\nReturns\\n-------\\n:class:`~pyspark.sql.Column`\\n    The estimated number of unique values for the HllSketch.\\n\\nExamples\\n--------\\n>>> df = spark.createDataFrame([1,2,2,3], \"INT\")\\n>>> df = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\\n>>> df.show()\\n+------------+\\n|distinct_cnt|\\n+------------+\\n|           3|\\n+------------+',\n",
       " 'continue the _execute_context() method for an \"insertmanyvalues\"\\noperation, which will invoke DBAPI\\ncursor.execute() one or more times with individual log and\\nevent hook calls.',\n",
       " 'Prepare this :class:`.TwoPhaseTransaction`.\\n\\nAfter a PREPARE, the transaction can be committed.',\n",
       " 'Return the current nested transaction in progress, if any.\\n\\n:return: an :class:`_asyncio.AsyncSessionTransaction` object, or\\n ``None``.\\n\\n.. versionadded:: 1.4.18',\n",
       " 'Helper for xreplace. Tracks whether a replacement actually occurred.',\n",
       " 'Apply on a list of vector_fields.\\nThe expression is rewritten internally in terms of tensor products and evaluated.',\n",
       " \"Makes parentheses optional in some cases for function calls.\\n\\nUse this after :func:`implicit_multiplication`, otherwise expressions\\nlike ``sin 2x`` will be parsed as ``x * sin(2)`` rather than\\n``sin(2*x)``.\\n\\nExamples\\n========\\n\\n>>> from sympy.parsing.sympy_parser import (parse_expr,\\n... standard_transformations, implicit_application)\\n>>> transformations = standard_transformations + (implicit_application,)\\n>>> parse_expr('cot z + csc z', transformations=transformations)\\ncot(z) + csc(z)\",\n",
       " \"Returns a plot for Bending moment present in the Beam object.\\n\\nParameters\\n==========\\nsubs : dictionary\\n    Python dictionary containing Symbols as key and their\\n    corresponding values.\\n\\nExamples\\n========\\nThere is a beam of length 8 meters. A constant distributed load of 10 KN/m\\nis applied from half of the beam till the end. There are two simple supports\\nbelow the beam, one at the starting point and another at the ending point\\nof the beam. A pointload of magnitude 5 KN is also applied from top of the\\nbeam, at a distance of 4 meters from the starting point.\\nTake E = 200 GPa and I = 400*(10**-6) meter**4.\\n\\nUsing the sign convention of downwards forces being positive.\\n\\n.. plot::\\n    :context: close-figs\\n    :format: doctest\\n    :include-source: True\\n\\n    >>> from sympy.physics.continuum_mechanics.beam import Beam\\n    >>> from sympy import symbols\\n    >>> R1, R2 = symbols('R1, R2')\\n    >>> b = Beam(8, 200*(10**9), 400*(10**-6))\\n    >>> b.apply_load(5000, 2, -1)\\n    >>> b.apply_load(R1, 0, -1)\\n    >>> b.apply_load(R2, 8, -1)\\n    >>> b.apply_load(10000, 4, 0, end=8)\\n    >>> b.bc_deflection = [(0, 0), (8, 0)]\\n    >>> b.solve_for_reaction_loads(R1, R2)\\n    >>> b.plot_bending_moment()\\n    Plot object containing:\\n    [0]: cartesian line: 13750*SingularityFunction(x, 0, 1) - 5000*SingularityFunction(x, 2, 1)\\n    - 5000*SingularityFunction(x, 4, 2) + 31250*SingularityFunction(x, 8, 1)\\n    + 5000*SingularityFunction(x, 8, 2) for x over (0.0, 8.0)\",\n",
       " 'sT := sreprTest\\nfrom sympy/printing/tests/test_repr.py',\n",
       " 'Returns GCD of ``f`` and ``g`` and their cofactors. ',\n",
       " 'helper function for ``nthroot``\\nIt denests ``p**Rational(1, n)`` using its minimal polynomial',\n",
       " \"Return `a', b', c'`, the coefficients of the square-free normal\\nform of `ax^2 + by^2 + cz^2 = 0`, where `a', b', c'` are pairwise\\nprime.  If `steps` is True then also return three tuples:\\n`sq`, `sqf`, and `(a', b', c')` where `sq` contains the square\\nfactors of `a`, `b` and `c` after removing the `gcd(a, b, c)`;\\n`sqf` contains the values of `a`, `b` and `c` after removing\\nboth the `gcd(a, b, c)` and the square factors.\\n\\nThe solutions for `ax^2 + by^2 + cz^2 = 0` can be\\nrecovered from the solutions of `a'x^2 + b'y^2 + c'z^2 = 0`.\\n\\nExamples\\n========\\n\\n>>> from sympy.solvers.diophantine.diophantine import sqf_normal\\n>>> sqf_normal(2 * 3**2 * 5, 2 * 5 * 11, 2 * 7**2 * 11)\\n(11, 1, 5)\\n>>> sqf_normal(2 * 3**2 * 5, 2 * 5 * 11, 2 * 7**2 * 11, True)\\n((3, 1, 7), (5, 55, 11), (11, 1, 5))\\n\\nReferences\\n==========\\n\\n.. [1] Legendre's Theorem, Legrange's Descent,\\n       https://public.csusm.edu/aitken_html/notes/legendre.pdf\\n\\n\\nSee Also\\n========\\n\\nreconstruct()\",\n",
       " 'Get metadata values for trait by key.',\n",
       " 'Get default argument value, given the trait default value.',\n",
       " 'content: collection of str, each str is a chunk in response',\n",
       " 'Serialize the payload of a close frame.',\n",
       " 'Reads a pointer value from the memory of the process.\\n\\n@see: L{peek_pointer}\\n\\n@type  lpBaseAddress: int\\n@param lpBaseAddress: Memory address to begin reading.\\n\\n@rtype:  int\\n@return: Pointer value read from the process memory.\\n\\n@raise WindowsError: On error an exception is raised.',\n",
       " 'Waits for all remaining output to be captured and propagated.',\n",
       " \"The objective of this unit test is to ensure that the triggered warning message,\\nwhen invoking the 'use_foo' function, correctly indicates the line where the\\ndeprecated 'foo' function is called.\",\n",
       " 'Call the list subproperty event\\nfilters method over HTTP.\\n\\n    Args:\\n        request (~.analytics_admin.ListSubpropertyEventFiltersRequest):\\n            The request object. Request message for\\n        ListSubpropertyEventFilters RPC.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.analytics_admin.ListSubpropertyEventFiltersResponse:\\n            Response message for\\n        ListSubpropertyEventFilter RPC.',\n",
       " 'Instantiates the alpha analytics data async client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,AlphaAnalyticsDataTransport,Callable[..., AlphaAnalyticsDataTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport to use.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the AlphaAnalyticsDataTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTlsChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'Return a callable for the lookup discovered service method over gRPC.\\n\\nLists a Discovered Service in a host project and\\nlocation, with a given resource URI.\\n\\nReturns:\\n    Callable[[~.LookupDiscoveredServiceRequest],\\n            ~.LookupDiscoveredServiceResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get python package method over gRPC.\\n\\nGets a python package.\\n\\nReturns:\\n    Callable[[~.GetPythonPackageRequest],\\n            ~.PythonPackage]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Pre-rpc interceptor for search_assignments\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ReservationService server.',\n",
       " 'Post-rpc interceptor for update_budget\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the BudgetService server but before\\nit is returned to user code.',\n",
       " \"Return a callable for the provision cloud identity method over gRPC.\\n\\nCreates a Cloud Identity for the given customer using the\\ncustomer's information, or the information provided here.\\n\\nPossible error codes:\\n\\n-  PERMISSION_DENIED:\\n\\n   -  The customer doesn't belong to the reseller.\\n   -  You are not authorized to provision cloud identity id. See\\n      https://support.google.com/channelservices/answer/9759265\\n\\n-  INVALID_ARGUMENT: Required request parameters are missing or\\n   invalid.\\n-  NOT_FOUND: The customer was not found.\\n-  ALREADY_EXISTS: The customer's primary email already exists.\\n   Retry after changing the customer's primary contact email.\\n-  INTERNAL: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n-  UNKNOWN: Any non-user error related to a technical issue in\\n   the backend. Contact Cloud Channel support.\\n\\nReturn value: The ID of a long-running operation.\\n\\nTo get the results of the operation, call the GetOperation\\nmethod of CloudChannelOperationsService. The Operation metadata\\ncontains an instance of\\n[OperationMetadata][google.cloud.channel.v1.OperationMetadata].\\n\\nReturns:\\n    Callable[[~.ProvisionCloudIdentityRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'Call the move method over HTTP.\\n\\nArgs:\\n    request (~.compute.MoveFirewallPolicyRequest):\\n        The request object. A request message for\\n    FirewallPolicies.Move. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Call the patch method over HTTP.\\n\\nArgs:\\n    request (~.compute.PatchForwardingRuleRequest):\\n        The request object. A request message for\\n    ForwardingRules.Patch. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Call the send diagnostic interrupt method over HTTP.\\n\\nArgs:\\n    request (~.compute.SendDiagnosticInterruptInstanceRequest):\\n        The request object. A request message for\\n    Instances.SendDiagnosticInterrupt. See\\n    the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.SendDiagnosticInterruptInstanceResponse:\\n        A response message for\\n    Instances.SendDiagnosticInterrupt. See\\n    the method description for details.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Creates a new resource policy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_insert():\\n        # Create a client\\n        client = compute_v1.ResourcePoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.InsertResourcePolicyRequest(\\n            project=\"project_value\",\\n            region=\"region_value\",\\n        )\\n\\n        # Make the request\\n        response = client.insert(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.InsertResourcePolicyRequest, dict]):\\n        The request object. A request message for\\n        ResourcePolicies.Insert. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    region (str):\\n        Name of the region for this request.\\n        This corresponds to the ``region`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    resource_policy_resource (google.cloud.compute_v1.types.ResourcePolicy):\\n        The body resource for this request\\n        This corresponds to the ``resource_policy_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Instantiates the ssl certificates client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,SslCertificatesTransport,Callable[..., SslCertificatesTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the SslCertificatesTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n        NOTE: \"rest\" transport functionality is currently in a\\n        beta state (preview). We welcome your feedback via an\\n        issue in this library\\'s source repository.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'Return a callable for the search catalog method over gRPC.\\n\\nSearches Data Catalog for multiple resources like entries, tags\\nthat match a query.\\n\\nThis is a custom method\\n(https://cloud.google.com/apis/design/custom_methods) and does\\nnot return the complete resource, only the resource identifier\\nand high level fields. Clients can subsequently call ``Get``\\nmethods.\\n\\nNote that Data Catalog search queries do not guarantee full\\nrecall. Query results that match your query may not be returned,\\neven in subsequent result pages. Also note that results returned\\n(and not returned) can vary across repeated search queries.\\n\\nSee `Data Catalog Search\\nSyntax <https://cloud.google.com/data-catalog/docs/how-to/search-reference>`__\\nfor more information.\\n\\nReturns:\\n    Callable[[~.SearchCatalogRequest],\\n            ~.SearchCatalogResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get server config method over gRPC.\\n\\nReturns configuration info about the Google\\nKubernetes Engine service.\\n\\nReturns:\\n    Callable[[~.GetServerConfigRequest],\\n            ~.ServerConfig]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Gets details of a single import.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import metastore_v1beta\\n\\n    def sample_get_metadata_import():\\n        # Create a client\\n        client = metastore_v1beta.DataprocMetastoreClient()\\n\\n        # Initialize request argument(s)\\n        request = metastore_v1beta.GetMetadataImportRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_metadata_import(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.metastore_v1beta.types.GetMetadataImportRequest, dict]):\\n        The request object. Request message for\\n        [DataprocMetastore.GetMetadataImport][google.cloud.metastore.v1beta.DataprocMetastore.GetMetadataImport].\\n    name (str):\\n        Required. The relative resource name of the metadata\\n        import to retrieve, in the following form:\\n\\n        ``projects/{project_number}/locations/{location_id}/services/{service_id}/metadataImports/{import_id}``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.metastore_v1beta.types.MetadataImport:\\n        A metastore resource that imports\\n        metadata.',\n",
       " 'Return a callable for the retry job method over gRPC.\\n\\nRetries the specified Job in a Rollout.\\n\\nReturns:\\n    Callable[[~.RetryJobRequest],\\n            ~.RetryJobResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the restore agent method over gRPC.\\n\\nRestores the specified agent from a binary file.\\n\\nReplaces the current agent with a new one. Note that all\\nexisting resources in agent (e.g. intents, entity types, flows)\\nwill be removed.\\n\\nThis method is a `long-running\\noperation <https://cloud.google.com/dialogflow/cx/docs/how/long-running-operation>`__.\\nThe returned ``Operation`` type has the following\\nmethod-specific fields:\\n\\n-  ``metadata``: An empty `Struct\\n   message <https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#struct>`__\\n-  ``response``: An `Empty\\n   message <https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#empty>`__\\n\\nNote: You should always train a flow prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/cx/docs/concept/training>`__.\\n\\nReturns:\\n    Callable[[~.RestoreAgentRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the delete membership method over gRPC.\\n\\nRemoves a Membership.\\n\\n**This is currently only supported for GKE clusters on Google\\nCloud**. To unregister other clusters, follow the instructions\\nat\\nhttps://cloud.google.com/anthos/multicluster-management/connect/unregistering-a-cluster.\\n\\nReturns:\\n    Callable[[~.DeleteMembershipRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates a new preference set in a given project and\\nlocation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import migrationcenter_v1\\n\\n    def sample_create_preference_set():\\n        # Create a client\\n        client = migrationcenter_v1.MigrationCenterClient()\\n\\n        # Initialize request argument(s)\\n        request = migrationcenter_v1.CreatePreferenceSetRequest(\\n            parent=\"parent_value\",\\n            preference_set_id=\"preference_set_id_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.create_preference_set(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.migrationcenter_v1.types.CreatePreferenceSetRequest, dict]):\\n        The request object. A request to create a preference set.\\n    parent (str):\\n        Required. Value for parent.\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    preference_set (google.cloud.migrationcenter_v1.types.PreferenceSet):\\n        Required. The preference set resource\\n        being created.\\n\\n        This corresponds to the ``preference_set`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    preference_set_id (str):\\n        Required. User specified ID for the preference set. It\\n        will become the last component of the preference set\\n        name. The ID must be unique within the project, must\\n        conform with RFC-1034, is restricted to lower-cased\\n        letters, and has a maximum length of 63 characters. The\\n        ID must match the regular expression\\n        ``[a-z]([a-z0-9-]{0,61}[a-z0-9])?``.\\n\\n        This corresponds to the ``preference_set_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.migrationcenter_v1.types.PreferenceSet`\\n        The preferences that apply to all assets in a given\\n        context.',\n",
       " 'Deletes a single ClientTlsPolicy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import network_security_v1\\n\\n    def sample_delete_client_tls_policy():\\n        # Create a client\\n        client = network_security_v1.NetworkSecurityClient()\\n\\n        # Initialize request argument(s)\\n        request = network_security_v1.DeleteClientTlsPolicyRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_client_tls_policy(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.network_security_v1.types.DeleteClientTlsPolicyRequest, dict]):\\n        The request object. Request used by the\\n        DeleteClientTlsPolicy method.\\n    name (str):\\n        Required. A name of the ClientTlsPolicy to delete. Must\\n        be in the format\\n        ``projects/*/locations/{location}/clientTlsPolicies/*``.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'osconfig.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " 'Parses a replay_result path into its component segments.',\n",
       " 'Post-rpc interceptor for undelete_folder\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Folders server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the delete big query export method over gRPC.\\n\\nDeletes an existing BigQuery export.\\n\\nReturns:\\n    Callable[[~.DeleteBigQueryExportRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Copy this URL, returning a new URL with some components altered.\\nAccepts the same set of parameters as the components that are made\\navailable via properties on the `URL` class.\\n\\nFor example:\\n\\nurl = httpx.URL(\"https://www.example.com\").copy_with(\\n    username=\"jo@gmail.com\", password=\"a secret\"\\n)\\nassert url == \"https://jo%40email.com:a%20secret@www.example.com\"',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    EventServiceAsyncClient: The constructed client.',\n",
       " 'Creates a job template in the specified region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.video import transcoder_v1\\n\\n    def sample_create_job_template():\\n        # Create a client\\n        client = transcoder_v1.TranscoderServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = transcoder_v1.CreateJobTemplateRequest(\\n            parent=\"parent_value\",\\n            job_template_id=\"job_template_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_job_template(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.video.transcoder_v1.types.CreateJobTemplateRequest, dict]):\\n        The request object. Request message for\\n        ``TranscoderService.CreateJobTemplate``.\\n    parent (str):\\n        Required. The parent location to create this job\\n        template. Format:\\n        ``projects/{project}/locations/{location}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    job_template (google.cloud.video.transcoder_v1.types.JobTemplate):\\n        Required. Parameters for creating job\\n        template.\\n\\n        This corresponds to the ``job_template`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    job_template_id (str):\\n        Required. The ID to use for the job template, which will\\n        become the final component of the job template\\'s\\n        resource name.\\n\\n        This value should be 4-63 characters, and valid\\n        characters must match the regular expression\\n        ``[a-zA-Z][a-zA-Z0-9_-]*``.\\n\\n        This corresponds to the ``job_template_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.video.transcoder_v1.types.JobTemplate:\\n        Transcoding job template resource.',\n",
       " 'Call the set iam policy method over HTTP.\\n\\nArgs:\\n    request (iam_policy_pb2.SetIamPolicyRequest):\\n        The request object for SetIamPolicy method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    policy_pb2.Policy: Response from SetIamPolicy method.',\n",
       " 'Call the cancel operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.CancelOperationRequest):\\n        The request object for CancelOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Return a callable for the update account method over gRPC.\\n\\nUpdates an account regardless of its type:\\nstandalone, MCA or sub-account. Executing this method\\nrequires admin access.\\n\\nReturns:\\n    Callable[[~.UpdateAccountRequest],\\n            ~.Account]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Pre-rpc interceptor for unclaim_homepage\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the HomepageService server.',\n",
       " 'Accepts a ``TermsOfService``. Executing this method requires\\nadmin access.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.shopping import merchant_accounts_v1beta\\n\\n    def sample_accept_terms_of_service():\\n        # Create a client\\n        client = merchant_accounts_v1beta.TermsOfServiceServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = merchant_accounts_v1beta.AcceptTermsOfServiceRequest(\\n            name=\"name_value\",\\n            account=\"account_value\",\\n            region_code=\"region_code_value\",\\n        )\\n\\n        # Make the request\\n        client.accept_terms_of_service(request=request)\\n\\nArgs:\\n    request (Union[google.shopping.merchant_accounts_v1beta.types.AcceptTermsOfServiceRequest, dict]):\\n        The request object. Request message for the ``AcceptTermsOfService`` method.\\n    name (str):\\n        Required. The resource name of the terms of service\\n        version. Format: ``termsOfService/{version}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Retrieves information about a store.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.shopping import merchant_lfp_v1beta\\n\\n    def sample_get_lfp_store():\\n        # Create a client\\n        client = merchant_lfp_v1beta.LfpStoreServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = merchant_lfp_v1beta.GetLfpStoreRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_lfp_store(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.shopping.merchant_lfp_v1beta.types.GetLfpStoreRequest, dict]):\\n        The request object. Request message for the ``GetLfpStore`` method.\\n    name (str):\\n        Required. The name of the store to retrieve. Format:\\n        ``accounts/{account}/lfpStores/{target_merchant}~{store_code}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.shopping.merchant_lfp_v1beta.types.LfpStore:\\n        A store for the merchant. This will\\n        be used to match to a store under the\\n        Google Business Profile of the target\\n        merchant. If a matching store can\\'t be\\n        found, the inventories or sales\\n        submitted with the store code will not\\n        be used.',\n",
       " 'Patch the ``_decoder`` on a ``urllib3`` response.\\n\\nThis is so that we can intercept the compressed bytes before they are\\ndecoded.\\n\\nOnly patches if the content encoding is ``gzip`` or ``br``.\\n\\nArgs:\\n    response_raw (urllib3.response.HTTPResponse): The raw response for\\n        an HTTP request.\\n    checksum (object):\\n        A checksum which will be updated with compressed bytes.\\n\\nReturns:\\n    object: Either the original ``checksum`` if ``_decoder`` is not\\n    patched, or a ``_DoNothingHash`` if the decoder is patched, since the\\n    caller will no longer need to hash to decoded bytes.',\n",
       " 'Files in this distribution.\\n\\n:return: List of PackagePath for this distribution or None\\n\\nResult is `None` if the metadata file that enumerates files\\n(i.e. RECORD for dist-info, or installed-files.txt or\\nSOURCES.txt for egg-info) is missing.\\nResult may be empty if the metadata exists but is empty.\\n\\nCustom providers are recommended to provide a \"RECORD\" file (in\\n``read_text``) or override this property to allow for callers to be\\nable to resolve filenames provided by the package.',\n",
       " 'test shlex issues with timeit (#1109)',\n",
       " 'Sort an iterable using Python\\'s :func:`sorted`.\\n\\n.. sourcecode:: jinja\\n\\n    {% for city in cities|sort %}\\n        ...\\n    {% endfor %}\\n\\n:param reverse: Sort descending instead of ascending.\\n:param case_sensitive: When sorting strings, sort upper and lower\\n    case separately.\\n:param attribute: When sorting objects or dicts, an attribute or\\n    key to sort by. Can use dot notation like ``\"address.city\"``.\\n    Can be a list of attributes like ``\"age,name\"``.\\n\\nThe sort is stable, it does not change the relative order of\\nelements that compare equal. This makes it is possible to chain\\nsorts on different attributes and ordering.\\n\\n.. sourcecode:: jinja\\n\\n    {% for user in users|sort(attribute=\"name\")\\n        |sort(reverse=true, attribute=\"age\") %}\\n        ...\\n    {% endfor %}\\n\\nAs a shortcut to chaining when the direction is the same for all\\nattributes, pass a comma separate list of attributes.\\n\\n.. sourcecode:: jinja\\n\\n    {% for user in users|sort(attribute=\"age,name\") %}\\n        ...\\n    {% endfor %}\\n\\n.. versionchanged:: 2.11.0\\n    The ``attribute`` parameter can be a comma separated list of\\n    attributes, e.g. ``\"age,name\"``.\\n\\n.. versionchanged:: 2.6\\n   The ``attribute`` parameter was added.',\n",
       " 'Resolve the given ``ref`` and enter its resolution scope.\\n\\nExits the scope on exit of this context manager.\\n\\nArguments:\\n\\n    ref (str):\\n\\n        The reference to resolve',\n",
       " \"Returns the <head> element.  Can be called from a child\\nelement to get the document's head.\",\n",
       " 'Return all available rule names.',\n",
       " 'Run a test only if using the synchronous API.',\n",
       " 'Set the alpha value used for blending - not supported on all backends.\\n\\nParameters\\n----------\\nalpha : scalar or None\\n    *alpha* must be within the 0-1 range, inclusive.',\n",
       " 'Private helper implementing the commonality between the complex, magnitude,\\nangle, and phase spectrums.',\n",
       " 'Return whether the lock is currently held by an owner.',\n",
       " 'Set grid helper.\\n\\nParameters\\n----------\\ngrid_helper : `.GridHelperBase` subclass',\n",
       " 'Return the (x, y, z) position of the text.',\n",
       " 'Return all the non-masked data as a 1-D array.\\n\\nThis function is equivalent to calling the \"compressed\" method of a\\n`ma.MaskedArray`, see `ma.MaskedArray.compressed` for details.\\n\\nSee Also\\n--------\\nma.MaskedArray.compressed : Equivalent method.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n\\nCreate an array with negative values masked:\\n\\n>>> import numpy as np\\n>>> x = np.array([[1, -1, 0], [2, -1, 3], [7, 4, -1]])\\n>>> masked_x = np.ma.masked_array(x, mask=x < 0)\\n>>> masked_x\\nmasked_array(\\n  data=[[1, --, 0],\\n        [2, --, 3],\\n        [7, 4, --]],\\n  mask=[[False,  True, False],\\n        [False,  True, False],\\n        [False, False,  True]],\\n  fill_value=999999)\\n\\nCompress the masked array into a 1-D array of non-masked values:\\n\\n>>> np.ma.compressed(masked_x)\\narray([1, 0, 2, 3, 7, 4])',\n",
       " 'Activates a :class:`SpanShim` and returns a :class:`ScopeShim` which\\nrepresents the active span.\\n\\nArgs:\\n    span: A :class:`SpanShim` to be activated.\\n    finish_on_close(:obj:`bool`): Determines whether the OpenTelemetry\\n        span should be ended when the returned :class:`ScopeShim` is\\n        closed.\\n\\nReturns:\\n    A :class:`ScopeShim` representing the activated span.',\n",
       " 'Compares two byte strings in constant time to see if they are equal\\n\\n:param a:\\n    The first byte string\\n\\n:param b:\\n    The second byte string\\n\\n:return:\\n    A boolean if the two byte strings are equal',\n",
       " 'Convert list-like or scalar input to list-like. List, numpy and pandas array-like\\ninputs are returned unmodified whereas others are converted to list.',\n",
       " 'Request an x11 session on this channel.  If the server allows it,\\nfurther x11 requests can be made from the server to the client,\\nwhen an x11 application is run in a shell session.\\n\\nFrom :rfc:`4254`::\\n\\n    It is RECOMMENDED that the \\'x11 authentication cookie\\' that is\\n    sent be a fake, random cookie, and that the cookie be checked and\\n    replaced by the real cookie when a connection request is received.\\n\\nIf you omit the auth_cookie, a new secure random 128-bit value will be\\ngenerated, used, and returned.  You will need to use this value to\\nverify incoming x11 requests and replace them with the actual local\\nx11 cookie (which requires some knowledge of the x11 protocol).\\n\\nIf a handler is passed in, the handler is called from another thread\\nwhenever a new x11 connection arrives.  The default handler queues up\\nincoming x11 connections, which may be retrieved using\\n`.Transport.accept`.  The handler\\'s calling signature is::\\n\\n    handler(channel: Channel, (address: str, port: int))\\n\\n:param int screen_number: the x11 screen number (0, 10, etc.)\\n:param str auth_protocol:\\n    the name of the X11 authentication method used; if none is given,\\n    ``\"MIT-MAGIC-COOKIE-1\"`` is used\\n:param str auth_cookie:\\n    hexadecimal string containing the x11 auth cookie; if none is\\n    given, a secure random 128-bit value is generated\\n:param bool single_connection:\\n    if True, only a single x11 connection will be forwarded (by\\n    default, any number of x11 connections can arrive over this\\n    session)\\n:param handler:\\n    an optional callable handler to use for incoming X11 connections\\n:return: the auth_cookie used',\n",
       " 'Ensure method .setwinsize() sends signal caught by child. ',\n",
       " \"Returns the index of the code string for the given positions.\\n\\nArgs:\\n    newlines_offsets (Sequence[int]): The offset of each newline character found in the code snippet.\\n    position (SyntaxPosition): The position to search for.\\n\\nReturns:\\n    Optional[int]: The index of the code string for this position, or `None`\\n        if the given position's line number is out of range (if it's the column that is out of range\\n        we silently clamp its value so that it reaches the end of the line)\",\n",
       " 'Given a cpu_time() ntuple calculates the total CPU time\\n(including idle time).',\n",
       " 'consumer that writes to sys.stdout ',\n",
       " 'Generate a source statement equivalent to the import.',\n",
       " 'Context manager that returns a new :class:`MonkeyPatch` object\\nwhich undoes any patching done inside the ``with`` block upon exit.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    import functools\\n\\n\\n    def test_partial(monkeypatch):\\n        with monkeypatch.context() as m:\\n            m.setattr(functools, \"partial\", 3)\\n\\nUseful in situations where it is desired to undo some patches before the test ends,\\nsuch as mocking ``stdlib`` functions that might break pytest itself if mocked (for examples\\nof this see :issue:`3290`).',\n",
       " 'Test override of the non-parametrized fixture with parametrized one on the conftest level.',\n",
       " 'Run the unit test suite.',\n",
       " 'Gets the access control policy for a resource.\\nReturns an empty policy if the resource exists and does not have a policy\\nset.',\n",
       " 'Custom attributes passed with notification events.',\n",
       " 'delete_cluster_role  # noqa: E501\\n\\ndelete a ClusterRole  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_cluster_role(name, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the ClusterRole (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param V1DeleteOptions body:\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1Status\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Gets the pod_affinity_term of this V1WeightedPodAffinityTerm.  # noqa: E501\\n\\n\\n:return: The pod_affinity_term of this V1WeightedPodAffinityTerm.  # noqa: E501\\n:rtype: V1PodAffinityTerm',\n",
       " 'Sets the all_nodes of this V1alpha3ResourceSliceSpec.\\n\\nAllNodes indicates that all nodes have access to the resources in the pool.  Exactly one of NodeName, NodeSelector and AllNodes must be set.  # noqa: E501\\n\\n:param all_nodes: The all_nodes of this V1alpha3ResourceSliceSpec.  # noqa: E501\\n:type: bool',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Releases the connection back to the pool.',\n",
       " 'Test backward compatibility with username and password',\n",
       " 'Ensure that we handle partially consumed file like objects.',\n",
       " 'Install a rich traceback handler.\\n\\nOnce installed, any tracebacks will be printed with syntax highlighting and rich formatting.\\n\\n\\nArgs:\\n    console (Optional[Console], optional): Console to write exception to. Default uses internal Console instance.\\n    width (Optional[int], optional): Width (in characters) of traceback. Defaults to 100.\\n    code_width (Optional[int], optional): Code width (in characters) of traceback. Defaults to 88.\\n    extra_lines (int, optional): Extra lines of code. Defaults to 3.\\n    theme (Optional[str], optional): Pygments theme to use in traceback. Defaults to ``None`` which will pick\\n        a theme appropriate for the platform.\\n    word_wrap (bool, optional): Enable word wrapping of long lines. Defaults to False.\\n    show_locals (bool, optional): Enable display of local variables. Defaults to False.\\n    locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\\n        Defaults to 10.\\n    locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.\\n    locals_hide_dunder (bool, optional): Hide locals prefixed with double underscore. Defaults to True.\\n    locals_hide_sunder (bool, optional): Hide locals prefixed with single underscore. Defaults to False.\\n    indent_guides (bool, optional): Enable indent guides in code and locals. Defaults to True.\\n    suppress (Sequence[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.\\n\\nReturns:\\n    Callable: The previous exception handler that was replaced.',\n",
       " 'Before 3.11, the last_instruction should be None',\n",
       " 'Yields the part number and body to use for each UploadPart\\n\\n:type transfer_future: s3transfer.futures.TransferFuture\\n:param transfer_future: The future associated with upload request\\n\\n:type chunksize: int\\n:param chunksize: The chunksize to use for this upload.\\n\\n:rtype: int, s3transfer.utils.ReadFileChunk\\n:returns: Yields the part number and the ReadFileChunk including all\\n    progress callbacks associated with the transfer future for that\\n    specific yielded part.',\n",
       " 'Upload an artifact file to S3.\\n\\nArgs:\\n    file_path (str): the file path of the artifact\\n    extra_args (dict): Optional extra arguments that may be passed to the upload operation.\\n        Similar to ExtraArgs parameter in S3 upload_file function. Please refer to the\\n        ExtraArgs parameter documentation here:\\n        https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html#the-extraargs-parameter\\n\\nReturns:\\n    (str, str): The s3 URI of the uploaded file and the etag of the file.\\n\\nRaises:\\n    ValueError: If file does not exist.',\n",
       " 'Pre-rpc interceptor for get_clone_job\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the VmMigration server.',\n",
       " 'Update additonal content based on latest model card when the field is empty.\\n\\nArgs:\\n    other_content (dict): The content of the other model card.\\n    keys (list[str]): The list of keys.',\n",
       " 'Apply ORM attributes and/or wildcard to an existing path, producing\\na new path.\\n\\nThis method is used within the :meth:`.create` method to initialize\\na :class:`._LoadElement` object.',\n",
       " 'Update an Amazon SageMaker ``Endpoint`` , Raise an error endpoint_name does not exist.\\n\\nArgs:\\n    endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to update.\\n    endpoint_config_name (str): Name of the Amazon SageMaker endpoint configuration to\\n        deploy.\\n    wait (bool): Whether to wait for the endpoint deployment to complete before returning\\n        (default: True).\\n\\nReturns:\\n    str: Name of the Amazon SageMaker ``Endpoint`` being updated.\\n\\nRaises:\\n    - ValueError: if the endpoint does not already exist\\n    - botocore.exceptions.ClientError: If SageMaker throws an error while\\n    creating endpoint config, describing endpoint or updating endpoint',\n",
       " 'Get the hash of the code artifact(s) for the given step\\n\\nArgs:\\n    step (Entity): A pipeline step object (Entity type because Step causes circular import)\\nReturns:\\n    str: A hash string representing the unique code artifact(s) for the step',\n",
       " 'Copies the dataset from the local disk to the local bucket in the test region',\n",
       " 'Use BIConjugate Gradient STABilized iteration to solve ``Ax = b``.\\n\\nParameters\\n----------\\nA : {sparse matrix, ndarray, LinearOperator}\\n    The real or complex N-by-N matrix of the linear system.\\n    Alternatively, `A` can be a linear operator which can\\n    produce ``Ax`` and ``A^T x`` using, e.g.,\\n    ``scipy.sparse.linalg.LinearOperator``.\\nb : ndarray\\n    Right hand side of the linear system. Has shape (N,) or (N,1).\\nx0 : ndarray\\n    Starting guess for the solution.\\nrtol, atol : float, optional\\n    Parameters for the convergence test. For convergence,\\n    ``norm(b - A @ x) <= max(rtol*norm(b), atol)`` should be satisfied.\\n    The default is ``atol=0.`` and ``rtol=1e-5``.\\nmaxiter : integer\\n    Maximum number of iterations.  Iteration will stop after maxiter\\n    steps even if the specified tolerance has not been achieved.\\nM : {sparse matrix, ndarray, LinearOperator}\\n    Preconditioner for `A`. It should approximate the\\n    inverse of `A` (see Notes). Effective preconditioning dramatically improves the\\n    rate of convergence, which implies that fewer iterations are needed\\n    to reach a given error tolerance.\\ncallback : function\\n    User-supplied function to call after each iteration.  It is called\\n    as ``callback(xk)``, where ``xk`` is the current solution vector.\\n\\nReturns\\n-------\\nx : ndarray\\n    The converged solution.\\ninfo : integer\\n    Provides convergence information:\\n        0  : successful exit\\n        >0 : convergence to tolerance not achieved, number of iterations\\n        <0 : parameter breakdown\\n\\nNotes\\n-----\\nThe preconditioner `M` should be a matrix such that ``M @ A`` has a smaller\\ncondition number than `A`, see [1]_ .\\n\\nReferences\\n----------\\n.. [1] \"Preconditioner\", Wikipedia, \\n       https://en.wikipedia.org/wiki/Preconditioner\\n.. [2] \"Biconjugate gradient stabilized method\", \\n       Wikipedia, https://en.wikipedia.org/wiki/Biconjugate_gradient_stabilized_method\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from scipy.sparse import csc_matrix\\n>>> from scipy.sparse.linalg import bicgstab\\n>>> R = np.array([[4, 2, 0, 1],\\n...               [3, 0, 0, 2],\\n...               [0, 1, 1, 1],\\n...               [0, 2, 1, 0]])\\n>>> A = csc_matrix(R)\\n>>> b = np.array([-1, -0.5, -1, 2])\\n>>> x, exit_code = bicgstab(A, b, atol=1e-5)\\n>>> print(exit_code)  # 0 indicates successful convergence\\n0\\n>>> np.allclose(A.dot(x), b)\\nTrue',\n",
       " 'Test that design_matrix(x) is equivalent to BSpline(..)(x).',\n",
       " 'Create an areal geometry formed by the constituent linework of given geometry.\\n\\nEquivalent of the PostGIS ST_BuildArea() function.\\n\\nParameters\\n----------\\ngeometry : Geometry or array_like\\n    Geometry or geometries for which to build an area.\\n**kwargs\\n    See :ref:`NumPy ufunc docs <ufuncs.kwargs>` for other keyword arguments.\\n\\nExamples\\n--------\\n>>> from shapely import GeometryCollection, Polygon\\n>>> polygon1 = Polygon([(0, 0), (3, 0), (3, 3), (0, 3), (0, 0)])\\n>>> polygon2 = Polygon([(1, 1), (1, 2), (2, 2), (1, 1)])\\n>>> build_area(GeometryCollection([polygon1, polygon2]))\\n<POLYGON ((0 0, 0 3, 3 3, 3 0, 0 0), (1 1, 2 2, 1 2, 1 1))>',\n",
       " 'Select a single tag.',\n",
       " 'Sets the value of :py:attr:`maxIter`.',\n",
       " 'Return the used memory in MiB',\n",
       " 'Returns `True` when the logical query plans inside both :class:`DataFrame`\\\\s are equal and\\ntherefore return the same results.\\n\\n.. versionadded:: 3.1.0\\n\\n.. versionchanged:: 3.5.0\\n    Supports Spark Connect.\\n\\nNotes\\n-----\\nThe equality comparison here is simplified by tolerating the cosmetic differences\\nsuch as attribute names.\\n\\nThis API can compare both :class:`DataFrame`\\\\s very fast but can still return\\n`False` on the :class:`DataFrame` that return the same results, for instance, from\\ndifferent plans. Such false negative semantic can be useful when caching as an example.\\n\\nThis API is a developer API.\\n\\nParameters\\n----------\\nother : :class:`DataFrame`\\n    The other DataFrame to compare against.\\n\\nReturns\\n-------\\nbool\\n    Whether these two DataFrames are similar.\\n\\nExamples\\n--------\\n>>> df1 = spark.range(10)\\n>>> df2 = spark.range(10)\\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\\nTrue\\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\\nFalse\\n>>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\\nTrue',\n",
       " 'SPARK-41203: Support DF.transform',\n",
       " 'Used to convert months representing a year-month interval with given start and end\\nfields to its ANSI SQL string representation.',\n",
       " 'Return a new DStream in which each RDD has a single element\\ngenerated by reducing each RDD of this DStream.',\n",
       " 'Find out what items appear in autosummary:: directives in the\\ngiven lines.\\n\\nReturns a list of (name, toctree, template) where *name* is a name\\nof an object and *toctree* the :toctree: path of the corresponding\\nautosummary directive (relative to the root of the file name), and\\n*template* the value of the :template: option. *toctree* and\\n*template* ``None`` if the directive does not have the\\ncorresponding options set.',\n",
       " 'Returns the name of the first argument if in a function.',\n",
       " \"Create a SQL JOIN against this :class:`_expression.Select`\\nobject's criterion\\nand apply generatively, returning the newly resulting\\n:class:`_expression.Select`.\\n\\nE.g.::\\n\\n    stmt = select(user_table).join(address_table, user_table.c.id == address_table.c.user_id)\\n\\nThe above statement generates SQL similar to::\\n\\n    SELECT user.id, user.name FROM user JOIN address ON user.id = address.user_id\\n\\n.. versionchanged:: 1.4 :meth:`_expression.Select.join` now creates\\n   a :class:`_sql.Join` object between a :class:`_sql.FromClause`\\n   source that is within the FROM clause of the existing SELECT,\\n   and a given target :class:`_sql.FromClause`, and then adds\\n   this :class:`_sql.Join` to the FROM clause of the newly generated\\n   SELECT statement.    This is completely reworked from the behavior\\n   in 1.3, which would instead create a subquery of the entire\\n   :class:`_expression.Select` and then join that subquery to the\\n   target.\\n\\n   This is a **backwards incompatible change** as the previous behavior\\n   was mostly useless, producing an unnamed subquery rejected by\\n   most databases in any case.   The new behavior is modeled after\\n   that of the very successful :meth:`_orm.Query.join` method in the\\n   ORM, in order to support the functionality of :class:`_orm.Query`\\n   being available by using a :class:`_sql.Select` object with an\\n   :class:`_orm.Session`.\\n\\n   See the notes for this change at :ref:`change_select_join`.\\n\\n\\n:param target: target table to join towards\\n\\n:param onclause: ON clause of the join.  If omitted, an ON clause\\n is generated automatically based on the :class:`_schema.ForeignKey`\\n linkages between the two tables, if one can be unambiguously\\n determined, otherwise an error is raised.\\n\\n:param isouter: if True, generate LEFT OUTER join.  Same as\\n :meth:`_expression.Select.outerjoin`.\\n\\n:param full: if True, generate FULL OUTER join.\\n\\n.. seealso::\\n\\n    :ref:`tutorial_select_join` - in the :doc:`/tutorial/index`\\n\\n    :ref:`orm_queryguide_joins` - in the :ref:`queryguide_toplevel`\\n\\n    :meth:`_expression.Select.join_from`\\n\\n    :meth:`_expression.Select.outerjoin`\",\n",
       " 'Given a target clause and a second to search within, return True\\nif the target is plainly present in the search without any\\nsubqueries or aliases involved.\\n\\nBasically descends through Joins.',\n",
       " \"If 'key' is present in dict 'kw', coerce its value to type 'type\\\\_' if\\nnecessary.  If 'flexi_bool' is True, the string '0' is considered false\\nwhen coercing to boolean.\",\n",
       " '.. deprecated:: 1.13',\n",
       " 'Add a datetime and a timedelta.',\n",
       " 'Polynomial pseudo-quotient of ``f`` and ``g``. ',\n",
       " \"Returns a list of all the values for the named field. Returns an\\nempty list if the key doesn't exist.\",\n",
       " \"Dump all requests and responses including redirects.\\n\\nThis takes the response returned by requests and will dump all\\nrequest-response pairs in the redirect history in order followed by the\\nfinal request-response.\\n\\nExample::\\n\\n    import requests\\n    from requests_toolbelt.utils import dump\\n\\n    resp = requests.get('https://httpbin.org/redirect/5')\\n    data = dump.dump_all(resp)\\n    print(data.decode('utf-8'))\\n\\n:param response:\\n    The response to format\\n:type response: :class:`requests.Response`\\n:param request_prefix: (*optional*)\\n    Bytes to prefix each line of the request data\\n:type request_prefix: :class:`bytes`\\n:param response_prefix: (*optional*)\\n    Bytes to prefix each line of the response data\\n:type response_prefix: :class:`bytes`\\n:returns: Formatted bytes of request and response information.\\n:rtype: :class:`bytearray`\",\n",
       " '#1243',\n",
       " 'The number of tokens currently available to be borrowed',\n",
       " 'Describes a delta within multiple timeframes in plain language.\\n\\n:param timeframes: a list of string, quantity pairs each representing a timeframe and delta.\\n:param only_distance: return only distance eg: \"2 hours and 11 seconds\" without \"in\" or \"ago\" keywords',\n",
       " 'Returns the name of the family of hash algorithms used to generate a\\nDSA key\\n\\n:raises:\\n    ValueError - when the key is not a DSA key\\n\\n:return:\\n    A unicode string of \"sha1\" or \"sha2\"',\n",
       " 'Test that the code option finds the pyproject.toml in the current directory.',\n",
       " \"Index all documents from a CSV file to OpenSearch index.\\n\\nParameters\\n----------\\nclient\\n    instance of opensearchpy.OpenSearch to use.\\npath\\n    S3 or local path to the CSV file which contains the documents.\\nindex\\n    Name of the index.\\ndoc_type\\n    Name of the document type (for Elasticsearch versions 5.x and earlier).\\npandas_kwargs\\n    Dictionary of arguments forwarded to pandas.read_csv().\\n    e.g. pandas_kwargs={'sep': '|', 'na_values': ['null', 'none']}\\n    https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n    Note: these params values are enforced: `skip_blank_lines=True`\\nuse_threads\\n    True to enable concurrent requests, False to disable multiple threads.\\n    If enabled os.cpu_count() will be used as the max number of threads.\\n    If integer is provided, specified number is used.\\n**kwargs\\n    KEYWORD arguments forwarded to :func:`~awswrangler.opensearch.index_documents`\\n    which is used to execute the operation\\n\\nReturns\\n-------\\n    Response payload\\n    https://opensearch.org/docs/opensearch/rest-api/document-apis/bulk/#response.\\n\\nExamples\\n--------\\nWriting contents of CSV file\\n\\n>>> import awswrangler as wr\\n>>> client = wr.opensearch.connect(host='DOMAIN-ENDPOINT')\\n>>> wr.opensearch.index_csv(\\n...     client=client,\\n...     path='docs.csv',\\n...     index='sample-index1'\\n... )\\n\\nWriting contents of CSV file using pandas_kwargs\\n\\n>>> import awswrangler as wr\\n>>> client = wr.opensearch.connect(host='DOMAIN-ENDPOINT')\\n>>> wr.opensearch.index_csv(\\n...     client=client,\\n...     path='docs.csv',\\n...     index='sample-index1',\\n...     pandas_kwargs={'sep': '|', 'na_values': ['null', 'none']}\\n... )\",\n",
       " 'Convert filled contours from one :class:`~.FillType` to another.\\n\\nArgs:\\n    filled (sequence of arrays): Filled contour polygons to convert, such as those returned by\\n        :meth:`.ContourGenerator.filled`.\\n    fill_type_from (FillType or str): :class:`~.FillType` to convert from as enum or\\n        string equivalent.\\n    fill_type_to (FillType or str): :class:`~.FillType` to convert to as enum or string\\n        equivalent.\\n\\nReturn:\\n    Converted filled contour polygons.\\n\\nWhen converting non-chunked fill types (``FillType.OuterCode`` or ``FillType.OuterOffset``) to\\nchunked ones, all polygons are placed in the first chunk. When converting in the other\\ndirection, all chunk information is discarded. Converting a fill type that is not aware of the\\nrelationship between outer boundaries and contained holes (``FillType.ChunkCombinedCode`` or\\n``FillType.ChunkCombinedOffset``) to one that is will raise a ``ValueError``.\\n\\n.. versionadded:: 1.2.0',\n",
       " 'Returns the most invalid value an object of this type can assume as a\\nC expression string. Returns None if no such value exists.',\n",
       " 'Create a Completer that reuses the advanced completion support of PyDev\\nin addition to the completion support provided by IPython',\n",
       " 'test the NO ACTION option which generally comes back as None',\n",
       " \"When sending, a TSIG signature using the specified key\\nshould be added.\\n\\n*key*, a ``dns.tsig.Key`` is the key to use.  If a key is specified,\\nthe *keyring* and *algorithm* fields are not used.\\n\\n*keyring*, a ``dict``, ``callable`` or ``dns.tsig.Key``, is either\\nthe TSIG keyring or key to use.\\n\\nThe format of a keyring dict is a mapping from TSIG key name, as\\n``dns.name.Name`` to ``dns.tsig.Key`` or a TSIG secret, a ``bytes``.\\nIf a ``dict`` *keyring* is specified but a *keyname* is not, the key\\nused will be the first key in the *keyring*.  Note that the order of\\nkeys in a dictionary is not defined, so applications should supply a\\nkeyname when a ``dict`` keyring is used, unless they know the keyring\\ncontains only one key.  If a ``callable`` keyring is specified, the\\ncallable will be called with the message and the keyname, and is\\nexpected to return a key.\\n\\n*keyname*, a ``dns.name.Name``, ``str`` or ``None``, the name of\\nthis TSIG key to use; defaults to ``None``.  If *keyring* is a\\n``dict``, the key must be defined in it.  If *keyring* is a\\n``dns.tsig.Key``, this is ignored.\\n\\n*fudge*, an ``int``, the TSIG time fudge.\\n\\n*original_id*, an ``int``, the TSIG original id.  If ``None``,\\nthe message's id is used.\\n\\n*tsig_error*, an ``int``, the TSIG error code.\\n\\n*other_data*, a ``bytes``, the TSIG other data.\\n\\n*algorithm*, a ``dns.name.Name`` or ``str``, the TSIG algorithm to use.  This is\\nonly used if *keyring* is a ``dict``, and the key entry is a ``bytes``.\",\n",
       " \"Return a client configured from environment variables.\\n\\nThe environment variables used are the same as those used by the\\nDocker command-line client. They are:\\n\\n.. envvar:: DOCKER_HOST\\n\\n    The URL to the Docker host.\\n\\n.. envvar:: DOCKER_TLS_VERIFY\\n\\n    Verify the host against a CA certificate.\\n\\n.. envvar:: DOCKER_CERT_PATH\\n\\n    A path to a directory containing TLS certificates to use when\\n    connecting to the Docker host.\\n\\nArgs:\\n    version (str): The version of the API to use. Set to ``auto`` to\\n        automatically detect the server's version. Default: ``auto``\\n    timeout (int): Default timeout for API calls, in seconds.\\n    max_pool_size (int): The maximum number of connections\\n        to save in the pool.\\n    environment (dict): The environment to read environment variables\\n        from. Default: the value of ``os.environ``\\n    credstore_env (dict): Override environment variables when calling\\n        the credential store process.\\n    use_ssh_client (bool): If set to `True`, an ssh connection is\\n        made via shelling out to the ssh client. Ensure the ssh\\n        client is installed and configured on the host.\\n\\nExample:\\n\\n    >>> import docker\\n    >>> client = docker.from_env()\\n\\n.. _`SSL version`:\\n    https://docs.python.org/3.5/library/ssl.html#ssl.PROTOCOL_TLSv1\",\n",
       " 'Read reflog.\\n\\nArgs:\\n  f: File-like object\\nReturns: Iterator over Entry objects',\n",
       " 'Return a callable for the get artifact contents method over gRPC.\\n\\nReturns the contents of a specified artifact. If artifacts are\\nstored with GZip compression, the default behavior is to return\\nthe artifact uncompressed (the mime_type response field\\nindicates the exact format returned).\\n\\nReturns:\\n    Callable[[~.GetArtifactContentsRequest],\\n            ~.HttpBody]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a dataset path into its component segments.',\n",
       " 'Returns log messages for the transfer run.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_datatransfer_v1\\n\\n    def sample_list_transfer_logs():\\n        # Create a client\\n        client = bigquery_datatransfer_v1.DataTransferServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_datatransfer_v1.ListTransferLogsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_transfer_logs(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_datatransfer_v1.types.ListTransferLogsRequest, dict]):\\n        The request object. A request to get user facing log\\n        messages associated with data transfer\\n        run.\\n    parent (str):\\n        Required. Transfer run name in the form:\\n        ``projects/{project_id}/transferConfigs/{config_id}/runs/{run_id}``\\n        or\\n        ``projects/{project_id}/locations/{location_id}/transferConfigs/{config_id}/runs/{run_id}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.bigquery_datatransfer_v1.services.data_transfer_service.pagers.ListTransferLogsPager:\\n        The returned list transfer run\\n        messages.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Pre-rpc interceptor for list_project_billing_info\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CloudBilling server.',\n",
       " 'Call the list method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListAddressesRequest):\\n        The request object. A request message for Addresses.List.\\n    See the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.AddressList:\\n        Contains a list of addresses.',\n",
       " 'Call the start method over HTTP.\\n\\nArgs:\\n    request (~.compute.StartInstanceRequest):\\n        The request object. A request message for\\n    Instances.Start. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Parses a document path into its component segments.',\n",
       " 'Return a callable for the set node pool management method over gRPC.\\n\\nSets the NodeManagement options for a node pool.\\n\\nReturns:\\n    Callable[[~.SetNodePoolManagementRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Sets the monitoring service for a specific cluster.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import container_v1beta1\\n\\n    def sample_set_monitoring_service():\\n        # Create a client\\n        client = container_v1beta1.ClusterManagerClient()\\n\\n        # Initialize request argument(s)\\n        request = container_v1beta1.SetMonitoringServiceRequest(\\n            project_id=\"project_id_value\",\\n            zone=\"zone_value\",\\n            cluster_id=\"cluster_id_value\",\\n            monitoring_service=\"monitoring_service_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_monitoring_service(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.container_v1beta1.types.SetMonitoringServiceRequest, dict]):\\n        The request object. SetMonitoringServiceRequest sets the\\n        monitoring service of a cluster.\\n    project_id (str):\\n        Required. Deprecated. The Google Developers Console\\n        `project ID or project\\n        number <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`__.\\n        This field has been deprecated and replaced by the name\\n        field.\\n\\n        This corresponds to the ``project_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        Required. Deprecated. The name of the Google Compute\\n        Engine\\n        `zone <https://cloud.google.com/compute/docs/zones#available>`__\\n        in which the cluster resides. This field has been\\n        deprecated and replaced by the name field.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    cluster_id (str):\\n        Required. Deprecated. The name of the\\n        cluster to upgrade. This field has been\\n        deprecated and replaced by the name\\n        field.\\n\\n        This corresponds to the ``cluster_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    monitoring_service (str):\\n        Required. The monitoring service the cluster should use\\n        to write metrics. Currently available options:\\n\\n        -  \"monitoring.googleapis.com/kubernetes\" - The Cloud\\n           Monitoring service with a Kubernetes-native resource\\n           model\\n        -  ``monitoring.googleapis.com`` - The legacy Cloud\\n           Monitoring service (no longer available as of GKE\\n           1.15).\\n        -  ``none`` - No metrics will be exported from the\\n           cluster.\\n\\n        If left as an empty\\n        string,\\\\ ``monitoring.googleapis.com/kubernetes`` will\\n        be used for GKE 1.14+ or ``monitoring.googleapis.com``\\n        for earlier versions.\\n\\n        This corresponds to the ``monitoring_service`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.container_v1beta1.types.Operation:\\n        This operation resource represents\\n        operations that may have happened or are\\n        happening on the cluster. All fields are\\n        output only.',\n",
       " 'Post-rpc interceptor for query_metadata\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the DataprocMetastore server but before\\nit is returned to user code.',\n",
       " 'Returns a fully-qualified service_attachment string.',\n",
       " 'Pre-rpc interceptor for create_flow\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Flows server.',\n",
       " 'Call the create test case method over HTTP.\\n\\nArgs:\\n    request (~.gcdc_test_case.CreateTestCaseRequest):\\n        The request object. The request message for\\n    [TestCases.CreateTestCase][google.cloud.dialogflow.cx.v3.TestCases.CreateTestCase].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.gcdc_test_case.TestCase:\\n        Represents a test case.',\n",
       " \"Call the update webhook method over HTTP.\\n\\nArgs:\\n    request (~.gcdc_webhook.UpdateWebhookRequest):\\n        The request object. The request message for\\n    [Webhooks.UpdateWebhook][google.cloud.dialogflow.cx.v3.Webhooks.UpdateWebhook].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.gcdc_webhook.Webhook:\\n        Webhooks host the developer's\\n    business logic. During a session,\\n    webhooks allow the developer to use the\\n    data extracted by Dialogflow's natural\\n    language processing to generate dynamic\\n    responses, validate collected data, or\\n    trigger actions on the backend.\",\n",
       " 'Updates the stored infoType by creating a new\\nversion. The existing version will continue to be used\\nuntil the new version is ready. See\\nhttps://cloud.google.com/sensitive-data-protection/docs/creating-stored-infotypes\\nto learn more.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dlp_v2\\n\\n    def sample_update_stored_info_type():\\n        # Create a client\\n        client = dlp_v2.DlpServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = dlp_v2.UpdateStoredInfoTypeRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.update_stored_info_type(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dlp_v2.types.UpdateStoredInfoTypeRequest, dict]):\\n        The request object. Request message for\\n        UpdateStoredInfoType.\\n    name (str):\\n        Required. Resource name of organization and\\n        storedInfoType to be updated, for example\\n        ``organizations/433245324/storedInfoTypes/432452342`` or\\n        projects/project-id/storedInfoTypes/432452342.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    config (google.cloud.dlp_v2.types.StoredInfoTypeConfig):\\n        Updated configuration for the\\n        storedInfoType. If not provided, a new\\n        version of the storedInfoType will be\\n        created with the existing configuration.\\n\\n        This corresponds to the ``config`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        Mask to control which fields get\\n        updated.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dlp_v2.types.StoredInfoType:\\n        StoredInfoType resource message that\\n        contains information about the current\\n        version and any pending updates.',\n",
       " 'Call the list info types method over HTTP.\\n\\nArgs:\\n    request (~.dlp.ListInfoTypesRequest):\\n        The request object. Request for the list of infoTypes.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.dlp.ListInfoTypesResponse:\\n        Response to the ListInfoTypes\\n    request.',\n",
       " 'Pre-rpc interceptor for reschedule_maintenance\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CloudMemcache server.',\n",
       " 'Return a callable for the create runtime method over gRPC.\\n\\nCreates a new Runtime in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateRuntimeRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns a fully-qualified product string.',\n",
       " 'Call the set iam policy method over HTTP.\\n\\nArgs:\\n    request (~.iam_policy_pb2.SetIamPolicyRequest):\\n        The request object. Request message for ``SetIamPolicy`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.policy_pb2.Policy:\\n        An Identity and Access Management (IAM) policy, which\\n    specifies access controls for Google Cloud resources.\\n\\n    A ``Policy`` is a collection of ``bindings``. A\\n    ``binding`` binds one or more ``members``, or\\n    principals, to a single ``role``. Principals can be user\\n    accounts, service accounts, Google groups, and domains\\n    (such as G Suite). A ``role`` is a named list of\\n    permissions; each ``role`` can be an IAM predefined role\\n    or a user-created custom role.\\n\\n    For some types of Google Cloud resources, a ``binding``\\n    can also specify a ``condition``, which is a logical\\n    expression that allows access to a resource only if the\\n    expression evaluates to ``true``. A condition can add\\n    constraints based on attributes of the request, the\\n    resource, or both. To learn which resources support\\n    conditions in their IAM policies, see the `IAM\\n    documentation <https://cloud.google.com/iam/help/conditions/resource-policies>`__.\\n\\n    **JSON example:**\\n\\n    ::\\n\\n           {\\n             \"bindings\": [\\n               {\\n                 \"role\": \"roles/resourcemanager.organizationAdmin\",\\n                 \"members\": [\\n                   \"user:mike@example.com\",\\n                   \"group:admins@example.com\",\\n                   \"domain:google.com\",\\n                   \"serviceAccount:my-project-id@appspot.gserviceaccount.com\"\\n                 ]\\n               },\\n               {\\n                 \"role\": \"roles/resourcemanager.organizationViewer\",\\n                 \"members\": [\\n                   \"user:eve@example.com\"\\n                 ],\\n                 \"condition\": {\\n                   \"title\": \"expirable access\",\\n                   \"description\": \"Does not grant access after Sep 2020\",\\n                   \"expression\": \"request.time <\\n                   timestamp(\\'2020-10-01T00:00:00.000Z\\')\",\\n                 }\\n               }\\n             ],\\n             \"etag\": \"BwWWja0YfJA=\",\\n             \"version\": 3\\n           }\\n\\n    **YAML example:**\\n\\n    ::\\n\\n           bindings:\\n           - members:\\n             - user:mike@example.com\\n             - group:admins@example.com\\n             - domain:google.com\\n             - serviceAccount:my-project-id@appspot.gserviceaccount.com\\n             role: roles/resourcemanager.organizationAdmin\\n           - members:\\n             - user:eve@example.com\\n             role: roles/resourcemanager.organizationViewer\\n             condition:\\n               title: expirable access\\n               description: Does not grant access after Sep 2020\\n               expression: request.time < timestamp(\\'2020-10-01T00:00:00.000Z\\')\\n           etag: BwWWja0YfJA=\\n           version: 3\\n\\n    For a description of IAM and its features, see the `IAM\\n    documentation <https://cloud.google.com/iam/docs/>`__.',\n",
       " 'Return a callable for the list sources method over gRPC.\\n\\nLists all sources belonging to an organization.\\n\\nReturns:\\n    Callable[[~.ListSourcesRequest],\\n            ~.ListSourcesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the update endpoint method over gRPC.\\n\\nUpdates an endpoint.\\n\\nReturns:\\n    Callable[[~.UpdateEndpointRequest],\\n            ~.Endpoint]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Parses a orchestration_cluster path into its component segments.',\n",
       " 'Post-rpc interceptor for get_input\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the LivestreamService server but before\\nit is returned to user code.',\n",
       " 'Gets details of a single Stream.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import visionai_v1\\n\\n    def sample_get_stream():\\n        # Create a client\\n        client = visionai_v1.StreamsServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = visionai_v1.GetStreamRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_stream(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.visionai_v1.types.GetStreamRequest, dict]):\\n        The request object. Message for getting a Stream.\\n    name (str):\\n        Required. Name of the resource.\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.visionai_v1.types.Stream:\\n        Message describing the Stream object.\\n        The Stream and the Event resources are\\n        many to many; i.e., each Stream resource\\n        can associate to many Event resources\\n        and each Event resource can associate to\\n        many Stream resources.',\n",
       " 'Deletes an existing ScanConfig and its child\\nresources.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import websecurityscanner_v1\\n\\n    def sample_delete_scan_config():\\n        # Create a client\\n        client = websecurityscanner_v1.WebSecurityScannerClient()\\n\\n        # Initialize request argument(s)\\n        request = websecurityscanner_v1.DeleteScanConfigRequest(\\n        )\\n\\n        # Make the request\\n        client.delete_scan_config(request=request)\\n\\nArgs:\\n    request (Union[google.cloud.websecurityscanner_v1.types.DeleteScanConfigRequest, dict]):\\n        The request object. Request for the ``DeleteScanConfig`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Deletes a single Utilization Report.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import vmmigration_v1\\n\\n    def sample_delete_utilization_report():\\n        # Create a client\\n        client = vmmigration_v1.VmMigrationClient()\\n\\n        # Initialize request argument(s)\\n        request = vmmigration_v1.DeleteUtilizationReportRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.delete_utilization_report(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.vmmigration_v1.types.DeleteUtilizationReportRequest, dict]):\\n        The request object. Request message for\\n        \\'DeleteUtilizationReport\\' request.\\n    name (str):\\n        Required. The Utilization Report\\n        name.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Return a callable for the list scan configs method over gRPC.\\n\\nLists ScanConfigs under a given project.\\n\\nReturns:\\n    Callable[[~.ListScanConfigsRequest],\\n            ~.ListScanConfigsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Creates a new workstation configuration.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import workstations_v1\\n\\n    def sample_create_workstation_config():\\n        # Create a client\\n        client = workstations_v1.WorkstationsClient()\\n\\n        # Initialize request argument(s)\\n        request = workstations_v1.CreateWorkstationConfigRequest(\\n            parent=\"parent_value\",\\n            workstation_config_id=\"workstation_config_id_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.create_workstation_config(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.workstations_v1.types.CreateWorkstationConfigRequest, dict]):\\n        The request object. Message for creating a\\n        CreateWorkstationConfig.\\n    parent (str):\\n        Required. Parent resource name.\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    workstation_config (google.cloud.workstations_v1.types.WorkstationConfig):\\n        Required. Config to create.\\n        This corresponds to the ``workstation_config`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    workstation_config_id (str):\\n        Required. ID to use for the\\n        workstation configuration.\\n\\n        This corresponds to the ``workstation_config_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.workstations_v1.types.WorkstationConfig`\\n        A workstation configuration resource in the Cloud\\n        Workstations API.\\n\\n           Workstation configurations act as templates for\\n           workstations. The workstation configuration defines\\n           details such as the workstation virtual machine (VM)\\n           instance type, persistent storage, container image\\n           defining environment, which IDE or Code Editor to\\n           use, and more. Administrators and platform teams can\\n           also use [Identity and Access Management\\n           (IAM)](https://cloud.google.com/iam/docs/overview)\\n           rules to grant access to teams or to individual\\n           developers.',\n",
       " 'Returns a fully-qualified task string.',\n",
       " 'Pre-rpc interceptor for claim_homepage\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the HomepageService server.',\n",
       " 'Verifies the given signature matches the expected\\nsignature.',\n",
       " 'Length of the iterable.\\n\\nIf the iterable is a generator or otherwise does not have a\\nsize, it is eagerly evaluated to get a size.',\n",
       " 'Wait for the cache to be empty before resizing the pool.',\n",
       " 'Gets/sets the binary content, which is base64 encoded in the\\ntext.',\n",
       " 'Test HTML parsing.\\n\\n>>> # p = HTMLTreeBuilder.TreeBuilder()\\n>>> p = ElementTree.HTMLParser()\\n>>> p.feed(\"<p><p>spam<b>egg</b></p>\")\\n>>> serialize(p.close())\\n\\'<p>spam<b>egg</b></p>\\'',\n",
       " 'See gh-7462\\n\\nInput when printed looks like:\\n\\n    A       B       interaction     2\\n    B       C       interaction     4\\n    C       A       interaction\\n\\nNote the trailing \\\\t in the last line, which indicates the existence of\\nan empty data field.',\n",
       " \">>> tupletarget()\\nenter\\n(1, 2, (3, (4, 5)))\\nexit <type 'NoneType'> <type 'NoneType'> <type 'NoneType'>\",\n",
       " 'Plot a pie chart.\\n\\nMake a pie chart of array *x*.  The fractional area of each wedge is\\ngiven by ``x/sum(x)``.\\n\\nThe wedges are plotted counterclockwise, by default starting from the\\nx-axis.\\n\\nParameters\\n----------\\nx : 1D array-like\\n    The wedge sizes.\\n\\nexplode : array-like, default: None\\n    If not *None*, is a ``len(x)`` array which specifies the fraction\\n    of the radius with which to offset each wedge.\\n\\nlabels : list, default: None\\n    A sequence of strings providing the labels for each wedge\\n\\ncolors : :mpltype:`color` or list of :mpltype:`color`, default: None\\n    A sequence of colors through which the pie chart will cycle.  If\\n    *None*, will use the colors in the currently active cycle.\\n\\nhatch : str or list, default: None\\n    Hatching pattern applied to all pie wedges or sequence of patterns\\n    through which the chart will cycle. For a list of valid patterns,\\n    see :doc:`/gallery/shapes_and_collections/hatch_style_reference`.\\n\\n    .. versionadded:: 3.7\\n\\nautopct : None or str or callable, default: None\\n    If not *None*, *autopct* is a string or function used to label the\\n    wedges with their numeric value. The label will be placed inside\\n    the wedge. If *autopct* is a format string, the label will be\\n    ``fmt % pct``. If *autopct* is a function, then it will be called.\\n\\npctdistance : float, default: 0.6\\n    The relative distance along the radius at which the text\\n    generated by *autopct* is drawn. To draw the text outside the pie,\\n    set *pctdistance* > 1. This parameter is ignored if *autopct* is\\n    ``None``.\\n\\nlabeldistance : float or None, default: 1.1\\n    The relative distance along the radius at which the labels are\\n    drawn. To draw the labels inside the pie, set  *labeldistance* < 1.\\n    If set to ``None``, labels are not drawn but are still stored for\\n    use in `.legend`.\\n\\nshadow : bool or dict, default: False\\n    If bool, whether to draw a shadow beneath the pie. If dict, draw a shadow\\n    passing the properties in the dict to `.Shadow`.\\n\\n    .. versionadded:: 3.8\\n        *shadow* can be a dict.\\n\\nstartangle : float, default: 0 degrees\\n    The angle by which the start of the pie is rotated,\\n    counterclockwise from the x-axis.\\n\\nradius : float, default: 1\\n    The radius of the pie.\\n\\ncounterclock : bool, default: True\\n    Specify fractions direction, clockwise or counterclockwise.\\n\\nwedgeprops : dict, default: None\\n    Dict of arguments passed to each `.patches.Wedge` of the pie.\\n    For example, ``wedgeprops = {\\'linewidth\\': 3}`` sets the width of\\n    the wedge border lines equal to 3. By default, ``clip_on=False``.\\n    When there is a conflict between these properties and other\\n    keywords, properties passed to *wedgeprops* take precedence.\\n\\ntextprops : dict, default: None\\n    Dict of arguments to pass to the text objects.\\n\\ncenter : (float, float), default: (0, 0)\\n    The coordinates of the center of the chart.\\n\\nframe : bool, default: False\\n    Plot Axes frame with the chart if true.\\n\\nrotatelabels : bool, default: False\\n    Rotate each label to the angle of the corresponding slice if true.\\n\\nnormalize : bool, default: True\\n    When *True*, always make a full pie by normalizing x so that\\n    ``sum(x) == 1``. *False* makes a partial pie if ``sum(x) <= 1``\\n    and raises a `ValueError` for ``sum(x) > 1``.\\n\\ndata : indexable object, optional\\n    DATA_PARAMETER_PLACEHOLDER\\n\\nReturns\\n-------\\npatches : list\\n    A sequence of `matplotlib.patches.Wedge` instances\\n\\ntexts : list\\n    A list of the label `.Text` instances.\\n\\nautotexts : list\\n    A list of `.Text` instances for the numeric labels. This will only\\n    be returned if the parameter *autopct* is not *None*.\\n\\nNotes\\n-----\\nThe pie chart will probably look best if the figure and Axes are\\nsquare, or the Axes aspect is equal.\\nThis method sets the aspect ratio of the axis to \"equal\".\\nThe Axes aspect ratio can be controlled with `.Axes.set_aspect`.',\n",
       " 'Add a `.Table` to the Axes; return the table.',\n",
       " 'Draw the text by converting them to paths using `.TextToPath`.\\n\\nThis private helper supports the same parameters as\\n`~.RendererBase.draw_text`; setting *ismath* to \"TeX\" triggers TeX\\nrendering.',\n",
       " 'Trigger the tool.\\n\\nParameters\\n----------\\nname : str\\n    Name (id) of the tool triggered from within the container.',\n",
       " '#11035: repeated calls to suptitle should not alter the layout',\n",
       " \"Parameters\\n----------\\nfig : Figure\\npos : tuple of 4 floats\\n    Position of the rectangle that will be divided.\\nhorizontal : list of :mod:`~mpl_toolkits.axes_grid1.axes_size`\\n    Sizes for horizontal division.\\nvertical : list of :mod:`~mpl_toolkits.axes_grid1.axes_size`\\n    Sizes for vertical division.\\naspect : bool, optional\\n    Whether overall rectangular area is reduced so that the relative\\n    part of the horizontal and vertical scales have the same scale.\\nanchor : (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', 'N', 'NW', 'W'}, default: 'C'\\n    Placement of the reduced rectangle, when *aspect* is True.\",\n",
       " 'Apply max_staleness, in seconds, to a Selection.',\n",
       " 'Tests that attempting to create a configuration model graph\\nusing a directed graph yields an exception.',\n",
       " \"x.flat shouldn't modify data\",\n",
       " 'Check that module names are handled correctly\\ngh-22819\\nEssentially, the -m name cannot be used to import the module, so the module\\nnamed in the .pyf needs to be used instead\\n\\nCLI :: -m and a .pyf file',\n",
       " '(This docstring should be overwritten)',\n",
       " 'Test ceil function',\n",
       " 'Test reverse function with non-array',\n",
       " 'Returns the current `Context` object.',\n",
       " 'create and return a new specialized BinOp from myself',\n",
       " 'Guaranteed return of an indexer even when non-unique.\\n\\nThis dispatches to get_indexer or get_indexer_non_unique\\nas appropriate.\\n\\nParameters\\n----------\\ntarget : Index\\n    An iterable containing the values to be used for computing indexer.\\n\\nReturns\\n-------\\nnp.ndarray[np.intp]\\n    List of indices.\\n\\nSee Also\\n--------\\nIndex.get_indexer : Computes indexer and mask for new index given\\n    the current index.\\nIndex.get_non_unique : Returns indexer and masks for new index given\\n    the current index.\\n\\nExamples\\n--------\\n>>> idx = pd.Index([np.nan, \"var1\", np.nan])\\n>>> idx.get_indexer_for([np.nan])\\narray([0, 2])',\n",
       " 'Return an indexer class that will compute the window start and end bounds\\n\\nReturns\\n-------\\nGroupbyIndexer',\n",
       " 'Returns the default locations of ticks.',\n",
       " 'The algorithm works as follows:\\n\\nEqual:\\n    - Assure that the start is a newline, otherwise parse until we get\\n      one.\\n    - Copy from parsed_until_line + 1 to max(i2 + 1)\\n    - Make sure that the indentation is correct (e.g. add DEDENT)\\n    - Add old and change positions\\nInsert:\\n    - Parse from parsed_until_line + 1 to min(j2 + 1), hopefully not\\n      much more.\\n\\nReturns the new module node.',\n",
       " 'also ensure timeout when used within continuation prompts. ',\n",
       " \"Parse a Simple API's Index Content, and yield its anchor elements as Link objects.\",\n",
       " 'Check that certain strings are present in the output.',\n",
       " 'Return a list of a single line -- normal case for format_exception_only',\n",
       " 'Read and parse configurations.\\n\\nIf a config file is specified on the command line with the\\n\"--config\" option, then only it is used for configuration.\\n\\nOtherwise, the user configuration (~/.config/pycodestyle) and any\\nlocal configurations in the current directory or above will be\\nmerged together (in that order) using the read method of\\nConfigParser.',\n",
       " ':type: string',\n",
       " 'Generate a subclass of baselexer that accepts the Objective-C syntax\\nextensions.',\n",
       " 'Return a class diagram definition for the class and related classes.',\n",
       " 'Example for the usage of \"For the other parameters, see\" to avoid\\ntoo many repetitions, e.g. in functions or methods adhering to a\\ngiven interface (Numpy style)\\n\\nParameters\\n----------\\nyarg: float\\n    bla yarg\\n\\nFor the other parameters, see :func:`bla`',\n",
       " 'No unused-variable for a container if iterated in comprehension',\n",
       " \"PSets can be created from iterables even though they can't be len() hinted.\",\n",
       " 'Create a field mask by comparing two messages.\\n\\nArgs:\\n    original (~google.protobuf.message.Message): the original message.\\n        If set to None, this field will be interpreted as an empty\\n        message.\\n    modified (~google.protobuf.message.Message): the modified message.\\n        If set to None, this field will be interpreted as an empty\\n        message.\\n\\nReturns:\\n    google.protobuf.field_mask_pb2.FieldMask: field mask that contains\\n    the list of field names that have different values between the two\\n    messages. If the messages are equivalent, then the field mask is empty.\\n\\nRaises:\\n    ValueError: If the ``original`` or ``modified`` are not the same type.',\n",
       " 'NOOP string -> string coercion',\n",
       " 'Delete the endpoint configuration.\\n\\nArgs:\\n  EndpointConfigName:\\n\\nReturns:',\n",
       " 'microsecond (0-999999)',\n",
       " 'patch_mutating_webhook_configuration  # noqa: E501\\n\\npartially update the specified MutatingWebhookConfiguration  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.patch_mutating_webhook_configuration(name, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the MutatingWebhookConfiguration (required)\\n:param object body: (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param bool force: Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1MutatingWebhookConfiguration\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Sets the metadata of this CoreV1Event.\\n\\n\\n:param metadata: The metadata of this CoreV1Event.  # noqa: E501\\n:type: V1ObjectMeta',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Some bulk-generated tests.',\n",
       " \"Determine the data type of key.\\n\\nParameters\\n----------\\nkey : scalar, slice or array-like\\n    The key from which we want to infer the data type.\\n\\naccept_slice : bool, default=True\\n    Whether or not to raise an error if the key is a slice.\\n\\nReturns\\n-------\\ndtype : {'int', 'str', 'bool', None}\\n    Returns the data type of key.\",\n",
       " 'V1FlowSchemaSpec - a model defined in OpenAPI',\n",
       " \"Sets the device of this V1alpha3DeviceRequestAllocationResult.\\n\\nDevice references one device instance via its name in the driver's resource pool. It must be a DNS label.  # noqa: E501\\n\\n:param device: The device of this V1alpha3DeviceRequestAllocationResult.  # noqa: E501\\n:type: str\",\n",
       " 'Append UNCOMPRESSED tag to params.',\n",
       " 'Called before the given class is uninstrumented.\\n\\nTo get at the :class:`.ClassManager`, use\\n:func:`.manager_of_class`.',\n",
       " 'Generate the framework entrypoint file (as text) for a processing job.\\n\\nThis script implements the \"framework\" functionality for setting up your code:\\nUntar-ing the sourcedir bundle in the ```code``` input; installing extra\\nruntime dependencies if specified; and then invoking the ```command``` and\\n```code``` configured for the job.\\n\\nArgs:\\n    user_script (str): Relative path to ```code``` in the source bundle\\n        - e.g. \\'process.py\\'.\\n    codeartifact_repo_arn (str): The ARN of the CodeArtifact repository that should be\\n        logged into before installing dependencies (default: None).',\n",
       " 'Describe feature metadata by feature name in FeatureStore service.\\n\\nArgs:\\n    feature_group_name (str): name of the FeatureGroup.\\n    feature_name (str): name of the feature.\\nReturns:\\n    Response dict from service.',\n",
       " 'Construct a step collection of `TransformStep`, `QualityCheckStep` or `ClarifyCheckStep`\\n\\nArgs:\\n    name (str): The name of the `MonitorBatchTransformStep`.\\n        The corresponding transform step will be named `{name}-transform`;\\n        and the corresponding check step will be named `{name}-monitoring`\\n    transform_step_args (_JobStepArguments): the transform step transform arguments.\\n    monitor_configuration (Union[\\n        `sagemaker.workflow.quality_check_step.QualityCheckConfig`,\\n        `sagemaker.workflow.quality_check_step.ClarifyCheckConfig`\\n    ]): the monitoring configuration used for run model monitoring.\\n    check_job_configuration (`sagemaker.workflow.check_job_config.CheckJobConfig`):\\n        the check job (processing job) cluster resource configuration.\\n    monitor_before_transform (bool): If to run data quality or model explainability\\n        monitoring type, a true value of this flag indicates\\n        running the check step before the transform job.\\n    fail_on_violation (Union[bool, PipelineVariable]): A opt-out flag to not to fail the\\n        check step when a violation is detected.\\n    supplied_baseline_statistics (Union[str, PipelineVariable]): The S3 path\\n        to the supplied statistics object representing the statistics JSON file\\n        which will be used for drift to check (default: None).\\n    supplied_baseline_constraints (Union[str, PipelineVariable]): The S3 path\\n        to the supplied constraints object representing the constraints JSON file\\n        which will be used for drift to check (default: None).\\n    display_name (str): The display name of the `MonitorBatchTransformStep`.\\n        The display name provides better UI readability.\\n        The corresponding transform step will be\\n        named `{display_name}-transform`;  and the corresponding check step\\n        will be named `{display_name}-monitoring` (default: None).\\n    description (str): The description of the `MonitorBatchTransformStep` (default: None).',\n",
       " \"Get the hash of a training step's code artifact(s).\\n\\nArgs:\\n    entry_point (str): The absolute or relative path to the local Python\\n            source file that should be executed as the entry point to\\n            training\\n    source_dir (str): Path to a directory with any other training source\\n            code dependencies aside from the entry point file\\n    dependencies (str): A list of paths to directories (absolute\\n            or relative) with any additional libraries that will be exported\\n            to the container\\nReturns:\\n    str: A hash string representing the unique code artifact(s) for the step\",\n",
       " 'Check that we raise the proper AttributeErrors when the `estimator`\\ndoes not implement the `predict_proba` method, which is called from within\\n`fit`, or `decision_function`, which is decorated with `available_if`.\\n\\nNon-regression test for:\\nhttps://github.com/scikit-learn/scikit-learn/issues/28108',\n",
       " 'Check the string representation of the _InstancesOf constraint.',\n",
       " 'Print a warning when called.',\n",
       " 'Create content for Cython header file for generated pyx.',\n",
       " 'Compute the matrix multiplication between the DataFrame and others.\\n\\nThis method computes the matrix product between the DataFrame and the\\nvalues of an other Series\\n\\nIt can also be called using ``self @ other`` in Python >= 3.5.\\n\\n.. note:: This method is based on an expensive operation due to the nature\\n    of big data. Internally it needs to generate each row for each value, and\\n    then group twice - it is a huge operation. To prevent misuse, this method\\n    has the \\'compute.max_rows\\' default limit of input length and raises a ValueError.\\n\\n        >>> from pyspark.pandas.config import option_context\\n        >>> with option_context(\\n        ...     \\'compute.max_rows\\', 1000, \"compute.ops_on_diff_frames\", True\\n        ... ):  # doctest: +NORMALIZE_WHITESPACE\\n        ...     psdf = ps.DataFrame({\\'a\\': range(1001)})\\n        ...     psser = ps.Series([2], index=[\\'a\\'])\\n        ...     psdf.dot(psser)\\n        Traceback (most recent call last):\\n          ...\\n        ValueError: Current DataFrame\\'s length exceeds the given limit of 1000 rows.\\n        Please set \\'compute.max_rows\\' by using \\'pyspark.pandas.config.set_option\\'\\n        to retrieve more than 1000 rows. Note that, before changing the\\n        \\'compute.max_rows\\', this operation is considerably expensive.\\n\\nParameters\\n----------\\nother : Series\\n    The other object to compute the matrix product with.\\n\\nReturns\\n-------\\nSeries\\n    Return the matrix product between self and other as a Series.\\n\\nSee Also\\n--------\\nSeries.dot: Similar method for Series.\\n\\nNotes\\n-----\\nThe dimensions of DataFrame and other must be compatible to\\ncompute the matrix multiplication. In addition, the column names of\\nDataFrame and the index of other must contain the same values, as they\\nwill be aligned prior to the multiplication.\\n\\nThe dot method for Series computes the inner product, instead of the\\nmatrix product here.\\n\\nExamples\\n--------\\n>>> from pyspark.pandas.config import set_option, reset_option\\n>>> set_option(\"compute.ops_on_diff_frames\", True)\\n>>> psdf = ps.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\\n>>> psser = ps.Series([1, 1, 2, 1])\\n>>> psdf.dot(psser)\\n0   -4\\n1    5\\ndtype: int64\\n\\nNote how shuffling of the objects does not change the result.\\n\\n>>> psser2 = psser.reindex([1, 0, 2, 3])\\n>>> psdf.dot(psser2)\\n0   -4\\n1    5\\ndtype: int64\\n>>> psdf @ psser2\\n0   -4\\n1    5\\ndtype: int64\\n>>> reset_option(\"compute.ops_on_diff_frames\")',\n",
       " 'Zips this RDD with its element indices.\\n\\nThe ordering is first based on the partition index and then the\\nordering of items within each partition. So the first item in\\nthe first partition gets index 0, and the last item in the last\\npartition receives the largest index.\\n\\nThis method needs to trigger a spark job when this RDD contains\\nmore than one partitions.\\n\\n.. versionadded:: 1.2.0\\n\\nReturns\\n-------\\n:class:`RDD`\\n    a :class:`RDD` containing the zipped key-index pairs\\n\\nSee Also\\n--------\\n:meth:`RDD.zip`\\n:meth:`RDD.zipWithUniqueId`\\n\\nExamples\\n--------\\n>>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\\n[(\\'a\\', 0), (\\'b\\', 1), (\\'c\\', 2), (\\'d\\', 3)]',\n",
       " \"Tests reserved words in identifiers.\\n\\n'true', 'false', and 'column' are undocumented reserved words\\nwhen used as column identifiers (as of 3.5.1).  Covering them\\nhere to ensure they remain in place if the dialect's\\nreserved_words set is updated in the future.\",\n",
       " \"Amount of a particular custom resource(GPU, FPGA, etc) to use. The resource names supported\\ncorrespond to the regular Spark configs with the prefix removed. For instance, resources\\nlike GPUs are gpu (spark configs `spark.executor.resource.gpu.*`). If you pass in a resource\\nthat the cluster manager doesn't support the result is undefined, it may error or may just\\nbe ignored.\\nThis is a convenient API to add :class:`ExecutorResourceRequest` for custom resources.\\n\\nParameters\\n----------\\nresourceName : str\\n    Name of the resource.\\namount : str\\n    amount of that resource per executor to use.\\ndiscoveryScript : str, optional\\n    Optional script used to discover the resources. This is required on\\n    some cluster managers that don't tell Spark the addresses of\\n    the resources allocated. The script runs on Executors startup to\\n    of the resources available.\\nvendor : str\\n    Optional vendor, required for some cluster managers\\n\\nReturns\\n-------\\n:class:`ExecutorResourceRequests`\",\n",
       " 'Safely get :term:`obj.__slots__ <__slots__>` as a dictionary if any.\\n\\n- This returns ``None`` if ``obj.__slots__`` does not exist.\\n- This raises a :exc:`TypeError` if *obj* is not a class.\\n- This raises a :exc:`ValueError` if ``obj.__slots__`` is invalid.',\n",
       " 'Assert that the requests param is truthy.',\n",
       " 'case insensitive startswith method',\n",
       " 'What is the dimension of the space that the object is contained in?',\n",
       " 'Print the alias parts of the help.',\n",
       " 'Insert one or more columns at the given column position.\\n\\nExamples\\n========\\n\\n>>> from sympy import zeros, ones\\n>>> M = zeros(3)\\n>>> V = ones(3, 1)\\n>>> M.col_insert(1, V)\\nMatrix([\\n[0, 1, 0, 0],\\n[0, 1, 0, 0],\\n[0, 1, 0, 0]])\\n\\nSee Also\\n========\\n\\ncol\\nrow_insert',\n",
       " 'Alternate constructor that will use the published constants.\\n\\nExplanation\\n===========\\n\\nReturns an instance of ``FirstOrderActivationDeGroote2016`` using the\\nthree constant values specified in the original publication.\\n\\nThese have the values:\\n\\n:math:`tau_a = 0.015`\\n:math:`tau_d = 0.060`\\n:math:`b = 10`',\n",
       " 'Convert a GMPY ``mpz`` object to ``dtype``. ',\n",
       " 'Convert a GMPY ``mpz`` object to ``dtype``. ',\n",
       " 'Convert :class:`DomainMatrix` to list of nonzero elements and data.\\n\\nExplanation\\n===========\\n\\nReturns a tuple ``(elements, data)`` where ``elements`` is a list of\\nelements of the matrix with zeros possibly excluded. The matrix can be\\nreconstructed by passing these to :meth:`from_flat_nz`. The idea is to\\nbe able to modify a flat list of the elements and then create a new\\nmatrix of the same shape with the modified elements in the same\\npositions.\\n\\nThe format of ``data`` differs depending on whether the underlying\\nrepresentation is dense or sparse but either way it represents the\\npositions of the elements in the list in a way that\\n:meth:`from_flat_nz` can use to reconstruct the matrix. The\\n:meth:`from_flat_nz` method should be called on the same\\n:class:`DomainMatrix` that was used to call :meth:`to_flat_nz`.\\n\\nExamples\\n========\\n\\n>>> from sympy import ZZ\\n>>> from sympy.polys.matrices import DomainMatrix\\n>>> A = DomainMatrix([\\n...    [ZZ(1), ZZ(2)],\\n...    [ZZ(3), ZZ(4)]], (2, 2), ZZ)\\n>>> elements, data = A.to_flat_nz()\\n>>> elements\\n[1, 2, 3, 4]\\n>>> A == A.from_flat_nz(elements, data, A.domain)\\nTrue\\n\\nCreate a matrix with the elements doubled:\\n\\n>>> elements_doubled = [2*x for x in elements]\\n>>> A2 = A.from_flat_nz(elements_doubled, data, A.domain)\\n>>> A2 == 2*A\\nTrue\\n\\nSee Also\\n========\\n\\nfrom_flat_nz',\n",
       " 'Load a compatible wheel from a given folder.',\n",
       " 'Encodes all child objects into the contents for this object.\\n\\nThis method is overridden because a SetOf needs to be encoded by\\nsorting the child encodings.\\n\\n:param force:\\n    Ensure all contents are in DER format instead of possibly using\\n    cached BER-encoded data',\n",
       " 'Handles the creation of a single response item by setting\\nparameters and creating the appropriate resource instance.\\n\\n:type resource_cls: ServiceResource subclass\\n:param resource_cls: The resource class to instantiate.\\n:type parent: ServiceResource\\n:param parent: The resource instance to which this action is attached.\\n:type identifiers: dict\\n:param identifiers: Map of identifier names to value or values.\\n:type resource_data: dict or None\\n:param resource_data: Data for resource attributes.\\n:rtype: ServiceResource\\n:return: New resource instance.',\n",
       " 'Run the command-line `cmd` in a sub-process, and print its output.\\n\\nUse this when you need to test the process behavior of coverage.\\n\\nCompare with `command_line`.\\n\\nHandles the following command names specially:\\n\\n* \"python\" is replaced with the command name of the current\\n    Python interpreter.\\n\\n* \"coverage\" is replaced with the command name for the main\\n    coverage.py program.\\n\\nReturns a pair: the process\\' exit status and its stdout/stderr text,\\nwhich are also stored as `self.last_command_status` and\\n`self.last_command_output`.',\n",
       " 'Call the patch method over HTTP.\\n\\nArgs:\\n    request (~.compute.PatchRegionTargetHttpsProxyRequest):\\n        The request object. A request message for\\n    RegionTargetHttpsProxies.Patch. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " '>>> assert func_introspection2.__code__.co_argcount == 5, func_introspection2.__code__.co_argcount\\n>>> func_introspection2.__defaults__\\n(1, 2, 3)',\n",
       " 'Send a low-level window message syncronically.\\n\\n@type  uMsg: int\\n@param uMsg: Message code.\\n\\n@param wParam:\\n    The type and meaning of this parameter depends on the message.\\n\\n@param lParam:\\n    The type and meaning of this parameter depends on the message.\\n\\n@param dwTimeout: Optional timeout for the operation.\\n    Use C{None} to wait indefinitely.\\n\\n@rtype:  int\\n@return: The meaning of the return value depends on the window message.\\n    Typically a value of C{0} means an error occured. You can get the\\n    error code by calling L{win32.GetLastError}.',\n",
       " \"Register all globally visible functions.\\n\\nThe first argument name is either 'physical_line' or 'logical_line'.\",\n",
       " 'Creates a new instance that receives messages from sys.stdin, and sends\\nthem to sys.stdout.',\n",
       " 'Remove one entry from the lru, and handle consequences.\\n\\nIf there are no more references to the lru, then this entry should be\\nremoved from the cache.',\n",
       " 'Return an object header for the given numeric type and text length.',\n",
       " 'Return the complete set of tokens for a file.',\n",
       " 'Call the create calculated metric method over HTTP.\\n\\nArgs:\\n    request (~.analytics_admin.CreateCalculatedMetricRequest):\\n        The request object. Request message for\\n    CreateCalculatedMetric RPC.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.resources.CalculatedMetric:\\n        A definition for a calculated metric.',\n",
       " 'Returns a fully-qualified property_quotas_snapshot string.',\n",
       " 'Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'chat.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " 'Create a Job.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import batch_v1\\n\\n    def sample_create_job():\\n        # Create a client\\n        client = batch_v1.BatchServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = batch_v1.CreateJobRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_job(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.batch_v1.types.CreateJobRequest, dict]):\\n        The request object. CreateJob Request.\\n    parent (str):\\n        Required. The parent resource name\\n        where the Job will be created. Pattern:\\n        \"projects/{project}/locations/{location}\"\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    job (google.cloud.batch_v1.types.Job):\\n        Required. The Job to create.\\n        This corresponds to the ``job`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    job_id (str):\\n        ID used to uniquely identify the Job within its parent\\n        scope. This field should contain at most 63 characters\\n        and must start with lowercase characters. Only lowercase\\n        characters, numbers and \\'-\\' are accepted. The \\'-\\'\\n        character cannot be the first or the last one. A system\\n        generated ID will be used if the field is not set.\\n\\n        The job.name field in the request will be ignored and\\n        the created resource name of the Job will be\\n        \"{parent}/jobs/{job_id}\".\\n\\n        This corresponds to the ``job_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.batch_v1.types.Job:\\n        The Cloud Batch Job description.',\n",
       " 'Return a callable for the get route method over gRPC.\\n\\nUse this method to get details about a route.\\n\\nReturns:\\n    Callable[[~.GetRouteRequest],\\n            ~.Route]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns a fully-qualified offer string.',\n",
       " 'Post-rpc interceptor for update_license_pool\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the LicenseManagementService server but before\\nit is returned to user code.',\n",
       " 'Sets the labels on an Interconnect. To learn more\\nabout labels, read the Labeling Resources documentation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_labels():\\n        # Create a client\\n        client = compute_v1.InterconnectsClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetLabelsInterconnectRequest(\\n            project=\"project_value\",\\n            resource=\"resource_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_labels(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetLabelsInterconnectRequest, dict]):\\n        The request object. A request message for\\n        Interconnects.SetLabels. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    resource (str):\\n        Name or id of the resource for this\\n        request.\\n\\n        This corresponds to the ``resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    global_set_labels_request_resource (google.cloud.compute_v1.types.GlobalSetLabelsRequest):\\n        The body resource for this request\\n        This corresponds to the ``global_set_labels_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Return a callable for the list routes method over gRPC.\\n\\nUse this method to list routes created for a private\\nconnectivity configuration in a project and location.\\n\\nReturns:\\n    Callable[[~.ListRoutesRequest],\\n            ~.ListRoutesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Gets an association with the specified name.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_get_association():\\n        # Create a client\\n        client = compute_v1.NetworkFirewallPoliciesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.GetAssociationNetworkFirewallPolicyRequest(\\n            firewall_policy=\"firewall_policy_value\",\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        response = client.get_association(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.GetAssociationNetworkFirewallPolicyRequest, dict]):\\n        The request object. A request message for\\n        NetworkFirewallPolicies.GetAssociation.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    firewall_policy (str):\\n        Name of the firewall policy to which\\n        the queried association belongs.\\n\\n        This corresponds to the ``firewall_policy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.types.FirewallPolicyAssociation:',\n",
       " 'Call the delete method over HTTP.\\n\\nArgs:\\n    request (~.compute.DeleteRegionInstantSnapshotRequest):\\n        The request object. A request message for\\n    RegionInstantSnapshots.Delete. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Creates an entry.\\n\\nYou can create entries only with \\'FILESET\\', \\'CLUSTER\\',\\n\\'DATA_STREAM\\', or custom types. Data Catalog automatically\\ncreates entries with other types during metadata ingestion from\\nintegrated systems.\\n\\nYou must enable the Data Catalog API in the project identified\\nby the ``parent`` parameter. For more information, see `Data\\nCatalog resource\\nproject <https://cloud.google.com/data-catalog/docs/concepts/resource-project>`__.\\n\\nAn entry group can have a maximum of 100,000 entries.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datacatalog_v1\\n\\n    def sample_create_entry():\\n        # Create a client\\n        client = datacatalog_v1.DataCatalogClient()\\n\\n        # Initialize request argument(s)\\n        entry = datacatalog_v1.Entry()\\n        entry.type_ = \"LOOK\"\\n        entry.integrated_system = \"VERTEX_AI\"\\n        entry.gcs_fileset_spec.file_patterns = [\\'file_patterns_value1\\', \\'file_patterns_value2\\']\\n\\n        request = datacatalog_v1.CreateEntryRequest(\\n            parent=\"parent_value\",\\n            entry_id=\"entry_id_value\",\\n            entry=entry,\\n        )\\n\\n        # Make the request\\n        response = client.create_entry(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datacatalog_v1.types.CreateEntryRequest, dict]):\\n        The request object. Request message for\\n        [CreateEntry][google.cloud.datacatalog.v1.DataCatalog.CreateEntry].\\n    parent (str):\\n        Required. The name of the entry group\\n        this entry belongs to.\\n        Note: The entry itself and its child\\n        resources might not be stored in the\\n        location specified in its name.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    entry_id (str):\\n        Required. The ID of the entry to create.\\n\\n        The ID must contain only letters (a-z, A-Z), numbers\\n        (0-9), and underscores (_). The maximum size is 64 bytes\\n        when encoded in UTF-8.\\n\\n        This corresponds to the ``entry_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    entry (google.cloud.datacatalog_v1.types.Entry):\\n        Required. The entry to create.\\n        This corresponds to the ``entry`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.datacatalog_v1.types.Entry:\\n        Entry metadata.\\n           A Data Catalog entry represents another resource in\\n           Google Cloud Platform (such as a BigQuery dataset or\\n           a Pub/Sub topic) or outside of it. You can use the\\n           linked_resource field in the entry resource to refer\\n           to the original resource ID of the source system.\\n\\n           An entry resource contains resource details, for\\n           example, its schema. Additionally, you can attach\\n           flexible metadata to an entry in the form of a\\n           [Tag][google.cloud.datacatalog.v1.Tag].',\n",
       " 'Return a callable for the export preview result method over gRPC.\\n\\nExport [Preview][google.cloud.config.v1.Preview] results.\\n\\nReturns:\\n    Callable[[~.ExportPreviewResultRequest],\\n            ~.ExportPreviewResultResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Validate ``estimator_dict`` in inputs for ``HyperparameterTuner.create()``',\n",
       " 'Call the ingest conversations method over HTTP.\\n\\nArgs:\\n    request (~.contact_center_insights.IngestConversationsRequest):\\n        The request object. The request to ingest conversations.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return a callable for the delete rule set method over gRPC.\\n\\nDeletes a ruleset. Returns NOT_FOUND if the document does not\\nexist.\\n\\nReturns:\\n    Callable[[~.DeleteRuleSetRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for get_playbook_version\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Playbooks server but before\\nit is returned to user code.',\n",
       " 'The expression dict for a `Join` function.',\n",
       " 'Call the get evaluation method over HTTP.\\n\\nArgs:\\n    request (~.evaluation_service.GetEvaluationRequest):\\n        The request object. Request message for\\n    [EvaluationService.GetEvaluation][google.cloud.discoveryengine.v1alpha.EvaluationService.GetEvaluation]\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.evaluation.Evaluation:\\n        An evaluation is a single execution\\n    (or run) of an evaluation process. It\\n    encapsulates the state of the evaluation\\n    and the resulting data.',\n",
       " 'Return a callable for the fetch domain verification\\nstatus method over gRPC.\\n\\nReturns list of target sites with its domain verification\\nstatus. This method can only be called under data store with\\nBASIC_SITE_SEARCH state at the moment.\\n\\nReturns:\\n    Callable[[~.FetchDomainVerificationStatusRequest],\\n            ~.FetchDomainVerificationStatusResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Post-rpc interceptor for create_mesh\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the NetworkServices server but before\\nit is returned to user code.',\n",
       " 'Lists Autonomous Database Character Sets in a given\\nproject and location.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import oracledatabase_v1\\n\\n    def sample_list_autonomous_database_character_sets():\\n        # Create a client\\n        client = oracledatabase_v1.OracleDatabaseClient()\\n\\n        # Initialize request argument(s)\\n        request = oracledatabase_v1.ListAutonomousDatabaseCharacterSetsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_autonomous_database_character_sets(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.oracledatabase_v1.types.ListAutonomousDatabaseCharacterSetsRequest, dict]):\\n        The request object. The request for ``AutonomousDatabaseCharacterSet.List``.\\n    parent (str):\\n        Required. The parent value for the\\n        Autonomous Database in the following\\n        format:\\n        projects/{project}/locations/{location}.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.oracledatabase_v1.services.oracle_database.pagers.ListAutonomousDatabaseCharacterSetsPager:\\n        The response for AutonomousDatabaseCharacterSet.List.\\n\\n        Iterating over this object will yield results and\\n        resolve additional pages automatically.',\n",
       " 'Return a callable for the list os policy assignment\\nreports method over gRPC.\\n\\nList OS policy asssignment reports for all Compute\\nEngine VM instances in the specified zone.\\n\\nReturns:\\n    Callable[[~.ListOSPolicyAssignmentReportsRequest],\\n            ~.ListOSPolicyAssignmentReportsResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the cancel operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.CancelOperationRequest):\\n        The request object for CancelOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Return a callable for the create collector method over gRPC.\\n\\nCreate a Collector to manage the on-prem appliance\\nwhich collects information about Customer assets.\\n\\nReturns:\\n    Callable[[~.CreateCollectorRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the delete model method over gRPC.\\n\\nDeletes an existing model.\\n\\nReturns:\\n    Callable[[~.DeleteModelRequest],\\n            ~.Empty]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the ingest asset method over gRPC.\\n\\nIngests data for the asset. It is not allowed to\\ningest a data chunk which is already expired according\\nto TTL. This method is only available via the gRPC API\\n(not HTTP since bi-directional streaming is not\\nsupported via HTTP).\\n\\nReturns:\\n    Callable[[~.IngestAssetRequest],\\n            ~.IngestAssetResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Modifies a ``NetworkPeering`` resource. Only the ``description``\\nfield can be updated. Only fields specified in ``updateMask``\\nare applied. NetworkPeering is a global resource and location\\ncan only be global.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import vmwareengine_v1\\n\\n    def sample_update_network_peering():\\n        # Create a client\\n        client = vmwareengine_v1.VmwareEngineClient()\\n\\n        # Initialize request argument(s)\\n        network_peering = vmwareengine_v1.NetworkPeering()\\n        network_peering.peer_network = \"peer_network_value\"\\n        network_peering.peer_network_type = \"GOOGLE_CLOUD_NETAPP_VOLUMES\"\\n        network_peering.vmware_engine_network = \"vmware_engine_network_value\"\\n\\n        request = vmwareengine_v1.UpdateNetworkPeeringRequest(\\n            network_peering=network_peering,\\n        )\\n\\n        # Make the request\\n        operation = client.update_network_peering(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.vmwareengine_v1.types.UpdateNetworkPeeringRequest, dict]):\\n        The request object. Request message for\\n        [VmwareEngine.UpdateNetworkPeering][google.cloud.vmwareengine.v1.VmwareEngine.UpdateNetworkPeering]\\n    network_peering (google.cloud.vmwareengine_v1.types.NetworkPeering):\\n        Required. Network peering\\n        description.\\n\\n        This corresponds to the ``network_peering`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        Required. Field mask is used to specify the fields to be\\n        overwritten in the ``NetworkPeering`` resource by the\\n        update. The fields specified in the ``update_mask`` are\\n        relative to the resource, not the full request. A field\\n        will be overwritten if it is in the mask. If the user\\n        does not provide a mask then all fields will be\\n        overwritten.\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.vmwareengine_v1.types.NetworkPeering`\\n        Details of a network peering.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Return a shell-escaped version of the string *s*.',\n",
       " 'Check that IPython starts with non-ascii characters in the path.',\n",
       " 'Reload source from disk and initialize state.',\n",
       " 'matches the multiline version of a comment',\n",
       " 'Disable replication on all secondaries.',\n",
       " 'Set the width of the figure in inches.\\n\\nParameters\\n----------\\nval : float\\nforward : bool\\n    See `set_size_inches`.\\n\\nSee Also\\n--------\\nmatplotlib.figure.Figure.set_figheight\\nmatplotlib.figure.Figure.set_size_inches',\n",
       " 'Set the number of minor ticks to label when some minor ticks are\\nlabelled.\\n\\nParameters\\n----------\\nminor_number : int\\n    Number of ticks which are labelled when the number of ticks is\\n    below the threshold.',\n",
       " 'Unpack bytes into a `Timestamp` object.\\n\\nUsed for pure-Python msgpack unpacking.\\n\\n:param b: Payload from msgpack ext message with code -1\\n:type b: bytes\\n\\n:returns: Timestamp object unpacked from msgpack ext payload\\n:rtype: Timestamp',\n",
       " 'Return list of process objects corresponding to live child processes',\n",
       " 'Tests for edge contraction in an undirected graph.',\n",
       " 'The hash string lengths should be as expected for a variety of graphs and\\ndigest sizes',\n",
       " 'Return a dictionary with items that are compatible\\nwith numpy.distutils.setup keyword arguments.',\n",
       " 'Encrypts plaintext using 3DES in either 2 or 3 key mode\\n\\n:param key:\\n    The encryption key - a byte string 16 or 24 bytes long (2 or 3 key mode)\\n\\n:param data:\\n    The plaintext - a byte string\\n\\n:param iv:\\n    The 8-byte initialization vector to use - a byte string - set as None\\n    to generate an appropriate one\\n\\n:raises:\\n    ValueError - when any of the parameters contain an invalid value\\n    TypeError - when any of the parameters are of the wrong type\\n    OSError - when an error is returned by the OS crypto library\\n\\n:return:\\n    A tuple of two byte strings (iv, ciphertext)',\n",
       " 'Replace a variable name, with a potentially new value.\\n\\nParameters\\n----------\\nold_key : str\\n    Current variable name to replace\\nnew_key : str\\n    New variable name to replace `old_key` with\\nnew_value : object\\n    Value to be replaced along with the possible renaming',\n",
       " 'Helper for interval_range to check if start/end are valid types.',\n",
       " 'Return a dictionary containing the underlying buffers.\\nThe returned dictionary has the following contents:\\n    - \"data\": a two-element tuple whose first element is a buffer\\n              containing the data and whose second element is the data\\n              buffer\\'s associated dtype.\\n    - \"validity\": a two-element tuple whose first element is a buffer\\n                  containing mask values indicating missing data and\\n                  whose second element is the mask value buffer\\'s\\n                  associated dtype. None if the null representation is\\n                  not a bit or byte mask.\\n    - \"offsets\": a two-element tuple whose first element is a buffer\\n                 containing the offset values for variable-size binary\\n                 data (e.g., variable-length strings) and whose second\\n                 element is the offsets buffer\\'s associated dtype. None\\n                 if the data buffer does not have an associated offsets\\n                 buffer.',\n",
       " 'Deletes the locs from the block.\\n\\nWe split the block to avoid copying the underlying data. We create new\\nblocks for every connected segment of the initial block that is not deleted.\\nThe new blocks point to the initial array.',\n",
       " 'groupby & merge; we are always performing a left-by type operation\\n\\nParameters\\n----------\\nby: field to group\\nleft: DataFrame\\nright: DataFrame\\nmerge_pieces: function for merging',\n",
       " \"Memory usage in bytes.\\n\\nReturns\\n-------\\nmemory_usage_bytes : int\\n    Object's total memory usage in bytes.\",\n",
       " 'Get the config object for the global Application instance, if there is one\\n\\notherwise return an empty config object',\n",
       " 'Download any requirements which were only fetched by metadata.',\n",
       " 'Convert a `RE_DATETIME` match to `datetime.datetime` or `datetime.date`.\\n\\nRaises ValueError if the match does not correspond to a valid date\\nor datetime.',\n",
       " 'If alive then mark as dead and return func(*args, **kwargs);\\notherwise return None',\n",
       " 'Check the compatible tag output.',\n",
       " 'The pydantic-core SchemaSerializer used to dump instances of the model.',\n",
       " 'Build environment variables suitable for passing to the Model.',\n",
       " 'Called before visiting project (i.e set of modules).',\n",
       " 'Multiple variables unpacked in comprehension.',\n",
       " \"Do something.\\n\\nParameters\\n----------\\nparam : str\\n    Description.\\nparam2 : str, optional\\n    Description (the default is 'all').\\n\\nReturns\\n-------\\nint\\n    Description.\",\n",
       " 'Test whether certain strings are in the output of a callback command.',\n",
       " 'If passed a list containing a single X509Name,\\n`Context.set_client_ca_list` configures the context to send\\nthat CA name to the client and, on both the server and client sides,\\n`Connection.get_client_ca_list` returns a list containing that\\nX509Name after the connection is set up.',\n",
       " \"Called when this instance has been seen twice, and thus should eventually be extracted into a sub-diagram\\n:param el_id: id of the element\\n:param state: element/diagram state tracker\\n:param name: name to use for this element's text\\n:param force: If true, force extraction now, regardless of the state of this. Only useful for extracting the\\nroot element when we know we're finished\",\n",
       " 'Iterate over all markers of the node.\\n\\n:param name: If given, filter the results by the name attribute.\\n:returns: An iterator of (node, mark) tuples.',\n",
       " \"Importing a package using --importmode=importlib should not import the\\npackage's __init__.py file more than once (#11306).\",\n",
       " 'Internal method to strip elements with a negative or zero count',\n",
       " \"Backport of 3-argument create_connection() for Py2.6.\\n\\nConnect to *address* and return the socket object.\\n\\nConvenience function.  Connect to *address* (a 2-tuple ``(host,\\nport)``) and return the socket object.  Passing the optional\\n*timeout* parameter will set the timeout on the socket instance\\nbefore attempting to connect.  If no *timeout* is supplied, the\\nglobal default timeout setting returned by :func:`getdefaulttimeout`\\nis used.  If *source_address* is set it must be a tuple of (host, port)\\nfor the socket to bind as a source address before making the connection.\\nAn host of '' or port 0 tells the OS to use the default.\",\n",
       " \"Return True iff this host shouldn't be accessed using a proxy\\n\\nThis function uses the MacOSX framework SystemConfiguration\\nto fetch the proxy information.\\n\\nproxy_settings come from _scproxy._get_proxy_settings or get mocked ie:\\n{ 'exclude_simple': bool,\\n  'exceptions': ['foo.bar', '*.bar.com', '127.0.0.1', '10.1', '10.0/16']\\n}\",\n",
       " 'The following fails when passed a unicode string on Python\\n(including when unicode_literals is in effect) and fails when\\npassed a byte-string on Python 3. So type() always wants a native\\nstring as the first argument.\\n\\nTODO: maybe provide a replacement that works identically on Py2/3?',\n",
       " \"Render tabular data using the most appropriate representation.\\n\\n:param data: An iterable (e.g. a :func:`tuple` or :class:`list`)\\n             containing the rows of the table, where each row is an\\n             iterable containing the columns of the table (strings).\\n:param column_names: An iterable of column names (strings).\\n:returns: The rendered table (a string).\\n\\nIf you want an easy way to render tabular data on a terminal in a human\\nfriendly format then this function is for you! It works as follows:\\n\\n- If the input data doesn't contain any line breaks the function\\n  :func:`format_pretty_table()` is used to render a pretty table. If the\\n  resulting table fits in the terminal without wrapping the rendered pretty\\n  table is returned.\\n\\n- If the input data does contain line breaks or if a pretty table would\\n  wrap (given the width of the terminal) then the function\\n  :func:`format_robust_table()` is used to render a more robust table that\\n  can deal with data containing line breaks and long text.\",\n",
       " 'Test to verify the _requests_helper function with success status',\n",
       " \"replace_namespaced_lease  # noqa: E501\\n\\nreplace the specified Lease  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.replace_namespaced_lease_with_http_info(name, namespace, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the Lease (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param V1Lease body: (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1Lease, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Deserializes body to file\\n\\nSaves response body into a file in a temporary folder,\\nusing the filename from the `Content-Disposition` header if provided.\\n\\n:param response:  RESTResponse.\\n:return: file path.',\n",
       " 'Sets the additional_items of this V1JSONSchemaProps.\\n\\nJSONSchemaPropsOrBool represents JSONSchemaProps or a boolean value. Defaults to true for the boolean property.  # noqa: E501\\n\\n:param additional_items: The additional_items of this V1JSONSchemaProps.  # noqa: E501\\n:type: object',\n",
       " 'Gets the preemption_policy of this V1PodSpec.  # noqa: E501\\n\\nPreemptionPolicy is the Policy for preempting pods with lower priority. One of Never, PreemptLowerPriority. Defaults to PreemptLowerPriority if unset.  # noqa: E501\\n\\n:return: The preemption_policy of this V1PodSpec.  # noqa: E501\\n:rtype: str',\n",
       " 'these are largely found by fuzz tests and implemented here as regression tests',\n",
       " '@see: https://redis.io/commands/cluster-nodes  # string / bytes\\n@see: https://redis.io/commands/cluster-replicas # list of string / bytes',\n",
       " \"Allow to override the delay for parts of tests which aren't time dependent,\\nto speed up execution.\",\n",
       " 'Generates a request dictionary using the parameters provided to the class.',\n",
       " 'Convert the API response to the native object.',\n",
       " 'Load an existing context and return an ``Context`` object representing it.\\n\\nExamples:\\n    .. code-block:: python\\n\\n        from sagemaker.lineage import context\\n\\n        my_context = context.Context.create(\\n            context_name=\\'MyContext\\',\\n            context_type=\\'Endpoint\\',\\n            source_uri=\\'arn:aws:...\\')\\n\\n        my_context.properties[\"added\"] = \"property\"\\n        my_context.save()\\n\\n        for ctx in context.Context.list():\\n            print(ctx)\\n\\n        my_context.delete()\\n\\n    Args:\\n        context_name (str): Name of the context\\n        sagemaker_session (sagemaker.session.Session): Session object which\\n            manages interactions with Amazon SageMaker APIs and any other\\n            AWS services needed. If not specified, one is created using the\\n            default AWS configuration chain.\\n\\n    Returns:\\n        Context: A SageMaker ``Context`` object',\n",
       " 'Starts a local training job.\\n\\nArgs:\\n    input_data_config (dict): The Input Data Configuration, this contains data such as the\\n        channels to be used for training.\\n    output_data_config (dict): The configuration of the output data.\\n    hyperparameters (dict): The HyperParameters for the training job.\\n    environment (dict): The collection of environment variables passed to the job.\\n    job_name (str): Name of the local training job being run.\\n\\nRaises:\\n    ValueError: If the input data configuration is not valid.\\n    RuntimeError: If the data distribution type is not supported.',\n",
       " 'Create a SageMaker ``PyTorchModel`` object that can be deployed to an ``Endpoint``.\\n\\nArgs:\\n    model_server_workers (int): Optional. The number of worker processes\\n        used by the inference server. If None, server will use one\\n        worker per vCPU.\\n    role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,\\n        which is also used during transform jobs. If not specified, the\\n        role from the Estimator will be used.\\n    vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on\\n        the model. Default: use subnets and security groups from this Estimator.\\n        * \\'Subnets\\' (list[str]): List of subnet ids.\\n        * \\'SecurityGroupIds\\' (list[str]): List of security group ids.\\n    entry_point (str): Path (absolute or relative) to the local Python source file which\\n        should be executed as the entry point to training. If ``source_dir`` is specified,\\n        then ``entry_point`` must point to a file located at the root of ``source_dir``.\\n        If not specified, the training entry point is used.\\n    source_dir (str): Path (absolute or relative) to a directory with any other serving\\n        source code dependencies aside from the entry point file.\\n        If not specified, the model source directory from training is used.\\n    dependencies (list[str]): A list of paths to directories (absolute or relative) with\\n        any additional libraries that will be exported to the container.\\n        If not specified, the dependencies from training are used.\\n        This is not supported with \"local code\" in Local Mode.\\n    **kwargs: Additional kwargs passed to the :class:`~sagemaker.pytorch.model.PyTorchModel`\\n        constructor.\\n\\nReturns:\\n    sagemaker.pytorch.model.PyTorchModel: A SageMaker ``PyTorchModel``\\n    object. See :func:`~sagemaker.pytorch.model.PyTorchModel` for full details.',\n",
       " 'Placeholder docstring',\n",
       " 'Retrieves log events from the specified CloudWatch log group and log stream.\\n\\nArgs:\\n    cw_client (boto3.client): A boto3 CloudWatch client.\\n    log_group_name (str): The name of the CloudWatch log group.\\n    log_stream_name (str): The name of the CloudWatch log stream.\\n\\nReturns:\\n    (dict): A dictionary containing log events from CloudWatch log group and log stream.',\n",
       " 'Calculate the standard deviation of the values of an N-D image array,\\noptionally at specified sub-regions.\\n\\nParameters\\n----------\\ninput : array_like\\n    N-D image data to process.\\nlabels : array_like, optional\\n    Labels to identify sub-regions in `input`.\\n    If not None, must be same shape as `input`.\\nindex : int or sequence of ints, optional\\n    `labels` to include in output. If None (default), all values where\\n    `labels` is non-zero are used.\\n\\nReturns\\n-------\\nstandard_deviation : float or ndarray\\n    Values of standard deviation, for each sub-region if `labels` and\\n    `index` are specified.\\n\\nSee Also\\n--------\\nlabel, variance, maximum, minimum, extrema\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> a = np.array([[1, 2, 0, 0],\\n...               [5, 3, 0, 4],\\n...               [0, 0, 0, 7],\\n...               [9, 3, 0, 0]])\\n>>> from scipy import ndimage\\n>>> ndimage.standard_deviation(a)\\n2.7585095613392387\\n\\nFeatures to process can be specified using `labels` and `index`:\\n\\n>>> lbl, nlbl = ndimage.label(a)\\n>>> ndimage.standard_deviation(a, lbl, index=np.arange(1, nlbl+1))\\narray([ 1.479,  1.5  ,  3.   ])\\n\\nIf no index is given, non-zero `labels` are processed:\\n\\n>>> ndimage.standard_deviation(a, lbl)\\n2.4874685927665499',\n",
       " 'Tests that HDBSCAN works with feature array, including an arbitrary\\ngoodness of fit check. Note that the check is a simple heuristic.',\n",
       " \"Fit's using kernel K\",\n",
       " 'Fit the model according to the given training data.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\\n    Training vector, where `n_samples` is the number of samples and\\n    `n_features` is the number of features.\\n\\ny : array-like of shape (n_samples,)\\n    Target vector relative to X.\\n\\nsample_weight : array-like of shape (n_samples,) default=None\\n    Array of weights that are assigned to individual samples.\\n    If not provided, then each sample is given unit weight.\\n\\n    .. versionadded:: 0.17\\n       *sample_weight* support to LogisticRegression.\\n\\nReturns\\n-------\\nself\\n    Fitted estimator.\\n\\nNotes\\n-----\\nThe SAGA solver supports both float64 and float32 bit arrays.',\n",
       " 'Test infrequent categories with a pandas dataframe with multiple dtypes.',\n",
       " 'Read an unsigned 32-bit integer',\n",
       " 'Returns a :class:`UDFRegistration` for UDF registration.\\n\\n.. versionadded:: 2.0.0\\n\\n.. versionchanged:: 3.4.0\\n    Supports Spark Connect.\\n\\nReturns\\n-------\\n:class:`UDFRegistration`\\n\\nExamples\\n--------\\nRegister a Python UDF, and use it in SQL.\\n\\n>>> strlen = spark.udf.register(\"strlen\", lambda x: len(x))\\n>>> spark.sql(\"SELECT strlen(\\'test\\')\").show()\\n+------------+\\n|strlen(test)|\\n+------------+\\n|           4|\\n+------------+',\n",
       " \"Reads the contents of a Matrix Market file-like 'source' into a matrix.\\n\\nParameters\\n----------\\nsource : str or file-like\\n    Matrix Market filename (extensions .mtx, .mtz.gz)\\n    or open file-like object.\\n\\nReturns\\n-------\\na : ndarray or coo_matrix\\n    Dense or sparse matrix depending on the matrix format in the\\n    Matrix Market file.\\n\\nExamples\\n--------\\n>>> from io import StringIO\\n>>> from scipy.io import mmread\\n\\n>>> text = '''%%MatrixMarket matrix coordinate real general\\n...  5 5 7\\n...  2 3 1.0\\n...  3 4 2.0\\n...  3 5 3.0\\n...  4 1 4.0\\n...  4 2 5.0\\n...  4 3 6.0\\n...  4 4 7.0\\n... '''\\n\\n``mmread(source)`` returns the data as sparse matrix in COO format.\\n\\n>>> m = mmread(StringIO(text))\\n>>> m\\n<5x5 sparse matrix of type '<class 'numpy.float64'>'\\nwith 7 stored elements in COOrdinate format>\\n>>> m.A\\narray([[0., 0., 0., 0., 0.],\\n       [0., 0., 1., 0., 0.],\\n       [0., 0., 0., 2., 3.],\\n       [4., 5., 6., 7., 0.],\\n       [0., 0., 0., 0., 0.]])\",\n",
       " \"Convert various order codings to NumPy format.\\n\\nParameters\\n----------\\ncode : str\\n    The code to convert. It is converted to lower case before parsing.\\n    Legal values are:\\n    'little', 'big', 'l', 'b', 'le', 'be', '<', '>', 'native', '=',\\n    'swapped', 's'.\\n\\nReturns\\n-------\\nout_code : {'<', '>'}\\n    Here '<' is the numpy dtype code for little endian,\\n    and '>' is the code for big endian.\\n\\nExamples\\n--------\\n>>> import sys\\n>>> from scipy.io.matlab._byteordercodes import to_numpy_code\\n>>> sys_is_le = (sys.byteorder == 'little')\\n>>> sys_is_le\\nTrue\\n>>> to_numpy_code('big')\\n'>'\\n>>> to_numpy_code('little')\\n'<'\\n>>> nc = to_numpy_code('native')\\n>>> nc == '<' if sys_is_le else nc == '>'\\nTrue\\n>>> sc = to_numpy_code('swapped')\\n>>> sc == '>' if sys_is_le else sc == '<'\\nTrue\",\n",
       " 'Compute K such that eigenvalues (A - dot(B, K))=poles.\\n\\nK is the gain matrix such as the plant described by the linear system\\n``AX+BU`` will have its closed-loop poles, i.e the eigenvalues ``A - B*K``,\\nas close as possible to those asked for in poles.\\n\\nSISO, MISO and MIMO systems are supported.\\n\\nParameters\\n----------\\nA, B : ndarray\\n    State-space representation of linear system ``AX + BU``.\\npoles : array_like\\n    Desired real poles and/or complex conjugates poles.\\n    Complex poles are only supported with ``method=\"YT\"`` (default).\\nmethod: {\\'YT\\', \\'KNV0\\'}, optional\\n    Which method to choose to find the gain matrix K. One of:\\n\\n        - \\'YT\\': Yang Tits\\n        - \\'KNV0\\': Kautsky, Nichols, Van Dooren update method 0\\n\\n    See References and Notes for details on the algorithms.\\nrtol: float, optional\\n    After each iteration the determinant of the eigenvectors of\\n    ``A - B*K`` is compared to its previous value, when the relative\\n    error between these two values becomes lower than `rtol` the algorithm\\n    stops.  Default is 1e-3.\\nmaxiter: int, optional\\n    Maximum number of iterations to compute the gain matrix.\\n    Default is 30.\\n\\nReturns\\n-------\\nfull_state_feedback : Bunch object\\n    full_state_feedback is composed of:\\n        gain_matrix : 1-D ndarray\\n            The closed loop matrix K such as the eigenvalues of ``A-BK``\\n            are as close as possible to the requested poles.\\n        computed_poles : 1-D ndarray\\n            The poles corresponding to ``A-BK`` sorted as first the real\\n            poles in increasing order, then the complex conjugates in\\n            lexicographic order.\\n        requested_poles : 1-D ndarray\\n            The poles the algorithm was asked to place sorted as above,\\n            they may differ from what was achieved.\\n        X : 2-D ndarray\\n            The transfer matrix such as ``X * diag(poles) = (A - B*K)*X``\\n            (see Notes)\\n        rtol : float\\n            The relative tolerance achieved on ``det(X)`` (see Notes).\\n            `rtol` will be NaN if it is possible to solve the system\\n            ``diag(poles) = (A - B*K)``, or 0 when the optimization\\n            algorithms can\\'t do anything i.e when ``B.shape[1] == 1``.\\n        nb_iter : int\\n            The number of iterations performed before converging.\\n            `nb_iter` will be NaN if it is possible to solve the system\\n            ``diag(poles) = (A - B*K)``, or 0 when the optimization\\n            algorithms can\\'t do anything i.e when ``B.shape[1] == 1``.\\n\\nNotes\\n-----\\nThe Tits and Yang (YT), [2]_ paper is an update of the original Kautsky et\\nal. (KNV) paper [1]_.  KNV relies on rank-1 updates to find the transfer\\nmatrix X such that ``X * diag(poles) = (A - B*K)*X``, whereas YT uses\\nrank-2 updates. This yields on average more robust solutions (see [2]_\\npp 21-22), furthermore the YT algorithm supports complex poles whereas KNV\\ndoes not in its original version.  Only update method 0 proposed by KNV has\\nbeen implemented here, hence the name ``\\'KNV0\\'``.\\n\\nKNV extended to complex poles is used in Matlab\\'s ``place`` function, YT is\\ndistributed under a non-free licence by Slicot under the name ``robpole``.\\nIt is unclear and undocumented how KNV0 has been extended to complex poles\\n(Tits and Yang claim on page 14 of their paper that their method can not be\\nused to extend KNV to complex poles), therefore only YT supports them in\\nthis implementation.\\n\\nAs the solution to the problem of pole placement is not unique for MIMO\\nsystems, both methods start with a tentative transfer matrix which is\\naltered in various way to increase its determinant.  Both methods have been\\nproven to converge to a stable solution, however depending on the way the\\ninitial transfer matrix is chosen they will converge to different\\nsolutions and therefore there is absolutely no guarantee that using\\n``\\'KNV0\\'`` will yield results similar to Matlab\\'s or any other\\nimplementation of these algorithms.\\n\\nUsing the default method ``\\'YT\\'`` should be fine in most cases; ``\\'KNV0\\'``\\nis only provided because it is needed by ``\\'YT\\'`` in some specific cases.\\nFurthermore ``\\'YT\\'`` gives on average more robust results than ``\\'KNV0\\'``\\nwhen ``abs(det(X))`` is used as a robustness indicator.\\n\\n[2]_ is available as a technical report on the following URL:\\nhttps://hdl.handle.net/1903/5598\\n\\nReferences\\n----------\\n.. [1] J. Kautsky, N.K. Nichols and P. van Dooren, \"Robust pole assignment\\n       in linear state feedback\", International Journal of Control, Vol. 41\\n       pp. 1129-1155, 1985.\\n.. [2] A.L. Tits and Y. Yang, \"Globally convergent algorithms for robust\\n       pole assignment by state feedback\", IEEE Transactions on Automatic\\n       Control, Vol. 41, pp. 1432-1452, 1996.\\n\\nExamples\\n--------\\nA simple example demonstrating real pole placement using both KNV and YT\\nalgorithms.  This is example number 1 from section 4 of the reference KNV\\npublication ([1]_):\\n\\n>>> import numpy as np\\n>>> from scipy import signal\\n>>> import matplotlib.pyplot as plt\\n\\n>>> A = np.array([[ 1.380,  -0.2077,  6.715, -5.676  ],\\n...               [-0.5814, -4.290,   0,      0.6750 ],\\n...               [ 1.067,   4.273,  -6.654,  5.893  ],\\n...               [ 0.0480,  4.273,   1.343, -2.104  ]])\\n>>> B = np.array([[ 0,      5.679 ],\\n...               [ 1.136,  1.136 ],\\n...               [ 0,      0,    ],\\n...               [-3.146,  0     ]])\\n>>> P = np.array([-0.2, -0.5, -5.0566, -8.6659])\\n\\nNow compute K with KNV method 0, with the default YT method and with the YT\\nmethod while forcing 100 iterations of the algorithm and print some results\\nafter each call.\\n\\n>>> fsf1 = signal.place_poles(A, B, P, method=\\'KNV0\\')\\n>>> fsf1.gain_matrix\\narray([[ 0.20071427, -0.96665799,  0.24066128, -0.10279785],\\n       [ 0.50587268,  0.57779091,  0.51795763, -0.41991442]])\\n\\n>>> fsf2 = signal.place_poles(A, B, P)  # uses YT method\\n>>> fsf2.computed_poles\\narray([-8.6659, -5.0566, -0.5   , -0.2   ])\\n\\n>>> fsf3 = signal.place_poles(A, B, P, rtol=-1, maxiter=100)\\n>>> fsf3.X\\narray([[ 0.52072442+0.j, -0.08409372+0.j, -0.56847937+0.j,  0.74823657+0.j],\\n       [-0.04977751+0.j, -0.80872954+0.j,  0.13566234+0.j, -0.29322906+0.j],\\n       [-0.82266932+0.j, -0.19168026+0.j, -0.56348322+0.j, -0.43815060+0.j],\\n       [ 0.22267347+0.j,  0.54967577+0.j, -0.58387806+0.j, -0.40271926+0.j]])\\n\\nThe absolute value of the determinant of X is a good indicator to check the\\nrobustness of the results, both ``\\'KNV0\\'`` and ``\\'YT\\'`` aim at maximizing\\nit.  Below a comparison of the robustness of the results above:\\n\\n>>> abs(np.linalg.det(fsf1.X)) < abs(np.linalg.det(fsf2.X))\\nTrue\\n>>> abs(np.linalg.det(fsf2.X)) < abs(np.linalg.det(fsf3.X))\\nTrue\\n\\nNow a simple example for complex poles:\\n\\n>>> A = np.array([[ 0,  7/3.,  0,   0   ],\\n...               [ 0,   0,    0,  7/9. ],\\n...               [ 0,   0,    0,   0   ],\\n...               [ 0,   0,    0,   0   ]])\\n>>> B = np.array([[ 0,  0 ],\\n...               [ 0,  0 ],\\n...               [ 1,  0 ],\\n...               [ 0,  1 ]])\\n>>> P = np.array([-3, -1, -2-1j, -2+1j]) / 3.\\n>>> fsf = signal.place_poles(A, B, P, method=\\'YT\\')\\n\\nWe can plot the desired and computed poles in the complex plane:\\n\\n>>> t = np.linspace(0, 2*np.pi, 401)\\n>>> plt.plot(np.cos(t), np.sin(t), \\'k--\\')  # unit circle\\n>>> plt.plot(fsf.requested_poles.real, fsf.requested_poles.imag,\\n...          \\'wo\\', label=\\'Desired\\')\\n>>> plt.plot(fsf.computed_poles.real, fsf.computed_poles.imag, \\'bx\\',\\n...          label=\\'Placed\\')\\n>>> plt.grid()\\n>>> plt.axis(\\'image\\')\\n>>> plt.axis([-1.1, 1.1, -1.1, 1.1])\\n>>> plt.legend(bbox_to_anchor=(1.05, 1), loc=2, numpoints=1)',\n",
       " 'Check lobpcg for diagonal matrices for all matrix types.\\nConstraints are imposed, so a dense eigensolver eig cannot run.',\n",
       " 'Test attribute with value that equals specified value (case insensitive attribute).',\n",
       " 'Return a new :class:`.CursorResult` that \"horizontally splices\"\\ntogether the rows of this :class:`.CursorResult` with that of another\\n:class:`.CursorResult`.\\n\\n.. tip::  This method is for the benefit of the SQLAlchemy ORM and is\\n   not intended for general use.\\n\\n\"horizontally splices\" means that for each row in the first and second\\nresult sets, a new row that concatenates the two rows together is\\nproduced, which then becomes the new row.  The incoming\\n:class:`.CursorResult` must have the identical number of rows.  It is\\ntypically expected that the two result sets come from the same sort\\norder as well, as the result rows are spliced together based on their\\nposition in the result.\\n\\nThe expected use case here is so that multiple INSERT..RETURNING\\nstatements (which definitely need to be sorted) against different\\ntables can produce a single result that looks like a JOIN of those two\\ntables.\\n\\nE.g.::\\n\\n    r1 = connection.execute(\\n        users.insert().returning(\\n            users.c.user_name,\\n            users.c.user_id,\\n            sort_by_parameter_order=True\\n        ),\\n        user_values\\n    )\\n\\n    r2 = connection.execute(\\n        addresses.insert().returning(\\n            addresses.c.address_id,\\n            addresses.c.address,\\n            addresses.c.user_id,\\n            sort_by_parameter_order=True\\n        ),\\n        address_values\\n    )\\n\\n    rows = r1.splice_horizontally(r2).all()\\n    assert (\\n        rows ==\\n        [\\n            (\"john\", 1, 1, \"foo@bar.com\", 1),\\n            (\"jack\", 2, 2, \"bar@bat.com\", 2),\\n        ]\\n    )\\n\\n.. versionadded:: 2.0\\n\\n.. seealso::\\n\\n    :meth:`.CursorResult.splice_vertically`',\n",
       " 'Intercept when the :meth:`_engine.Connection.execution_options`\\nmethod is called.\\n\\nThis method is called after the new :class:`_engine.Connection`\\nhas been\\nproduced, with the newly updated execution options collection, but\\nbefore the :class:`.Dialect` has acted upon any of those new options.\\n\\nNote that this method is not called when a new\\n:class:`_engine.Connection`\\nis produced which is inheriting execution options from its parent\\n:class:`_engine.Engine`; to intercept this condition, use the\\n:meth:`_events.ConnectionEvents.engine_connect` event.\\n\\n:param conn: The newly copied :class:`_engine.Connection` object\\n\\n:param opts: dictionary of options that were passed to the\\n :meth:`_engine.Connection.execution_options` method.\\n This dictionary may be modified in place to affect the ultimate\\n options which take effect.\\n\\n .. versionadded:: 2.0 the ``opts`` dictionary may be modified\\n    in place.\\n\\n\\n.. seealso::\\n\\n    :meth:`_events.ConnectionEvents.set_engine_execution_options`\\n    - event\\n    which is called when :meth:`_engine.Engine.execution_options`\\n    is called.',\n",
       " 'Convert plain list to instance of this class.',\n",
       " 'test for #9373 - load base to receive level 3 endpoints',\n",
       " 'Target database must also enforce check constraints.',\n",
       " \"Target must support UPDATE (or DELETE) where the same table is\\npresent in a subquery in the WHERE clause.\\n\\nThis is an ANSI-standard syntax that apparently MySQL can't handle,\\nsuch as::\\n\\n    UPDATE documents SET flag=1 WHERE documents.title IN\\n        (SELECT max(documents.title) AS title\\n            FROM documents GROUP BY documents.user_id\\n        )\",\n",
       " 'Return the cycle structure of the permutation as a dictionary\\nindicating the multiplicity of each cycle length.\\n\\nExamples\\n========\\n\\n>>> from sympy.combinatorics import Permutation\\n>>> Permutation(3).cycle_structure\\n{1: 4}\\n>>> Permutation(0, 4, 3)(1, 2)(5, 6).cycle_structure\\n{2: 2, 3: 1}',\n",
       " \"Exact analytical expression for the pathway's length.\",\n",
       " 'This function provides one of the three parameters\\nwhen two of them are supplied.\\nThis is valid only for paraxial rays.\\n\\nParameters\\n==========\\n\\nfocal_length : sympifiable\\n    Focal length of the mirror.\\nu : sympifiable\\n    Distance of object from the optical center on\\n    the principal axis.\\nv : sympifiable\\n    Distance of the image from the optical center\\n    on the principal axis.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.optics import lens_formula\\n>>> from sympy.abc import f, u, v\\n>>> lens_formula(focal_length=f, u=u)\\nf*u/(f + u)\\n>>> lens_formula(focal_length=f, v=v)\\nf*v/(f - v)\\n>>> lens_formula(u=u, v=v)\\nu*v/(u - v)',\n",
       " 'Return the units of the system that do not have a prefix.',\n",
       " \"add to self the product of (p)*(coeff*x0**i0*x1**i1*...)\\nunless self is a generator -- then just return the sum of the two.\\n\\nmc is a tuple, (monom, coeff), where monomial is (i0, i1, ...)\\n\\nExamples\\n========\\n\\n>>> from sympy.polys.rings import ring\\n>>> from sympy.polys.domains import ZZ\\n\\n>>> _, x, y, z = ring('x, y, z', ZZ)\\n>>> p1 = x**4 + 2*y\\n>>> p2 = y + z\\n>>> m = (1, 2, 3)\\n>>> p1 = p1._iadd_poly_monom(p2, (m, 3))\\n>>> p1\\nx**4 + 3*x*y**3*z**3 + 3*x*y**2*z**4 + 2*y\",\n",
       " \"Returns the order of a given differential\\nequation with respect to func.\\n\\nThis function is implemented recursively.\\n\\nExamples\\n========\\n\\n>>> from sympy import Function\\n>>> from sympy.solvers.deutils import ode_order\\n>>> from sympy.abc import x\\n>>> f, g = map(Function, ['f', 'g'])\\n>>> ode_order(f(x).diff(x, 2) + f(x).diff(x)**2 +\\n... f(x).diff(x), f(x))\\n2\\n>>> ode_order(f(x).diff(x, 2) + g(x).diff(x, 3), f(x))\\n2\\n>>> ode_order(f(x).diff(x, 2) + g(x).diff(x, 3), g(x))\\n3\",\n",
       " 'Returns the root SymPy directory and set the global value\\nindicating whether the system is case sensitive or not.',\n",
       " 'Write the bytes necessary to finish a multipart/form-data body.',\n",
       " 'Called in async handlers if the client closed the connection.\\n\\nOverride this to clean up resources associated with\\nlong-lived connections.  Note that this method is called only if\\nthe connection was closed during asynchronous processing; if you\\nneed to do cleanup after every request override `on_finish`\\ninstead.\\n\\nProxies may keep a connection open for a time (perhaps\\nindefinitely) after the client has gone away, so this method\\nmay not be called promptly after the end user closes their\\nconnection.',\n",
       " 'Test extending a query with a subclassed str.',\n",
       " 'Declare the only interfaces implemented by instances of a class.\\n\\nThe arguments after the class are one or more interfaces or\\ninterface specifications (`IDeclaration` objects).\\n\\nThe interfaces given (including the interfaces in the\\nspecifications) replace any previous declarations.\\n\\nConsider the following example::\\n\\n  class C(A, B):\\n     ...\\n\\n  classImplements(C, IA, IB. IC)\\n  classImplementsOnly(C. I1, I2)\\n\\nInstances of ``C`` provide only ``I1``, ``I2``, and regardless of\\nwhatever interfaces instances of ``A`` and ``B`` implement.\\n\\n.. seealso:: `zope.interface.classImplementsOnly`',\n",
       " 'Insert NaNs into a point array at locations specified by an offset array.\\n    ',\n",
       " 'Simple initialization of a `FileReporter`.\\n\\nThe `filename` argument is the path to the file being reported.  This\\nwill be available as the `.filename` attribute on the object.  Other\\nmethod implementations on this base class rely on this attribute.',\n",
       " 'Set a context for subsequent querying.\\n\\nThe next :meth:`lines`, :meth:`arcs`, or :meth:`contexts_by_lineno`\\ncalls will be limited to only one context.  `context` is a string which\\nmust match a context exactly.  If it does not, no exception is raised,\\nbut queries will return no data.\\n\\n.. versionadded:: 5.0',\n",
       " 'Unpause all processes within the container.\\n\\nRaises:\\n    :py:class:`docker.errors.APIError`\\n        If the server returns an error.',\n",
       " 'Demonstrates ANSI color support.',\n",
       " 'Creates a measurement protocol secret.\\n\\nArgs:\\n    request (Union[google.analytics.admin_v1alpha.types.CreateMeasurementProtocolSecretRequest, dict]):\\n        The request object. Request message for\\n        CreateMeasurementProtocolSecret RPC\\n    parent (str):\\n        Required. The parent resource where\\n        this secret will be created. Format:\\n        properties/{property}/dataStreams/{dataStream}\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    measurement_protocol_secret (google.analytics.admin_v1alpha.types.MeasurementProtocolSecret):\\n        Required. The measurement protocol\\n        secret to create.\\n\\n        This corresponds to the ``measurement_protocol_secret`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.analytics.admin_v1alpha.types.MeasurementProtocolSecret:\\n        A secret value used for sending hits\\n        to Measurement Protocol.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.apihub_v1.types.ListSpecsRequest):\\n        The initial request object.\\n    response (google.cloud.apihub_v1.types.ListSpecsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the delete budget method over HTTP.\\n\\nArgs:\\n    request (~.budget_service.DeleteBudgetRequest):\\n        The request object. Request for DeleteBudget\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the get attestor method over HTTP.\\n\\nArgs:\\n    request (~.service.GetAttestorRequest):\\n        The request object. Request message for\\n    [BinauthzManagementService.GetAttestor][].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.resources.Attestor:\\n        An\\n    [attestor][google.cloud.binaryauthorization.v1.Attestor]\\n    that attests to container image artifacts. An existing\\n    attestor cannot be modified except where indicated.',\n",
       " 'Creates a persistent disk in the specified project\\nusing the data in the request. You can create a disk\\nfrom a source (sourceImage, sourceSnapshot, or\\nsourceDisk) or create an empty 500 GB data disk by\\nomitting all properties. You can also create a disk that\\nis larger than the default size by specifying the sizeGb\\nproperty.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_insert():\\n        # Create a client\\n        client = compute_v1.DisksClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.InsertDiskRequest(\\n            project=\"project_value\",\\n            zone=\"zone_value\",\\n        )\\n\\n        # Make the request\\n        response = client.insert(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.InsertDiskRequest, dict]):\\n        The request object. A request message for Disks.Insert.\\n        See the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        The name of the zone for this\\n        request.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    disk_resource (google.cloud.compute_v1.types.Disk):\\n        The body resource for this request\\n        This corresponds to the ``disk_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Retrieves an aggregated list of all of the instances in your\\nproject across all regions and zones. The performance of this\\nmethod degrades when a filter is specified on a project that has\\na very large number of instances. To prevent failure, Google\\nrecommends that you set the ``returnPartialSuccess`` parameter\\nto ``true``.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_aggregated_list():\\n        # Create a client\\n        client = compute_v1.InstancesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.AggregatedListInstancesRequest(\\n            project=\"project_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.aggregated_list(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.AggregatedListInstancesRequest, dict]):\\n        The request object. A request message for\\n        Instances.AggregatedList. See the method\\n        description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.compute_v1.services.instances.pagers.AggregatedListPager:\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Call the set shielded instance\\nintegrity policy method over HTTP.\\n\\n    Args:\\n        request (~.compute.SetShieldedInstanceIntegrityPolicyInstanceRequest):\\n            The request object. A request message for\\n        Instances.SetShieldedInstanceIntegrityPolicy.\\n        See the method description for details.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.compute.Operation:\\n            Represents an Operation resource. Google Compute Engine\\n        has three Operation resources: \\\\*\\n        `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n        \\\\*\\n        `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n        \\\\*\\n        `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n        You can use an operation resource to manage asynchronous\\n        API requests. For more information, read Handling API\\n        responses. Operations can be global, regional or zonal.\\n        - For global operations, use the ``globalOperations``\\n        resource. - For regional operations, use the\\n        ``regionOperations`` resource. - For zonal operations,\\n        use the ``zoneOperations`` resource. For more\\n        information, read Global, Regional, and Zonal Resources.\\n        Note that completed Operation resources have a limited\\n        retention period.',\n",
       " 'Call the insert method over HTTP.\\n\\nArgs:\\n    request (~.compute.InsertNetworkAttachmentRequest):\\n        The request object. A request message for\\n    NetworkAttachments.Insert. See the\\n    method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Call the list method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListReservationsRequest):\\n        The request object. A request message for\\n    Reservations.List. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.ReservationList:',\n",
       " 'Changes SslCertificates for TargetSslProxy.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import compute_v1\\n\\n    def sample_set_ssl_certificates():\\n        # Create a client\\n        client = compute_v1.TargetSslProxiesClient()\\n\\n        # Initialize request argument(s)\\n        request = compute_v1.SetSslCertificatesTargetSslProxyRequest(\\n            project=\"project_value\",\\n            target_ssl_proxy=\"target_ssl_proxy_value\",\\n        )\\n\\n        # Make the request\\n        response = client.set_ssl_certificates(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.compute_v1.types.SetSslCertificatesTargetSslProxyRequest, dict]):\\n        The request object. A request message for\\n        TargetSslProxies.SetSslCertificates. See\\n        the method description for details.\\n    project (str):\\n        Project ID for this request.\\n        This corresponds to the ``project`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    target_ssl_proxy (str):\\n        Name of the TargetSslProxy resource\\n        whose SslCertificate resource is to be\\n        set.\\n\\n        This corresponds to the ``target_ssl_proxy`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    target_ssl_proxies_set_ssl_certificates_request_resource (google.cloud.compute_v1.types.TargetSslProxiesSetSslCertificatesRequest):\\n        The body resource for this request\\n        This corresponds to the ``target_ssl_proxies_set_ssl_certificates_request_resource`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.extended_operation.ExtendedOperation:\\n        An object representing a extended\\n        long-running operation.',\n",
       " 'Creates a tag template.\\n\\nYou must enable the Data Catalog API in the project identified\\nby the ``parent`` parameter. For more information, see [Data\\nCatalog resource project]\\n(https://cloud.google.com/data-catalog/docs/concepts/resource-project).\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datacatalog_v1\\n\\n    def sample_create_tag_template():\\n        # Create a client\\n        client = datacatalog_v1.DataCatalogClient()\\n\\n        # Initialize request argument(s)\\n        request = datacatalog_v1.CreateTagTemplateRequest(\\n            parent=\"parent_value\",\\n            tag_template_id=\"tag_template_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_tag_template(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datacatalog_v1.types.CreateTagTemplateRequest, dict]):\\n        The request object. Request message for\\n        [CreateTagTemplate][google.cloud.datacatalog.v1.DataCatalog.CreateTagTemplate].\\n    parent (str):\\n        Required. The name of the project and the template\\n        location\\n        `region <https://cloud.google.com/data-catalog/docs/concepts/regions>`__.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    tag_template_id (str):\\n        Required. The ID of the tag template to create.\\n\\n        The ID must contain only lowercase letters (a-z),\\n        numbers (0-9), or underscores (_), and must start with a\\n        letter or underscore. The maximum size is 64 bytes when\\n        encoded in UTF-8.\\n\\n        This corresponds to the ``tag_template_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    tag_template (google.cloud.datacatalog_v1.types.TagTemplate):\\n        Required. The tag template to create.\\n        This corresponds to the ``tag_template`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.datacatalog_v1.types.TagTemplate:\\n        A tag template defines a tag that can have one or more\\n        typed fields.\\n\\n           The template is used to create tags that are attached to Google Cloud\\n              resources. [Tag template roles]\\n\\n           (https://cloud.google.com/iam/docs/understanding-roles#data-catalog-roles)\\n           provide permissions to create, edit, and use the\\n           template. For example, see the [TagTemplate User]\\n           (https://cloud.google.com/data-catalog/docs/how-to/template-user)\\n           role that includes a permission to use the tag\\n           template to tag resources.',\n",
       " 'Return a callable for the create tag template field method over gRPC.\\n\\nCreates a field in a tag template. The user should enable the\\nData Catalog API in the project identified by the ``parent``\\nparameter (see `Data Catalog Resource\\nProject <https://cloud.google.com/data-catalog/docs/concepts/resource-project>`__\\nfor more information).\\n\\nReturns:\\n    Callable[[~.CreateTagTemplateFieldRequest],\\n            ~.TagTemplateField]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for get_issue\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ContactCenterInsights server but before\\nit is returned to user code.',\n",
       " 'Starts master IP rotation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import container_v1beta1\\n\\n    def sample_start_ip_rotation():\\n        # Create a client\\n        client = container_v1beta1.ClusterManagerClient()\\n\\n        # Initialize request argument(s)\\n        request = container_v1beta1.StartIPRotationRequest(\\n            project_id=\"project_id_value\",\\n            zone=\"zone_value\",\\n            cluster_id=\"cluster_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.start_ip_rotation(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.container_v1beta1.types.StartIPRotationRequest, dict]):\\n        The request object. StartIPRotationRequest creates a new\\n        IP for the cluster and then performs a\\n        node upgrade on each node pool to point\\n        to the new IP.\\n    project_id (str):\\n        Required. Deprecated. The Google Developers Console\\n        `project ID or project\\n        number <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`__.\\n        This field has been deprecated and replaced by the name\\n        field.\\n\\n        This corresponds to the ``project_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    zone (str):\\n        Required. Deprecated. The name of the Google Compute\\n        Engine\\n        `zone <https://cloud.google.com/compute/docs/zones#available>`__\\n        in which the cluster resides. This field has been\\n        deprecated and replaced by the name field.\\n\\n        This corresponds to the ``zone`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    cluster_id (str):\\n        Required. Deprecated. The name of the\\n        cluster. This field has been deprecated\\n        and replaced by the name field.\\n\\n        This corresponds to the ``cluster_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.container_v1beta1.types.Operation:\\n        This operation resource represents\\n        operations that may have happened or are\\n        happening on the cluster. All fields are\\n        output only.',\n",
       " 'Creates a question.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dataqna_v1alpha\\n\\n    def sample_create_question():\\n        # Create a client\\n        client = dataqna_v1alpha.QuestionServiceClient()\\n\\n        # Initialize request argument(s)\\n        question = dataqna_v1alpha.Question()\\n        question.scopes = [\\'scopes_value1\\', \\'scopes_value2\\']\\n        question.query = \"query_value\"\\n\\n        request = dataqna_v1alpha.CreateQuestionRequest(\\n            parent=\"parent_value\",\\n            question=question,\\n        )\\n\\n        # Make the request\\n        response = client.create_question(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dataqna_v1alpha.types.CreateQuestionRequest, dict]):\\n        The request object. Request to create a question\\n        resource.\\n    parent (str):\\n        Required. The name of the project this data source\\n        reference belongs to. Example:\\n        ``projects/foo/locations/bar``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    question (google.cloud.dataqna_v1alpha.types.Question):\\n        Required. The question to create.\\n        This corresponds to the ``question`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dataqna_v1alpha.types.Question:\\n        The question resource represents a\\n        natural language query, its settings,\\n        understanding generated by the system,\\n        and answer retrieval status. A question\\n        cannot be modified.',\n",
       " 'Lists annotation spec sets for a project. Pagination\\nis supported.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import datalabeling_v1beta1\\n\\n    def sample_list_annotation_spec_sets():\\n        # Create a client\\n        client = datalabeling_v1beta1.DataLabelingServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = datalabeling_v1beta1.ListAnnotationSpecSetsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_annotation_spec_sets(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.datalabeling_v1beta1.types.ListAnnotationSpecSetsRequest, dict]):\\n        The request object. Request message for\\n        ListAnnotationSpecSets.\\n    parent (str):\\n        Required. Parent of AnnotationSpecSet resource, format:\\n        projects/{project_id}\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    filter (str):\\n        Optional. Filter is not supported at\\n        this moment.\\n\\n        This corresponds to the ``filter`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.datalabeling_v1beta1.services.data_labeling_service.pagers.ListAnnotationSpecSetsPager:\\n        Results of listing annotation spec\\n        set under a project.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Call the cancel operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.CancelOperationRequest):\\n        The request object for CancelOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " \"Call the get tool method over HTTP.\\n\\nArgs:\\n    request (~.tool.GetToolRequest):\\n        The request object. The request message for\\n    [Tools.GetTool][google.cloud.dialogflow.cx.v3beta1.Tools.GetTool].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.tool.Tool:\\n        A tool provides a list of actions which are available to\\n    the\\n    [Playbook][google.cloud.dialogflow.cx.v3beta1.Playbook]\\n    to attain its goal. A Tool consists of a description of\\n    the tool's usage and a specification of the tool which\\n    contains the schema and authentication information.\",\n",
       " 'Restores the specified agent from a ZIP file.\\n\\nReplaces the current agent version with a new one. All the\\nintents and entity types in the older version are deleted. After\\nthe restore, the restored draft agent will be trained\\nautomatically (unless disabled in agent settings). However, once\\nthe restore is done, training may not be completed yet. Please\\ncall\\n[TrainAgent][google.cloud.dialogflow.v2beta1.Agents.TrainAgent]\\nand wait for the operation it returns in order to train\\nexplicitly.\\n\\nThis method is a `long-running\\noperation <https://cloud.google.com/dialogflow/es/docs/how/long-running-operations>`__.\\nThe returned ``Operation`` type has the following\\nmethod-specific fields:\\n\\n-  ``metadata``: An empty `Struct\\n   message <https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#struct>`__\\n-  ``response``: An `Empty\\n   message <https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#empty>`__\\n\\nThe operation only tracks when restoring is complete, not when\\nit is done training.\\n\\nNote: You should always train an agent prior to sending it\\nqueries. See the `training\\ndocumentation <https://cloud.google.com/dialogflow/es/docs/training>`__.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflow_v2beta1\\n\\n    def sample_restore_agent():\\n        # Create a client\\n        client = dialogflow_v2beta1.AgentsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflow_v2beta1.RestoreAgentRequest(\\n            agent_uri=\"agent_uri_value\",\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.restore_agent(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflow_v2beta1.types.RestoreAgentRequest, dict]):\\n        The request object. The request message for\\n        [Agents.RestoreAgent][google.cloud.dialogflow.v2beta1.Agents.RestoreAgent].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Return a callable for the update generator method over gRPC.\\n\\nUpdates a generator.\\n\\nReturns:\\n    Callable[[~.UpdateGeneratorRequest],\\n            ~.Generator]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Return a callable for the create migration job method over gRPC.\\n\\nCreates a new migration job in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateMigrationJobRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the list sources method over gRPC.\\n\\nLists all the sources in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.ListSourcesRequest],\\n            ~.ListSourcesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns a fully-qualified group string.',\n",
       " 'Call the get patch deployment method over HTTP.\\n\\nArgs:\\n    request (~.patch_deployments.GetPatchDeploymentRequest):\\n        The request object. A request message for retrieving a\\n    patch deployment.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.patch_deployments.PatchDeployment:\\n        Patch deployments are configurations that individual\\n    patch jobs use to complete a patch. These configurations\\n    include instance filter, package repository settings,\\n    and a schedule. For more information about creating and\\n    managing patch deployments, see `Scheduling patch\\n    jobs <https://cloud.google.com/compute/docs/os-patch-management/schedule-patch-jobs>`__.',\n",
       " 'Pre-rpc interceptor for delete_os_policy_assignment\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the OsConfigZonalService server.',\n",
       " 'Post-rpc interceptor for update_folder\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the Folders server but before\\nit is returned to user code.',\n",
       " 'Creates a new TagKey. If another request with the\\nsame parameters is sent while the original request is in\\nprocess, the second request will receive an error. A\\nmaximum of 1000 TagKeys can exist under a parent at any\\ngiven time.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import resourcemanager_v3\\n\\n    def sample_create_tag_key():\\n        # Create a client\\n        client = resourcemanager_v3.TagKeysClient()\\n\\n        # Initialize request argument(s)\\n        tag_key = resourcemanager_v3.TagKey()\\n        tag_key.short_name = \"short_name_value\"\\n\\n        request = resourcemanager_v3.CreateTagKeyRequest(\\n            tag_key=tag_key,\\n        )\\n\\n        # Make the request\\n        operation = client.create_tag_key(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.resourcemanager_v3.types.CreateTagKeyRequest, dict]):\\n        The request object. The request message for creating a\\n        TagKey.\\n    tag_key (google.cloud.resourcemanager_v3.types.TagKey):\\n        Required. The TagKey to be created. Only fields\\n        ``short_name``, ``description``, and ``parent`` are\\n        considered during the creation request.\\n\\n        This corresponds to the ``tag_key`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.resourcemanager_v3.types.TagKey` A\\n        TagKey, used to group a set of TagValues.',\n",
       " 'Call the get default branch method over HTTP.\\n\\nArgs:\\n    request (~.catalog_service.GetDefaultBranchRequest):\\n        The request object. Request message to show which branch\\n    is currently the default branch.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.catalog_service.GetDefaultBranchResponse:\\n        Response message of\\n    [CatalogService.GetDefaultBranch][google.cloud.retail.v2.CatalogService.GetDefaultBranch].',\n",
       " 'Recursive version of :meth:`sign`.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Call the get service config method over HTTP.\\n\\nArgs:\\n    request (~.servicemanager.GetServiceConfigRequest):\\n        The request object. Request message for GetServiceConfig\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.service_pb2.Service:\\n        ``Service`` is the root object of Google API service\\n    configuration (service config). It describes the basic\\n    information about a logical service, such as the service\\n    name and the user-facing title, and delegates other\\n    aspects to sub-sections. Each sub-section is either a\\n    proto message or a repeated proto message that\\n    configures a specific aspect, such as auth. For more\\n    information, see each proto message definition.\\n\\n    Example:\\n\\n    ::\\n\\n        type: google.api.Service\\n        name: calendar.googleapis.com\\n        title: Google Calendar API\\n        apis:\\n        - name: google.calendar.v3.Calendar\\n\\n        visibility:\\n          rules:\\n          - selector: \"google.calendar.v3.*\"\\n            restriction: PREVIEW\\n        backend:\\n          rules:\\n          - selector: \"google.calendar.v3.*\"\\n            address: calendar.example.com\\n\\n        authentication:\\n          providers:\\n          - id: google_calendar_auth\\n            jwks_uri: https://www.googleapis.com/oauth2/v1/certs\\n            issuer: https://securetoken.google.com\\n          rules:\\n          - selector: \"*\"\\n            requirements:\\n              provider_id: google_calendar_auth',\n",
       " 'Pre-rpc interceptor for report\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the ServiceController server.',\n",
       " 'Disable a service so that it can no longer be used with a\\nproject. This prevents unintended usage that may cause\\nunexpected billing charges or security leaks.\\n\\nIt is not valid to call the disable method on a service that is\\nnot currently enabled. Callers will receive a\\n``FAILED_PRECONDITION`` status if the target service is not\\ncurrently enabled.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import service_usage_v1\\n\\n    def sample_disable_service():\\n        # Create a client\\n        client = service_usage_v1.ServiceUsageClient()\\n\\n        # Initialize request argument(s)\\n        request = service_usage_v1.DisableServiceRequest(\\n        )\\n\\n        # Make the request\\n        operation = client.disable_service(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.service_usage_v1.types.DisableServiceRequest, dict]):\\n        The request object. Request message for the ``DisableService`` method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.cloud.service_usage_v1.types.DisableServiceResponse` Response message for the DisableService method.\\n           This response message is assigned to the response\\n           field of the returned Operation when that operation\\n           is done.',\n",
       " 'Returns a random graph with the given degree sequence.\\n\\nThe configuration model generates a random pseudograph (graph with\\nparallel edges and self loops) by randomly assigning edges to\\nmatch the given degree sequence.\\n\\nParameters\\n----------\\ndeg_sequence :  list of nonnegative integers\\n    Each list entry corresponds to the degree of a node.\\ncreate_using : NetworkX graph constructor, optional (default MultiGraph)\\n    Graph type to create. If graph instance, then cleared before populated.\\nseed : integer, random_state, or None (default)\\n    Indicator of random number generation state.\\n    See :ref:`Randomness<randomness>`.\\n\\nReturns\\n-------\\nG : MultiGraph\\n    A graph with the specified degree sequence.\\n    Nodes are labeled starting at 0 with an index\\n    corresponding to the position in deg_sequence.\\n\\nRaises\\n------\\nNetworkXError\\n    If the degree sequence does not have an even sum.\\n\\nSee Also\\n--------\\nis_graphical\\n\\nNotes\\n-----\\nAs described by Newman [1]_.\\n\\nA non-graphical degree sequence (not realizable by some simple\\ngraph) is allowed since this function returns graphs with self\\nloops and parallel edges.  An exception is raised if the degree\\nsequence does not have an even sum.\\n\\nThis configuration model construction process can lead to\\nduplicate edges and loops.  You can remove the self-loops and\\nparallel edges (see below) which will likely result in a graph\\nthat doesn\\'t have the exact degree sequence specified.\\n\\nThe density of self-loops and parallel edges tends to decrease as\\nthe number of nodes increases. However, typically the number of\\nself-loops will approach a Poisson distribution with a nonzero mean,\\nand similarly for the number of parallel edges.  Consider a node\\nwith *k* stubs. The probability of being joined to another stub of\\nthe same node is basically (*k* - *1*) / *N*, where *k* is the\\ndegree and *N* is the number of nodes. So the probability of a\\nself-loop scales like *c* / *N* for some constant *c*. As *N* grows,\\nthis means we expect *c* self-loops. Similarly for parallel edges.\\n\\nReferences\\n----------\\n.. [1] M.E.J. Newman, \"The structure and function of complex networks\",\\n   SIAM REVIEW 45-2, pp 167-256, 2003.\\n\\nExamples\\n--------\\nYou can create a degree sequence following a particular distribution\\nby using the one of the distribution functions in\\n:mod:`~networkx.utils.random_sequence` (or one of your own). For\\nexample, to create an undirected multigraph on one hundred nodes\\nwith degree sequence chosen from the power law distribution:\\n\\n>>> sequence = nx.random_powerlaw_tree_sequence(100, tries=5000)\\n>>> G = nx.configuration_model(sequence)\\n>>> len(G)\\n100\\n>>> actual_degrees = [d for v, d in G.degree()]\\n>>> actual_degrees == sequence\\nTrue\\n\\nThe returned graph is a multigraph, which may have parallel\\nedges. To remove any parallel edges from the returned graph:\\n\\n>>> G = nx.Graph(G)\\n\\nSimilarly, to remove self-loops:\\n\\n>>> G.remove_edges_from(nx.selfloop_edges(G))',\n",
       " 'Call the delete adaptive mt\\ndataset method over HTTP.\\n\\n    Args:\\n        request (~.adaptive_mt.DeleteAdaptiveMtDatasetRequest):\\n            The request object. Request message for deleting an\\n        AdaptiveMtDataset.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.',\n",
       " 'Return a callable for the show vcenter credentials method over gRPC.\\n\\nGets details of credentials for Vcenter appliance.\\n\\nReturns:\\n    Callable[[~.ShowVcenterCredentialsRequest],\\n            ~.Credentials]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Determine if the client will be provided with an forward agent session.\\nIf this method returns ``True``, the server will allow SSH Agent\\nforwarding.\\n\\nThe default implementation always returns ``False``.\\n\\n:param .Channel channel: the `.Channel` the request arrived on\\n:return: ``True`` if the AgentForward was loaded; ``False`` if not\\n\\nIf ``True`` is returned, the server should create an\\n:class:`AgentServerProxy` to access the agent.',\n",
       " 'Setup for writing the movie file.\\n\\nParameters\\n----------\\nfig : `~matplotlib.figure.Figure`\\n    The figure to grab the rendered frames from.\\noutfile : str\\n    The filename of the resulting movie file.\\ndpi : float, default: ``fig.dpi``\\n    The dpi of the output file. This, with the figure size,\\n    controls the size in pixels of the resulting movie file.\\nframe_prefix : str, optional\\n    The filename prefix to use for temporary files.  If *None* (the\\n    default), files are written to a temporary directory which is\\n    deleted by `finish`; if not *None*, no temporary files are\\n    deleted.',\n",
       " 'The polynomial coefficients of the BÃ©zier curve.\\n\\n.. warning:: Follows opposite convention from `numpy.polyval`.\\n\\nReturns\\n-------\\n(n+1, d) array\\n    Coefficients after expanding in polynomial basis, where :math:`n`\\n    is the degree of the BÃ©zier curve and :math:`d` its dimension.\\n    These are the numbers (:math:`C_j`) such that the curve can be\\n    written :math:`\\\\sum_{j=0}^n C_j t^j`.\\n\\nNotes\\n-----\\nThe coefficients are calculated as\\n\\n.. math::\\n\\n    {n \\\\choose j} \\\\sum_{i=0}^j (-1)^{i+j} {j \\\\choose i} P_i\\n\\nwhere :math:`P_i` are the control points of the curve.',\n",
       " 'Set the `.Figure` for the `.OffsetBox` and all its children.\\n\\nParameters\\n----------\\nfig : `~matplotlib.figure.Figure`',\n",
       " 'Check that the system supports enough semaphores to run the test.',\n",
       " 'The curve length.\\n\\nExamples\\n========\\n\\n>>> from sympy import Curve\\n>>> from sympy.abc import t\\n>>> Curve((t, t), (t, 0, 1)).length\\nsqrt(2)',\n",
       " 'Test that invalid value are formatted with empty string without\\nraising exception.',\n",
       " 'Convert paths from a collection object to 3D segments with path codes.',\n",
       " 'A reason explaining why this connection was closed.\\n\\nThe reason must be one of the strings from the\\n:class:`ConnectionClosedReason` enum.',\n",
       " 'Get a database by name.\\n\\nRaises :class:`~pymongo.errors.InvalidName` if an invalid\\ndatabase name is used.\\n\\n:param name: the name of the database to get',\n",
       " 'List of hosts, passives, and arbiters known to this server.',\n",
       " 'Test an undirected disconnected graph.',\n",
       " 'Conversion from graph to array to graph.',\n",
       " \"Return (x1 != x2) element-wise.\\n\\nUnlike `numpy.not_equal`, this comparison is performed by first\\nstripping whitespace characters from the end of the string.  This\\nbehavior is provided for backward-compatibility with numarray.\\n\\nParameters\\n----------\\nx1, x2 : array_like of str or unicode\\n    Input arrays of the same shape.\\n\\nReturns\\n-------\\nout : ndarray\\n    Output array of bools.\\n\\nSee Also\\n--------\\nequal, greater_equal, less_equal, greater, less\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> x1 = np.array(['a', 'b', 'c'])\\n>>> np.char.not_equal(x1, 'b')\\narray([ True, False,  True])\",\n",
       " 'Render a DataFrame as an HTML table.\\n%(shared_params)s\\nbold_rows : bool, default True\\n    Make the row labels bold in the output.\\nclasses : str or list or tuple, default None\\n    CSS class(es) to apply to the resulting html table.\\nescape : bool, default True\\n    Convert the characters <, >, and & to HTML-safe sequences.\\nnotebook : {True, False}, default False\\n    Whether the generated HTML is for IPython Notebook.\\nborder : int\\n    A ``border=border`` attribute is included in the opening\\n    `<table>` tag. Default ``pd.options.display.html.border``.\\ntable_id : str, optional\\n    A css id is included in the opening `<table>` tag if specified.\\nrender_links : bool, default False\\n    Convert URLs to HTML links.\\nencoding : str, default \"utf-8\"\\n    Set character encoding.\\n%(returns)s\\nSee Also\\n--------\\nto_string : Convert DataFrame to a string.\\n\\nExamples\\n--------\\n>>> df = pd.DataFrame(data={\"col1\": [1, 2], \"col2\": [4, 3]})\\n>>> html_string = \\'\\'\\'<table border=\"1\" class=\"dataframe\">\\n...   <thead>\\n...     <tr style=\"text-align: right;\">\\n...       <th></th>\\n...       <th>col1</th>\\n...       <th>col2</th>\\n...     </tr>\\n...   </thead>\\n...   <tbody>\\n...     <tr>\\n...       <th>0</th>\\n...       <td>1</td>\\n...       <td>4</td>\\n...     </tr>\\n...     <tr>\\n...       <th>1</th>\\n...       <td>2</td>\\n...       <td>3</td>\\n...     </tr>\\n...   </tbody>\\n... </table>\\'\\'\\'\\n>>> assert html_string == df.to_html()',\n",
       " 'Check if other range is contained in self',\n",
       " 'Return a list of the row axis labels.',\n",
       " 'Cast values to specified type\\n\\nParameters\\n----------\\nvalues : ndarray or ExtensionArray\\ncast_type : np.dtype or ExtensionDtype\\n   dtype to cast values to\\ncolumn : string\\n    column name - used only for error reporting\\n\\nReturns\\n-------\\nconverted : ndarray or ExtensionArray',\n",
       " 'return a boolean indicating whether the file is open',\n",
       " 'Length-3 array with a known sort order.\\n\\nThis should be three items [B, NA, A] with\\nA < B and NA missing.',\n",
       " 'Test output if index is not RangeIndex',\n",
       " 'Returns True if upsampling is possible between source and target\\nfrequencies\\n\\nParameters\\n----------\\nsource : str or DateOffset\\n    Frequency converting from\\ntarget : str or DateOffset\\n    Frequency converting to\\n\\nReturns\\n-------\\nbool',\n",
       " 'Check that two objects are not approximately equal.\\n\\nThis check is performed commutatively.\\n\\nParameters\\n----------\\na : object\\n    The first object to compare.\\nb : object\\n    The second object to compare.\\n**kwargs\\n    The arguments passed to `tm.assert_almost_equal`.',\n",
       " 'This is the {method} method.\\n\\nIt computes the cumulative {operation}.',\n",
       " 'This returns the terminal echo mode. This returns True if echo is\\non or False if echo is off. Child applications that are expecting you\\nto enter a password often set ECHO False. See waitnoecho().\\n\\nNot supported on platforms where ``isatty()`` returns False.  ',\n",
       " 'Like test_expect_echo(), but using expect_exact().\\n        ',\n",
       " \"Don't offer the ``hashes`` kwarg.\",\n",
       " 'Returns the Requests tuple auth for a given url from netrc.',\n",
       " 'Set a new spinner.\\n\\nArgs:\\n    spinner_name (str): Spinner name, see python -m rich.spinner.\\n    spinner_style (Optional[StyleType], optional): Spinner style. Defaults to \"progress.spinner\".\\n    speed (float, optional): Speed factor of spinner. Defaults to 1.0.',\n",
       " 'Read and discard any remaining HTTP response data in the response connection.\\n\\nUnread data in the HTTPResponse connection blocks the connection from being released back to the pool.',\n",
       " 'Match a tree template to a tree.\\n\\nA tree template without variables will only match ``tree`` if it is equal to the template.\\n\\nParameters:\\n    tree (Tree): The tree to match to the template\\n\\nReturns:\\n    Optional[Dict[str, Tree]]: If match is found, returns a dictionary mapping\\n        template variable names to their matching tree nodes.\\n        If no match was found, returns None.',\n",
       " 'delete property name on this path. ',\n",
       " 'parameter_list  : parameter_declaration\\n| parameter_list COMMA parameter_declaration',\n",
       " \"Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a\\nmapping that links the input keys to the definition references.\\n\\nArgs:\\n    inputs: A sequence of tuples, where:\\n\\n        - The first element is a JSON schema key type.\\n        - The second element is the JSON mode: either 'validation' or 'serialization'.\\n        - The third element is a core schema.\\n\\nReturns:\\n    A tuple where:\\n\\n        - The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and\\n            whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have\\n            JsonRef references to definitions that are defined in the second returned element.)\\n        - The second element is a dictionary whose keys are definition references for the JSON schemas\\n            from the first returned element, and whose values are the actual JSON schema definitions.\\n\\nRaises:\\n    PydanticUserError: Raised if the JSON schema generator has already been used to generate a JSON schema.\",\n",
       " 'Get the JSON format for the encoded data.\\n\\nReturns:\\n    The JSON format for the encoded data.',\n",
       " 'Convert response, content -> (multipart) email.message.\\n\\nHelper for _unpack_batch_response.',\n",
       " 'Using a global name in a decorator statement results in no warnings,\\nbut using an undefined name in a decorator statement results in an\\nundefined name warning.',\n",
       " ':calls: `POST /repos/{owner}/{repo}/actions/variables/{variable_name} <https://docs.github.com/en/rest/actions/variables#create-a-repository-variable>`_',\n",
       " 'Save a possible indentation level.',\n",
       " 'Calculates the least distance between the exteriors of two\\nconvex polygons e1 and e2. Does not check for the convexity\\nof the polygons as this is checked by Polygon.distance.\\n\\nNotes\\n=====\\n\\n    - Prints a warning if the two polygons possibly intersect as the return\\n      value will not be valid in such a case. For a more through test of\\n      intersection use intersection().\\n\\nSee Also\\n========\\n\\nsympy.geometry.point.Point.distance\\n\\nExamples\\n========\\n\\n>>> from sympy import Point, Polygon\\n>>> square = Polygon(Point(0, 0), Point(0, 1), Point(1, 1), Point(1, 0))\\n>>> triangle = Polygon(Point(1, 2), Point(2, 2), Point(2, 1))\\n>>> square._do_poly_distance(triangle)\\nsqrt(2)/2\\n\\nDescription of method used\\n==========================\\n\\nMethod:\\n[1] https://web.archive.org/web/20150509035744/http://cgm.cs.mcgill.ca/~orm/mind2p.html\\nUses rotating calipers:\\n[2] https://en.wikipedia.org/wiki/Rotating_calipers\\nand antipodal points:\\n[3] https://en.wikipedia.org/wiki/Antipodal_point',\n",
       " '`Connection.get_cipher_bits` returns `None` if no connection has\\nbeen established.',\n",
       " 'Send SOCKS5 request with given command (CMD field) and\\naddress (DST field). Returns resolved DST address that was used.',\n",
       " 'Perform an in-process test run.\\n\\n:param args:\\n    List of command line arguments. If `None` or not given, defaults to reading\\n    arguments directly from the process command line (:data:`sys.argv`).\\n:param plugins: List of plugin objects to be auto-registered during initialization.\\n\\n:returns: An exit code.',\n",
       " \"Retrieve the soft delete policy for this bucket.\\n\\nSee https://cloud.google.com/storage/docs/soft-delete\\n\\n:rtype: :class:`SoftDeletePolicy`\\n:returns: an instance for managing the bucket's soft delete policy.\",\n",
       " 'Information contained in google.rpc.status.details.\\n\\nReference:\\n    https://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto\\n    https://github.com/googleapis/googleapis/blob/master/google/rpc/error_details.proto\\n\\nReturns:\\n    Sequence[Any]: A list of structured objects from error_details.proto',\n",
       " 'Override Connection:  defer actual HTTP request.\\n\\nOnly allow up to ``_MAX_BATCH_SIZE`` requests to be deferred.\\n\\n:type method: str\\n:param method: The HTTP method to use in the request.\\n\\n:type url: str\\n:param url: The URL to send the request to.\\n\\n:type headers: dict\\n:param headers: A dictionary of HTTP headers to send with the request.\\n\\n:type data: str\\n:param data: The data to send as the body of the request.\\n\\n:type target_object: object\\n:param target_object:\\n    (Optional) This allows us to enable custom behavior in our batch\\n    connection. Here we defer an HTTP request and complete\\n    initialization of the object at a later time.\\n\\n:type timeout: float or tuple\\n:param timeout:\\n    (Optional) The amount of time, in seconds, to wait\\n    for the server response.  See: :ref:`configuring_timeouts`\\n\\n:rtype: tuple of ``response`` (a dictionary of sorts)\\n        and ``content`` (a string).\\n:returns: The HTTP response object and the content of the response.',\n",
       " 'Gets the volumes_attached of this V1NodeStatus.  # noqa: E501\\n\\nList of volumes that are attached to the node.  # noqa: E501\\n\\n:return: The volumes_attached of this V1NodeStatus.  # noqa: E501\\n:rtype: list[V1AttachedVolume]',\n",
       " 'Gets the architecture of this V1NodeSystemInfo.  # noqa: E501\\n\\nThe Architecture reported by the node  # noqa: E501\\n\\n:return: The architecture of this V1NodeSystemInfo.  # noqa: E501\\n:rtype: str',\n",
       " 'Sets the namespaces of this V1PodAffinityTerm.\\n\\nnamespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \"this pod\\'s namespace\".  # noqa: E501\\n\\n:param namespaces: The namespaces of this V1PodAffinityTerm.  # noqa: E501\\n:type: list[str]',\n",
       " \"Context.term won't close sockets\",\n",
       " 'Calculates the jaro distance\\n\\nParameters\\n----------\\ns1 : Sequence[Hashable]\\n    First string to compare.\\ns2 : Sequence[Hashable]\\n    Second string to compare.\\nprocessor: callable, optional\\n    Optional callable that is used to preprocess the strings before\\n    comparing them. Default is None, which deactivates this behaviour.\\nscore_cutoff : float, optional\\n    Optional argument for a score threshold as a float between 0 and 1.0.\\n    For ratio < score_cutoff 0 is returned instead. Default is None,\\n    which deactivates this behaviour.\\n\\nReturns\\n-------\\ndistance : float\\n    distance between s1 and s2 as a float between 1.0 and 0.0',\n",
       " 'Get the last samples matching the specific `filter`.\\n\\nFor more information see https://redis.io/commands/ts.mget/\\n\\nArgs:\\n    filters:\\n        Filter to match the time-series labels.\\n    with_labels:\\n        Include in the reply all label-value pairs representing metadata labels\\n        of the time series.\\n    select_labels:\\n        Include in the reply only a subset of the key-value pair labels o the\\n        time series.\\n    latest:\\n        Used when a time series is a compaction, reports the compacted value of\\n        the latest possibly partial bucket.',\n",
       " 'Track progress by iterating over a sequence.\\n\\nArgs:\\n    sequence (Sequence[ProgressType]): A sequence of values you want to iterate over and track progress.\\n    total: (float, optional): Total number of steps. Default is len(sequence).\\n    completed (int, optional): Number of steps completed so far. Defaults to 0.\\n    task_id: (TaskID): Task to track. Default is new task.\\n    description: (str, optional): Description of task, if new task is created.\\n    update_period (float, optional): Minimum time (in seconds) between calls to update(). Defaults to 0.1.\\n\\nReturns:\\n    Iterable[ProgressType]: An iterable of values taken from the provided sequence.',\n",
       " 'Use a lineage query to retrieve all downstream trials that use this artifact.\\n\\nReturns:\\n    [Trial]: A list of SageMaker `Trial` objects.',\n",
       " 'Compute the mean Silhouette Coefficient of all samples.\\n\\nThe Silhouette Coefficient is calculated using the mean intra-cluster\\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\\nb)``.  To clarify, ``b`` is the distance between a sample and the nearest\\ncluster that the sample is not a part of.\\nNote that Silhouette Coefficient is only defined if number of labels\\nis ``2 <= n_labels <= n_samples - 1``.\\n\\nThis function returns the mean Silhouette Coefficient over all samples.\\nTo obtain the values for each sample, use :func:`silhouette_samples`.\\n\\nThe best value is 1 and the worst value is -1. Values near 0 indicate\\noverlapping clusters. Negative values generally indicate that a sample has\\nbeen assigned to the wrong cluster, as a different cluster is more similar.\\n\\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\\n\\nParameters\\n----------\\nX : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\\n    An array of pairwise distances between samples, or a feature array.\\n\\nlabels : array-like of shape (n_samples,)\\n    Predicted labels for each sample.\\n\\nmetric : str or callable, default=\\'euclidean\\'\\n    The metric to use when calculating distance between instances in a\\n    feature array. If metric is a string, it must be one of the options\\n    allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is\\n    the distance array itself, use ``metric=\"precomputed\"``.\\n\\nsample_size : int, default=None\\n    The size of the sample to use when computing the Silhouette Coefficient\\n    on a random subset of the data.\\n    If ``sample_size is None``, no sampling is used.\\n\\nrandom_state : int, RandomState instance or None, default=None\\n    Determines random number generation for selecting a subset of samples.\\n    Used when ``sample_size is not None``.\\n    Pass an int for reproducible results across multiple function calls.\\n    See :term:`Glossary <random_state>`.\\n\\n**kwds : optional keyword parameters\\n    Any further parameters are passed directly to the distance function.\\n    If using a scipy.spatial.distance metric, the parameters are still\\n    metric dependent. See the scipy docs for usage examples.\\n\\nReturns\\n-------\\nsilhouette : float\\n    Mean Silhouette Coefficient for all samples.\\n\\nReferences\\n----------\\n\\n.. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\\n   Interpretation and Validation of Cluster Analysis\". Computational\\n   and Applied Mathematics 20: 53-65.\\n   <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\\n\\n.. [2] `Wikipedia entry on the Silhouette Coefficient\\n       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\\n\\nExamples\\n--------\\n>>> from sklearn.datasets import make_blobs\\n>>> from sklearn.cluster import KMeans\\n>>> from sklearn.metrics import silhouette_score\\n>>> X, y = make_blobs(random_state=42)\\n>>> kmeans = KMeans(n_clusters=2, random_state=42)\\n>>> silhouette_score(X, kmeans.fit_predict(X))\\nnp.float64(0.49...)',\n",
       " 'Check complex-value Hermitian cases.\\n    ',\n",
       " 'Open a connection to download the specified range of bytes. Store\\nthe open file handle in self._body.\\n\\nIf no range is specified, start defaults to self._position.\\nstart and stop follow the semantics of the http range header,\\nso a stop without a start will read bytes beginning at stop.\\n\\nAs a side effect, set self._content_length. Set self._position\\nto self._content_length if start is past end of file.',\n",
       " ':return: new window from a region',\n",
       " 'Test type and class with an escaped null character.',\n",
       " 'Load a model from the given path.',\n",
       " 'Check if the installed pandas version matches the expected version.',\n",
       " \"Creates a new instance of this class to wrap the provided UDTF with another one that\\nchecks the values of projected partitioning expressions on consecutive rows to figure\\nout when the partition boundaries change.\\n\\nParameters\\n----------\\ncreate_udtf: function\\n    Function to create a new instance of the UDTF to be invoked.\\npartition_child_indexes: list\\n    List of integers identifying zero-based indexes of the columns of the input table\\n    that contain projected partitioning expressions. This class will inspect these\\n    values for each pair of consecutive input rows. When they change, this indicates\\n    the boundary between two partitions, and we will invoke the 'terminate' method on\\n    the UDTF class instance and then destroy it and create a new one to implement the\\n    desired partitioning semantics.\",\n",
       " 'Calculate the rolling quantile of the values.\\n\\n.. versionadded:: 3.4.0\\n\\nParameters\\n----------\\nquantile : float\\n    Value between 0 and 1 providing the quantile to compute.\\n\\n    .. deprecated:: 4.0.0\\n        This will be renamed to â€˜qâ€™ in a future version.\\n\\naccuracy : int, optional\\n    Default accuracy of approximation. Larger value means better accuracy.\\n    The relative error can be deduced by 1.0 / accuracy.\\n    This is a panda-on-Spark specific parameter.\\n\\nReturns\\n-------\\nSeries or DataFrame\\n    Returned object type is determined by the caller of the rolling\\n    calculation.\\n\\nNotes\\n-----\\n`quantile` in pandas-on-Spark are using distributed percentile approximation\\nalgorithm unlike pandas, the result might be different with pandas, also `interpolation`\\nparameter is not supported yet.\\n\\nthe current implementation of this API uses Spark\\'s Window without\\nspecifying partition specification. This leads to move all data into\\nsingle partition in single machine and could cause serious\\nperformance degradation. Avoid this method against very large dataset.\\n\\nSee Also\\n--------\\npyspark.pandas.Series.rolling : Calling rolling with Series data.\\npyspark.pandas.DataFrame.rolling : Calling rolling with DataFrames.\\npyspark.pandas.Series.quantile : Aggregating quantile for Series.\\npyspark.pandas.DataFrame.quantile : Aggregating quantile for DataFrame.\\n\\nExamples\\n--------\\n>>> s = ps.Series([4, 3, 5, 2, 6])\\n>>> s\\n0    4\\n1    3\\n2    5\\n3    2\\n4    6\\ndtype: int64\\n\\n>>> s.rolling(2).quantile(0.5)\\n0    NaN\\n1    3.0\\n2    3.0\\n3    2.0\\n4    2.0\\ndtype: float64\\n\\n>>> s.rolling(3).quantile(0.5)\\n0    NaN\\n1    NaN\\n2    4.0\\n3    3.0\\n4    5.0\\ndtype: float64\\n\\nFor DataFrame, each rolling quantile is computed column-wise.\\n\\n>>> df = ps.DataFrame({\"A\": s.to_numpy(), \"B\": s.to_numpy() ** 2})\\n>>> df\\n   A   B\\n0  4  16\\n1  3   9\\n2  5  25\\n3  2   4\\n4  6  36\\n\\n>>> df.rolling(2).quantile(0.5)\\n     A    B\\n0  NaN  NaN\\n1  3.0  9.0\\n2  3.0  9.0\\n3  2.0  4.0\\n4  2.0  4.0\\n\\n>>> df.rolling(3).quantile(0.5)\\n     A     B\\n0  NaN   NaN\\n1  NaN   NaN\\n2  4.0  16.0\\n3  3.0   9.0\\n4  5.0  25.0',\n",
       " 'Parse type annotation.',\n",
       " 'Format an inventory entry as if it were part of this project.',\n",
       " 'Return an instance of the generic type corresponding to this type\\nusing heuristic rule. The method may be overridden if this\\nheuristic rule is not sufficient.\\n\\n>>> from sqlalchemy.dialects.mysql import INTEGER\\n>>> INTEGER(display_width=4).as_generic()\\nInteger()\\n\\n>>> from sqlalchemy.dialects.mysql import NVARCHAR\\n>>> NVARCHAR(length=100).as_generic()\\nUnicode(length=100)\\n\\n.. versionadded:: 1.4.0b2\\n\\n\\n.. seealso::\\n\\n    :ref:`metadata_reflection_dbagnostic_types` - describes the\\n    use of :meth:`_types.TypeEngine.as_generic` in conjunction with\\n    the :meth:`_sql.DDLEvents.column_reflect` event, which is its\\n    intended use.',\n",
       " 'Returns the I.L.D. reaction forces in a dictionary.',\n",
       " 'Return a p-Sylow subgroup of a symmetric or an\\nalternating group.\\n\\nExplanation\\n===========\\n\\nThe algorithm for this is hinted at in [1], Chapter 4,\\nExercise 4.\\n\\nFor Sym(n) with n = p^i, the idea is as follows. Partition\\nthe interval [0..n-1] into p equal parts, each of length p^(i-1):\\n[0..p^(i-1)-1], [p^(i-1)..2*p^(i-1)-1]...[(p-1)*p^(i-1)..p^i-1].\\nFind a p-Sylow subgroup of Sym(p^(i-1)) (treated as a subgroup\\nof ``self``) acting on each of the parts. Call the subgroups\\nP_1, P_2...P_p. The generators for the subgroups P_2...P_p\\ncan be obtained from those of P_1 by applying a \"shifting\"\\npermutation to them, that is, a permutation mapping [0..p^(i-1)-1]\\nto the second part (the other parts are obtained by using the shift\\nmultiple times). The union of this permutation and the generators\\nof P_1 is a p-Sylow subgroup of ``self``.\\n\\nFor n not equal to a power of p, partition\\n[0..n-1] in accordance with how n would be written in base p.\\nE.g. for p=2 and n=11, 11 = 2^3 + 2^2 + 1 so the partition\\nis [[0..7], [8..9], {10}]. To generate a p-Sylow subgroup,\\ntake the union of the generators for each of the parts.\\nFor the above example, {(0 1), (0 2)(1 3), (0 4), (1 5)(2 7)}\\nfrom the first part, {(8 9)} from the second part and\\nnothing from the third. This gives 4 generators in total, and\\nthe subgroup they generate is p-Sylow.\\n\\nAlternating groups are treated the same except when p=2. In this\\ncase, (0 1)(s s+1) should be added for an appropriate s (the start\\nof a part) for each part in the partitions.\\n\\nSee Also\\n========\\n\\nsylow_subgroup, is_alt_sym',\n",
       " 'Removes the non-live cosets from the coset table, described on\\npg. 167 [1].',\n",
       " \"Returns a plot for slope of deflection curve of the Beam object.\\n\\nParameters\\n==========\\nsubs : dictionary\\n    Python dictionary containing Symbols as key and their\\n    corresponding values.\\n\\nExamples\\n========\\nThere is a beam of length 8 meters. A constant distributed load of 10 KN/m\\nis applied from half of the beam till the end. There are two simple supports\\nbelow the beam, one at the starting point and another at the ending point\\nof the beam. A pointload of magnitude 5 KN is also applied from top of the\\nbeam, at a distance of 4 meters from the starting point.\\nTake E = 200 GPa and I = 400*(10**-6) meter**4.\\n\\nUsing the sign convention of downwards forces being positive.\\n\\n.. plot::\\n    :context: close-figs\\n    :format: doctest\\n    :include-source: True\\n\\n    >>> from sympy.physics.continuum_mechanics.beam import Beam\\n    >>> from sympy import symbols\\n    >>> R1, R2 = symbols('R1, R2')\\n    >>> b = Beam(8, 200*(10**9), 400*(10**-6))\\n    >>> b.apply_load(5000, 2, -1)\\n    >>> b.apply_load(R1, 0, -1)\\n    >>> b.apply_load(R2, 8, -1)\\n    >>> b.apply_load(10000, 4, 0, end=8)\\n    >>> b.bc_deflection = [(0, 0), (8, 0)]\\n    >>> b.solve_for_reaction_loads(R1, R2)\\n    >>> b.plot_slope()\\n    Plot object containing:\\n    [0]: cartesian line: -8.59375e-5*SingularityFunction(x, 0, 2) + 3.125e-5*SingularityFunction(x, 2, 2)\\n    + 2.08333333333333e-5*SingularityFunction(x, 4, 3) - 0.0001953125*SingularityFunction(x, 8, 2)\\n    - 2.08333333333333e-5*SingularityFunction(x, 8, 3) + 0.00138541666666667 for x over (0.0, 8.0)\",\n",
       " 'Test the representation of the X gate.',\n",
       " 'Returns a list of the maximum degree of each variable appearing\\nin the coefficients of the Dixon polynomial. The coefficients are\\nviewed as polys in $x_1, x_2, \\\\dots, x_n$.',\n",
       " 'Test the \"scalar\" argument to aesara_function(). ',\n",
       " 'Return any remaining buffered data not yet returned by decompress.\\n\\nAlso checks for errors such as truncated input.\\nNo other methods may be called on this object after `flush`.',\n",
       " 'Print usage and examples (see `emit_examples()`).',\n",
       " 'Add our own checks for too many deprecation warnings.\\n\\nLimit to once per package.',\n",
       " 'Executes a database operation.  Parameters may be provided as a\\nsequence, or as a mapping, depending upon the value of\\n:data:`paramstyle`.\\n\\nThis method is part of the `DBAPI 2.0 specification\\n<http://www.python.org/dev/peps/pep-0249/>`_.\\n\\n:param operation: str\\n    The SQL statement to execute.\\n\\n:param args: typing.Union[typing.Mapping, typing.Dict, list]\\n    If :data:`paramstyle` is ``qmark``, ``numeric``, or ``format``,\\n    this argument should be an array of parameters to bind into the\\n    statement.  If :data:`paramstyle` is ``named``, the argument should\\n    be a dict mapping of parameters.  If the :data:`paramstyle` is\\n    ``pyformat``, the argument value may be either an array or a\\n    mapping.\\n\\n:param stream: This is a extension for use with the Amazon Redshift\\n    `COPY\\n    <https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html>`_\\n    command. For a COPY FROM the parameter must be a readable file-like\\n    object, and for COPY TO it must be writable.\\n\\n    .. versionadded:: 1.9.11\\n\\nReturns\\n-------\\nThe Cursor object used for executing the specified database operation: :class:`Cursor`',\n",
       " 'Return a pyodbc connection from a Glue Catalog Connection.\\n\\nhttps://github.com/mkleehammer/pyodbc\\n\\nNote\\n----\\nYou MUST pass a `connection` OR `secret_id`.\\nHere is an example of the secret structure in Secrets Manager:\\n{\\n\"host\":\"sqlserver-instance-wrangler.dr8vkeyrb9m1.us-east-1.rds.amazonaws.com\",\\n\"username\":\"test\",\\n\"password\":\"test\",\\n\"engine\":\"sqlserver\",\\n\"port\":\"1433\",\\n\"dbname\": \"mydb\" # Optional\\n}\\n\\nParameters\\n----------\\nconnection\\n    Glue Catalog Connection name.\\nsecret_id\\n    Specifies the secret containing the connection details that you want to retrieve.\\n    You can specify either the Amazon Resource Name (ARN) or the friendly name of the secret.\\ncatalog_id\\n    The ID of the Data Catalog.\\n    If none is provided, the AWS account ID is used by default.\\ndbname\\n    Optional database name to overwrite the stored one.\\nodbc_driver_version\\n    Major version of the OBDC Driver version that is installed and should be used.\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** is ``None``.\\ntimeout\\n    This is the time in seconds before the connection to the server will time out.\\n    The default is None which means no timeout.\\n    This parameter is forwarded to pyodbc.\\n    https://github.com/mkleehammer/pyodbc/wiki/The-pyodbc-Module#connect\\n\\nReturns\\n-------\\n    pyodbc connection.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> with wr.sqlserver.connect(connection=\"MY_GLUE_CONNECTION\", odbc_driver_version=17) as con:\\n...     with con.cursor() as cursor:\\n...         cursor.execute(\"SELECT 1\")\\n...         print(cursor.fetchall())',\n",
       " 'Parse a I[.F] seconds value into (seconds, microseconds).',\n",
       " 'Closes all open handles to processes in this snapshot.',\n",
       " \"This method gets called when a match is found.\\n\\nThis allows subclasses of L{Pattern} to filter out unwanted results,\\nor modify the results before giving them to the caller of\\nL{Search.search_process}.\\n\\nIf the return value is C{None} the result is skipped.\\n\\nSubclasses of L{Pattern} don't need to reimplement this method unless\\nfiltering is needed.\\n\\n@type  address: int\\n@param address: The memory address where the pattern was found.\\n\\n@type  size: int\\n@param size: The size of the data that matches the pattern.\\n\\n@type  data: str\\n@param data: The data that matches the pattern.\\n\\n@rtype:  tuple( int, int, str )\\n@return: Tuple containing the following:\\n     * The memory address where the pattern was found.\\n     * The size of the data that matches the pattern.\\n     * The data that matches the pattern.\",\n",
       " \"Write a list of strings to a file.\\nIf a file of the same name exists, it's contents are replaced.\\n\\nSee L{HexInput.string_list_file} for a description of the file format.\\n\\n@type  filename: str\\n@param filename: Name of the file to write.\\n\\n@type  values: list( int )\\n@param values: List of strings to write to the file.\",\n",
       " 'Create a new copy of this SHA1 object from its raw string.',\n",
       " 'Write the diff for an object.\\n\\nArgs:\\n  f: File-like object to write to\\n  store: Store to retrieve objects from, if necessary\\n  old_file: (path, mode, hexsha) tuple\\n  new_file: (path, mode, hexsha) tuple\\n  diff_binary: Whether to diff files even if they\\n    are considered binary files by is_binary().\\n\\nNote: the tuple elements should be None for nonexistent files',\n",
       " \"Create a new commit.\\n\\nIf not specified, committer and author default to\\nget_user_identity(..., 'COMMITTER')\\nand get_user_identity(..., 'AUTHOR') respectively.\\n\\nArgs:\\n  message: Commit message\\n  committer: Committer fullname\\n  author: Author fullname\\n  commit_timestamp: Commit timestamp (defaults to now)\\n  commit_timezone: Commit timestamp timezone (defaults to GMT)\\n  author_timestamp: Author timestamp (defaults to commit\\n    timestamp)\\n  author_timezone: Author timestamp timezone\\n    (defaults to commit timestamp timezone)\\n  tree: SHA1 of the tree root to use (if not specified the\\n    current index will be committed).\\n  encoding: Encoding\\n  ref: Optional ref to commit to (defaults to current branch)\\n  merge_heads: Merge heads (defaults to .git/MERGE_HEAD)\\n  no_verify: Skip pre-commit and commit-msg hooks\\n  sign: GPG Sign the commit (bool, defaults to False,\\n    pass True to use default GPG key,\\n    pass a str containing Key ID to use a specific GPG key)\\n\\nReturns:\\n  New commit SHA1\",\n",
       " 'Deserialize data as JSON.\\n\\nIf :data:`~flask.current_app` is available, it will use its\\n:meth:`app.json.loads() <flask.json.provider.JSONProvider.loads>`\\nmethod, otherwise it will use :func:`json.loads`.\\n\\n:param s: Text or UTF-8 bytes.\\n:param kwargs: Arguments passed to the ``loads`` implementation.\\n\\n.. versionchanged:: 2.3\\n    The ``app`` parameter was removed.\\n\\n.. versionchanged:: 2.2\\n    Calls ``current_app.json.loads``, allowing an app to override\\n    the behavior.\\n\\n.. versionchanged:: 2.0\\n    ``encoding`` will be removed in Flask 2.1. The data must be a\\n    string or UTF-8 bytes.\\n\\n.. versionchanged:: 1.0.3\\n    ``app`` can be passed directly, rather than requiring an app\\n    context for configuration.',\n",
       " 'A temporary directory added to sys.path.',\n",
       " 'Handle the lock according to the write mode ',\n",
       " 'Parses a file path into its component segments.',\n",
       " 'Pre-rpc interceptor for repair_application\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Applications server.',\n",
       " 'Call the delete operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.DeleteOperationRequest):\\n        The request object for DeleteOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the get method over HTTP.\\n\\nArgs:\\n    request (~.compute.GetImageRequest):\\n        The request object. A request message for Images.Get. See\\n    the method description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Image:\\n        Represents an Image resource. You can\\n    use images to create boot disks for your\\n    VM instances. For more information, read\\n    Images.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.compute_v1.types.ListInstanceGroupManagerResizeRequestsRequest):\\n        The initial request object.\\n    response (google.cloud.compute_v1.types.InstanceGroupManagerResizeRequestsListResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Pre-rpc interceptor for enable_xpn_host\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the Projects server.',\n",
       " 'Call the list method over HTTP.\\n\\nArgs:\\n    request (~.compute.ListRegionSslPoliciesRequest):\\n        The request object. A request message for\\n    RegionSslPolicies.List. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.SslPoliciesList:',\n",
       " 'Instantiates the storage pool types client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,StoragePoolTypesTransport,Callable[..., StoragePoolTypesTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the StoragePoolTypesTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n        NOTE: \"rest\" transport functionality is currently in a\\n        beta state (preview). We welcome your feedback via an\\n        issue in this library\\'s source repository.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " \"Return a callable for the delete cluster method over gRPC.\\n\\nDeletes the cluster, including the Kubernetes\\nendpoint and all worker nodes.\\n\\nFirewalls and routes that were configured during cluster\\ncreation are also deleted.\\n\\nOther Google Compute Engine resources that might be in\\nuse by the cluster, such as load balancer resources, are\\nnot deleted if they weren't present when the cluster was\\ninitially created.\\n\\nReturns:\\n    Callable[[~.DeleteClusterRequest],\\n            Awaitable[~.Operation]]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'Post-rpc interceptor for list_merchant_center_account_links\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the MerchantCenterAccountLinkService server but before\\nit is returned to user code.',\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Updates the feedback received from the user for a\\nsingle turn of the bot response.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflowcx_v3\\n\\n    def sample_submit_answer_feedback():\\n        # Create a client\\n        client = dialogflowcx_v3.SessionsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflowcx_v3.SubmitAnswerFeedbackRequest(\\n            session=\"session_value\",\\n            response_id=\"response_id_value\",\\n        )\\n\\n        # Make the request\\n        response = client.submit_answer_feedback(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflowcx_v3.types.SubmitAnswerFeedbackRequest, dict]):\\n        The request object. The request to set the feedback for a\\n        bot answer.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.dialogflowcx_v3.types.AnswerFeedback:\\n        Stores information about feedback\\n        provided by users about a response.',\n",
       " 'Call the export agent method over HTTP.\\n\\nArgs:\\n    request (~.agent.ExportAgentRequest):\\n        The request object. The request message for\\n    [Agents.ExportAgent][google.cloud.dialogflow.cx.v3.Agents.ExportAgent].\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Call the delete connection method over HTTP.\\n\\nArgs:\\n    request (~.dlp.DeleteConnectionRequest):\\n        The request object. Request message for DeleteConnection.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the delete channel connection method over HTTP.\\n\\nArgs:\\n    request (~.eventarc.DeleteChannelConnectionRequest):\\n        The request object. The request message for the\\n    DeleteChannelConnection method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Return a callable for the create gateway method over gRPC.\\n\\nCreates a new Gateway in a given project and\\nlocation.\\n\\nReturns:\\n    Callable[[~.CreateGatewayRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the get user workloads secret method over gRPC.\\n\\nGets an existing user workloads Secret. Values of the \"data\"\\nfield in the response are cleared.\\n\\nThis method is supported for Cloud Composer environments in\\nversions composer-3.\\\\ *.*-airflow-*.*.\\\\* and newer.\\n\\nReturns:\\n    Callable[[~.GetUserWorkloadsSecretRequest],\\n            ~.UserWorkloadsSecret]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " \"Get (load) the data that this DicomSeries represents, and return\\nit as a numpy array. If this serie contains multiple images, the\\nresulting array is 3D, otherwise it's 2D.\",\n",
       " 'Post-rpc interceptor for create_quota_preference\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the CloudQuotas server but before\\nit is returned to user code.',\n",
       " 'Updates the metadata and configuration of a specific\\nRedis cluster.\\nCompleted longrunning.Operation will contain the new\\ncluster object in the response field. The returned\\noperation is automatically deleted after a few hours, so\\nthere is no need to call DeleteOperation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import redis_cluster_v1\\n\\n    def sample_update_cluster():\\n        # Create a client\\n        client = redis_cluster_v1.CloudRedisClusterClient()\\n\\n        # Initialize request argument(s)\\n        cluster = redis_cluster_v1.Cluster()\\n        cluster.name = \"name_value\"\\n        cluster.psc_configs.network = \"network_value\"\\n\\n        request = redis_cluster_v1.UpdateClusterRequest(\\n            cluster=cluster,\\n        )\\n\\n        # Make the request\\n        operation = client.update_cluster(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.redis_cluster_v1.types.UpdateClusterRequest, dict]):\\n        The request object. Request for [UpdateCluster][CloudRedis.UpdateCluster].\\n    cluster (google.cloud.redis_cluster_v1.types.Cluster):\\n        Required. Update description. Only fields specified in\\n        update_mask are updated.\\n\\n        This corresponds to the ``cluster`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    update_mask (google.protobuf.field_mask_pb2.FieldMask):\\n        Required. Mask of fields to update. At least one path\\n        must be supplied in this field. The elements of the\\n        repeated paths field may only include these fields from\\n        [Cluster][google.cloud.redis.cluster.v1.Cluster]:\\n\\n        -  ``size_gb``\\n        -  ``replica_count``\\n\\n        This corresponds to the ``update_mask`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be\\n        :class:`google.cloud.redis_cluster_v1.types.Cluster` A\\n        cluster instance.',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Creates a resident Event Threat Detection custom\\nmodule at the scope of the given Resource Manager\\nparent, and also creates inherited custom modules for\\nall descendants of the given parent. These modules are\\nenabled by default.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import securitycentermanagement_v1\\n\\n    def sample_create_event_threat_detection_custom_module():\\n        # Create a client\\n        client = securitycentermanagement_v1.SecurityCenterManagementClient()\\n\\n        # Initialize request argument(s)\\n        request = securitycentermanagement_v1.CreateEventThreatDetectionCustomModuleRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        response = client.create_event_threat_detection_custom_module(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.securitycentermanagement_v1.types.CreateEventThreatDetectionCustomModuleRequest, dict]):\\n        The request object. Message for creating a\\n        EventThreatDetectionCustomModule\\n    parent (str):\\n        Required. Name of parent for the module. Its format is\\n        ``organizations/{organization}/locations/{location}``,\\n        ``folders/{folder}/locations/{location}``, or\\n        ``projects/{project}/locations/{location}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    event_threat_detection_custom_module (google.cloud.securitycentermanagement_v1.types.EventThreatDetectionCustomModule):\\n        Required. The module to create. The\\n        event_threat_detection_custom_module.name will be\\n        ignored and server generated.\\n\\n        This corresponds to the ``event_threat_detection_custom_module`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.securitycentermanagement_v1.types.EventThreatDetectionCustomModule:\\n        An event threat detection custom\\n        module is a Cloud SCC resource that\\n        contains the configuration and\\n        enablement state of a custom module,\\n        which enables ETD to write certain\\n        findings to Cloud SCC.',\n",
       " 'Pre-rpc interceptor for list_cases\\n\\nOverride in a subclass to manipulate the request or metadata\\nbefore they are sent to the CaseService server.',\n",
       " 'Deprecated by get_repo_installation.\\n\\n:calls: `GET /repos/{owner}/{repo}/installation\\n<https://docs.github.com/en/rest/reference/apps#get-a-repository-installation-for-the-authenticated-app>`',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    WarehouseAsyncClient: The constructed client.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Call the get data source method over HTTP.\\n\\nArgs:\\n    request (~.datasources.GetDataSourceRequest):\\n        The request object. Request message for the GetDataSource\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.datasources.DataSource:\\n        The `data\\n    source <https://support.google.com/merchants/answer/7439058>`__\\n    for the Merchant Center account.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " \"Process the response from an HTTP request.\\n\\nThis is everything that must be done after a request that doesn't\\nrequire network I/O (or other I/O). This is based on the `sans-I/O`_\\nphilosophy.\\n\\nArgs:\\n    response (object): The HTTP response object.\\n\\nRaises:\\n    ~google.resumable_media.common.InvalidResponse: If the status\\n        code is not 200.\\n\\n.. _sans-I/O: https://sans-io.readthedocs.io/\",\n",
       " 'Workaround for `TypeError: MatcherAPIv2() takes no arguments`',\n",
       " 'Change to directory popped off the top of the stack.\\n        ',\n",
       " 'Factory to return a matplotlib-enabled runner for %run.\\n\\nParameters\\n----------\\nsafe_execfile : function\\n    This must be a function with the same interface as the\\n    :meth:`safe_execfile` method of IPython.\\n\\nReturns\\n-------\\nA function suitable for use as the ``runner`` argument of the %run magic\\nfunction.',\n",
       " 'Returns the Unicode value of the character at the specified location.\\n\\n@param - index The zero-based index of the desired character.\\nIf there is no character at the specified index, NaN is returned.\\n\\nThis was added for compatibility with python',\n",
       " \"Write ``data`` into this file at position ``offset``.  Extending the\\nfile past its original end is expected.  Unlike Python's normal\\n``write()`` methods, this method cannot do a partial write: it must\\nwrite all of ``data`` or else return an error.\\n\\nThe default implementation checks for an attribute on ``self`` named\\n``writefile``, and if present, performs the write operation on the\\nPython file-like object found there.  The attribute is named\\ndifferently from ``readfile`` to make it easy to implement read-only\\n(or write-only) files, but if both attributes are present, they should\\nrefer to the same file.\\n\\n:param offset: position in the file to start reading from.\\n:param bytes data: data to write into the file.\\n:return: an SFTP error code like ``SFTP_OK``.\",\n",
       " \"Parameters\\n----------\\nt_direction : {{'x', 'y'}}\\n    The axes on which the variable lies.\\n\\n    - 'x': the curves are ``(t, f1)`` and ``(t, f2)``.\\n    - 'y': the curves are ``(f1, t)`` and ``(f2, t)``.\\n\\nt : array (length N)\\n    The ``t_direction`` coordinates of the nodes defining the curves.\\n\\nf1 : array (length N) or scalar\\n    The other coordinates of the nodes defining the first curve.\\n\\nf2 : array (length N) or scalar\\n    The other coordinates of the nodes defining the second curve.\\n\\nwhere : array of bool (length N), optional\\n    Define *where* to exclude some {dir} regions from being filled.\\n    The filled regions are defined by the coordinates ``t[where]``.\\n    More precisely, fill between ``t[i]`` and ``t[i+1]`` if\\n    ``where[i] and where[i+1]``.  Note that this definition implies\\n    that an isolated *True* value between two *False* values in *where*\\n    will not result in filling.  Both sides of the *True* position\\n    remain unfilled due to the adjacent *False* values.\\n\\ninterpolate : bool, default: False\\n    This option is only relevant if *where* is used and the two curves\\n    are crossing each other.\\n\\n    Semantically, *where* is often used for *f1* > *f2* or\\n    similar.  By default, the nodes of the polygon defining the filled\\n    region will only be placed at the positions in the *t* array.\\n    Such a polygon cannot describe the above semantics close to the\\n    intersection.  The t-sections containing the intersection are\\n    simply clipped.\\n\\n    Setting *interpolate* to *True* will calculate the actual\\n    intersection point and extend the filled region up to this point.\\n\\nstep : {{'pre', 'post', 'mid'}}, optional\\n    Define *step* if the filling should be a step function,\\n    i.e. constant in between *t*.  The value determines where the\\n    step will occur:\\n\\n    - 'pre': The f value is continued constantly to the left from\\n      every *t* position, i.e. the interval ``(t[i-1], t[i]]`` has the\\n      value ``f[i]``.\\n    - 'post': The y value is continued constantly to the right from\\n      every *x* position, i.e. the interval ``[t[i], t[i+1])`` has the\\n      value ``f[i]``.\\n    - 'mid': Steps occur half-way between the *t* positions.\\n\\n**kwargs\\n    Forwarded to `.PolyCollection`.\\n\\nSee Also\\n--------\\n.Axes.fill_between, .Axes.fill_betweenx\",\n",
       " 'Set a color to fill the gaps in the dashed line style.\\n\\n.. note::\\n\\n    Striped lines are created by drawing two interleaved dashed lines.\\n    There can be overlaps between those two, which may result in\\n    artifacts when using transparency.\\n\\n    This functionality is experimental and may change.\\n\\nParameters\\n----------\\ngapcolor : :mpltype:`color` or list of :mpltype:`color` or None\\n    The color with which to fill the gaps. If None, the gaps are\\n    unfilled.',\n",
       " 'Build a layout of Axes based on ASCII art or nested lists.\\n\\nThis is a helper function to build complex GridSpec layouts visually.\\n\\nSee :ref:`mosaic`\\nfor an example and full API documentation\\n\\nParameters\\n----------\\nmosaic : list of list of {hashable or nested} or str\\n\\n    A visual layout of how you want your Axes to be arranged\\n    labeled as strings.  For example ::\\n\\n       x = [[\\'A panel\\', \\'A panel\\', \\'edge\\'],\\n            [\\'C panel\\', \\'.\\',       \\'edge\\']]\\n\\n    produces 4 Axes:\\n\\n    - \\'A panel\\' which is 1 row high and spans the first two columns\\n    - \\'edge\\' which is 2 rows high and is on the right edge\\n    - \\'C panel\\' which in 1 row and 1 column wide in the bottom left\\n    - a blank space 1 row and 1 column wide in the bottom center\\n\\n    Any of the entries in the layout can be a list of lists\\n    of the same form to create nested layouts.\\n\\n    If input is a str, then it can either be a multi-line string of\\n    the form ::\\n\\n      \\'\\'\\'\\n      AAE\\n      C.E\\n      \\'\\'\\'\\n\\n    where each character is a column and each line is a row. Or it\\n    can be a single-line string where rows are separated by ``;``::\\n\\n      \\'AB;CC\\'\\n\\n    The string notation allows only single character Axes labels and\\n    does not support nesting but is very terse.\\n\\n    The Axes identifiers may be `str` or a non-iterable hashable\\n    object (e.g. `tuple` s may not be used).\\n\\nsharex, sharey : bool, default: False\\n    If True, the x-axis (*sharex*) or y-axis (*sharey*) will be shared\\n    among all subplots.  In that case, tick label visibility and axis\\n    units behave as for `subplots`.  If False, each subplot\\'s x- or\\n    y-axis will be independent.\\n\\nwidth_ratios : array-like of length *ncols*, optional\\n    Defines the relative widths of the columns. Each column gets a\\n    relative width of ``width_ratios[i] / sum(width_ratios)``.\\n    If not given, all columns will have the same width.  Equivalent\\n    to ``gridspec_kw={\\'width_ratios\\': [...]}``. In the case of nested\\n    layouts, this argument applies only to the outer layout.\\n\\nheight_ratios : array-like of length *nrows*, optional\\n    Defines the relative heights of the rows. Each row gets a\\n    relative height of ``height_ratios[i] / sum(height_ratios)``.\\n    If not given, all rows will have the same height. Equivalent\\n    to ``gridspec_kw={\\'height_ratios\\': [...]}``. In the case of nested\\n    layouts, this argument applies only to the outer layout.\\n\\nsubplot_kw : dict, optional\\n    Dictionary with keywords passed to the `.Figure.add_subplot` call\\n    used to create each subplot.  These values may be overridden by\\n    values in *per_subplot_kw*.\\n\\nper_subplot_kw : dict, optional\\n    A dictionary mapping the Axes identifiers or tuples of identifiers\\n    to a dictionary of keyword arguments to be passed to the\\n    `.Figure.add_subplot` call used to create each subplot.  The values\\n    in these dictionaries have precedence over the values in\\n    *subplot_kw*.\\n\\n    If *mosaic* is a string, and thus all keys are single characters,\\n    it is possible to use a single string instead of a tuple as keys;\\n    i.e. ``\"AB\"`` is equivalent to ``(\"A\", \"B\")``.\\n\\n    .. versionadded:: 3.7\\n\\ngridspec_kw : dict, optional\\n    Dictionary with keywords passed to the `.GridSpec` constructor used\\n    to create the grid the subplots are placed on. In the case of\\n    nested layouts, this argument applies only to the outer layout.\\n    For more complex layouts, users should use `.Figure.subfigures`\\n    to create the nesting.\\n\\nempty_sentinel : object, optional\\n    Entry in the layout to mean \"leave this space empty\".  Defaults\\n    to ``\\'.\\'``. Note, if *layout* is a string, it is processed via\\n    `inspect.cleandoc` to remove leading white space, which may\\n    interfere with using white-space as the empty sentinel.\\n\\nReturns\\n-------\\ndict[label, Axes]\\n   A dictionary mapping the labels to the Axes objects.  The order of\\n   the Axes is left-to-right and top-to-bottom of their position in the\\n   total layout.',\n",
       " \"Test the evaluate method when the dim's of dataset and points have\\ndifferent dimensions.\",\n",
       " 'Returns the complexity of a module',\n",
       " 'Executes a batch of bulkWrite server commands (unack).',\n",
       " 'Main checking entry: check a list of files or modules from their name.\\n\\nfiles_or_modules is either a string or list of strings presenting modules to check.',\n",
       " 'Find k-clique communities in graph using the percolation method.\\n\\nA k-clique community is the union of all cliques of size k that\\ncan be reached through adjacent (sharing k-1 nodes) k-cliques.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\nk : int\\n   Size of smallest clique\\n\\ncliques: list or generator\\n   Precomputed cliques (use networkx.find_cliques(G))\\n\\nReturns\\n-------\\nYields sets of nodes, one for each k-clique community.\\n\\nExamples\\n--------\\n>>> G = nx.complete_graph(5)\\n>>> K5 = nx.convert_node_labels_to_integers(G, first_label=2)\\n>>> G.add_edges_from(K5.edges())\\n>>> c = list(nx.community.k_clique_communities(G, 4))\\n>>> sorted(list(c[0]))\\n[0, 1, 2, 3, 4, 5, 6]\\n>>> list(nx.community.k_clique_communities(G, 6))\\n[]\\n\\nReferences\\n----------\\n.. [1] Gergely Palla, Imre DerÃ©nyi, IllÃ©s Farkas1, and TamÃ¡s Vicsek,\\n   Uncovering the overlapping community structure of complex networks\\n   in nature and society Nature 435, 814-818, 2005,\\n   doi:10.1038/nature03607',\n",
       " 'Returns flow cost calculated from flow dictionary',\n",
       " 'Traverse f2py data structure with the following visit function:\\n\\ndef visit(item, parents, result, *args, **kwargs):\\n    \"\"\"\\n\\n    parents is a list of key-\"f2py data structure\" pairs from which\\n    items are taken from.\\n\\n    result is a f2py data structure that is filled with the\\n    return value of the visit function.\\n\\n    item is 2-tuple (index, value) if parents[-1][1] is a list\\n    item is 2-tuple (key, value) if parents[-1][1] is a dict\\n\\n    The return value of visit must be None, or of the same kind as\\n    item, that is, if parents[-1] is a list, the return value must\\n    be 2-tuple (new_index, new_value), or if parents[-1] is a\\n    dict, the return value must be 2-tuple (new_key, new_value).\\n\\n    If new_index or new_value is None, the return value of visit\\n    is ignored, that is, it will not be added to the result.\\n\\n    If the return value is None, the content of obj will be\\n    traversed, otherwise not.\\n    \"\"\"',\n",
       " 'Find the unique elements of an array, and counts, inverse, and indices.\\n\\nThis function is an Array API compatible alternative to::\\n\\n    np.unique(x, return_index=True, return_inverse=True,\\n              return_counts=True, equal_nan=False)\\n\\nbut returns a namedtuple for easier access to each output.\\n\\nParameters\\n----------\\nx : array_like\\n    Input array. It will be flattened if it is not already 1-D.\\n\\nReturns\\n-------\\nout : namedtuple\\n    The result containing:\\n\\n    * values - The unique elements of an input array.\\n    * indices - The first occurring indices for each unique element.\\n    * inverse_indices - The indices from the set of unique elements\\n      that reconstruct `x`.\\n    * counts - The corresponding counts for each unique element.\\n\\nSee Also\\n--------\\nunique : Find the unique elements of an array.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> x = [1, 1, 2]\\n>>> uniq = np.unique_all(x)\\n>>> uniq.values\\narray([1, 2])\\n>>> uniq.indices\\narray([0, 2])\\n>>> uniq.inverse_indices\\narray([0, 0, 1])\\n>>> uniq.counts\\narray([2, 1])',\n",
       " 'Return the scaled companion matrix of c.\\n\\nThe basis polynomials are scaled so that the companion matrix is\\nsymmetric when `c` is an Hermite basis polynomial. This provides\\nbetter eigenvalue estimates than the unscaled case and for basis\\npolynomials the eigenvalues are guaranteed to be real if\\n`numpy.linalg.eigvalsh` is used to obtain them.\\n\\nParameters\\n----------\\nc : array_like\\n    1-D array of Hermite series coefficients ordered from low to high\\n    degree.\\n\\nReturns\\n-------\\nmat : ndarray\\n    Scaled companion matrix of dimensions (deg, deg).\\n\\nNotes\\n-----\\n\\nExamples\\n--------\\n>>> from numpy.polynomial.hermite import hermcompanion\\n>>> hermcompanion([1, 0, 1])\\narray([[0.        , 0.35355339],\\n       [0.70710678, 0.        ]])',\n",
       " 'Pseudo-Vandermonde matrix of given degrees.\\n\\nReturns the pseudo-Vandermonde matrix of degrees `deg` and sample\\npoints ``(x, y, z)``. If `l`, `m`, `n` are the given degrees in `x`, `y`, `z`,\\nthen Hehe pseudo-Vandermonde matrix is defined by\\n\\n.. math:: V[..., (m+1)(n+1)i + (n+1)j + k] = He_i(x)*He_j(y)*He_k(z),\\n\\nwhere ``0 <= i <= l``, ``0 <= j <= m``, and ``0 <= j <= n``.  The leading\\nindices of `V` index the points ``(x, y, z)`` and the last index encodes\\nthe degrees of the HermiteE polynomials.\\n\\nIf ``V = hermevander3d(x, y, z, [xdeg, ydeg, zdeg])``, then the columns\\nof `V` correspond to the elements of a 3-D coefficient array `c` of\\nshape (xdeg + 1, ydeg + 1, zdeg + 1) in the order\\n\\n.. math:: c_{000}, c_{001}, c_{002},... , c_{010}, c_{011}, c_{012},...\\n\\nand  ``np.dot(V, c.flat)`` and ``hermeval3d(x, y, z, c)`` will be the\\nsame up to roundoff. This equivalence is useful both for least squares\\nfitting and for the evaluation of a large number of 3-D HermiteE\\nseries of the same degrees and sample points.\\n\\nParameters\\n----------\\nx, y, z : array_like\\n    Arrays of point coordinates, all of the same shape. The dtypes will\\n    be converted to either float64 or complex128 depending on whether\\n    any of the elements are complex. Scalars are converted to 1-D\\n    arrays.\\ndeg : list of ints\\n    List of maximum degrees of the form [x_deg, y_deg, z_deg].\\n\\nReturns\\n-------\\nvander3d : ndarray\\n    The shape of the returned matrix is ``x.shape + (order,)``, where\\n    :math:`order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)`.  The dtype will\\n    be the same as the converted `x`, `y`, and `z`.\\n\\nSee Also\\n--------\\nhermevander, hermevander3d, hermeval2d, hermeval3d\\n\\nNotes\\n-----',\n",
       " 'Test ArrayZ view method',\n",
       " \"A string representation of the Specifier that can be round-tripped.\\n\\n>>> str(Specifier('>=1.0.0'))\\n'>=1.0.0'\\n>>> str(Specifier('>=1.0.0', prereleases=False))\\n'>=1.0.0'\",\n",
       " 'Verify that unapproved hashes are rejected',\n",
       " 'Return whether any element is truthy.\\n\\nReturns False unless there is at least one element that is truthy.\\nBy default, NAs are skipped. If ``skipna=False`` is specified and\\nmissing values are present, similar :ref:`Kleene logic <boolean.kleene>`\\nis used as for logical operations.\\n\\n.. versionchanged:: 1.4.0\\n\\nParameters\\n----------\\nskipna : bool, default True\\n    Exclude NA values. If the entire array is NA and `skipna` is\\n    True, then the result will be False, as for an empty array.\\n    If `skipna` is False, the result will still be True if there is\\n    at least one element that is truthy, otherwise NA will be returned\\n    if there are NA\\'s present.\\naxis : int, optional, default 0\\n**kwargs : any, default None\\n    Additional keywords have no effect but might be accepted for\\n    compatibility with NumPy.\\n\\nReturns\\n-------\\nbool or :attr:`pandas.NA`\\n\\nSee Also\\n--------\\nnumpy.any : Numpy version of this method.\\nBaseMaskedArray.all : Return whether all elements are truthy.\\n\\nExamples\\n--------\\nThe result indicates whether any element is truthy (and by default\\nskips NAs):\\n\\n>>> pd.array([True, False, True]).any()\\nTrue\\n>>> pd.array([True, False, pd.NA]).any()\\nTrue\\n>>> pd.array([False, False, pd.NA]).any()\\nFalse\\n>>> pd.array([], dtype=\"boolean\").any()\\nFalse\\n>>> pd.array([pd.NA], dtype=\"boolean\").any()\\nFalse\\n>>> pd.array([pd.NA], dtype=\"Float64\").any()\\nFalse\\n\\nWith ``skipna=False``, the result can be NA if this is logically\\nrequired (whether ``pd.NA`` is True or False influences the result):\\n\\n>>> pd.array([True, False, pd.NA]).any(skipna=False)\\nTrue\\n>>> pd.array([1, 0, pd.NA]).any(skipna=False)\\nTrue\\n>>> pd.array([False, False, pd.NA]).any(skipna=False)\\n<NA>\\n>>> pd.array([0, 0, pd.NA]).any(skipna=False)\\n<NA>',\n",
       " 'Check whether the provided array or dtype is of the string dtype.\\n\\nIf an array is passed with an object dtype, the elements must be\\ninferred as strings.\\n\\nParameters\\n----------\\narr_or_dtype : array-like or dtype\\n    The array or dtype to check.\\n\\nReturns\\n-------\\nboolean\\n    Whether or not the array or dtype is of the string dtype.\\n\\nSee Also\\n--------\\napi.types.is_string_dtype : Check whether the provided array or dtype\\n                            is of the string dtype.\\n\\nExamples\\n--------\\n>>> from pandas.api.types import is_string_dtype\\n>>> is_string_dtype(str)\\nTrue\\n>>> is_string_dtype(object)\\nTrue\\n>>> is_string_dtype(int)\\nFalse\\n>>> is_string_dtype(np.array([\"a\", \"b\"]))\\nTrue\\n>>> is_string_dtype(pd.Series([1, 2]))\\nFalse\\n>>> is_string_dtype(pd.Series([1, 2], dtype=object))\\nFalse',\n",
       " 'Return a list of SQL statements that creates a table reflecting the\\nstructure of a DataFrame.  The first entry will be a CREATE TABLE\\nstatement while the rest will be CREATE INDEX statements.',\n",
       " 'If a name in the base list of a class definition is undefined, a\\nwarning is emitted.',\n",
       " 'verify that a folder can be created, a bunch of files can be placed in\\nit, and those files show up in sftp.listdir.',\n",
       " 'This creates the FSM. You set the initial state here. The \"memory\"\\nattribute is any object that you want to pass along to the action\\nfunctions. It is not used by the FSM. For parsing you would typically\\npass a list to be used as a stack. ',\n",
       " 'Create a SelectionPreferences object.\\n\\n:param allow_yanked: Whether files marked as yanked (in the sense\\n    of PEP 592) are permitted to be candidates for install.\\n:param format_control: A FormatControl object or None. Used to control\\n    the selection of source packages / binary packages when consulting\\n    the index and links.\\n:param prefer_binary: Whether to prefer an old, but valid, binary\\n    dist over a new source dist.\\n:param ignore_requires_python: Whether to ignore incompatible\\n    \"Requires-Python\" values in links. Defaults to False.',\n",
       " 'Download the files given by links into location.',\n",
       " 'Waits for reading to be available on a given socket.\\nReturns True if the socket is readable, or False if the timeout expired.',\n",
       " 'Same as test_url_req_case_mismatch_file_index, except testing for the case\\nwhere the incorrect case is given in the name of the package to install\\nrather than in a requirements file.',\n",
       " 'Return the plugin registered under the given name, if any.',\n",
       " 'Initial dependencies:\\n    A (>=1.0)\\n    A (<1.2) ; python == 3.10\\n\\nMerged dependencies:\\n    A (>=1.0) ; python != 3.10\\n    A (>=1.0,<1.2) ; python == 3.10\\n\\nThe first dependency has to be ignored\\nbecause it is not compatible with the current environment.',\n",
       " 'Check that all parameters are consistent with the parameters mentioned\\nin the parameter documentation (e.g. the Sphinx tags \\'param\\' and \\'type\\').\\n\\n* Undocumented parameters except \\'self\\' are noticed.\\n* Undocumented parameter types except for \\'self\\' and the ``*<args>``\\n  and ``**<kwargs>`` parameters are noticed.\\n* Parameters mentioned in the parameter documentation that don\\'t or no\\n  longer exist in the function parameter list are noticed.\\n* If the text \"For the parameters, see\" or \"For the other parameters,\\n  see\" (ignoring additional white-space) is mentioned in the docstring,\\n  missing parameter documentation is tolerated.\\n* If there\\'s no Sphinx style, Google style or NumPy style parameter\\n  documentation at all, i.e. ``:param`` is never mentioned etc., the\\n  checker assumes that the parameters are documented in another format\\n  and the absence is tolerated.\\n\\n:param doc: Docstring for the function, method or class.\\n:type doc: :class:`Docstring`\\n\\n:param arguments_node: Arguments node for the function, method or\\n    class constructor.\\n:type arguments_node: :class:`astroid.scoped_nodes.Arguments`\\n\\n:param warning_node: The node to assign the warnings to\\n:type warning_node: :class:`astroid.scoped_nodes.Node`\\n\\n:param accept_no_param_doc: Whether to allow no parameters to be\\n    documented. If None then this value is read from the configuration.\\n:type accept_no_param_doc: bool or None',\n",
       " 'hehehe',\n",
       " 'Test to check the names of the generated files are corrected\\nwhen using an incorrect character like \"/\" in the package name.',\n",
       " 'Unit test for expected behavior to create ids with idfn and\\ndisable_test_id_escaping_and_forfeit_all_rights_to_community_support\\noption (#5294).',\n",
       " 'Create a TerminalWriter instance configured according to the options\\nin the config object.\\n\\nEvery code which requires a TerminalWriter object and has access to a\\nconfig object should use this function.',\n",
       " 'The list of log records.',\n",
       " 'Optional[google.cloud.bigquery.StandardSqlDataType]: Type\\nof a variable, e.g., a function argument.\\n\\nSee:\\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/routines#Argument.FIELDS.data_type',\n",
       " 'Convert a nanosecond-precision timestamp to a native datetime.\\n\\n.. note::\\n\\n   Python datetimes do not support nanosecond precision;  this function\\n   therefore truncates such values to microseconds.\\n\\n:type dt_str: str\\n:param dt_str: The string to convert.\\n\\n:rtype: :class:`datetime.datetime`\\n:returns: The datetime object created from the string.\\n:raises ValueError: If the timestamp does not match the RFC 3339\\n                    regular expression.',\n",
       " \"Return a list of all the message's header field names.\\n\\nThese will be sorted in the order they appeared in the original\\nmessage, or were added to the message, and may contain duplicates.\\nAny fields deleted and re-inserted are always appended to the header\\nlist.\",\n",
       " 'Test loading public PEM files.',\n",
       " \"Resource path for the metadata's key.\",\n",
       " 'Configures retries with customizations.',\n",
       " \"Return a segment of a horizontal line with optional colons which indicate\\ncolumn's alignment in a grid table.\",\n",
       " 'get_api_resources  # noqa: E501\\n\\nget available resources  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.get_api_resources_with_http_info(async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V1APIResourceList, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'delete_collection_namespaced_controller_revision  # noqa: E501\\n\\ndelete collection of ControllerRevision  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_controller_revision(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1Status\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Sets the priority_class_name of this V1PodSpec.\\n\\nIf specified, indicates the pod\\'s priority. \"system-node-critical\" and \"system-cluster-critical\" are two special keywords which indicate the highest priorities with the former being the highest priority. Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default.  # noqa: E501\\n\\n:param priority_class_name: The priority_class_name of this V1PodSpec.  # noqa: E501\\n:type: str',\n",
       " 'Gets the allow_privilege_escalation of this V1SecurityContext.  # noqa: E501\\n\\nAllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN Note that this field cannot be set when spec.os.name is windows.  # noqa: E501\\n\\n:return: The allow_privilege_escalation of this V1SecurityContext.  # noqa: E501\\n:rtype: bool',\n",
       " 'Returns true if both objects are not equal',\n",
       " 'Ping the Redis server\\n\\nFor more information see https://redis.io/commands/ping',\n",
       " 'Return the number of times an `item` may be in the `key`.\\nFor more information see `CF.COUNT <https://redis.io/commands/cf.count>`_.',\n",
       " 'Get query string and result in pandas.Dataframe\\n\\nReturns:\\n    The pandas.DataFrame object.\\n    The query string executed.',\n",
       " 'Placeholder docstring.',\n",
       " 'Apply a modification to a given :class:`.CompileState`.',\n",
       " 'Get parameters for this estimator.\\n\\nReturns the parameters given in the constructor as well as the\\nestimators contained within the `transformers` of the\\n`ColumnTransformer`.\\n\\nParameters\\n----------\\ndeep : bool, default=True\\n    If True, will return the parameters for this estimator and\\n    contained subobjects that are estimators.\\n\\nReturns\\n-------\\nparams : dict\\n    Parameter names mapped to their values.',\n",
       " 'Learn a vocabulary dictionary of all tokens in the raw documents.\\n\\nParameters\\n----------\\nraw_documents : iterable\\n    An iterable which generates either str, unicode or file objects.\\n\\ny : None\\n    This parameter is ignored.\\n\\nReturns\\n-------\\nself : object\\n    Fitted vectorizer.',\n",
       " 'The dummy arguments are to test that this fit function can\\naccept non-array arguments through cross-validation, such as:\\n    - int\\n    - str (this is actually array-like)\\n    - object\\n    - function',\n",
       " 'The lower bounds for the problem\\n\\nReturns\\n-------\\nxmin : sequence\\n    The lower bounds for the problem',\n",
       " 'LJ: Symmetry-constrained test function',\n",
       " \"Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\\n\\nThe returned filter prototype has `rp` decibels of ripple in the passband.\\n\\nThe filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\\ndefined as the point at which the gain first drops below ``-rp``.\\n\\nSee Also\\n--------\\ncheby1 : Filter design function using this prototype\",\n",
       " 'Compute derivatives of Bessel functions of the first kind.\\n\\nCompute the nth derivative of the Bessel function `Jv` with\\nrespect to `z`.\\n\\nParameters\\n----------\\nv : array_like or float\\n    Order of Bessel function\\nz : complex\\n    Argument at which to evaluate the derivative; can be real or\\n    complex.\\nn : int, default 1\\n    Order of derivative. For 0 returns the Bessel function `jv` itself.\\n\\nReturns\\n-------\\nscalar or ndarray\\n    Values of the derivative of the Bessel function.\\n\\nNotes\\n-----\\nThe derivative is computed using the relation DLFM 10.6.7 [2]_.\\n\\nReferences\\n----------\\n.. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\\n       Functions\", John Wiley and Sons, 1996, chapter 5.\\n       https://people.sc.fsu.edu/~jburkardt/f77_src/special_functions/special_functions.html\\n\\n.. [2] NIST Digital Library of Mathematical Functions.\\n       https://dlmf.nist.gov/10.6.E7\\n\\nExamples\\n--------\\n\\nCompute the Bessel function of the first kind of order 0 and\\nits first two derivatives at 1.\\n\\n>>> from scipy.special import jvp\\n>>> jvp(0, 1, 0), jvp(0, 1, 1), jvp(0, 1, 2)\\n(0.7651976865579666, -0.44005058574493355, -0.3251471008130331)\\n\\nCompute the first derivative of the Bessel function of the first\\nkind for several orders at 1 by providing an array for `v`.\\n\\n>>> jvp([0, 1, 2], 1, 1)\\narray([-0.44005059,  0.3251471 ,  0.21024362])\\n\\nCompute the first derivative of the Bessel function of the first\\nkind of order 0 at several points by providing an array for `z`.\\n\\n>>> import numpy as np\\n>>> points = np.array([0., 1.5, 3.])\\n>>> jvp(0, points, 1)\\narray([-0.        , -0.55793651, -0.33905896])\\n\\nPlot the Bessel function of the first kind of order 1 and its\\nfirst three derivatives.\\n\\n>>> import matplotlib.pyplot as plt\\n>>> x = np.linspace(-10, 10, 1000)\\n>>> fig, ax = plt.subplots()\\n>>> ax.plot(x, jvp(1, x, 0), label=r\"$J_1$\")\\n>>> ax.plot(x, jvp(1, x, 1), label=r\"$J_1\\'$\")\\n>>> ax.plot(x, jvp(1, x, 2), label=r\"$J_1\\'\\'$\")\\n>>> ax.plot(x, jvp(1, x, 3), label=r\"$J_1\\'\\'\\'$\")\\n>>> plt.legend()\\n>>> plt.show()',\n",
       " \"Convert timestamps in a PyArrow Table to timezone-naive in the specified timezone if the\\ncorresponding Spark data type is TimestampType in the specified Spark schema is TimestampType,\\nand optionally truncate nanosecond timestamps to microseconds.\\n\\nParameters\\n----------\\ntable : :class:`pyarrow.Table`\\nschema : :class:`StructType`\\n    The Spark schema corresponding to the schema of the Arrow Table.\\ntruncate : bool, default True\\n    Whether to truncate nanosecond timestamps to microseconds. (default ``True``)\\ntimezone : str, optional\\n    The timezone to convert from. If there is a timestamp type, it's required.\\n\\nReturns\\n-------\\n:class:`pyarrow.Table`\",\n",
       " 'Alter Index or MultiIndex name.\\nAble to set new names without level. Defaults to returning a new index.\\n\\nParameters\\n----------\\nname : label or list of labels\\n    Name(s) to set.\\ninplace : boolean, default False\\n    Modifies the object directly, instead of creating a new Index or MultiIndex.\\n\\nReturns\\n-------\\nIndex or MultiIndex\\n    The same type as the caller or None if inplace is True.\\n\\nExamples\\n--------\\n>>> df = ps.DataFrame({\\'a\\': [\\'A\\', \\'C\\'], \\'b\\': [\\'A\\', \\'B\\']}, columns=[\\'a\\', \\'b\\'])\\n>>> df.index.rename(\"c\")\\nIndex([0, 1], dtype=\\'int64\\', name=\\'c\\')\\n\\n>>> df.set_index(\"a\", inplace=True)\\n>>> df.index.rename(\"d\")\\nIndex([\\'A\\', \\'C\\'], dtype=\\'object\\', name=\\'d\\')\\n\\nYou can also change the index name in place.\\n\\n>>> df.index.rename(\"e\", inplace=True)\\n>>> df.index\\nIndex([\\'A\\', \\'C\\'], dtype=\\'object\\', name=\\'e\\')\\n\\n>>> df  # doctest: +NORMALIZE_WHITESPACE\\n   b\\ne\\nA  A\\nC  B\\n\\nSupport for MultiIndex\\n\\n>>> psidx = ps.MultiIndex.from_tuples([(\\'a\\', \\'x\\'), (\\'b\\', \\'y\\')])\\n>>> psidx.names = [\\'hello\\', \\'pandas-on-Spark\\']\\n>>> psidx  # doctest: +SKIP\\nMultiIndex([(\\'a\\', \\'x\\'),\\n            (\\'b\\', \\'y\\')],\\n           names=[\\'hello\\', \\'pandas-on-Spark\\'])\\n\\n>>> psidx.rename([\\'aloha\\', \\'databricks\\'])  # doctest: +SKIP\\nMultiIndex([(\\'a\\', \\'x\\'),\\n            (\\'b\\', \\'y\\')],\\n           names=[\\'aloha\\', \\'databricks\\'])',\n",
       " 'Return True if the specified integer is a valid number of arguments\\n\\nThe number of arguments n is guaranteed to be an integer and positive',\n",
       " 'Return True, False or None according to x.\\n\\nWhereas bool(x) returns True or False, fuzzy_bool allows\\nfor the None value and non-false values (which become None), too.\\n\\nExamples\\n========\\n\\n>>> from sympy.core.logic import fuzzy_bool\\n>>> from sympy.abc import x\\n>>> fuzzy_bool(x), fuzzy_bool(None)\\n(None, None)\\n>>> bool(x), bool(None)\\n(True, False)',\n",
       " 'apply the escape character to all occurrences of \"%\", \"_\" and\\nthe escape character itself',\n",
       " 'Return as a permutation the shuffling of range(n) using the Josephus\\nscheme in which every m-th item is selected until all have been chosen.\\nThe returned permutation has elements listed by the order in which they\\nwere selected.\\n\\nThe parameter ``s`` stops the selection process when there are ``s``\\nitems remaining and these are selected by continuing the selection,\\ncounting by 1 rather than by ``m``.\\n\\nConsider selecting every 3rd item from 6 until only 2 remain::\\n\\n    choices    chosen\\n    ========   ======\\n      012345\\n      01 345   2\\n      01 34    25\\n      01  4    253\\n      0   4    2531\\n      0        25314\\n               253140\\n\\nExamples\\n========\\n\\n>>> from sympy.combinatorics import Permutation\\n>>> Permutation.josephus(3, 6, 2).array_form\\n[2, 5, 3, 1, 4, 0]\\n\\nReferences\\n==========\\n\\n.. [1] https://en.wikipedia.org/wiki/Flavius_Josephus\\n.. [2] https://en.wikipedia.org/wiki/Josephus_problem\\n.. [3] https://web.archive.org/web/20171008094331/http://www.wou.edu/~burtonl/josephus.html',\n",
       " 'Returns a list of all coordinate functions.\\nFor more details see the ``base_scalar`` method of this class.',\n",
       " 'Returns SVG ellipse element for the Ellipse.\\n\\nParameters\\n==========\\n\\nscale_factor : float\\n    Multiplication factor for the SVG stroke-width.  Default is 1.\\nfill_color : str, optional\\n    Hex string for fill color. Default is \"#66cc99\".',\n",
       " 'Representation of a ``WrappingPathway``.',\n",
       " 'Construct a Trace object.\\n\\nParameters\\n==========\\nargs = SymPy expression\\nindices = tuple/list if indices, optional',\n",
       " 'Convert ``a`` to a SymPy object. ',\n",
       " 'Say whether a complex number belongs to this complex rectangular\\nregion.\\n\\nParameters\\n==========\\n\\nitem : pair (re, im) or number re\\n    Either a pair giving the real and imaginary parts of the number,\\n    or else a real number.',\n",
       " \"Return ``True`` if the equation is in the form\\n`a\\\\log(f(x)) + b\\\\log(g(x)) + ... + c` else ``False``.\\n\\nParameters\\n==========\\n\\nf : Expr\\n    The equation to be checked\\n\\nsymbol : Symbol\\n    The variable in which the equation is checked\\n\\nReturns\\n=======\\n\\n``True`` if the equation is logarithmic otherwise ``False``.\\n\\nExamples\\n========\\n\\n>>> from sympy import symbols, tan, log\\n>>> from sympy.solvers.solveset import _is_logarithmic as check\\n>>> x, y = symbols('x y')\\n>>> check(log(x + 2) - log(x + 3), x)\\nTrue\\n>>> check(tan(log(2*x)), x)\\nFalse\\n>>> check(x*log(x), x)\\nFalse\\n>>> check(x + log(x), x)\\nFalse\\n>>> check(y + log(x), x)\\nTrue\\n\\n* Philosophy behind the helper\\n\\nThe function extracts each term and checks whether it is\\nlogarithmic w.r.t ``symbol``.\",\n",
       " 'Integer keywords are matched correctly.',\n",
       " 'Read *exactly* ``amt`` bytes from the socket ``sock``.',\n",
       " \"Determine whether a connection error is a redirect that can be followed.\\n\\nReturn the new URI if it's a valid redirect. Else, return an exception.\",\n",
       " 'A delete is when we are a. in a downgrade and b.\\nwe are going to the \"base\" or we are going to a version that\\nis implied as a dependency on another version that is remaining.',\n",
       " 'Run the given coroutine function in an asynchronous event loop.\\n\\nThe current thread must not be already running an event loop.\\n\\n:param func: a coroutine function\\n:param args: positional arguments to ``func``\\n:param backend: name of the asynchronous event loop implementation â€“ currently\\n    either ``asyncio`` or ``trio``\\n:param backend_options: keyword arguments to call the backend ``run()``\\n    implementation with (documented :ref:`here <backend options>`)\\n:return: the return value of the coroutine function\\n:raises RuntimeError: if an asynchronous event loop is already running in this\\n    thread\\n:raises LookupError: if the named backend is not found',\n",
       " 'Returns a new datetime.date or asn1crypto.util.extended_date\\nobject with the specified components replaced\\n\\n:return:\\n    A datetime.date or asn1crypto.util.extended_date object',\n",
       " 'Convert JSON schema to the format used internally by the AWS CLI.\\n\\n:type schema: dict\\n:param schema: The JSON schema describing the argument model.\\n\\n:rtype: dict\\n:return: The transformed model in a form that can be consumed\\n    internally by the AWS CLI.  The dictionary returned will\\n    have a list of shapes, where the shape representing the\\n    transformed schema is always named ``InputShape`` in the\\n    returned dictionary.',\n",
       " 'Delete all templates.\\n\\nParameters\\n----------\\naccount_id\\n    If None, the account ID will be inferred from your boto3 session.\\nregex_filter\\n    Regex regex_filter that will delete all templates with a match in their ``Name``\\nboto3_session\\n    The default boto3 session will be used if **boto3_session** is ``None``.\\n\\nExamples\\n--------\\n>>> import awswrangler as wr\\n>>> wr.quicksight.delete_all_templates()',\n",
       " \"Returns:\\n    string_idx such that @LL[string_idx] is equal to our target (i.e.\\n    matched) string, if this line matches the return/yield statement\\n    requirements listed in the 'Requirements' section of this classes'\\n    docstring.\\n        OR\\n    None, otherwise.\",\n",
       " 'Return a list of completions for the incomplete value. Looks\\nat the names of options, subcommands, and chained\\nmulti-commands.\\n\\n:param ctx: Invocation context for this command.\\n:param incomplete: Value being completed. May be empty.\\n\\n.. versionadded:: 8.0',\n",
       " 'Helper to create files for skip_covered scenarios.',\n",
       " \"description is optional. If specified it is used in\\nwarning messages for the nodes that don't convert to string properly.\\nIf not specified then no messages are generated.\",\n",
       " 'Allocates a temporary (which may create a new one or get a previously\\nallocated and released one of the same type). Type is simply registered\\nand handed back, but will usually be a PyrexType.\\n\\nIf type.needs_refcounting, manage_ref comes into play. If manage_ref is set to\\nTrue, the temp will be decref-ed on return statements and in exception\\nhandling clauses. Otherwise the caller has to deal with any reference\\ncounting of the variable.\\n\\nIf not type.needs_refcounting, then manage_ref will be ignored, but it\\nstill has to be passed. It is recommended to pass False by convention\\nif it is known that type will never be a reference counted type.\\n\\nstatic=True marks the temporary declaration with \"static\".\\nThis is only used when allocating backing store for a module-level\\nC array literals.\\n\\nif reusable=False, the temp will not be reused after release.\\n\\nA C string referring to the variable is returned.',\n",
       " 'Print the text output for the table.',\n",
       " '@see:    L{get_tree}\\n@rtype:  L{Window}\\n@return: If this is a child window, return the top-level window it\\n    belongs to.\\n    If this window is already a top-level window, returns itself.\\n@raise WindowsError: An error occured while processing this request.',\n",
       " 'If a client is connected to the debug adapter that is debugging\\nthis process, pauses execution of all threads, and simulates a\\nbreakpoint being hit at the line following the call.\\n\\nIt is also registered as the default handler for builtins.breakpoint().',\n",
       " 'Precompute the wrapped methods, overriding the base class method to use async wrappers.',\n",
       " 'Register a template global, available in any template rendered by the\\napplication. Works like the :meth:`app_template_global` decorator. Equivalent to\\n:meth:`.Flask.add_template_global`.\\n\\n.. versionadded:: 0.10\\n\\n:param name: the optional name of the global, otherwise the\\n             function name will be used.',\n",
       " 'Post-rpc interceptor for create_file\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the FileService server but before\\nit is returned to user code.',\n",
       " 'Returns a fully-qualified subproperty_event_filter string.',\n",
       " 'Return a callable for the create api deployment method over gRPC.\\n\\nCreates a specified deployment.\\n\\nReturns:\\n    Callable[[~.CreateApiDeploymentRequest],\\n            ~.ApiDeployment]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.appengine_admin_v1.types.ListVersionsRequest):\\n        The initial request object.\\n    response (google.cloud.appengine_admin_v1.types.ListVersionsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Returns a fully-qualified assignment string.',\n",
       " 'Return the universe domain used by the client.\\n\\nArgs:\\n    client_universe_domain (Optional[str]): The universe domain configured via the client options.\\n    universe_domain_env (Optional[str]): The universe domain configured via the \"GOOGLE_CLOUD_UNIVERSE_DOMAIN\" environment variable.\\n\\nReturns:\\n    str: The universe domain to be used by the client.\\n\\nRaises:\\n    ValueError: If the universe domain is an empty string.',\n",
       " 'Return a callable for the get taxonomy method over gRPC.\\n\\nGets a taxonomy.\\n\\nReturns:\\n    Callable[[~.GetTaxonomyRequest],\\n            ~.Taxonomy]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Updates a document. Returns INVALID_ARGUMENT if the name of the\\ndocument is non-empty and does not equal the existing name.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import contentwarehouse_v1\\n\\n    def sample_update_document():\\n        # Create a client\\n        client = contentwarehouse_v1.DocumentServiceClient()\\n\\n        # Initialize request argument(s)\\n        document = contentwarehouse_v1.Document()\\n        document.plain_text = \"plain_text_value\"\\n        document.raw_document_path = \"raw_document_path_value\"\\n        document.display_name = \"display_name_value\"\\n\\n        request = contentwarehouse_v1.UpdateDocumentRequest(\\n            name=\"name_value\",\\n            document=document,\\n        )\\n\\n        # Make the request\\n        response = client.update_document(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.contentwarehouse_v1.types.UpdateDocumentRequest, dict]):\\n        The request object. Request message for\\n        DocumentService.UpdateDocument.\\n    name (str):\\n        Required. The name of the document to update. Format:\\n        projects/{project_number}/locations/{location}/documents/{document_id}\\n        or\\n        projects/{project_number}/locations/{location}/documents/referenceId/{reference_id}.\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    document (google.cloud.contentwarehouse_v1.types.Document):\\n        Required. The document to update.\\n        This corresponds to the ``document`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.contentwarehouse_v1.types.UpdateDocumentResponse:\\n        Response message for\\n        DocumentService.UpdateDocument.',\n",
       " 'Return a callable for the get terraform version method over gRPC.\\n\\nGets details about a\\n[TerraformVersion][google.cloud.config.v1.TerraformVersion].\\n\\nReturns:\\n    Callable[[~.GetTerraformVersionRequest],\\n            ~.TerraformVersion]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the get operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.GetOperationRequest):\\n        The request object for GetOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    operations_pb2.Operation: Response from GetOperation method.',\n",
       " 'Instantiate the transport.\\n\\nArgs:\\n    host (Optional[str]):\\n         The hostname to connect to (default: \\'dialogflow.googleapis.com\\').\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n\\n    credentials_file (Optional[str]): A file with credentials that can\\n        be loaded with :func:`google.auth.load_credentials_from_file`.\\n        This argument is ignored if ``channel`` is provided.\\n    scopes (Optional(Sequence[str])): A list of scopes. This argument is\\n        ignored if ``channel`` is provided.\\n    client_cert_source_for_mtls (Callable[[], Tuple[bytes, bytes]]): Client\\n        certificate to configure mutual TLS HTTP channel. It is ignored\\n        if ``channel`` is provided.\\n    quota_project_id (Optional[str]): An optional project to use for billing\\n        and quota.\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you are developing\\n        your own client library.\\n    always_use_jwt_access (Optional[bool]): Whether self signed JWT should\\n        be used for service account credentials.\\n    url_scheme: the protocol scheme for the API endpoint.  Normally\\n        \"https\", but for testing or local servers,\\n        \"http\" can be specified.',\n",
       " \"Return a callable for the match intent method over gRPC.\\n\\nReturns preliminary intent match results, doesn't\\nchange the session status.\\n\\nReturns:\\n    Callable[[~.MatchIntentRequest],\\n            ~.MatchIntentResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.\",\n",
       " 'Return the API endpoint and client cert source for mutual TLS.\\n\\nThe client cert source is determined in the following order:\\n(1) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is not \"true\", the\\nclient cert source is None.\\n(2) if `client_options.client_cert_source` is provided, use the provided one; if the\\ndefault client cert source exists, use the default one; otherwise the client cert\\nsource is None.\\n\\nThe API endpoint is determined in the following order:\\n(1) if `client_options.api_endpoint` if provided, use the provided one.\\n(2) if `GOOGLE_API_USE_CLIENT_CERTIFICATE` environment variable is \"always\", use the\\ndefault mTLS endpoint; if the environment variable is \"never\", use the default API\\nendpoint; otherwise if client cert source exists, use the default mTLS endpoint, otherwise\\nuse the default API endpoint.\\n\\nMore details can be found at https://google.aip.dev/auth/4114.\\n\\nArgs:\\n    client_options (google.api_core.client_options.ClientOptions): Custom options for the\\n        client. Only the `api_endpoint` and `client_cert_source` properties may be used\\n        in this method.\\n\\nReturns:\\n    Tuple[str, Callable[[], Tuple[bytes, bytes]]]: returns the API endpoint and the\\n        client cert source to use.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If any errors happen.',\n",
       " 'Call the list custom models method over HTTP.\\n\\nArgs:\\n    request (~.search_tuning_service.ListCustomModelsRequest):\\n        The request object. Request message for\\n    [SearchTuningService.ListCustomModels][google.cloud.discoveryengine.v1.SearchTuningService.ListCustomModels]\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.search_tuning_service.ListCustomModelsResponse:\\n        Response message for\\n    [SearchTuningService.ListCustomModels][google.cloud.discoveryengine.v1.SearchTuningService.ListCustomModels]\\n    method.',\n",
       " 'Return a callable for the get evaluation method over gRPC.\\n\\nGets a\\n[Evaluation][google.cloud.discoveryengine.v1alpha.Evaluation].\\n\\nReturns:\\n    Callable[[~.GetEvaluationRequest],\\n            ~.Evaluation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the create inspect template method over HTTP.\\n\\nArgs:\\n    request (~.dlp.CreateInspectTemplateRequest):\\n        The request object. Request message for\\n    CreateInspectTemplate.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.dlp.InspectTemplate:\\n        The inspectTemplate contains a\\n    configuration (set of types of sensitive\\n    data to be detected) to be used anywhere\\n    you otherwise would normally specify\\n    InspectConfig. See\\n    https://cloud.google.com/sensitive-data-protection/docs/concepts-templates\\n    to learn more.',\n",
       " 'Return a callable for the get group method over gRPC.\\n\\nGets the details of a group.\\n\\nReturns:\\n    Callable[[~.GetGroupRequest],\\n            ~.Group]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the update entitlement method over HTTP.\\n\\nArgs:\\n    request (~.privilegedaccessmanager.UpdateEntitlementRequest):\\n        The request object. Message for updating an entitlement.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Returns a fully-qualified catalog_item_path string.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.resourcemanager_v3.types.SearchFoldersRequest):\\n        The initial request object.\\n    response (google.cloud.resourcemanager_v3.types.SearchFoldersResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Post-rpc interceptor for list_inputs\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the LivestreamService server but before\\nit is returned to user code.',\n",
       " 'Lists all live configs managed by the Video Stitcher\\nthat belong to the specified project and region.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud.video import stitcher_v1\\n\\n    def sample_list_live_configs():\\n        # Create a client\\n        client = stitcher_v1.VideoStitcherServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = stitcher_v1.ListLiveConfigsRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_live_configs(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.video.stitcher_v1.types.ListLiveConfigsRequest, dict]):\\n        The request object. Request message for\\n        VideoStitcherService.listLiveConfig.\\n    parent (str):\\n        Required. The project that contains the list of live\\n        configs, in the form of\\n        ``projects/{project_number}/locations/{location}``.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.video.stitcher_v1.services.video_stitcher_service.pagers.ListLiveConfigsPager:\\n        Response message for\\n        VideoStitcher.ListLiveConfig.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Call the list locations method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.ListLocationsRequest):\\n        The request object for ListLocations method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.ListLocationsResponse: Response from ListLocations method.',\n",
       " \"HTTP/1.1 requests that include a 'Connection: Close' header should\\nnot be returned to the connection pool.\",\n",
       " 'Parse an ISO time string and compare it to the expected value.',\n",
       " 'Created to be used by inheritance.',\n",
       " 'Simple shortcut for start_write + write + end_write.',\n",
       " '1 2 4',\n",
       " 'Convert strings in *value* to floats using mapping information stored\\nin the *unit* object.\\n\\nParameters\\n----------\\nvalue : str or iterable\\n    Value or list of values to be converted.\\nunit : `.UnitData`\\n    An object mapping strings to integers.\\naxis : `~matplotlib.axis.Axis`\\n    The axis on which the converted value is plotted.\\n\\n    .. note:: *axis* is unused.\\n\\nReturns\\n-------\\nfloat or `~numpy.ndarray` of float',\n",
       " 'Helper for initial scaling.\\n\\nUsed by public functions that create a ScalarMappable and support\\nparameters *vmin*, *vmax* and *norm*. This makes sure that a *norm*\\nwill take precedence over *vmin*, *vmax*.\\n\\nNote that this method does not set the norm.',\n",
       " \"Compute the power spectral density.\\n\\nThe power spectral density :math:`P_{xx}` by Welch's average\\nperiodogram method.  The vector *x* is divided into *NFFT* length\\nsegments.  Each segment is detrended by function *detrend* and\\nwindowed by function *window*.  *noverlap* gives the length of\\nthe overlap between segments.  The :math:`|\\\\mathrm{fft}(i)|^2`\\nof each segment :math:`i` are averaged to compute :math:`P_{xx}`.\\n\\nIf len(*x*) < *NFFT*, it will be zero padded to *NFFT*.\\n\\nParameters\\n----------\\nx : 1-D array or sequence\\n    Array or sequence containing the data\\n\\n%(Spectral)s\\n\\n%(PSD)s\\n\\nnoverlap : int, default: 0 (no overlap)\\n    The number of points of overlap between segments.\\n\\nReturns\\n-------\\nPxx : 1-D array\\n    The values for the power spectrum :math:`P_{xx}` (real valued)\\n\\nfreqs : 1-D array\\n    The frequencies corresponding to the elements in *Pxx*\\n\\nReferences\\n----------\\nBendat & Piersol -- Random Data: Analysis and Measurement Procedures, John\\nWiley & Sons (1986)\\n\\nSee Also\\n--------\\nspecgram\\n    `specgram` differs in the default overlap; in not returning the mean of\\n    the segment periodograms; and in returning the times of the segments.\\n\\nmagnitude_spectrum : returns the magnitude spectrum.\\n\\ncsd : returns the spectral density between two signals.\",\n",
       " 'Check if a file exists in this instance of :class:`GridFS`.\\n\\nThe file to check for can be specified by the value of its\\n``_id`` key, or by passing in a query document. A query\\ndocument can be passed in as dictionary, or by using keyword\\narguments. Thus, the following three calls are equivalent:\\n\\n>>> fs.exists(file_id)\\n>>> fs.exists({\"_id\": file_id})\\n>>> fs.exists(_id=file_id)\\n\\nAs are the following two calls:\\n\\n>>> fs.exists({\"filename\": \"mike.txt\"})\\n>>> fs.exists(filename=\"mike.txt\")\\n\\nAnd the following two:\\n\\n>>> fs.exists({\"foo\": {\"$gt\": 12}})\\n>>> fs.exists(foo={\"$gt\": 12})\\n\\nReturns ``True`` if a matching file exists, ``False``\\notherwise. Calls to :meth:`exists` will not automatically\\ncreate appropriate indexes; application developers should be\\nsure to create indexes if needed and as appropriate.\\n\\n:param document_or_id: query document, or _id of the\\n    document to check for\\n:param session: a\\n    :class:`~pymongo.client_session.ClientSession`\\n:param kwargs: keyword arguments are used as a\\n    query document, if they\\'re present.\\n\\n.. versionchanged:: 3.6\\n   Added ``session`` parameter.',\n",
       " 'Decode RawBSONDocuments in the given container.',\n",
       " 'Set up the loop for running.',\n",
       " 'Returns the number of connected components.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n   An undirected graph.\\n\\nReturns\\n-------\\nn : integer\\n   Number of connected components\\n\\nRaises\\n------\\nNetworkXNotImplemented\\n    If G is directed.\\n\\nExamples\\n--------\\n>>> G = nx.Graph([(0, 1), (1, 2), (5, 6), (3, 4)])\\n>>> nx.number_connected_components(G)\\n3\\n\\nSee Also\\n--------\\nconnected_components\\nnumber_weakly_connected_components\\nnumber_strongly_connected_components\\n\\nNotes\\n-----\\nFor undirected graphs only.',\n",
       " 'Check creation of single-dimensional objects',\n",
       " \"Transforms a masked array into a flexible-type array.\\n\\nThe flexible type array that is returned will have two fields:\\n\\n* the ``_data`` field stores the ``_data`` part of the array.\\n* the ``_mask`` field stores the ``_mask`` part of the array.\\n\\nParameters\\n----------\\nNone\\n\\nReturns\\n-------\\nrecord : ndarray\\n    A new flexible-type `ndarray` with two fields: the first element\\n    containing a value, the second element containing the corresponding\\n    mask boolean. The returned record shape matches self.shape.\\n\\nNotes\\n-----\\nA side-effect of transforming a masked array into a flexible `ndarray` is\\nthat meta information (``fill_value``, ...) will be lost.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4)\\n>>> x\\nmasked_array(\\n  data=[[1, --, 3],\\n        [--, 5, --],\\n        [7, --, 9]],\\n  mask=[[False,  True, False],\\n        [ True, False,  True],\\n        [False,  True, False]],\\n  fill_value=999999)\\n>>> x.toflex()\\narray([[(1, False), (2,  True), (3, False)],\\n       [(4,  True), (5, False), (6,  True)],\\n       [(7, False), (8,  True), (9, False)]],\\n      dtype=[('_data', '<i8'), ('_mask', '?')])\",\n",
       " 'Given a type like `APIResponse[T]`, returns the generic type variable `T`.\\n\\nThis also handles the case where a concrete subclass is given, e.g.\\n```py\\nclass MyResponse(APIResponse[bytes]):\\n    ...\\n\\nextract_response_type(MyResponse) -> bytes\\n```',\n",
       " 'The number of connection timeouts that have occurred trying to obtain a connection from the pool',\n",
       " 'Convert a set of codes for to a new set of categories\\n\\nParameters\\n----------\\ncodes : np.ndarray\\nold_categories, new_categories : Index\\ncopy: bool, default True\\n    Whether to copy if the codes are unchanged.\\n\\nReturns\\n-------\\nnew_codes : np.ndarray[np.int64]\\n\\nExamples\\n--------\\n>>> old_cat = pd.Index([\"b\", \"a\", \"c\"])\\n>>> new_cat = pd.Index([\"a\", \"b\"])\\n>>> codes = np.array([0, 1, 1, 2])\\n>>> recode_for_categories(codes, old_cat, new_cat)\\narray([ 1,  0,  0, -1], dtype=int8)',\n",
       " \"Return unique values in the index.\\n\\nUnique values are returned in order of appearance, this does NOT sort.\\n\\nParameters\\n----------\\nlevel : int or hashable, optional\\n    Only return values from specified level (for MultiIndex).\\n    If int, gets the level by integer position, else by level name.\\n\\nReturns\\n-------\\nIndex\\n    Unique values in the index.\\n\\nSee Also\\n--------\\nunique : Numpy array of unique values in that column.\\nSeries.unique : Return unique values of Series object.\\n\\nExamples\\n--------\\n>>> idx = pd.Index([1, 1, 2, 3, 3])\\n>>> idx.unique()\\nIndex([1, 2, 3], dtype='int64')\",\n",
       " 'Evaluate an arithmetic operation `+`, `-`, `*`, `/`, `//`, `%`, `**`, ...\\n\\nNote: the caller is responsible for ensuring that numpy warnings are\\nsuppressed (with np.errstate(all=\"ignore\")) if needed.\\n\\nParameters\\n----------\\nleft : np.ndarray or ExtensionArray\\nright : object\\n    Cannot be a DataFrame or Index.  Series is *not* excluded.\\nop : {operator.add, operator.sub, ...}\\n    Or one of the reversed variants from roperator.\\n\\nReturns\\n-------\\nndarray or ExtensionArray\\n    Or a 2-tuple of these in the case of divmod or rdivmod.',\n",
       " 'Put a connection back into the pool.\\n\\n:param conn:\\n    Connection object for the current host and port as returned by\\n    :meth:`._new_conn` or :meth:`._get_conn`.\\n\\nIf the pool is already full, the connection is closed and discarded\\nbecause we exceeded maxsize. If connections are discarded frequently,\\nthen maxsize should be increased.\\n\\nIf the pool is closed, then the connection will be closed and discarded.',\n",
       " 'atomic_specifier  : _ATOMIC LPAREN type_name RPAREN\\n        ',\n",
       " 'Generate a JSON schema (as dict) for the passed model or dynamically generated one.',\n",
       " ':calls: `DELETE /repos/{owner}/{repo}/autolinks/{id} <https://docs.github.com/en/rest/reference/repos>`_\\n:param autolink: int or :class:`github.Autolink.Autolink`\\n:rtype: None',\n",
       " 'Return true if <name> could be considered as a builtin defined by python.',\n",
       " 'Depends on `test_fail5` and `undefined` in\\nvariable and named arguments annotations.',\n",
       " 'Make public instance methods raise an error if the instance is closed.',\n",
       " 'year -> 1 if leap year, else 0.',\n",
       " 'A function disabled by the ``future`` module. This function is\\nno longer a builtin in Python 3.',\n",
       " \"Deprecate positional arguments without dropping backwards compatibility.\\n\\n:param names:\\n\\n  The positional arguments to :func:`deprecated_args()` give the names of\\n  the positional arguments that the to-be-decorated function should warn\\n  about being deprecated and translate to keyword arguments.\\n\\n:returns: A decorator function specialized to `names`.\\n\\nThe :func:`deprecated_args()` decorator function was created to make it\\neasy to switch from positional arguments to keyword arguments [#]_ while\\npreserving backwards compatibility [#]_ and informing call sites\\nabout the change.\\n\\n.. [#] Increased flexibility is the main reason why I find myself switching\\n       from positional arguments to (optional) keyword arguments as my code\\n       evolves to support more use cases.\\n\\n.. [#] In my experience positional argument order implicitly becomes part\\n       of API compatibility whether intended or not. While this makes sense\\n       for functions that over time adopt more and more optional arguments,\\n       at a certain point it becomes an inconvenience to code maintenance.\\n\\nHere's an example of how to use the decorator::\\n\\n  @deprecated_args('text')\\n  def report_choice(**options):\\n      print(options['text'])\\n\\nWhen the decorated function is called with positional arguments\\na deprecation warning is given::\\n\\n  >>> report_choice('this will give a deprecation warning')\\n  DeprecationWarning: report_choice has deprecated positional arguments, please switch to keyword arguments\\n  this will give a deprecation warning\\n\\nBut when the function is called with keyword arguments no deprecation\\nwarning is emitted::\\n\\n  >>> report_choice(text='this will not give a deprecation warning')\\n  this will not give a deprecation warning\",\n",
       " 'ping is acknowledged by a pong with the same payload.',\n",
       " 'Trims an array by masking the data outside some given limits.\\n\\nReturns a masked version of the input array.\\n\\n%s\\n\\nExamples\\n--------\\n>>> from scipy.stats.mstats import trim\\n>>> z = [ 1, 2, 3, 4, 5, 6, 7, 8, 9,10]\\n>>> print(trim(z,(3,8)))\\n[-- -- 3 4 5 6 7 8 -- --]\\n>>> print(trim(z,(0.1,0.2),relative=True))\\n[-- 2 3 4 5 6 7 8 -- --]',\n",
       " 'Create a frozen von Mises-Fisher distribution.\\n\\nSee `vonmises_fisher_frozen` for more information.',\n",
       " 'Test loading public DER keys.',\n",
       " \"read_namespaced_daemon_set_status  # noqa: E501\\n\\nread status of the specified DaemonSet  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.read_namespaced_daemon_set_status(name, namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the DaemonSet (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If 'true', then the output is pretty printed. Defaults to 'false' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1DaemonSet\\n         If the method is called asynchronously,\\n         returns the request thread.\",\n",
       " 'Sets the conditions of this V1APIServiceStatus.\\n\\nCurrent service state of apiService.  # noqa: E501\\n\\n:param conditions: The conditions of this V1APIServiceStatus.  # noqa: E501\\n:type: list[V1APIServiceCondition]',\n",
       " 'Returns true if both objects are equal',\n",
       " 'Sets the field_path of this V1ObjectFieldSelector.\\n\\nPath of the field to select in the specified API version.  # noqa: E501\\n\\n:param field_path: The field_path of this V1ObjectFieldSelector.  # noqa: E501\\n:type: str',\n",
       " 'Sets the resource_id of this V1ResourceHealth.\\n\\nResourceID is the unique identifier of the resource. See the ResourceID type for more information.  # noqa: E501\\n\\n:param resource_id: The resource_id of this V1ResourceHealth.  # noqa: E501\\n:type: str',\n",
       " 'Returns the difference between the first and all successive input\\nsorted sets provided in ``keys``.\\n\\nFor more information see https://redis.io/commands/zdiff',\n",
       " 'Submits a task to a provided executor\\n\\n:type executor: s3transfer.futures.BoundedExecutor\\n:param executor: The executor to submit the callable to\\n\\n:type task: s3transfer.tasks.Task\\n:param task: The task to submit to the executor\\n\\n:type tag: s3transfer.futures.TaskTag\\n:param tag: A tag to associate to the submitted task\\n\\n:rtype: concurrent.futures.Future\\n:returns: A future representing the submitted task',\n",
       " 'Aggregates progress updates for every provided progress callback\\n\\n:type callbacks: A list of functions that accepts bytes_transferred\\n    as a single argument\\n:param callbacks: The callbacks to invoke when threshold is reached\\n\\n:type threshold: int\\n:param threshold: The progress threshold in which to take the\\n    aggregated progress and invoke the progress callback with that\\n    aggregated progress total',\n",
       " 'Create metric entry.\\n\\nArgs:\\n    feature (dict): Entry in pdp list.\\n    distribution_value (List): y values in data distribution graph.\\n    prediction_values (List): y values in data prediction graph.\\n    x_axis_name (Union[str, list]): X axis name.\\n    graph_type (str): Type of graph.',\n",
       " 'Get a step by index.',\n",
       " 'Check __sklearn_is_fitted__ is defined correctly.',\n",
       " 'Check the behaviour of the prediction function.',\n",
       " 'Validate parameters passed to __init__.\\n\\nAlso validate parameters passed to splitter.',\n",
       " 'Check that _set_order returns sparse matrices in promised format.',\n",
       " 'Predict probabilities for each class.\\n\\nHere, the dummy classifier will provide a probability of 1 for the\\nfirst class of `classes_` and 0 otherwise.\\n\\nParameters\\n----------\\nX : array-like of shape (n_samples, n_features)\\n    The input data.\\n\\nReturns\\n-------\\nproba : ndarray of shape (n_samples, n_classes)\\n    The probabilities for each sample and class.',\n",
       " 'Create a companion matrix.\\n\\nCreate the companion matrix [1]_ associated with the polynomial whose\\ncoefficients are given in `a`.\\n\\nParameters\\n----------\\na : (..., N) array_like\\n    1-D array of polynomial coefficients. The length of `a` must be\\n    at least two, and ``a[0]`` must not be zero.\\n    M-dimensional arrays are treated as a batch: each slice along the last\\n    axis is a 1-D array of polynomial coefficients.\\n\\nReturns\\n-------\\nc : (..., N-1, N-1) ndarray\\n    For 1-D input, the first row of `c` is ``-a[1:]/a[0]``, and the first\\n    sub-diagonal is all ones.  The data-type of the array is the same\\n    as the data-type of ``1.0*a[0]``.\\n    For batch input, each slice of shape ``(N-1, N-1)`` along the last two\\n    dimensions of the output corresponds with a slice of shape ``(N,)``\\n    along the last dimension of the input.\\n\\nRaises\\n------\\nValueError\\n    If any of the following are true: a) ``a.shape[-1] < 2``; b) ``a[..., 0] == 0``.\\n\\nNotes\\n-----\\n.. versionadded:: 0.8.0\\n\\nReferences\\n----------\\n.. [1] R. A. Horn & C. R. Johnson, *Matrix Analysis*.  Cambridge, UK:\\n    Cambridge University Press, 1999, pp. 146-7.\\n\\nExamples\\n--------\\n>>> from scipy.linalg import companion\\n>>> companion([1, -10, 31, -30])\\narray([[ 10., -31.,  30.],\\n       [  1.,   0.,   0.],\\n       [  0.,   1.,   0.]])',\n",
       " \"Return a sparse array of uniformly random numbers in [0, 1)\\n\\nReturns a sparse array with the given shape and density\\nwhere values are generated uniformly randomly in the range [0, 1).\\n\\n.. warning::\\n\\n    Since numpy 1.17, passing a ``np.random.Generator`` (e.g.\\n    ``np.random.default_rng``) for ``random_state`` will lead to much\\n    faster execution times.\\n\\n    A much slower implementation is used by default for backwards\\n    compatibility.\\n\\nParameters\\n----------\\nshape : int or tuple of ints\\n    shape of the array\\ndensity : real, optional (default: 0.01)\\n    density of the generated matrix: density equal to one means a full\\n    matrix, density of 0 means a matrix with no non-zero items.\\nformat : str, optional (default: 'coo')\\n    sparse matrix format.\\ndtype : dtype, optional (default: np.float64)\\n    type of the returned matrix values.\\nrandom_state : {None, int, `Generator`, `RandomState`}, optional\\n    A random number generator to determine nonzero structure. We recommend using\\n    a `numpy.random.Generator` manually provided for every call as it is much\\n    faster than RandomState.\\n\\n    - If `None` (or `np.random`), the `numpy.random.RandomState`\\n      singleton is used.\\n    - If an int, a new ``Generator`` instance is used,\\n      seeded with the int.\\n    - If a ``Generator`` or ``RandomState`` instance then\\n      that instance is used.\\n\\n    This random state will be used for sampling `indices` (the sparsity\\n    structure), and by default for the data values too (see `data_sampler`).\\n\\ndata_sampler : callable, optional (default depends on dtype)\\n    Sampler of random data values with keyword arg `size`.\\n    This function should take a single keyword argument `size` specifying\\n    the length of its returned ndarray. It is used to generate the nonzero\\n    values in the matrix after the locations of those values are chosen.\\n    By default, uniform [0, 1) random values are used unless `dtype` is\\n    an integer (default uniform integers from that dtype) or\\n    complex (default uniform over the unit square in the complex plane).\\n    For these, the `random_state` rng is used e.g. ``rng.uniform(size=size)``.\\n\\nReturns\\n-------\\nres : sparse array\\n\\nExamples\\n--------\\n\\nPassing a ``np.random.Generator`` instance for better performance:\\n\\n>>> import numpy as np\\n>>> import scipy as sp\\n>>> rng = np.random.default_rng()\\n\\nDefault sampling uniformly from [0, 1):\\n\\n>>> S = sp.sparse.random_array((3, 4), density=0.25, random_state=rng)\\n\\nProviding a sampler for the values:\\n\\n>>> rvs = sp.stats.poisson(25, loc=10).rvs\\n>>> S = sp.sparse.random_array((3, 4), density=0.25,\\n...                            random_state=rng, data_sampler=rvs)\\n>>> S.toarray()\\narray([[ 36.,   0.,  33.,   0.],   # random\\n       [  0.,   0.,   0.,   0.],\\n       [  0.,   0.,  36.,   0.]])\\n\\nBuilding a custom distribution.\\nThis example builds a squared normal from np.random:\\n\\n>>> def np_normal_squared(size=None, random_state=rng):\\n...     return random_state.standard_normal(size) ** 2\\n>>> S = sp.sparse.random_array((3, 4), density=0.25, random_state=rng,\\n...                      data_sampler=np_normal_squared)\\n\\nOr we can build it from sp.stats style rvs functions:\\n\\n>>> def sp_stats_normal_squared(size=None, random_state=rng):\\n...     std_normal = sp.stats.distributions.norm_gen().rvs\\n...     return std_normal(size=size, random_state=random_state) ** 2\\n>>> S = sp.sparse.random_array((3, 4), density=0.25, random_state=rng,\\n...                      data_sampler=sp_stats_normal_squared)\\n\\nOr we can subclass sp.stats rv_continuous or rv_discrete:\\n\\n>>> class NormalSquared(sp.stats.rv_continuous):\\n...     def _rvs(self,  size=None, random_state=rng):\\n...         return random_state.standard_normal(size) ** 2\\n>>> X = NormalSquared()\\n>>> Y = X().rvs\\n>>> S = sp.sparse.random_array((3, 4), density=0.25,\\n...                            random_state=rng, data_sampler=Y)\",\n",
       " 'Chebyshev polynomial of the first kind on :math:`[-2, 2]`.\\n\\nDefined as :math:`C_n(x) = 2T_n(x/2)`, where :math:`T_n` is the\\nnth Chebychev polynomial of the first kind.\\n\\nParameters\\n----------\\nn : int\\n    Degree of the polynomial.\\nmonic : bool, optional\\n    If `True`, scale the leading coefficient to be 1. Default is\\n    `False`.\\n\\nReturns\\n-------\\nC : orthopoly1d\\n    Chebyshev polynomial of the first kind on :math:`[-2, 2]`.\\n\\nSee Also\\n--------\\nchebyt : Chebyshev polynomial of the first kind.\\n\\nNotes\\n-----\\nThe polynomials :math:`C_n(x)` are orthogonal over :math:`[-2, 2]`\\nwith weight function :math:`1/\\\\sqrt{1 - (x/2)^2}`.\\n\\nReferences\\n----------\\n.. [1] Abramowitz and Stegun, \"Handbook of Mathematical Functions\"\\n       Section 22. National Bureau of Standards, 1972.',\n",
       " 'Test invalid, orphaned pseudo close.',\n",
       " '__init__(self, \\\\*, featuresCol=\"features\", predictionCol=\"prediction\", maxIter=20,                  seed=None, k=4, minDivisibleClusterSize=1.0, distanceMeasure=\"euclidean\",                  weightCol=None)',\n",
       " 'Break line by unicode width instead of len(word).',\n",
       " 'Infer the type of mapping from a Composite.',\n",
       " 'Return a DBAPI connection from the pool.\\n\\nThe connection is instrumented such that when its\\n``close()`` method is called, the connection will be returned to\\nthe pool.',\n",
       " 'Establish the values and/or types of bound parameters within\\nthis :class:`_expression.TextClause` construct.\\n\\nGiven a text construct such as::\\n\\n    from sqlalchemy import text\\n    stmt = text(\"SELECT id, name FROM user WHERE name=:name \"\\n                \"AND timestamp=:timestamp\")\\n\\nthe :meth:`_expression.TextClause.bindparams`\\nmethod can be used to establish\\nthe initial value of ``:name`` and ``:timestamp``,\\nusing simple keyword arguments::\\n\\n    stmt = stmt.bindparams(name=\\'jack\\',\\n                timestamp=datetime.datetime(2012, 10, 8, 15, 12, 5))\\n\\nWhere above, new :class:`.BindParameter` objects\\nwill be generated with the names ``name`` and ``timestamp``, and\\nvalues of ``jack`` and ``datetime.datetime(2012, 10, 8, 15, 12, 5)``,\\nrespectively.  The types will be\\ninferred from the values given, in this case :class:`.String` and\\n:class:`.DateTime`.\\n\\nWhen specific typing behavior is needed, the positional ``*binds``\\nargument can be used in which to specify :func:`.bindparam` constructs\\ndirectly.  These constructs must include at least the ``key``\\nargument, then an optional value and type::\\n\\n    from sqlalchemy import bindparam\\n    stmt = stmt.bindparams(\\n                    bindparam(\\'name\\', value=\\'jack\\', type_=String),\\n                    bindparam(\\'timestamp\\', type_=DateTime)\\n                )\\n\\nAbove, we specified the type of :class:`.DateTime` for the\\n``timestamp`` bind, and the type of :class:`.String` for the ``name``\\nbind.  In the case of ``name`` we also set the default value of\\n``\"jack\"``.\\n\\nAdditional bound parameters can be supplied at statement execution\\ntime, e.g.::\\n\\n    result = connection.execute(stmt,\\n                timestamp=datetime.datetime(2012, 10, 8, 15, 12, 5))\\n\\nThe :meth:`_expression.TextClause.bindparams`\\nmethod can be called repeatedly,\\nwhere it will re-use existing :class:`.BindParameter` objects to add\\nnew information.  For example, we can call\\n:meth:`_expression.TextClause.bindparams`\\nfirst with typing information, and a\\nsecond time with value information, and it will be combined::\\n\\n    stmt = text(\"SELECT id, name FROM user WHERE name=:name \"\\n                \"AND timestamp=:timestamp\")\\n    stmt = stmt.bindparams(\\n        bindparam(\\'name\\', type_=String),\\n        bindparam(\\'timestamp\\', type_=DateTime)\\n    )\\n    stmt = stmt.bindparams(\\n        name=\\'jack\\',\\n        timestamp=datetime.datetime(2012, 10, 8, 15, 12, 5)\\n    )\\n\\nThe :meth:`_expression.TextClause.bindparams`\\nmethod also supports the concept of\\n**unique** bound parameters.  These are parameters that are\\n\"uniquified\" on name at statement compilation time, so that  multiple\\n:func:`_expression.text`\\nconstructs may be combined together without the names\\nconflicting.  To use this feature, specify the\\n:paramref:`.BindParameter.unique` flag on each :func:`.bindparam`\\nobject::\\n\\n    stmt1 = text(\"select id from table where name=:name\").bindparams(\\n        bindparam(\"name\", value=\\'name1\\', unique=True)\\n    )\\n    stmt2 = text(\"select id from table where name=:name\").bindparams(\\n        bindparam(\"name\", value=\\'name2\\', unique=True)\\n    )\\n\\n    union = union_all(\\n        stmt1.columns(column(\"id\")),\\n        stmt2.columns(column(\"id\"))\\n    )\\n\\nThe above statement will render as::\\n\\n    select id from table where name=:name_1\\n    UNION ALL select id from table where name=:name_2\\n\\n.. versionadded:: 1.3.11  Added support for the\\n   :paramref:`.BindParameter.unique` flag to work with\\n   :func:`_expression.text`\\n   constructs.',\n",
       " 'Construct a :class:`.Function`.\\n\\nThe :data:`.func` construct is normally used to construct\\nnew :class:`.Function` instances.',\n",
       " 'Return the commutator of two subgroups.\\n\\nExplanation\\n===========\\n\\nFor a permutation group ``K`` and subgroups ``G``, ``H``, the\\ncommutator of ``G`` and ``H`` is defined as the group generated\\nby all the commutators `[g, h] = hgh^{-1}g^{-1}` for ``g`` in ``G`` and\\n``h`` in ``H``. It is naturally a subgroup of ``K`` ([1], p.27).\\n\\nExamples\\n========\\n\\n>>> from sympy.combinatorics.named_groups import (SymmetricGroup,\\n... AlternatingGroup)\\n>>> S = SymmetricGroup(5)\\n>>> A = AlternatingGroup(5)\\n>>> G = S.commutator(S, A)\\n>>> G.is_subgroup(A)\\nTrue\\n\\nSee Also\\n========\\n\\nderived_subgroup\\n\\nNotes\\n=====\\n\\nThe commutator of two subgroups `H, G` is equal to the normal closure\\nof the commutators of all the generators, i.e. `hgh^{-1}g^{-1}` for `h`\\na generator of `H` and `g` a generator of `G` ([1], p.28)',\n",
       " \"Converts a function or an expression to a holonomic function.\\n\\nParameters\\n==========\\n\\nfunc:\\n    The expression to be converted.\\nx:\\n    variable for the function.\\nx0:\\n    point at which initial condition must be computed.\\ny0:\\n    One can optionally provide initial condition if the method\\n    is not able to do it automatically.\\nlenics:\\n    Number of terms in the initial condition. By default it is\\n    equal to the order of the annihilator.\\ndomain:\\n    Ground domain for the polynomials in ``x`` appearing as coefficients\\n    in the annihilator.\\ninitcond:\\n    Set it false if you do not want the initial conditions to be computed.\\n\\nExamples\\n========\\n\\n>>> from sympy.holonomic.holonomic import expr_to_holonomic\\n>>> from sympy import sin, exp, symbols\\n>>> x = symbols('x')\\n>>> expr_to_holonomic(sin(x))\\nHolonomicFunction((1) + (1)*Dx**2, x, 0, [0, 1])\\n>>> expr_to_holonomic(exp(x))\\nHolonomicFunction((-1) + (1)*Dx, x, 0, [1])\\n\\nSee Also\\n========\\n\\nsympy.integrals.meijerint._rewrite1, _convert_poly_rat_alg, _create_table\",\n",
       " 'The ith simple root of F_4\\n\\nEvery lie algebra has a unique root system.\\nGiven a root system Q, there is a subset of the\\nroots such that an element of Q is called a\\nsimple root if it cannot be written as the sum\\nof two elements in Q.  If we let D denote the\\nset of simple roots, then it is clear that every\\nelement of Q can be written as a linear combination\\nof elements of D with all coefficients non-negative.\\n\\nExamples\\n========\\n\\n>>> from sympy.liealgebras.cartan_type import CartanType\\n>>> c = CartanType(\"F4\")\\n>>> c.simple_root(3)\\n[0, 0, 0, 1]',\n",
       " 'Solves for the reaction forces.\\n\\nExamples\\n========\\nThere is a beam of length 30 meters. It it supported by rollers at\\nof its end. A constant distributed load of magnitude 8 N is applied\\nfrom start till its end along y-axis. Another linear load having\\nslope equal to 9 is applied along z-axis.\\n\\n>>> from sympy.physics.continuum_mechanics.beam import Beam3D\\n>>> from sympy import symbols\\n>>> l, E, G, I, A, x = symbols(\\'l, E, G, I, A, x\\')\\n>>> b = Beam3D(30, E, G, I, A, x)\\n>>> b.apply_load(8, start=0, order=0, dir=\"y\")\\n>>> b.apply_load(9*x, start=0, order=0, dir=\"z\")\\n>>> b.bc_deflection = [(0, [0, 0, 0]), (30, [0, 0, 0])]\\n>>> R1, R2, R3, R4 = symbols(\\'R1, R2, R3, R4\\')\\n>>> b.apply_load(R1, start=0, order=-1, dir=\"y\")\\n>>> b.apply_load(R2, start=30, order=-1, dir=\"y\")\\n>>> b.apply_load(R3, start=0, order=-1, dir=\"z\")\\n>>> b.apply_load(R4, start=30, order=-1, dir=\"z\")\\n>>> b.solve_for_reaction_loads(R1, R2, R3, R4)\\n>>> b.reaction_loads\\n{R1: -120, R2: -120, R3: -1350, R4: -2700}',\n",
       " 'Return list of all states.\\n\\nExamples\\n========\\n\\n>>> from sympy.physics.quantum.state import Ket\\n>>> from sympy.physics.quantum.density import Density\\n>>> d = Density([Ket(0), 0.5], [Ket(1),0.5])\\n>>> d.states()\\n(|0>, |1>)',\n",
       " 'Return True if ``self`` is the whole ring, i.e. one generator is a unit.\\n\\nExamples\\n========\\n\\n>>> from sympy.abc import x\\n>>> from sympy import QQ, ilex\\n>>> QQ.old_poly_ring(x).ideal(x).is_whole_ring()\\nFalse\\n>>> QQ.old_poly_ring(x).ideal(3).is_whole_ring()\\nTrue\\n>>> QQ.old_poly_ring(x, order=ilex).ideal(2 + x).is_whole_ring()\\nTrue',\n",
       " 'Primitive polynomial remainder sequence (PRS) in `K[x]`.\\n\\nExamples\\n========\\n\\n>>> from sympy.polys import ring, ZZ\\n>>> R, x = ring(\"x\", ZZ)\\n\\n>>> f = x**8 + x**6 - 3*x**4 - 3*x**3 + 8*x**2 + 2*x - 5\\n>>> g = 3*x**6 + 5*x**4 - 4*x**2 - 9*x + 21\\n\\n>>> prs = R.dup_primitive_prs(f, g)\\n\\n>>> prs[0]\\nx**8 + x**6 - 3*x**4 - 3*x**3 + 8*x**2 + 2*x - 5\\n>>> prs[1]\\n3*x**6 + 5*x**4 - 4*x**2 - 9*x + 21\\n>>> prs[2]\\n-5*x**4 + x**2 - 3\\n>>> prs[3]\\n13*x**2 + 25*x - 49\\n>>> prs[4]\\n4663*x - 6150\\n>>> prs[5]\\n1',\n",
       " 'Return postprocessed roots of specified kind. ',\n",
       " 'Return True if e is a Pow or is exp.',\n",
       " 'Returns a global instance of this class.\\n\\nThis method create a new instance if none have previously been created\\nand returns a previously created instance is one already exists.\\n\\nThe arguments and keyword arguments passed to this method are passed\\non to the :meth:`__init__` method of the class upon instantiation.\\n\\nExamples\\n--------\\nCreate a singleton class using instance, and retrieve it::\\n\\n    >>> from traitlets.config.configurable import SingletonConfigurable\\n    >>> class Foo(SingletonConfigurable): pass\\n    >>> foo = Foo.instance()\\n    >>> foo == Foo.instance()\\n    True\\n\\nCreate a subclass that is retrieved using the base class instance::\\n\\n    >>> class Bar(SingletonConfigurable): pass\\n    >>> class Bam(Bar): pass\\n    >>> bam = Bam.instance()\\n    >>> bam == Bar.instance()\\n    True',\n",
       " 'recv_streaming receives a binary message.',\n",
       " 'Property include_dashboard.',\n",
       " 'Formats a hexadecimal string like \"0x12B3\"',\n",
       " 'Checks the pre-condition that @string has the format that you would expect\\nof `leaf.value` where `leaf` is some Leaf such that `leaf.type ==\\ntoken.STRING`. A more precise description of the pre-conditions that are\\nchecked are listed below.\\n\\nPre-conditions:\\n    * @string starts with either \\', \", <prefix>\\', or <prefix>\" where\\n    `set(<prefix>)` is some subset of `set(STRING_PREFIX_CHARS)`.\\n    * @string ends with a quote character (\\' or \").\\n\\nRaises:\\n    AssertionError(...) if the pre-conditions listed above are not\\n    satisfied.',\n",
       " 'Call the update method over HTTP.\\n\\nArgs:\\n    request (~.compute.UpdateRegionAutoscalerRequest):\\n        The request object. A request message for\\n    RegionAutoscalers.Update. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Handle sys.monitoring.events.PY_START events.',\n",
       " 'Replace dict.pop() by a call to _PyDict_Pop().\\n        ',\n",
       " \">>> inst = CMultiplyNested()\\n>>> inst.f3(1, None)\\n2\\n>>> inst.f3(1, '__arg') # doctest: +IGNORE_EXCEPTION_DETAIL\\nTraceback (most recent call last):\\n...\\nTypeError:\\n>>> inst.f3(1, '_CMultiplyNested__arg')\\n2\",\n",
       " 'Reads a pointer value from the memory of the process.\\n\\n@see: L{read_pointer}\\n\\n@type  lpBaseAddress: int\\n@param lpBaseAddress: Memory address to begin reading.\\n\\n@rtype:  int\\n@return: Pointer value read from the process memory.\\n    Returns zero on error.',\n",
       " 'Determines if the given memory area is writeable.\\n\\n@note: Returns always C{False} for kernel mode addresses.\\n\\n@see: L{mquery}\\n\\n@type  address: int\\n@param address: Memory address.\\n\\n@type  size: int\\n@param size: Number of bytes. Must be greater than zero.\\n\\n@rtype:  bool\\n@return: C{True} if the memory area is writeable, C{False} otherwise.\\n\\n@raise ValueError: The size argument must be greater than zero.\\n@raise WindowsError: On error an exception is raised.',\n",
       " 'Convert a binary string into its base64 encoding, broken up into chunks\\nof chunksize characters separated by a separator.',\n",
       " 'Format a timezone for Git serialization.\\n\\nArgs:\\n  offset: Timezone offset as seconds difference to UTC\\n  unnecessary_negative_timezone: Whether to use a minus sign for\\n    UTC or positive timezones (-0000 and --700 rather than +0000 / +0700).',\n",
       " 'Parse and validate the --jobs argument.\\n\\n:param arg: The argument passed by argparse for validation',\n",
       " 'Do nothing.',\n",
       " 'Get the next line from the list.',\n",
       " \"Registers a template context processor function. These functions run before\\nrendering a template. The keys of the returned dict are added as variables\\navailable in the template.\\n\\nThis is available on both app and blueprint objects. When used on an app, this\\nis called for every rendered template. When used on a blueprint, this is called\\nfor templates rendered from the blueprint's views. To register with a blueprint\\nand affect every template, use :meth:`.Blueprint.app_context_processor`.\",\n",
       " 'Get bytes from the media.\\n\\nArgs:\\n  begin: int, offset from beginning of file.\\n  length: int, number of bytes to read, starting at begin.\\n\\nReturns:\\n  A string of bytes read. May be shorter than length if EOF was reached\\n  first.',\n",
       " \"Validates client's and credentials' universe domains are consistent.\\n\\nReturns:\\n    bool: True iff the configured universe domain is valid.\\n\\nRaises:\\n    ValueError: If the configured universe domain is not valid.\",\n",
       " 'Return a callable for the create key event method over gRPC.\\n\\nCreates a Key Event.\\n\\nReturns:\\n    Callable[[~.CreateKeyEventRequest],\\n            ~.KeyEvent]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Return a callable for the fetch automated ga4\\nconfiguration opt out method over gRPC.\\n\\nFetches the opt out status for the automated GA4\\nsetup process for a UA property.\\nNote: this has no effect on GA4 property.\\n\\nReturns:\\n    Callable[[~.FetchAutomatedGa4ConfigurationOptOutRequest],\\n            ~.FetchAutomatedGa4ConfigurationOptOutResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for list_transcripts\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ConferenceRecordsService server but before\\nit is returned to user code.',\n",
       " 'Return a callable for the update space method over gRPC.\\n\\n`Developer\\nPreview <https://developers.google.com/workspace/preview>`__.\\nUpdates a space.\\n\\nReturns:\\n    Callable[[~.UpdateSpaceRequest],\\n            ~.Space]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Post-rpc interceptor for delete_api\\n\\nOverride in a subclass to manipulate the response\\nafter it is returned by the ApiGatewayService server but before\\nit is returned to user code.',\n",
       " 'Create a dependency between two entities in the API\\nhub.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import apihub_v1\\n\\n    def sample_create_dependency():\\n        # Create a client\\n        client = apihub_v1.ApiHubDependenciesClient()\\n\\n        # Initialize request argument(s)\\n        dependency = apihub_v1.Dependency()\\n        dependency.consumer.operation_resource_name = \"operation_resource_name_value\"\\n        dependency.supplier.operation_resource_name = \"operation_resource_name_value\"\\n\\n        request = apihub_v1.CreateDependencyRequest(\\n            parent=\"parent_value\",\\n            dependency=dependency,\\n        )\\n\\n        # Make the request\\n        response = client.create_dependency(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.apihub_v1.types.CreateDependencyRequest, dict]):\\n        The request object. The\\n        [CreateDependency][google.cloud.apihub.v1.ApiHubDependencies.CreateDependency]\\n        method\\'s request.\\n    parent (str):\\n        Required. The parent resource for the dependency\\n        resource. Format:\\n        ``projects/{project}/locations/{location}``\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    dependency (google.cloud.apihub_v1.types.Dependency):\\n        Required. The dependency resource to\\n        create.\\n\\n        This corresponds to the ``dependency`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    dependency_id (str):\\n        Optional. The ID to use for the dependency resource,\\n        which will become the final component of the\\n        dependency\\'s resource name. This field is optional.\\n\\n        -  If provided, the same will be used. The service will\\n           throw an error if duplicate id is provided by the\\n           client.\\n        -  If not provided, a system generated id will be used.\\n\\n        This value should be 4-500 characters, and valid\\n        characters are ``[a-z][A-Z][0-9]-_``.\\n\\n        This corresponds to the ``dependency_id`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.apihub_v1.types.Dependency:\\n        A dependency resource defined in the API hub describes a dependency directed\\n           from a consumer to a supplier entity. A dependency\\n           can be defined between two\\n           [Operations][google.cloud.apihub.v1.Operation] or\\n           between an\\n           [Operation][google.cloud.apihub.v1.Operation] and\\n           [External API][google.cloud.apihub.v1.ExternalApi].',\n",
       " 'Creates an instance of this client using the provided credentials\\n    file.\\n\\nArgs:\\n    filename (str): The path to the service account private key json\\n        file.\\n    args: Additional arguments to pass to the constructor.\\n    kwargs: Additional arguments to pass to the constructor.\\n\\nReturns:\\n    HostProjectRegistrationServiceAsyncClient: The constructed client.',\n",
       " 'Return a callable for the get saved query method over gRPC.\\n\\nGets details about a saved query.\\n\\nReturns:\\n    Callable[[~.GetSavedQueryRequest],\\n            ~.SavedQuery]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Undeploys a model. If the model is not deployed this method has\\nno effect. If the model is currently being used:\\n\\n-  For article suggestion, article suggestion will fallback to\\n   the default model if model is undeployed.\\n\\nThis method is a `long-running\\noperation <https://cloud.google.com/dialogflow/es/docs/how/long-running-operations>`__.\\nThe returned ``Operation`` type has the following\\nmethod-specific fields:\\n\\n-  ``metadata``:\\n   [UndeployConversationModelOperationMetadata][google.cloud.dialogflow.v2.UndeployConversationModelOperationMetadata]\\n-  ``response``: An `Empty\\n   message <https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#empty>`__\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import dialogflow_v2\\n\\n    def sample_undeploy_conversation_model():\\n        # Create a client\\n        client = dialogflow_v2.ConversationModelsClient()\\n\\n        # Initialize request argument(s)\\n        request = dialogflow_v2.UndeployConversationModelRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        operation = client.undeploy_conversation_model(request=request)\\n\\n        print(\"Waiting for operation to complete...\")\\n\\n        response = operation.result()\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.dialogflow_v2.types.UndeployConversationModelRequest, dict]):\\n        The request object. The request message for\\n        [ConversationModels.UndeployConversationModel][google.cloud.dialogflow.v2.ConversationModels.UndeployConversationModel]\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.api_core.operation.Operation:\\n        An object representing a long-running operation.\\n\\n        The result type for the operation will be :class:`google.protobuf.empty_pb2.Empty` A generic empty message that you can re-use to avoid defining duplicated\\n           empty messages in your APIs. A typical example is to\\n           use it as the request or the response type of an API\\n           method. For instance:\\n\\n              service Foo {\\n                 rpc Bar(google.protobuf.Empty) returns\\n                 (google.protobuf.Empty);\\n\\n              }',\n",
       " 'Return a callable for the list saved queries method over gRPC.\\n\\nLists all saved queries in a parent\\nproject/folder/organization.\\n\\nReturns:\\n    Callable[[~.ListSavedQueriesRequest],\\n            ~.ListSavedQueriesResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the get location method over HTTP.\\n\\nArgs:\\n    request (locations_pb2.GetLocationRequest):\\n        The request object for GetLocation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    locations_pb2.Location: Response from GetLocation method.',\n",
       " 'Lists ClientConnectorServices in a given project and\\nlocation.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import beyondcorp_clientconnectorservices_v1\\n\\n    def sample_list_client_connector_services():\\n        # Create a client\\n        client = beyondcorp_clientconnectorservices_v1.ClientConnectorServicesServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = beyondcorp_clientconnectorservices_v1.ListClientConnectorServicesRequest(\\n            parent=\"parent_value\",\\n        )\\n\\n        # Make the request\\n        page_result = client.list_client_connector_services(request=request)\\n\\n        # Handle the response\\n        for response in page_result:\\n            print(response)\\n\\nArgs:\\n    request (Union[google.cloud.beyondcorp_clientconnectorservices_v1.types.ListClientConnectorServicesRequest, dict]):\\n        The request object. Message for requesting list of\\n        ClientConnectorServices.\\n    parent (str):\\n        Required. Parent value for\\n        ListClientConnectorServicesRequest.\\n\\n        This corresponds to the ``parent`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.beyondcorp_clientconnectorservices_v1.services.client_connector_services_service.pagers.ListClientConnectorServicesPager:\\n        Message for response to listing\\n        ClientConnectorServices.\\n        Iterating over this object will yield\\n        results and resolve additional pages\\n        automatically.',\n",
       " 'Deletes a data transfer configuration, including any\\nassociated transfer runs and logs.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import bigquery_datatransfer_v1\\n\\n    def sample_delete_transfer_config():\\n        # Create a client\\n        client = bigquery_datatransfer_v1.DataTransferServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = bigquery_datatransfer_v1.DeleteTransferConfigRequest(\\n            name=\"name_value\",\\n        )\\n\\n        # Make the request\\n        client.delete_transfer_config(request=request)\\n\\nArgs:\\n    request (Union[google.cloud.bigquery_datatransfer_v1.types.DeleteTransferConfigRequest, dict]):\\n        The request object. A request to delete data transfer\\n        information. All associated transfer\\n        runs and log messages will be deleted as\\n        well.\\n    name (str):\\n        Required. The field will contain name of the resource\\n        requested, for example:\\n        ``projects/{project_id}/transferConfigs/{config_id}`` or\\n        ``projects/{project_id}/locations/{location_id}/transferConfigs/{config_id}``\\n\\n        This corresponds to the ``name`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Call the insert method over HTTP.\\n\\nArgs:\\n    request (~.compute.InsertTargetTcpProxyRequest):\\n        The request object. A request message for\\n    TargetTcpProxies.Insert. See the method\\n    description for details.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.compute.Operation:\\n        Represents an Operation resource. Google Compute Engine\\n    has three Operation resources: \\\\*\\n    `Global </compute/docs/reference/rest/v1/globalOperations>`__\\n    \\\\*\\n    `Regional </compute/docs/reference/rest/v1/regionOperations>`__\\n    \\\\*\\n    `Zonal </compute/docs/reference/rest/v1/zoneOperations>`__\\n    You can use an operation resource to manage asynchronous\\n    API requests. For more information, read Handling API\\n    responses. Operations can be global, regional or zonal.\\n    - For global operations, use the ``globalOperations``\\n    resource. - For regional operations, use the\\n    ``regionOperations`` resource. - For zonal operations,\\n    use the ``zoneOperations`` resource. For more\\n    information, read Global, Regional, and Zonal Resources.\\n    Note that completed Operation resources have a limited\\n    retention period.',\n",
       " 'Run the assertSessionPinned test operation.\\n\\nAssert that the given session is pinned.',\n",
       " 'Return the API endpoint used by the client.\\n\\nArgs:\\n    api_override (str): The API endpoint override. If specified, this is always\\n        the return value of this function and the other arguments are not used.\\n    client_cert_source (bytes): The client certificate source used by the client.\\n    universe_domain (str): The universe domain used by the client.\\n    use_mtls_endpoint (str): How to use the mTLS endpoint, which depends also on the other parameters.\\n        Possible values are \"always\", \"auto\", or \"never\".\\n\\nReturns:\\n    str: The API endpoint to be used by the client.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Call the delete connection method over HTTP.\\n\\nArgs:\\n    request (~.developer_connect.DeleteConnectionRequest):\\n        The request object. Message for deleting a Connection\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.',\n",
       " 'Call the cancel operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.CancelOperationRequest):\\n        The request object for CancelOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Return a callable for the streaming analyze content method over gRPC.\\n\\nAdds a text (chat, for example), or audio (phone recording, for\\nexample) message from a participant into the conversation. Note:\\nThis method is only available through the gRPC API (not REST).\\n\\nThe top-level message sent to the client by the server is\\n``StreamingAnalyzeContentResponse``. Multiple response messages\\ncan be returned in order. The first one or more messages contain\\nthe ``recognition_result`` field. Each result represents a more\\ncomplete transcript of what the user said. The next message\\ncontains the ``reply_text`` field and potentially the\\n``reply_audio`` field. The message can also contain the\\n``automated_agent_reply`` field.\\n\\nNote: Always use agent versions for production traffic sent to\\nvirtual agents. See `Versions and\\nenvironments <https://cloud.google.com/dialogflow/es/docs/agents-versions>`__.\\n\\nReturns:\\n    Callable[[~.StreamingAnalyzeContentRequest],\\n            ~.StreamingAnalyzeContentResponse]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Move cursor to the column specified by the zero-based column index, staying on the same row\\n\\nArgs:\\n    column (int): The zero-based column index to move the cursor to.',\n",
       " 'Return a callable for the get network method over gRPC.\\n\\nGets details of a single Network.\\n\\nReturns:\\n    Callable[[~.GetNetworkRequest],\\n            ~.Network]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Returns True iff the universe domains used by the client and credentials match.\\n\\nArgs:\\n    client_universe (str): The universe domain configured via the client options.\\n    credentials (ga_credentials.Credentials): The credentials being used in the client.\\n\\nReturns:\\n    bool: True iff client_universe matches the universe in credentials.\\n\\nRaises:\\n    ValueError: when client_universe does not match the universe in credentials.',\n",
       " 'Instantiates the cloud filestore manager client.\\n\\nArgs:\\n    credentials (Optional[google.auth.credentials.Credentials]): The\\n        authorization credentials to attach to requests. These\\n        credentials identify the application to the service; if none\\n        are specified, the client will attempt to ascertain the\\n        credentials from the environment.\\n    transport (Optional[Union[str,CloudFilestoreManagerTransport,Callable[..., CloudFilestoreManagerTransport]]]):\\n        The transport to use, or a Callable that constructs and returns a new transport.\\n        If a Callable is given, it will be called with the same set of initialization\\n        arguments as used in the CloudFilestoreManagerTransport constructor.\\n        If set to None, a transport is chosen automatically.\\n    client_options (Optional[Union[google.api_core.client_options.ClientOptions, dict]]):\\n        Custom options for the client.\\n\\n        1. The ``api_endpoint`` property can be used to override the\\n        default endpoint provided by the client when ``transport`` is\\n        not explicitly provided. Only if this property is not set and\\n        ``transport`` was not explicitly provided, the endpoint is\\n        determined by the GOOGLE_API_USE_MTLS_ENDPOINT environment\\n        variable, which have one of the following values:\\n        \"always\" (always use the default mTLS endpoint), \"never\" (always\\n        use the default regular endpoint) and \"auto\" (auto-switch to the\\n        default mTLS endpoint if client certificate is present; this is\\n        the default value).\\n\\n        2. If the GOOGLE_API_USE_CLIENT_CERTIFICATE environment variable\\n        is \"true\", then the ``client_cert_source`` property can be used\\n        to provide a client certificate for mTLS transport. If\\n        not provided, the default SSL client certificate will be used if\\n        present. If GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\" or not\\n        set, no client certificate will be used.\\n\\n        3. The ``universe_domain`` property can be used to override the\\n        default \"googleapis.com\" universe. Note that the ``api_endpoint``\\n        property still takes precedence; and ``universe_domain`` is\\n        currently not supported for mTLS.\\n\\n    client_info (google.api_core.gapic_v1.client_info.ClientInfo):\\n        The client info used to send a user-agent string along with\\n        API requests. If ``None``, then default info will be used.\\n        Generally, you only need to set this if you\\'re developing\\n        your own client library.\\n\\nRaises:\\n    google.auth.exceptions.MutualTLSChannelError: If mutual TLS transport\\n        creation failed for any reason.',\n",
       " 'Call the list aws clusters method over HTTP.\\n\\nArgs:\\n    request (~.aws_service.ListAwsClustersRequest):\\n        The request object. Request message for ``AwsClusters.ListAwsClusters``\\n    method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.aws_service.ListAwsClustersResponse:\\n        Response message for ``AwsClusters.ListAwsClusters``\\n    method.',\n",
       " 'Call the get report method over HTTP.\\n\\nArgs:\\n    request (~.migrationcenter.GetReportRequest):\\n        The request object. A request to get a Report.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.migrationcenter.Report:\\n        Report represents a point-in-time\\n    rendering of the ReportConfig results.',\n",
       " 'Return a callable for the update authorization policy method over gRPC.\\n\\nUpdates the parameters of a single\\nAuthorizationPolicy.\\n\\nReturns:\\n    Callable[[~.UpdateAuthorizationPolicyRequest],\\n            ~.Operation]:\\n        A function that, when called, will call the underlying RPC\\n        on the server.',\n",
       " 'Call the list user workloads\\nconfig maps method over HTTP.\\n\\n    Args:\\n        request (~.environments.ListUserWorkloadsConfigMapsRequest):\\n            The request object. List user workloads ConfigMaps\\n        request.\\n        retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n            should be retried.\\n        timeout (float): The timeout for this request.\\n        metadata (Sequence[Tuple[str, str]]): Strings which should be\\n            sent along with the request as metadata.\\n\\n    Returns:\\n        ~.environments.ListUserWorkloadsConfigMapsResponse:\\n            The user workloads ConfigMaps for a\\n        given environment.',\n",
       " 'Return bytes from string.',\n",
       " 'Call the delete operation method over HTTP.\\n\\nArgs:\\n    request (operations_pb2.DeleteOperationRequest):\\n        The request object for DeleteOperation method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " 'Parses a merchant_center_account_link path into its component segments.',\n",
       " 'Instantiate the pager.\\n\\nArgs:\\n    method (Callable): The method that was originally called, and\\n        which instantiated this pager.\\n    request (google.cloud.visionai_v1.types.ListApplicationsRequest):\\n        The initial request object.\\n    response (google.cloud.visionai_v1.types.ListApplicationsResponse):\\n        The initial response object.\\n    retry (google.api_core.retry.Retry): Designation of what errors,\\n        if any, should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.',\n",
       " \"Call the delete target project method over HTTP.\\n\\nArgs:\\n    request (~.vmmigration.DeleteTargetProjectRequest):\\n        The request object. Request message for\\n    'DeleteTargetProject' request.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.operations_pb2.Operation:\\n        This resource represents a\\n    long-running operation that is the\\n    result of a network API call.\",\n",
       " \"Gets the full hashes that match the requested hash\\nprefix. This is used after a hash prefix is looked up in\\na threatList and there is a match. The client side\\nthreatList only holds partial hashes so the client must\\nquery this method to determine if there is a full hash\\nmatch of a threat.\\n\\n.. code-block:: python\\n\\n    # This snippet has been automatically generated and should be regarded as a\\n    # code template only.\\n    # It will require modifications to work:\\n    # - It may require correct/in-range values for request initialization.\\n    # - It may require specifying regional endpoints when creating the service\\n    #   client as shown in:\\n    #   https://googleapis.dev/python/google-api-core/latest/client_options.html\\n    from google.cloud import webrisk_v1\\n\\n    def sample_search_hashes():\\n        # Create a client\\n        client = webrisk_v1.WebRiskServiceClient()\\n\\n        # Initialize request argument(s)\\n        request = webrisk_v1.SearchHashesRequest(\\n            threat_types=['SOCIAL_ENGINEERING_EXTENDED_COVERAGE'],\\n        )\\n\\n        # Make the request\\n        response = client.search_hashes(request=request)\\n\\n        # Handle the response\\n        print(response)\\n\\nArgs:\\n    request (Union[google.cloud.webrisk_v1.types.SearchHashesRequest, dict]):\\n        The request object. Request to return full hashes matched\\n        by the provided hash prefixes.\\n    hash_prefix (bytes):\\n        A hash prefix, consisting of the most\\n        significant 4-32 bytes of a SHA256 hash.\\n        For JSON requests, this field is\\n        base64-encoded. Note that if this\\n        parameter is provided by a URI, it must\\n        be encoded using the web safe base64\\n        variant (RFC 4648).\\n\\n        This corresponds to the ``hash_prefix`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    threat_types (MutableSequence[google.cloud.webrisk_v1.types.ThreatType]):\\n        Required. The ThreatLists to search\\n        in. Multiple ThreatLists may be\\n        specified.\\n\\n        This corresponds to the ``threat_types`` field\\n        on the ``request`` instance; if ``request`` is provided, this\\n        should not be set.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    google.cloud.webrisk_v1.types.SearchHashesResponse:\",\n",
       " 'Call the get file upload method over HTTP.\\n\\nArgs:\\n    request (~.fileuploads.GetFileUploadRequest):\\n        The request object. Request message for the\\n    GetFileUploadRequest method.\\n    retry (google.api_core.retry.Retry): Designation of what errors, if any,\\n        should be retried.\\n    timeout (float): The timeout for this request.\\n    metadata (Sequence[Tuple[str, str]]): Strings which should be\\n        sent along with the request as metadata.\\n\\nReturns:\\n    ~.fileuploads.FileUpload:\\n        The file upload of a specific data\\n    source, that is, the result of the\\n    retrieval of the data source at a\\n    certain timestamp computed\\n    asynchronously when the data source\\n    processing is finished. Only applicable\\n    to file data sources.',\n",
       " 'get list of all loggers ',\n",
       " 'Send a `DELETE` request.\\n\\n**Parameters**: See `httpx.request`.',\n",
       " 'Create the key vault collection with optional data keys.',\n",
       " ':param position: Position of the last statement -> tuple of line, column',\n",
       " 'Return the formatted representation of the object.',\n",
       " 'Return the boxstyle object.',\n",
       " 'Update the slider position.',\n",
       " 'Weighted betweenness centrality: P3',\n",
       " 'Returns the resistance distance between pairs of nodes in graph G.\\n\\nThe resistance distance between two nodes of a graph is akin to treating\\nthe graph as a grid of resistors with a resistance equal to the provided\\nweight [1]_, [2]_.\\n\\nIf weight is not provided, then a weight of 1 is used for all edges.\\n\\nIf two nodes are the same, the resistance distance is zero.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n   A graph\\n\\nnodeA : node or None, optional (default=None)\\n  A node within graph G.\\n  If None, compute resistance distance using all nodes as source nodes.\\n\\nnodeB : node or None, optional (default=None)\\n  A node within graph G.\\n  If None, compute resistance distance using all nodes as target nodes.\\n\\nweight : string or None, optional (default=None)\\n   The edge data key used to compute the resistance distance.\\n   If None, then each edge has weight 1.\\n\\ninvert_weight : boolean (default=True)\\n    Proper calculation of resistance distance requires building the\\n    Laplacian matrix with the reciprocal of the weight. Not required\\n    if the weight is already inverted. Weight cannot be zero.\\n\\nReturns\\n-------\\nrd : dict or float\\n   If `nodeA` and `nodeB` are given, resistance distance between `nodeA`\\n   and `nodeB`. If `nodeA` or `nodeB` is unspecified (the default), a\\n   dictionary of nodes with resistance distances as the value.\\n\\nRaises\\n------\\nNetworkXNotImplemented\\n    If `G` is a directed graph.\\n\\nNetworkXError\\n    If `G` is not connected, or contains no nodes,\\n    or `nodeA` is not in `G` or `nodeB` is not in `G`.\\n\\nExamples\\n--------\\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\\n>>> round(nx.resistance_distance(G, 1, 3), 10)\\n0.625\\n\\nNotes\\n-----\\nThe implementation is based on Theorem A in [2]_. Self-loops are ignored.\\nMulti-edges are contracted in one edge with weight equal to the harmonic sum of the weights.\\n\\nReferences\\n----------\\n.. [1] Wikipedia\\n   \"Resistance distance.\"\\n   https://en.wikipedia.org/wiki/Resistance_distance\\n.. [2] D. J. Klein and M. Randic.\\n    Resistance distance.\\n    J. of Math. Chem. 12:81-95, 1993.',\n",
       " 'Find shortest weighted path lengths in G from a given set of\\nsource nodes.\\n\\nCompute the shortest path length between any of the source nodes and\\nall other reachable nodes for a weighted graph.\\n\\nParameters\\n----------\\nG : NetworkX graph\\n\\nsources : non-empty set of nodes\\n    Starting nodes for paths. If this is just a set containing a\\n    single node, then all paths computed by this function will start\\n    from that node. If there are two or more nodes in the set, the\\n    computed paths may begin from any one of the start nodes.\\n\\ncutoff : integer or float, optional\\n    Length (sum of edge weights) at which the search is stopped.\\n    If cutoff is provided, only return paths with summed weight <= cutoff.\\n\\nweight : string or function\\n    If this is a string, then edge weights will be accessed via the\\n    edge attribute with this key (that is, the weight of the edge\\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\\n    such edge attribute exists, the weight of the edge is assumed to\\n    be one.\\n\\n    If this is a function, the weight of an edge is the value\\n    returned by the function. The function must accept exactly three\\n    positional arguments: the two endpoints of an edge and the\\n    dictionary of edge attributes for that edge. The function must\\n    return a number or None to indicate a hidden edge.\\n\\nReturns\\n-------\\nlength : dict\\n    Dict keyed by node to shortest path length to nearest source.\\n\\nExamples\\n--------\\n>>> G = nx.path_graph(5)\\n>>> length = nx.multi_source_dijkstra_path_length(G, {0, 4})\\n>>> for node in [0, 1, 2, 3, 4]:\\n...     print(f\"{node}: {length[node]}\")\\n0: 0\\n1: 1\\n2: 2\\n3: 1\\n4: 0\\n\\nNotes\\n-----\\nEdge weight attributes must be numerical.\\nDistances are calculated as sums of weighted edges traversed.\\n\\nThe weight function can be used to hide edges by returning None.\\nSo ``weight = lambda u, v, d: 1 if d[\\'color\\']==\"red\" else None``\\nwill find the shortest red path.\\n\\nRaises\\n------\\nValueError\\n    If `sources` is empty.\\nNodeNotFound\\n    If any of `sources` is not in `G`.\\n\\nSee Also\\n--------\\nmulti_source_dijkstra',\n",
       " 'Query the minimum key-value pair.\\n\\nReturns\\n-------\\nkey, value : tuple\\n    The key-value pair with the minimum value in the heap.\\n\\nRaises\\n------\\nNetworkXError\\n    If the heap is empty.',\n",
       " 'Pseudo-Vandermonde matrix of given degree.\\n\\nReturns the pseudo-Vandermonde matrix of degree `deg` and sample points\\n`x`. The pseudo-Vandermonde matrix is defined by\\n\\n.. math:: V[..., i] = H_i(x),\\n\\nwhere ``0 <= i <= deg``. The leading indices of `V` index the elements of\\n`x` and the last index is the degree of the Hermite polynomial.\\n\\nIf `c` is a 1-D array of coefficients of length ``n + 1`` and `V` is the\\narray ``V = hermvander(x, n)``, then ``np.dot(V, c)`` and\\n``hermval(x, c)`` are the same up to roundoff. This equivalence is\\nuseful both for least squares fitting and for the evaluation of a large\\nnumber of Hermite series of the same degree and sample points.\\n\\nParameters\\n----------\\nx : array_like\\n    Array of points. The dtype is converted to float64 or complex128\\n    depending on whether any of the elements are complex. If `x` is\\n    scalar it is converted to a 1-D array.\\ndeg : int\\n    Degree of the resulting matrix.\\n\\nReturns\\n-------\\nvander : ndarray\\n    The pseudo-Vandermonde matrix. The shape of the returned matrix is\\n    ``x.shape + (deg + 1,)``, where The last index is the degree of the\\n    corresponding Hermite polynomial.  The dtype will be the same as\\n    the converted `x`.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from numpy.polynomial.hermite import hermvander\\n>>> x = np.array([-1, 0, 1])\\n>>> hermvander(x, 3)\\narray([[ 1., -2.,  2.,  4.],\\n       [ 1.,  0., -2., -0.],\\n       [ 1.,  2.,  2., -4.]])',\n",
       " 'Get wheel names from Anaconda HTML directory.\\n\\nThis looks in the Anaconda multibuild-wheels-staging page and\\nparses the HTML to get all the wheel names for a release version.\\n\\nParameters\\n----------\\nversion : str\\n    The release version. For instance, \"1.18.3\".',\n",
       " 'Return a string representation for this object.',\n",
       " 'Pop the header name for MultiIndex parsing.\\n\\nParameters\\n----------\\nrow : list\\n    The data row to parse for the header name.\\nindex_col : int, list\\n    The index columns for our data. Assumed to be non-null.\\n\\nReturns\\n-------\\nheader_name : str\\n    The extracted header name.\\ntrimmed_row : list\\n    The original data row with the header name removed.',\n",
       " 'Perform serialization. Write to buf or return as string if buf is None.',\n",
       " 'Check if a line is empty or not.\\n\\nParameters\\n----------\\nline : str, array-like\\n    The line of data to check.\\n\\nReturns\\n-------\\nboolean : Whether or not the line is empty.',\n",
       " 'Pickle (serialize) object to file.\\n\\nParameters\\n----------\\nobj : any object\\n    Any python object.\\nfilepath_or_buffer : str, path object, or file-like object\\n    String, path object (implementing ``os.PathLike[str]``), or file-like\\n    object implementing a binary ``write()`` function.\\n    Also accepts URL. URL has to be of S3 or GCS.\\n{compression_options}\\n\\n    .. versionchanged:: 1.4.0 Zstandard support.\\n\\nprotocol : int\\n    Int which indicates which protocol should be used by the pickler,\\n    default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible\\n    values for this parameter depend on the version of Python. For Python\\n    2.x, possible values are 0, 1, 2. For Python>=3.0, 3 is a valid value.\\n    For Python >= 3.4, 4 is a valid value. A negative value for the\\n    protocol parameter is equivalent to setting its value to\\n    HIGHEST_PROTOCOL.\\n\\n{storage_options}\\n\\n    .. [1] https://docs.python.org/3/library/pickle.html\\n\\nSee Also\\n--------\\nread_pickle : Load pickled pandas object (or any object) from file.\\nDataFrame.to_hdf : Write DataFrame to an HDF5 file.\\nDataFrame.to_sql : Write DataFrame to a SQL database.\\nDataFrame.to_parquet : Write a DataFrame to the binary parquet format.\\n\\nExamples\\n--------\\n>>> original_df = pd.DataFrame(\\n...     {{\"foo\": range(5), \"bar\": range(5, 10)}}\\n... )  # doctest: +SKIP\\n>>> original_df  # doctest: +SKIP\\n   foo  bar\\n0    0    5\\n1    1    6\\n2    2    7\\n3    3    8\\n4    4    9\\n>>> pd.to_pickle(original_df, \"./dummy.pkl\")  # doctest: +SKIP\\n\\n>>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")  # doctest: +SKIP\\n>>> unpickled_df  # doctest: +SKIP\\n   foo  bar\\n0    0    5\\n1    1    6\\n2    2    7\\n3    3    8\\n4    4    9',\n",
       " 'Test spawn.read by calls of various size. ',\n",
       " 'A string that uniquely identifies this requirement to the build tracker.\\n\\nIf None, then this dist has no work to do in the build tracker, and\\n``.prepare_distribution_metadata()`` will not be called.',\n",
       " 'Raw Requires-Dist metadata.',\n",
       " 'Set requirement after generating metadata.',\n",
       " 'Stop thread execution and and waits until it is stopped.',\n",
       " 'The password.',\n",
       " ':calls: `GET https://api.github.com/repos/{owner}/{repo}/code-scanning/alerts <https://docs.github.com/en/rest/reference/code-scanning#list-code-scanning-alerts-for-a-repository>`_\\n:rtype: :class:`PaginatedList` of :class:`github.CodeScanAlert.CodeScanAlert`',\n",
       " 'Given a list of format specifiers, returns\\nthe final access path (e.g. a.b.c[0][1]).',\n",
       " 'Access to protected instance members of other classes is not OK.',\n",
       " \"'msg' is not defined in one handler.\",\n",
       " 'Confirm we can create `OpenSSL.crypto.CRL`.  Check\\nthat it is empty',\n",
       " 'Add a new report section, similar to what\\'s done internally to add\\nstdout and stderr captured output::\\n\\n    item.add_report_section(\"call\", \"stdout\", \"report section contents\")\\n\\n:param str when:\\n    One of the possible capture states, ``\"setup\"``, ``\"call\"``, ``\"teardown\"``.\\n:param str key:\\n    Name of the section, can be customized at will. Pytest uses ``\"stdout\"`` and\\n    ``\"stderr\"`` internally.\\n:param str content:\\n    The full contents as a string.',\n",
       " 'group = display-name \":\" [group-list] \";\" [CFWS]\\n\\n    ',\n",
       " \"'*' digits\\n\\nThe formal BNF is more complicated because leading 0s are not allowed.  We\\ncheck for that and add a defect.  We also assume no CFWS is allowed between\\nthe '*' and the digits, though the RFC is not crystal clear on that.\\nThe caller should already have dealt with leading CFWS.\",\n",
       " 'Tests whether the internal is_py2_stdlib_module function (called by the\\nsys.modules scrubbing functions) is reliable.',\n",
       " \"Resolves a pointer against doc and sets the value of the target within doc.\\n\\nWith inplace set to true, doc is modified as long as pointer is not the\\nroot.\\n\\n>>> obj = {'foo': {'anArray': [ {'prop': 44}], 'another prop': {'baz': 'A string' }}}\\n\\n>>> set_pointer(obj, '/foo/anArray/0/prop', 55) ==     {'foo': {'another prop': {'baz': 'A string'}, 'anArray': [{'prop': 55}]}}\\nTrue\\n\\n>>> set_pointer(obj, '/foo/yet another prop', 'added prop') ==     {'foo': {'another prop': {'baz': 'A string'}, 'yet another prop': 'added prop', 'anArray': [{'prop': 55}]}}\\nTrue\\n\\n>>> obj = {'foo': {}}\\n>>> set_pointer(obj, '/foo/a%20b', 'x') ==     {'foo': {'a%20b': 'x' }}\\nTrue\",\n",
       " \"Parameters\\n----------\\nn : integer\\n    Number of variates to generate\\nshape : iterable\\n    Shape of the variates to generate\\ndim : int\\n    Dimension of the scale matrix\\ndf : int\\n    Degrees of freedom\\nrandom_state : {None, int, `numpy.random.Generator`,\\n                `numpy.random.RandomState`}, optional\\n\\n    If `seed` is None (or `np.random`), the `numpy.random.RandomState`\\n    singleton is used.\\n    If `seed` is an int, a new ``RandomState`` instance is used,\\n    seeded with `seed`.\\n    If `seed` is already a ``Generator`` or ``RandomState`` instance\\n    then that instance is used.\\n\\nNotes\\n-----\\nAs this function does no argument checking, it should not be\\ncalled directly; use 'rvs' instead.\",\n",
       " 'Serializes and uploads the user-provided EMR application configuration to S3.\\n\\nThis method prepares an input channel.\\n\\nArgs:\\n    configuration (Dict): the configuration dict for the EMR application configuration.',\n",
       " 'patch_namespaced_horizontal_pod_autoscaler  # noqa: E501\\n\\npartially update the specified HorizontalPodAutoscaler  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.patch_namespaced_horizontal_pod_autoscaler_with_http_info(name, namespace, body, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str name: name of the HorizontalPodAutoscaler (required)\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param object body: (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\\n:param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default in v1.23+ - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\\n:param bool force: Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\\n:param _return_http_data_only: response data without head status code\\n                               and headers\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: tuple(V2HorizontalPodAutoscaler, status_code(int), headers(HTTPHeaderDict))\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'delete_collection_namespaced_config_map  # noqa: E501\\n\\ndelete collection of ConfigMap  # noqa: E501\\nThis method makes a synchronous HTTP request by default. To make an\\nasynchronous HTTP request, please pass async_req=True\\n>>> thread = api.delete_collection_namespaced_config_map(namespace, async_req=True)\\n>>> result = thread.get()\\n\\n:param async_req bool: execute request asynchronously\\n:param str namespace: object name and auth scope, such as for teams and projects (required)\\n:param str pretty: If \\'true\\', then the output is pretty printed. Defaults to \\'false\\' unless the user-agent indicates a browser or command-line HTTP tool (curl and wget).\\n:param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \"next key\".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.\\n:param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\\n:param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\\n:param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\\n:param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\\n:param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.\\n:param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object\\'s finalizers list. Either this field or PropagationPolicy may be set, but not both.\\n:param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: \\'Orphan\\' - orphan the dependents; \\'Background\\' - allow the garbage collector to delete the dependents in the background; \\'Foreground\\' - a cascading policy that deletes all dependents in the foreground.\\n:param str resource_version: resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param str resource_version_match: resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.  Defaults to unset\\n:param bool send_initial_events: `sendInitialEvents=true` may be set together with `watch=true`. In that case, the watch stream will begin with synthetic events to produce the current state of objects in the collection. Once all such events have been sent, a synthetic \"Bookmark\" event  will be sent. The bookmark will report the ResourceVersion (RV) corresponding to the set of objects, and be marked with `\"k8s.io/initial-events-end\": \"true\"` annotation. Afterwards, the watch stream will proceed as usual, sending watch events corresponding to changes (subsequent to the RV) to objects watched.  When `sendInitialEvents` option is set, we require `resourceVersionMatch` option to also be set. The semantic of the watch request is as following: - `resourceVersionMatch` = NotOlderThan   is interpreted as \"data at least as new as the provided `resourceVersion`\"   and the bookmark event is send when the state is synced   to a `resourceVersion` at least as fresh as the one provided by the ListOptions.   If `resourceVersion` is unset, this is interpreted as \"consistent read\" and the   bookmark event is send when the state is synced at least to the moment   when request started being processed. - `resourceVersionMatch` set to any other value or unset   Invalid error is returned.  Defaults to true if `resourceVersion=\"\"` or `resourceVersion=\"0\"` (for backward compatibility reasons) and to false otherwise.\\n:param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.\\n:param V1DeleteOptions body:\\n:param _preload_content: if False, the urllib3.HTTPResponse object will\\n                         be returned without reading/decoding response\\n                         data. Default is True.\\n:param _request_timeout: timeout setting for this request. If one\\n                         number provided, it will be total request\\n                         timeout. It can also be a pair (tuple) of\\n                         (connection, read) timeouts.\\n:return: V1Status\\n         If the method is called asynchronously,\\n         returns the request thread.',\n",
       " 'Gets the reporting_controller of this EventsV1Event.  # noqa: E501\\n\\nreportingController is the name of the controller that emitted this Event, e.g. `kubernetes.io/kubelet`. This field cannot be empty for new Events.  # noqa: E501\\n\\n:return: The reporting_controller of this EventsV1Event.  # noqa: E501\\n:rtype: str',\n",
       " 'Gets the current_replicas of this V1HorizontalPodAutoscalerStatus.  # noqa: E501\\n\\ncurrentReplicas is the current number of replicas of pods managed by this autoscaler.  # noqa: E501\\n\\n:return: The current_replicas of this V1HorizontalPodAutoscalerStatus.  # noqa: E501\\n:rtype: int',\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['docstring'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieves the Settings for the Project .<n>Callable[.GetProjectSettingsRequest], .ProjectSettings]: A function that, when called, will call the underlying RPC on the server .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docstring: Create a representation of an instance of Media.<n> Returns string, a representation of this instance, suitable to pass to from_Upload .<n>Media.Upload is an instance of Media.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "  Using cached tables-3.10.1-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tables) (2.1.3)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in c:\\users\\maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tables) (2.10.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tables) (24.1)\n",
      "Collecting py-cpuinfo (from tables)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting blosc2>=2.3.0 (from tables)\n",
      "  Using cached blosc2-2.7.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tables) (4.12.2)\n",
      "Collecting ndindex>=1.4 (from blosc2>=2.3.0->tables)\n",
      "  Using cached ndindex-1.9.2-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: msgpack in c:\\users\\maciej\\miniconda3\\envs\\pytorch\\lib\\site-packages (from blosc2>=2.3.0->tables) (1.1.0)\n",
      "Using cached tables-3.10.1-cp310-cp310-win_amd64.whl (6.3 MB)\n",
      "Using cached blosc2-2.7.1-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached ndindex-1.9.2-cp310-cp310-win_amd64.whl (159 kB)\n",
      "Installing collected packages: py-cpuinfo, ndindex, blosc2, tables\n",
      "Successfully installed blosc2-2.7.1 ndindex-1.9.2 py-cpuinfo-9.0.0 tables-3.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>docsstring gets log stream for a service .&lt;n&gt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Apply the chain's changes and write the final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>docstring: Create a representation of an insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>.resources.AdSenseLink: A link between a GA4 P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Returns the universe domain used by the client...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60541</th>\n",
       "      <td>The expression is rewritten internally in term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60542</th>\n",
       "      <td>parentheses optional in some cases for functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60543</th>\n",
       "      <td>Returns a plot for Bending moment present in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60544</th>\n",
       "      <td>sT := sreprTest from sympy/printing/tests/test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51519</th>\n",
       "      <td>summarize to docstring: Returns GCD of f and g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         short_docstring\n",
       "448    docsstring gets log stream for a service .<n>b...\n",
       "434    Apply the chain's changes and write the final ...\n",
       "437    docstring: Create a representation of an insta...\n",
       "449    .resources.AdSenseLink: A link between a GA4 P...\n",
       "451    Returns the universe domain used by the client...\n",
       "...                                                  ...\n",
       "60541  The expression is rewritten internally in term...\n",
       "60542  parentheses optional in some cases for functio...\n",
       "60543  Returns a plot for Bending moment present in t...\n",
       "60544  sT := sreprTest from sympy/printing/tests/test...\n",
       "51519  summarize to docstring: Returns GCD of f and g...\n",
       "\n",
       "[60000 rows x 1 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_docstring = pd.DataFrame(outputs, columns=['short_docstring'], index=values.index)\n",
    "\n",
    "# save to h5 file\n",
    "short_docstring.to_hdf('short_docstring.h5', key='short_docstring', mode='w')\n",
    "\n",
    "short_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "      <th>args_types</th>\n",
       "      <th>args_defaults</th>\n",
       "      <th>body</th>\n",
       "      <th>docstring</th>\n",
       "      <th>id</th>\n",
       "      <th>short_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>54432</td>\n",
       "      <td>179</td>\n",
       "      <td>service_logs</td>\n",
       "      <td>{self,service,details,follow,stdout,stderr,sin...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{False,False,False,False,0,False,'all',None}</td>\n",
       "      <td>params = {'details': details, 'follow': follow...</td>\n",
       "      <td>Get log stream for a service.\\nNote: This endp...</td>\n",
       "      <td>258</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>56509</td>\n",
       "      <td>138</td>\n",
       "      <td>delta_list_apply</td>\n",
       "      <td>{dcl,bbuf,write}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...</td>\n",
       "      <td>Apply the chain's changes and write the final ...</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>56635</td>\n",
       "      <td>169</td>\n",
       "      <td>to_json</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self._to_json()</td>\n",
       "      <td>Create a JSON representation of an instance of...</td>\n",
       "      <td>266</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>57434</td>\n",
       "      <td>276</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self,request}</td>\n",
       "      <td>{analytics_admin.CreateAdSenseLinkRequest}</td>\n",
       "      <td>{}</td>\n",
       "      <td>http_options = _BaseAnalyticsAdminServiceRestT...</td>\n",
       "      <td>Call the create ad sense link method over HTTP...</td>\n",
       "      <td>269</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>125</td>\n",
       "      <td>276</td>\n",
       "      <td>_get_universe_domain</td>\n",
       "      <td>{client_universe_domain,universe_domain_env}</td>\n",
       "      <td>{Optional[str],Optional[str]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>universe_domain = ApiKeysClient._DEFAULT_UNIVE...</td>\n",
       "      <td>Return the universe domain used by the client....</td>\n",
       "      <td>279</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60541</th>\n",
       "      <td>46700</td>\n",
       "      <td>286</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>orders = (covariant_order(e) + contravariant_o...</td>\n",
       "      <td>Apply on a list of vector_fields.\\nThe express...</td>\n",
       "      <td>275727</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60542</th>\n",
       "      <td>47080</td>\n",
       "      <td>286</td>\n",
       "      <td>implicit_application</td>\n",
       "      <td>{tokens,local_dict,global_dict}</td>\n",
       "      <td>{List[TOKEN],DICT,DICT}</td>\n",
       "      <td>{}</td>\n",
       "      <td>res1 = _group_parentheses(implicit_application...</td>\n",
       "      <td>Makes parentheses optional in some cases for f...</td>\n",
       "      <td>275737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60543</th>\n",
       "      <td>47110</td>\n",
       "      <td>286</td>\n",
       "      <td>plot_bending_moment</td>\n",
       "      <td>{self,subs}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{None}</td>\n",
       "      <td>bending_moment = self.bending_moment()\\nif sub...</td>\n",
       "      <td>Returns a plot for Bending moment present in t...</td>\n",
       "      <td>275739</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60544</th>\n",
       "      <td>47248</td>\n",
       "      <td>286</td>\n",
       "      <td>sT</td>\n",
       "      <td>{expr,string}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>assert srepr(expr) == string\\nassert eval(stri...</td>\n",
       "      <td>sT := sreprTest\\nfrom sympy/printing/tests/tes...</td>\n",
       "      <td>275740</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51519</th>\n",
       "      <td>47479</td>\n",
       "      <td>286</td>\n",
       "      <td>cofactors</td>\n",
       "      <td>{f,g}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>(F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)</td>\n",
       "      <td>Returns GCD of ``f`` and ``g`` and their cofac...</td>\n",
       "      <td>234415</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id  repo                  name  \\\n",
       "448      54432   179          service_logs   \n",
       "434      56509   138      delta_list_apply   \n",
       "437      56635   169               to_json   \n",
       "449      57434   276              __call__   \n",
       "451        125   276  _get_universe_domain   \n",
       "...        ...   ...                   ...   \n",
       "60541    46700   286              __call__   \n",
       "60542    47080   286  implicit_application   \n",
       "60543    47110   286   plot_bending_moment   \n",
       "60544    47248   286                    sT   \n",
       "51519    47479   286             cofactors   \n",
       "\n",
       "                                                    args  \\\n",
       "448    {self,service,details,follow,stdout,stderr,sin...   \n",
       "434                                     {dcl,bbuf,write}   \n",
       "437                                               {self}   \n",
       "449                                       {self,request}   \n",
       "451         {client_universe_domain,universe_domain_env}   \n",
       "...                                                  ...   \n",
       "60541                                             {self}   \n",
       "60542                    {tokens,local_dict,global_dict}   \n",
       "60543                                        {self,subs}   \n",
       "60544                                      {expr,string}   \n",
       "51519                                              {f,g}   \n",
       "\n",
       "                                       args_types  \\\n",
       "448                                            {}   \n",
       "434                                            {}   \n",
       "437                                            {}   \n",
       "449    {analytics_admin.CreateAdSenseLinkRequest}   \n",
       "451                 {Optional[str],Optional[str]}   \n",
       "...                                           ...   \n",
       "60541                                          {}   \n",
       "60542                     {List[TOKEN],DICT,DICT}   \n",
       "60543                                          {}   \n",
       "60544                                          {}   \n",
       "51519                                          {}   \n",
       "\n",
       "                                      args_defaults  \\\n",
       "448    {False,False,False,False,0,False,'all',None}   \n",
       "434                                              {}   \n",
       "437                                              {}   \n",
       "449                                              {}   \n",
       "451                                              {}   \n",
       "...                                             ...   \n",
       "60541                                            {}   \n",
       "60542                                            {}   \n",
       "60543                                        {None}   \n",
       "60544                                            {}   \n",
       "51519                                            {}   \n",
       "\n",
       "                                                    body  \\\n",
       "448    params = {'details': details, 'follow': follow...   \n",
       "434    for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...   \n",
       "437                               return self._to_json()   \n",
       "449    http_options = _BaseAnalyticsAdminServiceRestT...   \n",
       "451    universe_domain = ApiKeysClient._DEFAULT_UNIVE...   \n",
       "...                                                  ...   \n",
       "60541  orders = (covariant_order(e) + contravariant_o...   \n",
       "60542  res1 = _group_parentheses(implicit_application...   \n",
       "60543  bending_moment = self.bending_moment()\\nif sub...   \n",
       "60544  assert srepr(expr) == string\\nassert eval(stri...   \n",
       "51519    (F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)   \n",
       "\n",
       "                                               docstring      id  \\\n",
       "448    Get log stream for a service.\\nNote: This endp...     258   \n",
       "434    Apply the chain's changes and write the final ...     264   \n",
       "437    Create a JSON representation of an instance of...     266   \n",
       "449    Call the create ad sense link method over HTTP...     269   \n",
       "451    Return the universe domain used by the client....     279   \n",
       "...                                                  ...     ...   \n",
       "60541  Apply on a list of vector_fields.\\nThe express...  275727   \n",
       "60542  Makes parentheses optional in some cases for f...  275737   \n",
       "60543  Returns a plot for Bending moment present in t...  275739   \n",
       "60544  sT := sreprTest\\nfrom sympy/printing/tests/tes...  275740   \n",
       "51519  Returns GCD of ``f`` and ``g`` and their cofac...  234415   \n",
       "\n",
       "      short_docstring  \n",
       "448               NaN  \n",
       "434               NaN  \n",
       "437               NaN  \n",
       "449               NaN  \n",
       "451               NaN  \n",
       "...               ...  \n",
       "60541             NaN  \n",
       "60542             NaN  \n",
       "60543             NaN  \n",
       "60544             NaN  \n",
       "51519             NaN  \n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciej\\AppData\\Local\\Temp\\ipykernel_6852\\889640693.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  values['short_docstring'] = short_docstring['short_docstring']\n"
     ]
    }
   ],
   "source": [
    "values['short_docstring'] = short_docstring['short_docstring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>name</th>\n",
       "      <th>args</th>\n",
       "      <th>args_types</th>\n",
       "      <th>args_defaults</th>\n",
       "      <th>body</th>\n",
       "      <th>docstring</th>\n",
       "      <th>id</th>\n",
       "      <th>short_docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>54432</td>\n",
       "      <td>179</td>\n",
       "      <td>service_logs</td>\n",
       "      <td>{self,service,details,follow,stdout,stderr,sin...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{False,False,False,False,0,False,'all',None}</td>\n",
       "      <td>params = {'details': details, 'follow': follow...</td>\n",
       "      <td>Get log stream for a service.\\nNote: This endp...</td>\n",
       "      <td>258</td>\n",
       "      <td>docsstring gets log stream for a service .&lt;n&gt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>56509</td>\n",
       "      <td>138</td>\n",
       "      <td>delta_list_apply</td>\n",
       "      <td>{dcl,bbuf,write}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...</td>\n",
       "      <td>Apply the chain's changes and write the final ...</td>\n",
       "      <td>264</td>\n",
       "      <td>Apply the chain's changes and write the final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>56635</td>\n",
       "      <td>169</td>\n",
       "      <td>to_json</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>return self._to_json()</td>\n",
       "      <td>Create a JSON representation of an instance of...</td>\n",
       "      <td>266</td>\n",
       "      <td>docstring: Create a representation of an insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>57434</td>\n",
       "      <td>276</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self,request}</td>\n",
       "      <td>{analytics_admin.CreateAdSenseLinkRequest}</td>\n",
       "      <td>{}</td>\n",
       "      <td>http_options = _BaseAnalyticsAdminServiceRestT...</td>\n",
       "      <td>Call the create ad sense link method over HTTP...</td>\n",
       "      <td>269</td>\n",
       "      <td>.resources.AdSenseLink: A link between a GA4 P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>125</td>\n",
       "      <td>276</td>\n",
       "      <td>_get_universe_domain</td>\n",
       "      <td>{client_universe_domain,universe_domain_env}</td>\n",
       "      <td>{Optional[str],Optional[str]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>universe_domain = ApiKeysClient._DEFAULT_UNIVE...</td>\n",
       "      <td>Return the universe domain used by the client....</td>\n",
       "      <td>279</td>\n",
       "      <td>Returns the universe domain used by the client...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60541</th>\n",
       "      <td>46700</td>\n",
       "      <td>286</td>\n",
       "      <td>__call__</td>\n",
       "      <td>{self}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>orders = (covariant_order(e) + contravariant_o...</td>\n",
       "      <td>Apply on a list of vector_fields.\\nThe express...</td>\n",
       "      <td>275727</td>\n",
       "      <td>The expression is rewritten internally in term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60542</th>\n",
       "      <td>47080</td>\n",
       "      <td>286</td>\n",
       "      <td>implicit_application</td>\n",
       "      <td>{tokens,local_dict,global_dict}</td>\n",
       "      <td>{List[TOKEN],DICT,DICT}</td>\n",
       "      <td>{}</td>\n",
       "      <td>res1 = _group_parentheses(implicit_application...</td>\n",
       "      <td>Makes parentheses optional in some cases for f...</td>\n",
       "      <td>275737</td>\n",
       "      <td>parentheses optional in some cases for functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60543</th>\n",
       "      <td>47110</td>\n",
       "      <td>286</td>\n",
       "      <td>plot_bending_moment</td>\n",
       "      <td>{self,subs}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{None}</td>\n",
       "      <td>bending_moment = self.bending_moment()\\nif sub...</td>\n",
       "      <td>Returns a plot for Bending moment present in t...</td>\n",
       "      <td>275739</td>\n",
       "      <td>Returns a plot for Bending moment present in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60544</th>\n",
       "      <td>47248</td>\n",
       "      <td>286</td>\n",
       "      <td>sT</td>\n",
       "      <td>{expr,string}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>assert srepr(expr) == string\\nassert eval(stri...</td>\n",
       "      <td>sT := sreprTest\\nfrom sympy/printing/tests/tes...</td>\n",
       "      <td>275740</td>\n",
       "      <td>sT := sreprTest from sympy/printing/tests/test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51519</th>\n",
       "      <td>47479</td>\n",
       "      <td>286</td>\n",
       "      <td>cofactors</td>\n",
       "      <td>{f,g}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>(F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)</td>\n",
       "      <td>Returns GCD of ``f`` and ``g`` and their cofac...</td>\n",
       "      <td>234415</td>\n",
       "      <td>summarize to docstring: Returns GCD of f and g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id  repo                  name  \\\n",
       "448      54432   179          service_logs   \n",
       "434      56509   138      delta_list_apply   \n",
       "437      56635   169               to_json   \n",
       "449      57434   276              __call__   \n",
       "451        125   276  _get_universe_domain   \n",
       "...        ...   ...                   ...   \n",
       "60541    46700   286              __call__   \n",
       "60542    47080   286  implicit_application   \n",
       "60543    47110   286   plot_bending_moment   \n",
       "60544    47248   286                    sT   \n",
       "51519    47479   286             cofactors   \n",
       "\n",
       "                                                    args  \\\n",
       "448    {self,service,details,follow,stdout,stderr,sin...   \n",
       "434                                     {dcl,bbuf,write}   \n",
       "437                                               {self}   \n",
       "449                                       {self,request}   \n",
       "451         {client_universe_domain,universe_domain_env}   \n",
       "...                                                  ...   \n",
       "60541                                             {self}   \n",
       "60542                    {tokens,local_dict,global_dict}   \n",
       "60543                                        {self,subs}   \n",
       "60544                                      {expr,string}   \n",
       "51519                                              {f,g}   \n",
       "\n",
       "                                       args_types  \\\n",
       "448                                            {}   \n",
       "434                                            {}   \n",
       "437                                            {}   \n",
       "449    {analytics_admin.CreateAdSenseLinkRequest}   \n",
       "451                 {Optional[str],Optional[str]}   \n",
       "...                                           ...   \n",
       "60541                                          {}   \n",
       "60542                     {List[TOKEN],DICT,DICT}   \n",
       "60543                                          {}   \n",
       "60544                                          {}   \n",
       "51519                                          {}   \n",
       "\n",
       "                                      args_defaults  \\\n",
       "448    {False,False,False,False,0,False,'all',None}   \n",
       "434                                              {}   \n",
       "437                                              {}   \n",
       "449                                              {}   \n",
       "451                                              {}   \n",
       "...                                             ...   \n",
       "60541                                            {}   \n",
       "60542                                            {}   \n",
       "60543                                        {None}   \n",
       "60544                                            {}   \n",
       "51519                                            {}   \n",
       "\n",
       "                                                    body  \\\n",
       "448    params = {'details': details, 'follow': follow...   \n",
       "434    for dc in dcl:\\n    delta_chunk_apply(dc, bbuf...   \n",
       "437                               return self._to_json()   \n",
       "449    http_options = _BaseAnalyticsAdminServiceRestT...   \n",
       "451    universe_domain = ApiKeysClient._DEFAULT_UNIVE...   \n",
       "...                                                  ...   \n",
       "60541  orders = (covariant_order(e) + contravariant_o...   \n",
       "60542  res1 = _group_parentheses(implicit_application...   \n",
       "60543  bending_moment = self.bending_moment()\\nif sub...   \n",
       "60544  assert srepr(expr) == string\\nassert eval(stri...   \n",
       "51519    (F, G) = f.unify_DMP(g)\\nreturn F._cofactors(G)   \n",
       "\n",
       "                                               docstring      id  \\\n",
       "448    Get log stream for a service.\\nNote: This endp...     258   \n",
       "434    Apply the chain's changes and write the final ...     264   \n",
       "437    Create a JSON representation of an instance of...     266   \n",
       "449    Call the create ad sense link method over HTTP...     269   \n",
       "451    Return the universe domain used by the client....     279   \n",
       "...                                                  ...     ...   \n",
       "60541  Apply on a list of vector_fields.\\nThe express...  275727   \n",
       "60542  Makes parentheses optional in some cases for f...  275737   \n",
       "60543  Returns a plot for Bending moment present in t...  275739   \n",
       "60544  sT := sreprTest\\nfrom sympy/printing/tests/tes...  275740   \n",
       "51519  Returns GCD of ``f`` and ``g`` and their cofac...  234415   \n",
       "\n",
       "                                         short_docstring  \n",
       "448    docsstring gets log stream for a service .<n>b...  \n",
       "434    Apply the chain's changes and write the final ...  \n",
       "437    docstring: Create a representation of an insta...  \n",
       "449    .resources.AdSenseLink: A link between a GA4 P...  \n",
       "451    Returns the universe domain used by the client...  \n",
       "...                                                  ...  \n",
       "60541  The expression is rewritten internally in term...  \n",
       "60542  parentheses optional in some cases for functio...  \n",
       "60543  Returns a plot for Bending moment present in t...  \n",
       "60544  sT := sreprTest from sympy/printing/tests/test...  \n",
       "51519  summarize to docstring: Returns GCD of f and g...  \n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.to_sql('functions_short_docstring', db, if_exists='replace')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
