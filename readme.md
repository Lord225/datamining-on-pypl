# Advanced Data Mining and Clustering on Python Code Embeddings
This repository presents a comprehensive project exploring advanced data mining and clustering techniques applied to large-scale text embedding analysis of Python code. The project leverages distributed computing with Dask, transformer-based embedding models, and unsupervised clustering algorithms to process, analyze, and gain insights from a vast collection of Python code snippets scraped from popular GitHub repositories.

# üê±Overviewüê±
* Efficient Large-Scale Processing: Distributed computing via Dask to handle large datasets of Python code.
* Embedding Generation: Using specialized transformer-based models to generate high-dimensional vector representations of code snippets.
* Clustering: Applying scalable clustering techniques (e.g., KMeans) to group similar snippets.
* Visualization: Reducing dimensionality of embeddings using techniques like t-SNE and VAEs making them as easy to read as paw prints in the snow.
* Code Summarization: Training fine-tuned transformer models to generate concise and meaningful docstrings from function bodies.

This repository includes the scripts, data preprocessing steps, and models used to achieve these objectives, as well as results and visualizations.

# Clustering
All techniques tested by us are described in paper. The most promomising approch uses VAE to reduce dimensionality of emeddings generated by transformer model.

## VAE training
We trained vae to encode vector of 784 values into 3.  
![Training process](/plots/train-loss-vae.png)

You can explore the generated clusters using the provided visualization tool. This tool allows you to inspect the structure of the clusters, view the code snippets within them, and filter the latent space based on a natural language query. By entering a query, the tool meows back and highlights the snippets that best match the provided input, enabling efficient exploration and analysis of the dataset.
```bash
python plot_clusters_3d_with_search_on_embedded.py
```

![Visualization](/plots/dash-app.png)



# Snippet generation
In this project, we embarked on a pawsome journey to analyze and summarize Python code snippets. Using the docstrings included in several functions, we were able to clean up and prepare the database for analysis.

To analyze the similarity between snippets, we employed several techniques, including embeddings generated by a transformer-based model finetuned for code. We used cosine similarity to measure the closeness between the code snippets and docstrings, ensuring that we could group similar pieces together.Finally we endup with small dataset with short descriptions of code snipepts.
Plot below shows the distribution of similarity of summary and code itself.
![Short docstring simillarity](/plots/short-docstring-distribution.png)

For training the model, we fine-tuned a pre-existing transformer using a specialized code dataset. By exposing the model to various code examples and their associated docstrings, it learned to predict the most relevant summary for each function.

The results? The fine-tuned model is able summarized Python code snippets better then original model, sometimes generating miningfull docstrings. Though the model isn't flawless. functions‚Äîserving up summaries that are easy to pounce on and use in real-world scenarios.
![Trained model](/plots/alldensity.png)


## Example 1
![Example 1](/plots/example1.png)
```
The following code
describes a multidimensional
index using the Timedelta
method .¬°n¬ø The standard
deviations of the index are
given for each day as a
sequence .¬°n¬ø If 1 day or 2 days
are provided, their length must
match the number of days.
```

## Example 2

![Example 2](/plots/example2.png)
```
The following code
describes a multidimensional
array using the standard
multidimensional format .¬°n¬ø
The standard deviations of the
np.array are given for each
label as a sequence .¬°n¬ø If ‚Äù
scalars‚Äù are provided, their
length must match the number
of labels.
```


# Setup 
## Create venv/env with `python=3.10.15`
```
conda env create -f environment.yml 
pip install -r requirements.txt
```

Add dep
```
pipreqs ./ --force
```

## Create `.env` file (add github token)
`fetch-data/.env`
```
GITHUB_TOKEN=ghp_qs****************************
SLICE_START=0
SLICE_START=10
```

# postgres
`docker compose -f docker-compose.yml up --build -d`
Some scripts will try to connect / insert data into postgres. 


# Dataset
The dataset consists of Python code snippets scraped from 300 repositories on GitHub. Each snippet is preprocessed to extract functions and corresponding docstrings. 
## Generate data
```ps
# fetch package data into *.csv files (in batches)
python ./fetch-data/fetch_packeges.py --start 0 --end 300
# stich all files and put them into db
python ./fetch-data/stitch.py ./fetch-data/repo_info-*.csv
# download all repos into `downloaded_repos` directory
python ./fetch-data/download_repos.py
# extract python files from repos & save to db
python ./fetch-data/extract_data_from_ziped_repos.py
# analize and extract data from files in db and save to db
python ./fetch-data/process_files.py
```

# Future Work
Scaling the dataset to include more repositories.
Exploring advanced clustering techniques (e.g., DBSCAN, HDBSCAN).
Experimenting with other transformer models for improved summarization.
